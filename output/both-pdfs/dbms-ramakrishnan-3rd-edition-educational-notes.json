{
  "concepts": {
    "relational-model-intro": {
      "id": "relational-model-intro",
      "title": "Introduction to Relational Databases",
      "definition": "Overview of database systems, relational model basics, and data independence",
      "difficulty": "beginner",
      "page_references": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20
      ],
      "sections": {
        "definition": {
          "text": "It's your choice!\nNew Modular Organization!\nRelational Model\nSQLDDL\nInfonnation Retrieval\nand XML Data\nManagement\nER Model\nConceptual Design\nAppncatirms emphasis: A course that covers the principles of database systems and emphasizes\nhow they are used in developing data-intensive applications.\n.\nf,;~tY'W';Yl~t';;:;,~7' A course that has a strong systems emphasis and assumes that students have\ngood programming skills in C and C++.\nHybrid course: Modular organization allows you to teach the course with the emphasis you want.\n......-\n:= Dependencies\n~~~\nI\nv\nI\nII\nIV\nVIr\nIII\n\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\n\nDATABASE MANAGEMENT\nSYSTEMS",
          "pages": [
            1,
            2,
            3,
            4,
            5
          ],
          "relevance": {
            "score": 0.32,
            "sql_score": 0.4,
            "concept_score": 0.4,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "DATABASE MANAGEMENT\nSYSTEMS\nThird Edition\nRaghu Ramakrishnan\nUniversity of Wisconsin\nMadison, Wisconsin, USA\n•\nJohannes Gehrke\nCornell University\nIthaca, New York, USA\nBoston\nBurr Ridge, IL\nDubuque, IA\nMadison, WI\nNew York\nSan Francisco\nSt. Louis\nBangkok\nBogota\nCaracas\nKuala Lumpur\nLisbon\nLondon\nMadrid\nMexico City\nMilan\nMontreal\nNew Delhi\nSantiago\nSeoul\nSingapore\nSydney\nTaipei\nToronto\n\nMcGraw-Hill Higher Education tz\nA Lhvision of The McGraw-Hill Companies\nDATABASE MANAGEMENT SYSTEMS, THIRD EDITION\nInternational Edition 2003\nExclusive rights by McGraw-Hill Education (Asia), for manufacture and export. This\nbook cannot be re-exported from the country to which it is sold by McGraw-Hill. The\nInternational Edition is not available in North America.\nPublished by McGraw-Hili, a business unit of The McGraw-Hili Companies, Inc., 1221\nAvenue of the Americas, New York, NY 10020. Copyright © 2003, 2000, 1998 by The\nMcGraw-Hill Companies, Inc. All rights reserved. No part of this publication may be\nreproduced or distributed in any form or by any means, or stored in a database or retrieval\nsystem, without the prior written consent of The McGraw-Hill Companies, Inc.,\nincluding, but not limited to, in any network or other electronic storage or transmission,\nor broadcast for distance learning.\nSome ancillaries, including electronic and print components, may not be available to\ncustomers outside the United States.\nCTF\nBJE\nLibrary of Congress Cataloging-in-Publication Data\nRamakrishnan, Raghu\nDatabase management systems / Raghu Ramakrishnan, Johannes Gehrke.~3rd ed.\np.\ncm.\nIncludes index.\nISBN 0-07-246563-8-ISBN 0-07-115110-9 (ISE)\n1.\nDatabase management. 1. Gehrke, Johannes. II. Title.\nQA76.9.D3 R237 2003\n005.74--Dc21\nCIP\nWhen ordering this title, use ISBN 0-07-123151-X\nPrinted in Singapore\nwww.mhhe.com\n\nTo Apu, Ketan, and Vivek with love\nTo Keiko and Elisa\n\n\n\nPREFACE\nPart I\nFOUNDATIONS\nCONTENTS\nXXIV\nOVERVIEW OF DATABASE SYSTEMS\n1.1\nManaging Data\n1.2\nA Historical Perspective\n1.3\nFile Systems versus a DBMS\n1.4\nAdvantages of a DBMS\n1.5\nDescribing and Storing Data in a DBMS\n1.5.1\nThe Relational Model\n1.5.2\nLevels of Abstraction in a DBMS\n1.5.3\nData Independence\n1.6\nQueries in a DBMS\n1.7\nTransaction Management\n1.7.1\nConcurrent Execution of Transactions\n1.7.2\nIncomplete Transactions and System Crashes\n1.7.3\nPoints to Note\n1.8\nStructure of a DBMS\n1.9\nPeople Who Work with Databases\n1.10\nReview Questions\nINTRODUCTION TO DATABASE DESIGN\n2.1\nDatabase Design and ER Diagrams\n2.1.1\nBeyond ER Design\n2.2\nEntities, Attributes, and Entity Sets\n2.3\nRelationships and Relationship Sets\n2.4\nAdditional Features of the ER Model\n2.4.1\nKey Constraints\n2.4.2\nParticipation Constraints\n2.4.3\nWeak Entities\n2.4.4\nClass Hierarchies\n2.4.5\nAggregation\nvii",
          "pages": [
            6,
            7,
            8,
            9,
            10
          ],
          "relevance": {
            "score": 0.72,
            "sql_score": 0.9,
            "concept_score": 0.9,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Vlll\nDATABASE \"NIANAGEMENT SYSTEivlS\nPreliminaries\nRelational Algebra\n4.2.1\nSelection and Projection\n4.2.2\nSet Operations\n2.5\nConceptual Design With the ER Model\n2..5.1\nEntity versus Attribute\n2.5.2\nEntity versus Relationship\n2.5.3\nBinary versus Ternary Relationships\n2..5.4\nAggregation versus Ternary Relationships\n2.6\nConceptual Design for Large Enterprises\n2.7\nThe Unified Modeling Language\n2.8\nCase Study: The Internet Shop\n2.8.1\nRequirements Analysis\n2.8.2\nConceptual Design\n2.9\nReview Questions\nTHE RELATIONAL MODEL\n3.1\nIntroduction to the Relational Model\n3.1.1\nCreating and Modifying Relations Using SQL\n3.2\nIntegrity Constraints over Relations\n3.2.1\nKey Constraints\n:3.2.2\nForeign Key Constraints\n3.2.3\nGeneral Constraints\n3.3\nEnforcing Integrity Constraints\n3.3.1\nTransactions and Constraints\n3.4\nQuerying Relational Data\n3.5\nLogical Database Design: ER to Relational\n3.5.1\nEntity Sets to Tables\n3.5.2\nRelationship Sets (without Constraints) to Tables\n3.5.3\nTranslating Relationship Sets with Key Constraints\n3.5.4\nTranslating Relationship Sets with Participation Constraints\n3.5.5\nTranslating Weak Entity Sets\n3.5.6\ncn'anslating Class Hierarchies\n3.5.7\nTranslating ER Diagrams with Aggregation\n3.5.8\nER to Relational: Additional Examples\n:3.6\nIntroduction to Views\n3.6.1\nViews, Data Independence, Security\n3.6.2\nUpdates on Views\n:3.7\nDestroying/Altering Tables and Views\n:3.8\nCase Study: The Internet Store\n:3.9\nReview Questions\nRELATIONAL ALGEBRA AND CALCULUS\n4.1\n4.2\n\nContents\nlX\n~\n4.2.3\nRenaming\n4.2.4\nJoins\n4.2.5\nDivision\n4.2.6\n1\\'lore Examples of Algebra Queries\n4.3\nRelational Calculus\n4.3.1\nTuple Relational Calculus\n4.3.2\nDomain Relational Calculus\n4.4\nExpressive Power of Algebra and Calculus\n4.5\nReview Questions\nSQL: QUERIES, CONSTRAINTS, TRIGGERS\n5.1\nOverview\n5.1.1\nChapter Organization\n.5.2\nThe Form of a Basic SQL Query\n5.2.1\nExamples of Basic SQL Queries\n5.2.2\nExpressions and Strings in the SELECT Command\n5.3\nUNION, INTERSECT, and EXCEPT\n5.4\nNested Queries\n5.4.1\nIntroduction to Nested Queries\n5.4.2\nCorrelated Nested Queries\n5.4.3\nSet-Comparison Operators\n5.4.4\nMore Examples of Nested Queries\n5.5\nAggregate Operators\n5.5.1\nThe GROUP BY and HAVING Clauses\n5.5.2\nMore Examples of Aggregate Queries\n5.6\nNull Values\n5.6.1\nComparisons Using Null Values\n5.6.2\nLogical Connectives AND, OR, and NOT\n5.6.3\nImpact 011 SQL Constructs\n5.6.4\nOuter Joins\n5.6.5\nDisallowing Null Values\n5.7\nComplex Integrity Constraints in SQL\n5.7.1\nConstraints over a Single Table\n5.7.2\nDomain Constraints and Distinct Types\n5.7.3\nAssertions: ICs over Several Tables\n5.8\nTriggers and Active Databases\n5.8.1\nExamples of Triggers in SQL\n5.9\nDesigning Active Databases\n5.9.1\nWhy Triggers Can Be Hard to Understand\n5.9.2\nConstraints versus Triggers\n5.9.:3\nOther Uses of Triggers\n5.10\nReview Questions\n17:3\n\nx\nDATABASE J\\;1ANAGEMENT SYSTEMS\nPart II\nAPPLICATION DEVELOPMENT\nDATABASE APPLICATION DEVELOPMENT\n6.1\nAccessing Databases from Applications\n6.1.1\nEmbedded SQL\n6.1.2\nCursors\n6.1.3\nDynamic SQL\n6.2\nAn Introduction to JDBC\n6.2.1\nArchitecture\n6.3\nJDBC Classes and Interfaces\n6.3.1\nJDBC Driver Management\n6.3.2\nConnections\n6.3.3\nExecuting SQL Statements\n6.3.4\nResultSets\n6.3.5\nExceptions and Warnings\n6.3.6\nExamining Database Metadata\n6.4\nSQLJ\n6.4.1\nWriting SQLJ Code\n6.5\nStored Procedures\n6.5.1\nCreating a Simple Stored Procedure\n6.5.2\nCalling Stored Procedures\n6.5.3\nSQL/PSM\n6.6\nCase Study: The Internet Book Shop\n6.7\nReview Questions\nINTERNET APPLICATIONS\n7.1\nIntroduction\n7.2\nInternet Concepts\n7.2.1\nUniform Resource Identifiers\n7.2.2\nThe Hypertext Transfer Protocol (HTTP)\n7.3\nHTML Documents\n7.4\nXML Documents\n7.4.1\nIntroduction to XML\n7.4.2\nXML DTDs\n7.4.3\nDomain-Specific DTDs\n7.5\nThe Three-Tier Application Architecture\n7.5.1\nSingle-Tier and Client-Server Architectures\n7.5.2\nThree-Tier Architectures\n7.5.3\nAdvantages of the Three-Tier Architecture\n7.6\nThe Presentation Layer\n7.6.1\nHTrvlL Forms\n7.6.2\nJavaScript\n7.6.3\nStyle Sheets",
          "pages": [
            11,
            12,
            13
          ],
          "relevance": {
            "score": 0.6,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "A relational database is an organized collection of data that is structured using tables. Each table consists of rows and columns, where each row represents a record and each column represents a field. Relational databases use SQL (Structured Query Language) to manage and manipulate the data.",
        "explanation": "Relational databases solve the problem of managing large amounts of structured data efficiently. They work by organizing data into tables that are linked together through relationships. This structure allows for easy querying, updating, and management of data. Relational databases are widely used in various applications because they provide a robust framework for storing and retrieving information. Key features include ACID transactions, data integrity constraints, and support for complex queries.",
        "key_points": [
          "Key point 1: Tables are the basic building blocks of relational databases.",
          "Key point 2: Relationships between tables allow for complex data management.",
          "Key point 3: SQL is used to interact with relational databases.",
          "Key point 4: ACID transactions ensure data integrity and consistency.",
          "Key point 5: Data independence allows applications to be decoupled from the database schema."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- SELECT all records FROM a TABLE SELECT * FROM employees;",
            "explanation": "This example demonstrates how to retrieve all data from an 'employees' table. The asterisk (*) is used to select all columns.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Find employees in a specific department\nSELECT name, position FROM employees WHERE department = 'Sales';",
            "explanation": "This practical example shows how to query data based on a condition. It selects the names and positions of employees who work in the Sales department."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to specify column names in SELECT statements",
            "incorrect_code": "-- Incorrect SQL\nSELECT FROM employees;",
            "correct_code": "-- Correct SQL\nSELECT * FROM employees;",
            "explanation": "This mistake happens when a student tries to select data without specifying any columns. The asterisk (*) is used to select all columns."
          }
        ],
        "practice": {
          "question": "Create a table named 'students' with columns for 'id', 'name', and 'age'. Insert three records into the table.",
          "solution": "-- Create students table\nCREATE TABLE students (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    age INT\n);\n-- Insert records\nINSERT INTO students (id, name, age) VALUES (1, 'Alice', 20);\nINSERT INTO students (id, name, age) VALUES (2, 'Bob', 22);\nINSERT INTO students (id, name, age) VALUES (3, 'Charlie', 21);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "It's your choice!\nNew Modular Organization!\nRelational Model\nSQLDDL\nInfonnation Retrieval\nand XML Data\nManagement\nER Model\nConceptual Design\nAppncatirms emphasis: A course that covers the principles o",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "data-independence": {
      "id": "data-independence",
      "title": "Data Independence",
      "definition": "Logical and physical data independence - how DBMS separates data from applications",
      "difficulty": "beginner",
      "page_references": [
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24
      ],
      "sections": {
        "definition": {
          "text": "Xll\nDATABASE\n~/IANAGE1'vIENT SYSTEMS\n9.3\nDisk Space Management\n9.3.1\nKeeping Track of Free Blocks\n9.3.2\nUsing as File Systems to il/ranage Disk Space\n9.4\nBuffer Manager\n9.4.1\nBuffer Replacement Policies\n9.4.2\nBuffer Management in DBMS versus OS\n9.5\nFiles of Records\n9.5.1\nImplementing Heap Files\n9.6\nPage Formats\n9.6.1\nFixed-Length Records\n9.6.2\nVariable-Length Records\n9.7\nRecord Formats\n9.7.1\nFixed-Length Records\n9.7.2\nVariable-Length Records\n9.8\nReview Questions\nTREE-STRUCTURED INDEXING\n10.1\nIntuition For Tree Indexes\n10.2\nIndexed Sequential Access Method (ISAM)\n10.2.1\nOverflow Pages, Locking Considerations\n10.3\nB+ Trees: A Dynamic Index Structure\n10.3.1\nFormat of a Node\n10.4\nSearch\n10.5\nInsert\n10.6\nDelete\n10.7\nDuplicates\n10.8\nB+ Trees in Practice\n10.8.1\nKey Compression\n10.8.2\nBulk-Loading a B+ Tl'ee\n10.8.3\nThe Order Concept\n10.8.4\nThe Effect of Inserts and Deletes on Rids\n10.9\nReview Questions\nHASH-BASED INDEXING\n11.1\nStatic Hashing\n11.1.1\nNotation and Conventions\n11.2\nExtendible HCkshing\n11.3\nLine~r Hashing\n11.4\nExtendible vs. Linear Ha\"lhing\nn.5\nReview Questions\nPart IV\nQUERY EVALUATION\n:316\n\nContents\nOVERVIEW OF QUERY EVALUATION\n12.1\nThe System Catalog\n12.1.1\nInformation in the Catalog\n12.2\nIntroduction to Operator Evaluation\n12.2.1\nThree Common Techniques\n12.2.2\nAccess Paths\n12.3\nAlgorithms for Relational Operations\n12.3.1\nSelection\n12.3.2\nProjection\n12.3.3\nJoin\n12.3.4\nOther Operations\n12.4\nIntroduction to Query Optimization\n12.4.1\nQuery Evaluation Plans\n12.4.2\nMulti-operator Queries: Pipelined Evaluation\n12.4.3\nThe Iterator Interface\n12.5\nAlternative Plans: A Motivating Example\n12.5.1\nPushing Selections\n12.5.2\nUsing Indexes\n12.6\nWhat a Typical Optimizer Does\n12.6.1\nAlternative Plans Considered\n12.6.2\nEstimating the Cost of a Plan\n12.7\nReview Questions\nEXTERNAL SORTING\n13.1\nWhen Does a DBMS Sort Data?\n13.2\nA Simple Two-Way Merge Sort\n13.3\nExternal Merge Sort\n13.3.1\nMinimizing the Number of Runs\n13.4\nMinimizing I/O Cost versus Number of I/Os\n13.4.1\nBlocked I/O\n13.4.2\nDouble Buffering\n13.5\nUsing B+ Trees for Sorting\n13.5.1\nClustered Index\n1:3.5.2\nUnclustered Index\n13.6\nReview Questions\nEVALUATING RELATIONAL OPERATORS\n14.1\nThe' Selection Operation\n14.1.1\nNo Index, Unsorted Data\n14.1.2\nNo Index, Sorted Data\n14.1.:3\nB+ Tree Index\n14.1.4\nHash Index, Equality Selection\n14.2\nGeneral Selection Conditions\n:394\n:39.5\n4:33\n\nXIV\nDATABASE ~11ANAGEMENT SYSTEMS\n14.2.1\nCNF and Index Matching\n14.2.2\nEvaluating Selections without Disjunction\n14.2.3\nSelections with Disjunction\n14.3\nThe Projection Operation\n14.3.1\nProjection Based on Sorting\n14.3.2\nProjection Based on Hashing\n14.3.3\nSorting Versus Hashing for Projections\n14.3.4\nUse of Indexes for Projections\n14.4\nThe Join Operation\n14.4.1\nNested Loops Join\n14.4.2\nSort-Merge Join\n14.4.3\nHash Join\n14.4.4\nGeneral Join Conditions\n14.5\nThe Set Operations\n14.5.1\nSorting for Union and Difference\n14.5.2\nHashing for Union and Difference\n14.6\nAggregate Operations\n14.6.1\nImplementing Aggregation by Using an Index\n14.7\nThe Impact of Buffering\n14.8\nReview Questions\nA TYPICAL RELATIONAL QUERY OPTIMIZER\n15.1\nTranslating SQL Queries into Algebra\n15.1.1\nDecomposition of a Query into Blocks\n15.1.2\nA Query Block as a Relational Algebra Expression\n15.2\nEstimating the Cost of a Plan\n15.2.1\nEstimating Result Sizes\n15.3\nRelational Algebra Equivalences\n15.3.1\nSelections\n15.3.2\nProjections\n15.3.3\nCross-Products and Joins\n15.3.4\nSelects, Projects, and Joins\n15.3.5\nOther Equivalences\n15.4\nEnumeration of Alternative Plans\n15.4.1\nSingle-Relation Queries\n15.4.2\nMultiple-Relation Queries\nIS.5\nNested Subqueries\n15.6\nThe System R Optimizer\n15.7\nOther Approaches to Query Optimization\nS07\n15.8\nReview Questions\nPart V\nTRANSACTION MANAGEMENT",
          "pages": [
            15,
            16,
            17
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Contents\nXfV\nOVERVIEW OF TRANSACTION MANAGEMENT\n16.1\nThe ACID Properties\n16.1.1\nConsistency and Isolation\n16.1.2\nAtomicity and Durability\n16.2\nTransactions and Schedules\n16.3\nConcurrent Execution of Transactions\n16.3.1\nrvlotivation for Concurrent Execution\n16.3.2\nSerializability\n16.3.3\nAnomalies Due to Interleaved Execution\n16.3.4\nSchedules Involving Aborted Transactions\n16.4\nLock-Based Concurrency Control\n16.4.1\nStrict Two-Phase Locking (Strict 2PL)\n16.4.2\nDeadlocks\n16.5\nPerformance of Locking\n16.6\nTransaction Support in SQL\n16.6.1\nCreating and Terminating Transactions\n16.6.2\nWhat Should We Lock?\n16.6.3\nTransaction Characteristics in SQL\n16.7\nIntroduction to Crash Recovery\n16.7.1\nStealing Frames and Forcing Pages\n16.7.2\nRecovery-Related Steps during Normal Execution\n16.7.3\nOverview of ARIES\n16.7.4\nAtomicity: Implementing Rollback\n16.8\nReview Questions\n17 CONCURRENCY CONTROL\n17.1\n2PL, Serializability, and Recoverability\n17.1.1\nView Serializability\n17.2\nIntroduction to Lock Management\n17.2.1\nImplementing Lock and Unlock Requests\n17.3\nLock Conversions\n17.4\nDealing With Deadlocks\n17.4.1\nDeadlock Prevention\n17.5\nSpecialized Locking Techniques\n17.5.1\nDynamic Databases and the Phantom Problem\n17.5.2\nConcurrency Control in B+ Trees\n17.5.3\nMultiple-Granularity Locking\n17.6\nConClurency Control without Locking\n17.6.1\nOptimistic Concurrency Control\n17.6.2\nTimestamp-Based Concurrency Control\n17.6.3\nMultiversion Concurrency Control\n17.7\nReviev Questions\n57:3\n\nXVI\nDATABASE rvlANAGEMENT SYSTEMS\nCRASH RECOVERY\n18.1\nIntroduction to ARIES\n18.2\nThe Log\n18.3\nOther Recovery-Related Structures\n18.4\nThe Write-Ahead Log Protocol\n18.5\nCheckpointing\n18.6\nRecovering from a System Crash\n18.6.1\nAnalysis Phase\n18.6.2\nRedo Phase\n18.6.3\nUndo Phase\n18.7\nMedia Recovery\n18.8\nOther Approaches and Interaction with Concurrency Control\n18.9\nReview Questions\nPart VI\nDATABASE DESIGN AND TUNING\nSCHEMA REFINEMENT AND NORMAL FORMS\n19.1\nIntroduction to Schema Refinement\n19.1.1\nProblems Caused by Redundancy\n19.1.2\nDecompositions\n19.1.3\nProblems Related to Decomposition\n19.2\nFunctional Dependencies\n19.3\nReasoning about FDs\n19.3.1\nClosure of a Set of FDs\n19.3.2\nAttribute Closure\n19.4\nNormal Forms\n19.4.1\nBoyce-Codd Normal Form\n19.4.2\nThird Normal Form\n19.5\nProperties of Decompositions\n19.5.1\nLossless-Join Decomposition\n19.5.2\nDependency-Preserving Decomposition\n19.6\nNormalization\n19.6.1\nDecomposition into BCNF\n19.6.2\nDecomposition into 3NF\n19.7\nSchema Refinement in Database Design\n19.7.1\nConstraints on an Entity Set\n19.7.2\nConstraints on a Relationship Set\n19.7.3\nIdentifying Attributes of Entities\n19.7.4\nIdentifying Entity Sets\n6:33\n19.8\nOther Kinds of Dependencies\n6:33\n19.8.1\nMultivalued Dependencies\n6:34\n19.8.2\nFourth Normal Form\n6:36\n19.8.:3\nJoin Dependencies\n(1:38\n\nContents\nXVll\n19.8.4\nFifth Normal Form\n6:38\n19.8.5\nInclusion Dependencies\n19.9\nCase Study: The Internet Shop\n19.10 Review Questions\nPHYSICAL DATABASE DESIGN AND TUNING\n20.1\nIntroduction to Physical Database Design\n20.1.1\nDatabase Workloads\n20.1.2\nPhysical Design and Tuning Decisions\n20.1.3\nNeed for Database Tuning\n20.2\nGuidelines for Index Selection\n20.3\nBasic Examples of Index Selection\n20.4\nClustering and Indexing\n20.4.1\nCo-clustering Two Relations\n20.5\nIndexes that Enable Index-Only Plans\n20.6\nTools to Assist in Index Selection\n20.6.1\nAutomatic Index Selection\n20.6.2\nHow Do Index Tuning Wizards Work?\n20.7\nOverview of Database Tuning\n20.7.1\nTuning Indexes\n20.7.2\nTuning the Conceptual Schema\n20.7.3\nTuning Queries and Views\n20.8\nChoices in Tuning the Conceptual Schema\n20.8.1\nSettling for a Weaker Normal Form\n20.8.2\nDenormalization\n20.8.3\nChoice of Decomposition\n20.8.4\nVertical Partitioning of BCNF Relations\n20.8.5\nHorizontal Decomposition\n20.9\nChoices in Tuning Queries and Views\n20.10 Impact of Concurrency\n20.10.1 Reducing Lock Durations\n20.10.2 Reducing Hot Spots\n20.11 Case Study: The Internet Shop\n20.11.11\\ming the Datab~'ie\n20.12 DBMS Benchmarking\n20.12.1 Well-Known DBMS Benchmarks\n20.12.2 Using a Benchmark\n20.13 Review Questions\nSECURITY AND AUTHORIZATION\n21.1\nIntroduction to Datab~\"e Security\n21.2\nAccess Control\n21.3\nDiscretionary Access Control",
          "pages": [
            18,
            19,
            20
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "xviii\nDATABASE ~/IANAGEMENT SYSTEMS\n21.3.1\nGrant and Revoke on Views and Integrity Constraints\n21.4\nMandatory Access Control\n21.4.1\nMultilevel Relations and Polyinstantiation\n21.4.2\nCovert Channels, DoD Security Levels\n21.5\nSecurity for Internet Applications\n21.5.1\nEncryption\n21.5.2\nCertifying Servers: The SSL Protocol\n21.5.3\nDigital Signatures\n21.6\nAdditional Issues Related to Security\n21.6.1\nRole of the Database Administrator\n21.6.2\nSecurity in Statistical Databases\n21.7\nDesign Case Study: The Internet Store\n21.8\nReview Questions\nPart VII\nADDITIONAL TOPICS\nPARALLEL AND DISTRIBUTED DATABASES\n22.1\nIntroduction\n22.2\nArchitectures for Parallel Databases\n22.3\nParallel Query Evaluation\n22.3.1\nData Partitioning\n22.3.2\nParallelizing Sequential Operator Evaluation Code\n22.4\nParallelizing Individual Operations\n22.4.1\nBulk Loading and Scanning\n22.4.2\nSorting\n22.4.3\nJoins\n22.5\nParallel Query Optimization\n22.6\nIntroduction to Distributed Databases\n22.6.1\nTypes of Distributed Databases\n22.7\nDistributed DBMS Architectures\n22.7.1\nClient-Server Systems\n22.7.2\nCollaborating Server Systems\n22.7.3\nMidclleware Systems\n22.8\nStoring Data in a Distributed DBMS\n22.8.1\nFragmentation\n22.8.2\nReplication\n22.9\nDistributed Catalog Management\n22.9.1\nNaming Objects\n22.9.2\nCatalog Structure\n22.9.3\nDistributed Data Independence\n22.10 Distributed Query Processing\n22.1.0.1 Nonjoin Queries in a Distributed DBMS\n22.10.2 Joins in a Distributed DBMS\n\nContents\nJ6x\n22.10.3 Cost-Based Query Optimization\n22.11 Updating Distributed Data\n22.11.1 Synchronous Replication\n22.11.2 Asynchronous Replication\n22.12 Distributed Transactions\n22.13 Distributed Concurrency Control\n22.13.1 Distributed Deadlock\n22.14 Distributed Recovery\n22.14.1 Normal Execution and Commit Protocols\n22.14.2 Restart after a Failure\n22.14.3 Two-Phase Commit Revisited\n22.14.4 Three-Phase Commit\n22.15 Review Questions\nOBJECT-DATABASE SYSTEMS\n23.1\nMotivating Example\n23.1.1\nNew Data Types\n23.1.2\nManipulating the New Data\n23.2\nStructured Data Types\n23.2.1\nCollection Types\n23.3\nOperations on Structured Data\n23.3.1\nOperations on Rows\n23.3.2\nOperations on Arrays\n23.3.3\nOperations on Other Collection Types\n23.3.4\nQueries Over Nested Collections\n23.4\nEncapsulation and ADTs\n23.4.1\nDefining Methods\n23.5\nInheritance\n23.5.1\nDefining Types with Inheritance\n23.5.2\nBinding Methods\n23.5.3\nCollection Hierarchies\n23.6\nObjects, aIDs, and Reference Types\n23.6.1\nNotions of Equality\n23.6.2\nDereferencing Reference Types\n23.6.3\nURLs and OIDs in SQL:1999\n23.7\nDatabase Design for an ORDBJ\\'IS\n23.7.1\nCollection Types and ADTs\n2~).7.2\nObject Identity\n23.7.3\nExtending the ER Model\n23.7.4\nUsing Nested Collections\n2:3.8\nORDBMS Implementation Challenges\n23.8.]\nStorage and Access Methods\n23.8.2\nQuery Processing",
          "pages": [
            21,
            22
          ],
          "relevance": {
            "score": 0.64,
            "sql_score": 0.8,
            "concept_score": 0.8,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Data independence is the ability to change one part of a database without affecting another part. It ensures that changes made to the physical structure of the database do not impact the logical data and vice versa.",
        "explanation": "Data independence is crucial in database management systems (DBMS) because it allows for flexibility and scalability. When data independence is maintained, modifications such as changing storage formats or adding new columns can be done without altering existing queries or applications that rely on the data. This ensures that the system remains robust and reliable even as it grows and evolves.",
        "key_points": [
          "Key point 1: Data independence separates physical and logical structures of the database, making it easier to manage and evolve.",
          "Key point 2: It allows changes in the physical design (e.g., storage format) without affecting the logical data or applications that use it.",
          "Key point 3: Common pitfall is not designing for future changes, leading to rigid systems that are difficult to update.",
          "Key point 4: Best practice is to design with flexibility in mind, using normalization and well-defined schemas.",
          "Key point 5: This concept connects to other database design principles like normalization and schema refinement."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Example of a simple SQL query that is independent of the physical structure\nSELECT customer_id, customer_name FROM customers;",
            "explanation": "This example demonstrates a query that retrieves customer data without specifying how the data is stored physically. The query remains valid even if the database is restructured."
          },
          {
            "title": "Practical Example",
            "code": "-- Real-world scenario where data independence helps\nSELECT order_id, product_id FROM orders WHERE order_date > '2023-01-01';",
            "explanation": "This practical example shows how a query can be written to retrieve specific data without being affected by changes in the underlying database schema or storage format."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not designing for future changes, leading to rigid systems.",
            "incorrect_code": "-- Example of a poorly designed query that might break with physical changes\nSELECT customer_id, customer_name FROM customers WHERE last_updated > '2023-01-01';",
            "correct_code": "-- Corrected version using logical data instead of physical attributes\nSELECT customer_id, customer_name FROM customers WHERE customer_status = 'active';",
            "explanation": "This mistake occurs when queries are written to rely on specific physical attributes (like last_updated) rather than logical data (like customer_status). Changing the physical structure can break these queries."
          }
        ],
        "practice": {
          "question": "Design a simple SQL query that retrieves employee details without being affected by changes in the physical storage format of the employees table.",
          "solution": "SELECT employee_id, first_name, last_name FROM employees;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Xll\nDATABASE\n~/IANAGE1'vIENT SYSTEMS\n9.3\nDisk Space Management\n9.3.1\nKeeping Track of Free Blocks\n9.3.2\nUsing as File Systems to il/ranage Disk Space\n9.4\nBuffer Manager\n9.4.1\nBuffer Replacement Polici",
      "content_relevance": {
        "score": 0.8,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.0,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "er-model": {
      "id": "er-model",
      "title": "Entity-Relationship Model",
      "definition": "Modeling database structure using entities, relationships, and attributes",
      "difficulty": "intermediate",
      "page_references": [
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42,
        43,
        44
      ],
      "sections": {
        "definition": {
          "text": "xxii\nDATABASE l\\1ANAGEMENT SYSTEMS\n27.2\nIntroduction to Information Retrieval\n27.2.1\nVector Space Model\n27.2.2 TFjIDF Weighting of Terms\n27.2.3\nRanking Document Similarity\n27.2.4\n:Measuring Success: Precision and Recall\n27.3\nIndexing for Text Search\n27.3.1\nInverted Indexes\n27.3.2\nSignature Files\n27.4\nWeb Search Engines\n27.4.1\nSearch Engine Architecture\n27.4.2\nUsing Link Information\n27.5\nManaging Text in a DBMS\n27.5.1\nLoosely Coupled Inverted Index\n27.6\nA Data Model for XML\n27.6.1\nMotivation for Loose Structure\n27.6.2\nA Graph Model\n27.7\nXQuery: Querying XML Data\n27.7.1\nPath Expressions\n27.7.2\nFLWR Expressions\n27.7.3\nOrdering of Elements\n27.7.4\nGrouping and Generation of Collection Values\n27.8\nEfficient Evaluation of XML Queries\n27.8.1\nStoring XML in RDBMS\n27.8.2\nIndexing XML Repositories\n27.9\nReview Questions\nSPATIAL DATA MANAGEMENT\n28.1\nTypes of Spatial Data and Queries\n28.2\nApplications Involving Spatial Data\n28.3\nIntroduction to Spatial Indexes\n28.3.1\nOverview of Proposed Index Structures\n28.4\nIndexing Based on Space-Filling Curves\n28.4.1\nRegion Quad Trees and Z-Ordering: Region Data\n28.4.2\nSpatial Queries Using Z-Ordering\n28.5\nGrid Files\n28..5.1\nAdapting Grid Files to Handle Regions\n28.6\nR Trees: Point and Region Data\n28.6~1\nQueries\n28.6.2\nInsert and Delete Operations\n28.6.3\nConcurrency Control\n28.6.4\nGeneralized Search Trees\n28.7\nIssues in High-Dimensional Indexing\n28.8\nReview Questions\n\nContents\nFURTHER READING\n29.1\nAdvanced Tl\"ansaction Processing\n29.1.1\nTransaction Processing Monitors\n29.1.2\nNew Transaction Models\n29.1.3\nReal-Time DBlvISs\n29.2\nData Integration\n29.3\nMobile Databases\n29.4\nMain Memory Databases\n29.5\nMultimedia Databases\n29.6\nGeographic Information Systems\n29.7\nTemporal Databases\n29.8\nBiological Databases\n29.9\nInformation Visualization\n29.10 Summary\nTHE MINIBASE SOFTWARE\n30.1\nWhat Is Available\n30.2\nOverview of Minibase Assignments\n30.3\nAcknowledgments\nREFERENCES\nAUTHOR INDEX\nSUBJECT INDEX\nxxm\n\nPREFACE\nThe advantage of doing one's praising for oneself is that one can lay it on so thick\nand exactly in the right places.\n--Samuel Butler\nDatabase management systems are now an indispensable tool for managing\ninformation, and a course on the principles and practice of database systems\nis now an integral part of computer science curricula.\nThis book covers the\nfundamentals of modern database management systems, in particular relational\ndatabase systems.\nWe have attempted to present the material in a clear, simple style. A quantita-\ntive approach is used throughout with many detailed examples. An extensive\nset of exercises (for which solutions are available online to instructors) accom-\npanies each chapter and reinforces students' ability to apply the concepts to\nreal problems.\nThe book can be used with the accompanying software and programming as-\nsignments in two distinct kinds of introductory courses:\n1. Applications Emphasis: A course that covers the principles of database\nsystems, and emphasizes how they are used in developing data-intensive ap-\nplications. Two new chapters on application development (one on database-\nbacked applications, and one on Java and Internet application architec-\ntures) have been added to the third edition, and the entire book has been\nextensively revised and reorganized to support such a course. A running\ncase-study and extensive online materials (e.g., code for SQL queries and\nJava applications, online databases and solutions) make it easy to teach a\nhands-on application-centric course.\n2. Systems Emphasis: A course that has a strong systems emphasis and\nassumes that students have good programming skills in C and C++. In\nthis case the accompanying Minibase software can be llsed as the basis\nfor projects in which students are asked to implement various parts of a\nrelational DBMS. Several central modules in the project software (e.g.,\nheap files, buffer manager, B+ trees, hash indexes, various join methods)\nxxiv\n\nPTeface\nXKV\nare described in sufficient detail in the text to enable students to implement\nthem, given the (C++) class interfaces.\nr..,1any instructors will no doubt teach a course that falls between these two\nextremes. The restructuring in the third edition offers a very modular orga-\nnization that facilitates such hybrid courses. The also book contains enough\nmaterial to support advanced courses in a two-course sequence.\nOrganization of the Third Edition\nThe book is organized into six main parts plus a collection of advanced topics, as\nshown in Figure 0.1. The Foundations chapters introduce database systems, the\n(1) Foundations\nBoth\n(2) Application Development\nApplications emphasis\n(3) Storage and Indexing\nSystems emphasis\n(4) Query Evaluation\nSystems emphasis\n(5) Transaction Management\nSystems emphasis\n(6) Database Design and Tuning\nApplications emphasis\n(7) Additional Topics\nBoth\nOrganization of Parts in the Third Edition\nER model and the relational model. They explain how databases are created\nand used, and cover the basics of database design and querying, including an\nin-depth treatment of SQL queries. While an instructor can omit some of this\nmaterial at their discretion (e.g., relational calculus, some sections on the ER\nmodel or SQL queries), this material is relevant to every student of database\nsystems, and we recommend that it be covered in as much detail as possible.\nEach of the remaining five main parts has either an application or a systems\nempha.sis. Each of the three Systems parts has an overview chapter, designed to\nprovide a self-contained treatment, e.g., Chapter 8 is an overview of storage and\nindexing. The overview chapters can be used to provide stand-alone coverage\nof the topic, or as the first chapter in a more detailed treatment. Thus, in an\napplication-oriented course, Chapter 8 might be the only material covered on\nfile organizations and indexing, whereas in a systems-oriented course it would be\nsupplemented by a selection from Chapters 9 through 11. The Database Design\nand Tuning part contains a discussion of performance tuning and designing for\nsecure access. These application topics are best covered after giving students\na good grasp of database system architecture, and are therefore placed later in\nthe chapter sequence.\n\nXXVI\nSuggested Course Outlines\nDATABASE ~1ANAGEMENT SYSTEMS\nThe book can be used in two kinds of introductory database courses, one with\nan applications emphasis and one with a systems empha..':iis.\nThe introductory applications-oriented course could cover the :Foundations chap-\nters, then the Application Development chapters, followed by the overview sys-\ntems chapters, and conclude with the Database Design and Tuning material.\nChapter dependencies have been kept to a minimum, enabling instructors to\neasily fine tune what material to include. The Foundations material, Part I,\nshould be covered first, and within Parts III, IV, and V, the overview chapters\nshould be covered first.\nThe only remaining dependencies between chapters\nin Parts I to VI are shown as arrows in Figure 0.2.\nThe chapters in Part I\nshould be covered in sequence. However, the coverage of algebra and calculus\ncan be skipped in order to get to SQL queries sooner (although we believe this\nmaterial is important and recommend that it should be covered before SQL).\nThe introductory systems-oriented course would cover the Foundations chap-\nters and a selection of Applications and Systems chapters. An important point\nfor systems-oriented courses is that the timing of programming projects (e.g.,\nusing Minibase) makes it desirable to cover some systems topics early. Chap-\nter dependencies have been carefully limited to allow the Systems chapters to\nbe covered as soon as Chapters 1 and 3 have been covered.\nThe remaining\nFoundations chapters and Applications chapters can be covered subsequently.\nThe book also has ample material to support a multi-course sequence. Obvi-\nously, choosing an applications or systems emphasis in the introductory course\nresults in dropping certain material from the course; the material in the book\nsupports a comprehensive two-course sequence that covers both applications\nand systems a.spects. The Additional Topics range over a broad set of issues,\nand can be used as the core material for an advanced course, supplemented\nwith further readings.\nSupplementary Material\nThis book comes with extensive online supplements:\n..\nOnline Chapter: To make space for new material such a.'3 application\ndevelopment, information retrieval, and XML, we've moved the coverage\nof QBE to an online chapter.\nStudents can freely download the chapter\nfrom the book's web site, and solutions to exercises from this chapter are\nincluded in solutions manual.",
          "pages": [
            25,
            26,
            27,
            28,
            29
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Preface\nxxvii;\n(\n, (\n(\nH\n5 J\nI\nI\n1~\n!---i\nRelational Model\nRelational Algebra\nI Introduction,\n:\nERModel\n1--1\nSQLDM~\n\"---~~~\n. Conceptual Design\nl\nSQLDDL\nand Calculus\nII\nDatabase Application\n~\nDevelopment\nDatabase-Backed\nInternet Applications\n]\n] [\n]\nIII\nOverview of\nJ\\\nStorage and Indexing\nData Storage\nTree Indexes\nHash Indexes\n\\\nIV\nOverview of\nEvaluation of\nI--\nA Typical\nQuery Evaluation 1\\\nExternal Sorting\nRelational Operators\nRelational Optimizer\n\\\nV\nOverview of\nConcurrency r--\nCrash\nTransaction Management 1\\\nControl\nRecovery\n\\ \\\n\\\nVI\nSchema Refinement,\nPhysical DB\nSecurity and\nFDs, Normalization\nDesign, Tuning\nAuthorization\nParallel and\nObject-Database\nDeductive\nData Warehousing\nDistributed DBs\nSystems\nDatabases\nand Decision Support\nVII\nC\nData\nInformation Retrieval\nSpatial\nFurther\nMining\nand XML Data\nDatabases\nReading\nChapter Organization and Dependencies\nlIII\nLecture Slides:\nLecture slides are freely available for all chapters in\nPostscript, and PDF formats.\nCourse instructors can also obtain these\nslides in Microsoft Powerpoint format, and can adapt them to their teach-\ning needs. Instructors also have access to all figures llsed in the book (in\nxfig format), and can use them to modify the slides.\n\nxxviii\nDATABASE IVIANAGEMENT SVSTErvIS\n•\nSolutions to Chapter Exercises: The book has an UnUS1H:l,lly extensive\nset of in-depth exercises. Students can obtain solutioIls to odd-numbered\nchapter exercises and a set of lecture slides for each chapter through the\nvVeb in Postscript and Adobe PDF formats. Course instructors can obtain\nsolutions to all exercises.\n•\nSoftware: The book comes with two kinds of software.\nFirst, we have\nJ\\!Iinibase, a small relational DBMS intended for use in systems-oriented\ncourses.\nMinibase comes with sample assignments and solutions, as de-\nscribed in Appendix 30. Access is restricted to course instructors. Second,\nwe offer code for all SQL and Java application development exercises in\nthe book, together with scripts to create sample databases, and scripts for\nsetting up several commercial DBMSs. Students can only access solution\ncode for odd-numbered exercises, whereas instructors have access to all\nsolutions.\n•\nInstructor's Manual: The book comes with an online manual that of-\nfers instructors comments on the material in each chapter. It provides a\nsummary of each chapter and identifies choices for material to emphasize\nor omit.\nThe manual also discusses the on-line supporting material for\nthat chapter and offers numerous suggestions for hands-on exercises and\nprojects. Finally, it includes samples of examination papers from courses\ntaught by the authors using the book. It is restricted to course instructors.\nFor More Information\nThe home page for this book is at URL:\nhttp://www.cs.wisc.edu/-dbbook\nIt contains a list of the changes between the 2nd and 3rd editions, and a fre-\nquently updated link to all known erTOT8 in the book and its accompanying\nsupplements.\nInstructors should visit this site periodically or register at this\nsite to be notified of important changes by email.\nAcknowledgments\nThis book grew out of lecture notes for CS564, the introductory (senior/graduate\nlevel) database course at UvV-Madison. David De\\Vitt developed this course\nand the Minirel project, in which students wrote several well-chosen parts of\na relational DBMS. My thinking about this material was shaped by teaching\nCS564, and Minirel was the inspiration for Minibase, which is more compre-\nhensive (e.g., it has a query optimizer and includes visualization software) but\n\nPreface\nXXIX\ntries to retain the spirit of MinireL lVEke Carey and I jointly designed much of\nMinibase. My lecture notes (and in turn this book) were influenced by Mike's\nlecture notes and by Yannis Ioannidis's lecture slides.\nJoe Hellerstein used the beta edition of the book at Berkeley and provided\ninvaluable feedback, assistance on slides, and hilarious quotes.\nvVriting the\nchapter on object-database systems with Joe was a lot of fun.\nC. Mohan provided invaluable assistance, patiently answering a number of ques-\ntions about implementation techniques used in various commercial systems, in\nparticular indexing, concurrency control, and recovery algorithms. Moshe Zloof\nanswered numerous questions about QBE semantics and commercial systems\nbased on QBE. Ron Fagin, Krishna Kulkarni, Len Shapiro, Jim Melton, Dennis\nShasha, and Dirk Van Gucht reviewed the book and provided detailed feedback,\ngreatly improving the content and presentation. Michael Goldweber at Beloit\nCollege, Matthew Haines at Wyoming, Michael Kifer at SUNY StonyBrook,\nJeff Naughton at Wisconsin, Praveen Seshadri at Cornell, and Stan Zdonik at\nBrown also used the beta edition in their database courses and offered feedback\nand bug reports. In particular, Michael Kifer pointed out an error in the (old)\nalgorithm for computing a minimal cover and suggested covering some SQL\nfeatures in Chapter 2 to improve modularity. Gio Wiederhold's bibliography,\nconverted to Latex format by S. Sudarshan, and Michael Ley's online bibliogra-\nphy on databases and logic programming were a great help while compiling the\nchapter bibliographies. Shaun Flisakowski and Uri Shaft helped me frequently\nin my never-ending battles with Latex.\nlowe a special thanks to the many, many students who have contributed to\nthe Minibase software. Emmanuel Ackaouy, Jim Pruyne, Lee Schumacher, and\nMichael Lee worked with me when I developed the first version of Minibase\n(much of which was subsequently discarded, but which influenced the next\nversion). Emmanuel Ackaouy and Bryan So were my TAs when I taught CS564\nusing this version and went well beyond the limits of a TAship in their efforts\nto refine the project.\nPaul Aoki struggled with a version of Minibase and\noffered lots of useful eomments as a TA at Berkeley. An entire class of CS764\nstudents (our graduate database course) developed much of the current version\nof Minibase in a large class project that was led and coordinated by Mike Carey\nand me. Amit Shukla and Michael Lee were my TAs when I first taught CS564\nusing this vers~on of Minibase and developed the software further.\nSeveral students worked with me on independent projects, over a long period\nof time, to develop Minibase components. These include visualization packages\nfor the buffer manager and B+ trees (Huseyin Bekta.'3, Harry Stavropoulos, and\nWeiqing Huang); a query optimizer and visualizer (Stephen Harris, Michael Lee,\nand Donko Donjerkovic); an ER diagram tool based on the Opossum schema\n\nxxx\nDATABASE NIANAGEMENT SYSTEMS\n~\neditor (Eben Haber); and a GUI-based tool for normalization (Andrew Prock\nand Andy Therber). In addition, Bill Kimmel worked to integrate and fix a\nlarge body of code (storage manager, buffer manager, files and access methods,\nrelational operators, and the query plan executor) produced by the CS764 class\nproject. Ranjani Ramamurty considerably extended Bill's work on cleaning up\nand integrating the various modules. Luke Blanshard, Uri Shaft, and Shaun\nFlisakowski worked on putting together the release version of the code and\ndeveloped test suites and exercises based on the Minibase software. Krishna\nKunchithapadam tested the optimizer and developed part of the Minibase GUI.\nClearly, the Minibase software would not exist without the contributions of a\ngreat many talented people. With this software available freely in the public\ndomain, I hope that more instructors will be able to teach a systems-oriented\ndatabase course with a blend of implementation and experimentation to com-\nplement the lecture material.\nI'd like to thank the many students who helped in developing and checking\nthe solutions to the exercises and provided useful feedback on draft versions of\nthe book. In alphabetical order: X. Bao, S. Biao, M. Chakrabarti, C. Chan,\nW. Chen, N. Cheung, D. Colwell, C. Fritz, V. Ganti, J. Gehrke, G. Glass, V.\nGopalakrishnan, M. Higgins, T. Jasmin, M. Krishnaprasad, Y. Lin, C. Liu, M.\nLusignan, H. Modi, S. Narayanan, D. Randolph, A. Ranganathan, J. Reminga,\nA. Therber, M. Thomas, Q. Wang, R. Wang, Z. Wang, and J. Yuan. Arcady\nGrenadeI', James Harrington, and Martin Reames at Wisconsin and Nina Tang\nat Berkeley provided especially detailed feedback.\nCharlie Fischer, Avi Silberschatz, and Jeff Ullman gave me invaluable advice\non working with a publisher. My editors at McGraw-Hill, Betsy Jones and Eric\nMunson, obtained extensive reviews and guided this book in its early stages.\nEmily Gray and Brad Kosirog were there whenever problems cropped up. At\nWisconsin, Ginny Werner really helped me to stay on top of things.\nFinally, this book was a thief of time, and in many ways it was harder on my\nfamily than on me. My sons expressed themselves forthrightly. From my (then)\nfive-year-old, Ketan: \"Dad, stop working on that silly book. You don't have\nany time for me.\" Two-year-old Vivek: \"You working boook? No no no come\nplay basketball me!\" All the seasons of their discontent were visited upon my\nwife, and Apu nonetheless cheerfully kept the family going in its usual chaotic,\nhappy way all the many evenings and weekends I was wrapped up in this book.\n(Not to mention the days when I was wrapped up in being a faculty member!)\nAs in all things, I can trace my parents' hand in much of this; my father,\nwith his love of learning, and my mother, with her love of us, shaped me. My\nbrother Kartik's contributions to this book consisted chiefly of phone calls in\nwhich he kept me from working, but if I don't acknowledge him, he's liable to\n\nPreface\nbe annoyed. I'd like to thank my family for being there and giving meaning to\neverything I do. (There! I knew I'd find a legitimate reason to thank Kartik.)\nAcknowledgments for the Second Edition\nEmily Gray and Betsy Jones at 1tfcGraw-Hill obtained extensive reviews and\nprovided guidance and support as we prepared the second edition. Jonathan\nGoldstein helped with the bibliography for spatial databases.\nThe following\nreviewers provided valuable feedback on content and organization: Liming Cai\nat Ohio University, Costas Tsatsoulis at University of Kansas, Kwok-Bun Vue\nat University of Houston, Clear Lake, William Grosky at Wayne State Univer-\nsity, Sang H. Son at University of Virginia, James M. Slack at Minnesota State\nUniversity, Mankato, Herman Balsters at University of Twente, Netherlands,\nKaren C. Davis at University of Cincinnati, Joachim Hammer at University of\nFlorida, Fred Petry at Tulane University, Gregory Speegle at Baylor Univer-\nsity, Salih Yurttas at Texas A&M University, and David Chao at San Francisco\nState University.\nA number of people reported bugs in the first edition. In particular, we wish\nto thank the following: Joseph Albert at Portland State University, Han-yin\nChen at University of Wisconsin, Lois Delcambre at Oregon Graduate Institute,\nMaggie Eich at Southern Methodist University, Raj Gopalan at Curtin Univer-\nsity of Technology, Davood Rafiei at University of Toronto, Michael Schrefl at\nUniversity of South Australia, Alex Thomasian at University of Connecticut,\nand Scott Vandenberg at Siena College.\nA special thanks to the many people who answered a detailed survey about how\ncommercial systems support various features: At IBM, Mike Carey, Bruce Lind-\nsay, C. Mohan, and James Teng; at Informix, M. Muralikrishna and Michael\nUbell; at Microsoft, David Campbell, Goetz Graefe, and Peter Spiro; at Oracle,\nHakan Jacobsson, Jonathan D. Klein, Muralidhar Krishnaprasad, and M. Zi-\nauddin; and at Sybase, Marc Chanliau, Lucien Dimino, Sangeeta Doraiswamy,\nHanuma Kodavalla, Roger MacNicol, and Tirumanjanam Rengarajan.\nAfter reading about himself in the acknowledgment to the first edition, Ketan\n(now 8) had a simple question: \"How come you didn't dedicate the book to us?\nWhy mom?\"\nK~tan, I took care of this inexplicable oversight. Vivek (now 5)\nwas more concerned about the extent of his fame: \"Daddy, is my name in evvy\ncopy of your book? Do they have it in evvy compooter science department in\nthe world'?\"\nVivek, I hope so. Finally, this revision would not have made it\nwithout Apu's and Keiko's support.\n\nxx.,xii\nDATABASE l\\IANAGEl'vIENT SYSTEMS\nAcknowledgments for the Third Edition\n\\rYe thank Raghav Kaushik for his contribution to the discussion of XML, and\nAlex Thomasian for his contribution to the coverage of concurrency control. A\nspecial thanks to Jim JVlelton for giving us a pre-publication copy of his book\non object-oriented extensions in the SQL: 1999 standard, and catching several\nbugs in a draft of this edition. Marti Hearst at Berkeley generously permitted\nus to adapt some of her slides on Information Retrieval, and Alon Levy and\nDan Sueiu were kind enough to let us adapt some of their lectures on X:NIL.\nMike Carey offered input on Web services.\nEmily Lupash at McGraw-Hill has been a source of constant support and en-\ncouragement. She coordinated extensive reviews from Ming Wang at Embry-\nRiddle Aeronautical University, Cheng Hsu at RPI, Paul Bergstein at Univ. of\nMassachusetts, Archana Sathaye at SJSU, Bharat Bhargava at Purdue, John\nFendrich at Bradley, Ahmet Ugur at Central Michigan, Richard Osborne at\nUniv.\nof Colorado, Akira Kawaguchi at CCNY, Mark Last at Ben Gurion,\nVassilis Tsotras at Univ. of California, and Ronald Eaglin at Univ. of Central\nFlorida. It is a pleasure to acknowledge the thoughtful input we received from\nthe reviewers, which greatly improved the design and content of this edition.\nGloria Schiesl and Jade Moran dealt cheerfully and efficiently with last-minute\nsnafus, and, with Sherry Kane, made a very tight schedule possible. Michelle\nWhitaker iterated many times on the cover and end-sheet design.\nOn a personal note for Raghu, Ketan, following the canny example of the\ncamel that shared a tent, observed that \"it is only fair\" that Raghu dedicate\nthis edition solely to him and Vivek, since \"mommy already had it dedicated\nonly to her.\" Despite this blatant attempt to hog the limelight, enthusiastically\nsupported by Vivek and viewed with the indulgent affection of a doting father,\nthis book is also dedicated to Apu, for being there through it all.\nFor Johannes, this revision would not have made it without Keiko's support\nand inspiration and the motivation from looking at Elisa's peacefully sleeping\nface.\n\nPART I\nFOUNDATIONS",
          "pages": [
            30,
            31,
            32,
            33,
            34,
            35,
            36
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "OVERVIEW OF\nDATABASE SYSTEMS\n--\nWhat is a DBMS, in particular, a relational DBMS?\n..\nWhy should we consider a DBMS to manage data?\n..\nHow is application data represented in a DBMS?\n--\nHow is data in a DBMS retrieved and manipulated?\n..\nHow does a DBMS support concurrent access and protect data during\nsystem failures?\n..\nWhat are the main components of a DBMS?\n..\nWho is involved with databases in real life?\n..\nKey concepts: database management, data independence, database\ndesign, data model; relational databases and queries; schemas, levels\nof abstraction; transactions, concurrency and locking, recovery and\nlogging; DBMS architecture; database administrator, application pro-\ngrammer, end user\nHas everyone noticed that all the letters of the word database are typed with\nthe left hand? Now the layout of the QWEHTY typewriter keyboard was designed,\namong other things, to facilitate the even use of both hands. It follows, therefore,\nthat writing about databases is not only unnatural, but a lot harder than it appears.\n---Anonymous\nThe alIlount of information available to us is literally exploding, and the value\nof data as an organizational asset is widely recognized. To get the most out of\ntheir large and complex datasets, users require tools that simplify the tasks of\n\nCHAPTER If\nThe area of database management systenls is a microcosm of computer sci-\nence in general. The issues addressed and the techniques used span a wide\nspectrum, including languages, object-orientation and other progTamming\nparadigms, compilation, operating systems, concurrent programming, data\nstructures, algorithms, theory, parallel and distributed systems, user inter-\nfaces, expert systems and artificial intelligence, statistical techniques, and\ndynamic programming. \\Ve cannot go into all these &<;jpects of database\nmanagement in one book, but we hope to give the reader a sense of the\nexcitement in this rich and vibrant discipline.\nmanaging the data and extracting useful information in a timely fashion. Oth-\nerwise, data can become a liability, with the cost of acquiring it and managing\nit far exceeding the value derived from it.\nA database is a collection of data, typically describing the activities of one or\nmore related organizations. For example, a university database might contain\ninformation about the following:\n•\nEntities such as students, faculty, courses, and classrooms.\n•\nRelationships between entities, such as students' enrollment in courses,\nfaculty teaching courses, and the use of rooms for courses.\nA database management system, or DBMS, is software designed to assist\nin maintaining and utilizing large collections of data. The need for such systems,\nas well as their use, is growing rapidly. The alternative to using a DBMS is\nto store the data in files and write application-specific code to manage it. The\nuse of a DBMS has several important advantages, as we will see in Section 1.4.\n1.1\nMANAGING DATA\nThe goal of this book is to present an in-depth introduction to database man-\nagement systems, with an empha.sis on how to design a database and\n'li8C a\nDBMS effectively. Not surprisingly, many decisions about how to use a DBIvIS\nfor a given application depend on what capabilities the DBMS supports effi-\nciently. Therefore, to use a DBMS well, it is necessary to also understand how\na DBMS work8.\nMany kinds of database management systems are in use, but this book concen-\ntrates on relational database systems (RDBMSs), which are by far the\ndominant type of DB~'IS today. The following questions are addressed in the\ncorc chapters of this hook:",
          "pages": [
            37,
            38,
            39
          ],
          "relevance": {
            "score": 0.64,
            "sql_score": 0.8,
            "concept_score": 0.8,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The Entity-Relationship (ER) model is a conceptual framework used to design and represent databases. It uses entities, attributes, and relationships to organize data into logical structures that can be easily understood and manipulated.",
        "explanation": "The ER model helps in designing a database by breaking down the real-world scenario into discrete objects (entities), their characteristics (attributes), and how these objects relate to each other (relationships). This model is crucial because it provides a visual and conceptual representation of data, making it easier for designers to understand and design databases. It ensures that the database is well-structured and can be easily maintained.",
        "key_points": [
          "Key point 1: Entities represent real-world objects or concepts.",
          "Key point 2: Attributes define the properties of entities.",
          "Key point 3: Relationships show how entities are connected to each other.",
          "Key point 4: ER diagrams help in visualizing database structures.",
          "Key point 5: It facilitates better data modeling and reduces errors."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Define an entity AND its attributes CREATE TABLE Employee ( EmployeeID INT PRIMARY KEY, FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "explanation": "This example demonstrates how to create a simple table (entity) with attributes.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- CREATE a relationship between two entities CREATE TABLE Department ( DepartmentID INT PRIMARY KEY, DepartmentName VARCHAR(50) ); ALTER TABLE Employee ADD COLUMN DepartmentID INT; ALTER TABLE Employee ADD CONSTRAINT FK_Department FOREIGN KEY (DepartmentID) REFERENCES Department(DepartmentID);",
            "explanation": "This practical example shows how to create a relationship between two entities using foreign keys.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to define primary and foreign keys.",
            "incorrect_code": "-- Incorrect SQL CREATE TABLE Employee ( EmployeeID INT, FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "correct_code": "-- Correct SQL CREATE TABLE Employee ( EmployeeID INT PRIMARY KEY, FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "explanation": "This mistake can lead to data integrity issues. Always ensure that primary and foreign keys are defined to maintain the relationships between tables."
          }
        ],
        "practice": {
          "question": "Create a simple ER model for a library system with entities such as 'Books', 'Authors', and 'Members'. Define attributes and relationships.",
          "solution": "Create three tables: Books (BookID, Title, AuthorID), Authors (AuthorID, FirstName, LastName), and Members (MemberID, FirstName, LastName). Establish relationships between these tables using foreign keys."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "xxii\nDATABASE l\\1ANAGEMENT SYSTEMS\n27.2\nIntroduction to Information Retrieval\n27.2.1\nVector Space Model\n27.2.2 TFjIDF Weighting of Terms\n27.2.3\nRanking Document Similarity\n27.2.4\n:Measuring Success: P",
      "content_relevance": {
        "score": 0.7,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "er-diagrams": {
      "id": "er-diagrams",
      "title": "ER Diagrams",
      "definition": "Visual representation of entity-relationship models",
      "difficulty": "intermediate",
      "page_references": [
        35,
        36,
        37,
        38,
        39,
        40,
        41,
        42,
        43,
        44,
        45,
        46,
        47,
        48,
        49,
        50
      ],
      "sections": {
        "definition": {
          "text": "xx.,xii\nDATABASE l\\IANAGEl'vIENT SYSTEMS\nAcknowledgments for the Third Edition\n\\rYe thank Raghav Kaushik for his contribution to the discussion of XML, and\nAlex Thomasian for his contribution to the coverage of concurrency control. A\nspecial thanks to Jim JVlelton for giving us a pre-publication copy of his book\non object-oriented extensions in the SQL: 1999 standard, and catching several\nbugs in a draft of this edition. Marti Hearst at Berkeley generously permitted\nus to adapt some of her slides on Information Retrieval, and Alon Levy and\nDan Sueiu were kind enough to let us adapt some of their lectures on X:NIL.\nMike Carey offered input on Web services.\nEmily Lupash at McGraw-Hill has been a source of constant support and en-\ncouragement. She coordinated extensive reviews from Ming Wang at Embry-\nRiddle Aeronautical University, Cheng Hsu at RPI, Paul Bergstein at Univ. of\nMassachusetts, Archana Sathaye at SJSU, Bharat Bhargava at Purdue, John\nFendrich at Bradley, Ahmet Ugur at Central Michigan, Richard Osborne at\nUniv.\nof Colorado, Akira Kawaguchi at CCNY, Mark Last at Ben Gurion,\nVassilis Tsotras at Univ. of California, and Ronald Eaglin at Univ. of Central\nFlorida. It is a pleasure to acknowledge the thoughtful input we received from\nthe reviewers, which greatly improved the design and content of this edition.\nGloria Schiesl and Jade Moran dealt cheerfully and efficiently with last-minute\nsnafus, and, with Sherry Kane, made a very tight schedule possible. Michelle\nWhitaker iterated many times on the cover and end-sheet design.\nOn a personal note for Raghu, Ketan, following the canny example of the\ncamel that shared a tent, observed that \"it is only fair\" that Raghu dedicate\nthis edition solely to him and Vivek, since \"mommy already had it dedicated\nonly to her.\" Despite this blatant attempt to hog the limelight, enthusiastically\nsupported by Vivek and viewed with the indulgent affection of a doting father,\nthis book is also dedicated to Apu, for being there through it all.\nFor Johannes, this revision would not have made it without Keiko's support\nand inspiration and the motivation from looking at Elisa's peacefully sleeping\nface.\n\nPART I\nFOUNDATIONS\n\nOVERVIEW OF\nDATABASE SYSTEMS\n--\nWhat is a DBMS, in particular, a relational DBMS?\n..\nWhy should we consider a DBMS to manage data?\n..\nHow is application data represented in a DBMS?\n--\nHow is data in a DBMS retrieved and manipulated?\n..\nHow does a DBMS support concurrent access and protect data during\nsystem failures?\n..\nWhat are the main components of a DBMS?\n..\nWho is involved with databases in real life?\n..\nKey concepts: database management, data independence, database\ndesign, data model; relational databases and queries; schemas, levels\nof abstraction; transactions, concurrency and locking, recovery and\nlogging; DBMS architecture; database administrator, application pro-\ngrammer, end user\nHas everyone noticed that all the letters of the word database are typed with\nthe left hand? Now the layout of the QWEHTY typewriter keyboard was designed,\namong other things, to facilitate the even use of both hands. It follows, therefore,\nthat writing about databases is not only unnatural, but a lot harder than it appears.\n---Anonymous\nThe alIlount of information available to us is literally exploding, and the value\nof data as an organizational asset is widely recognized. To get the most out of\ntheir large and complex datasets, users require tools that simplify the tasks of",
          "pages": [
            35,
            36,
            37,
            38
          ],
          "relevance": {
            "score": 0.56,
            "sql_score": 0.7,
            "concept_score": 0.7,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER If\nThe area of database management systenls is a microcosm of computer sci-\nence in general. The issues addressed and the techniques used span a wide\nspectrum, including languages, object-orientation and other progTamming\nparadigms, compilation, operating systems, concurrent programming, data\nstructures, algorithms, theory, parallel and distributed systems, user inter-\nfaces, expert systems and artificial intelligence, statistical techniques, and\ndynamic programming. \\Ve cannot go into all these &<;jpects of database\nmanagement in one book, but we hope to give the reader a sense of the\nexcitement in this rich and vibrant discipline.\nmanaging the data and extracting useful information in a timely fashion. Oth-\nerwise, data can become a liability, with the cost of acquiring it and managing\nit far exceeding the value derived from it.\nA database is a collection of data, typically describing the activities of one or\nmore related organizations. For example, a university database might contain\ninformation about the following:\n•\nEntities such as students, faculty, courses, and classrooms.\n•\nRelationships between entities, such as students' enrollment in courses,\nfaculty teaching courses, and the use of rooms for courses.\nA database management system, or DBMS, is software designed to assist\nin maintaining and utilizing large collections of data. The need for such systems,\nas well as their use, is growing rapidly. The alternative to using a DBMS is\nto store the data in files and write application-specific code to manage it. The\nuse of a DBMS has several important advantages, as we will see in Section 1.4.\n1.1\nMANAGING DATA\nThe goal of this book is to present an in-depth introduction to database man-\nagement systems, with an empha.sis on how to design a database and\n'li8C a\nDBMS effectively. Not surprisingly, many decisions about how to use a DBIvIS\nfor a given application depend on what capabilities the DBMS supports effi-\nciently. Therefore, to use a DBMS well, it is necessary to also understand how\na DBMS work8.\nMany kinds of database management systems are in use, but this book concen-\ntrates on relational database systems (RDBMSs), which are by far the\ndominant type of DB~'IS today. The following questions are addressed in the\ncorc chapters of this hook:\n\nOverview of Databa8e SY8tem8\n1. Database Design and Application Development: How can a user\ndescribe a real-world enterprise (e.g., a university) in terms of the data\nstored in a DBMS? \\Vhat factors must be considered in deciding how to\norganize the stored data? How can ,ve develop applications that rely upon\na DBMS? (Chapters 2, 3, 6, 7, 19, 20, and 21.)\n2. Data Analysis: How can a user answer questions about the enterprise by\nposing queries over the data in the DBMS? (Chapters 4 and 5.)1\n3. Concurrency and Robustness: How does a DBMS allow many users to\naccess data concurrently, and how does it protect the data in the event of\nsystem failures? (Chapters 16, 17, and 18.)\n4. Efficiency and Scalability: How does a DBMS store large datasets and\nanswer questions against this data efficiently? (Chapters 8, 9, la, 11, 12,\n13, 14, and 15.)\nLater chapters cover important and rapidly evolving topics, such as parallel and\ndistributed database management, data warehousing and complex queries for\ndecision support, data mining, databases and information retrieval, XML repos-\nitories, object databases, spatial data management, and rule-oriented DBMS\nextensions.\nIn the rest of this chapter, we introduce these issues. In Section 1.2, we be-\ngin with a brief history of the field and a discussion of the role of database\nmanagement in modern information systems. We then identify the benefits of\nstoring data in a DBMS instead of a file system in Section 1.3, and discuss\nthe advantages of using a DBMS to manage data in Section 1.4. In Section\n1.5, we consider how information about an enterprise should be organized and\nstored in a DBMS. A user probably thinks about this information in high-level\nterms that correspond to the entities in the organization and their relation-\nships, whereas the DBMS ultimately stores data in the form of (rnany, many)\nbits. The gap between how users think of their data and how the data is ul-\ntimately stored is bridged through several levels of abstract1:on supported by\nthe DBMS. Intuitively, a user can begin by describing the data in fairly high-\nlevel terms, then refine this description by considering additional storage and\nrepresentation details as needed.\nIn Section 1.6, we consider how users can retrieve data stored in a DBMS and\nthe need for techniques to efficiently compute answers to questions involving\nsuch data. In Section 1.7, we provide an overview of how a DBMS supports\nconcurrent access to data by several users and how it protects the data in the\nevent of system failures.\n1An online chapter on Query-by-Example (QBE) is also available.\n\nCHAPTERrl\nvVe then briefly describe the internal structure of a DBMS in Section 1.8, and\nmention various groups of people associated with the development and use of\na DBMS in Section 1.9.\n1.2\nA HISTORICAL PERSPECTIVE\nFrom the earliest days of computers, storing and manipulating data have been a\nmajor application focus. The first general-purpose DBMS, designed by Charles\nBachman at General Electric in the early 1960s, was called the Integrated Data\nStore. It formed the basis for the network data model, which was standardized\nby the Conference on Data Systems Languages (CODASYL) and strongly in-\nfluenced database systems through the 1960s. Bachman was the first recipient\nof ACM's Turing Award (the computer science equivalent of a Nobel Prize) for\nwork in the database area; he received the award in 1973.\nIn the late 1960s, IBM developed the Information Management System (IMS)\nDBMS, used even today in many major installations. IMS formed the basis for\nan alternative data representation framework called the hierarchical data model.\nThe SABRE system for making airline reservations was jointly developed by\nAmerican Airlines and IBM around the same time, and it allowed several people\nto access the same data through a computer network. Interestingly, today the\nsame SABRE system is used to power popular Web-based travel services such\nas Travelocity.\nIn 1970, Edgar Codd, at IBM's San Jose Research Laboratory, proposed a new\ndata representation framework called the relational data model. This proved to\nbe a watershed in the development of database systems: It sparked the rapid\ndevelopment of several DBMSs based on the relational model, along with a rich\nbody of theoretical results that placed the field on a firm foundation.\nCodd\nwon the 1981 Turing Award for his seminal work. Database systems matured\nas an academic discipline, and the popularity of relational DBMSs changed the\ncommercial landscape. Their benefits were widely recognized, and the use of\nDBMSs for managing corporate data became standard practice.\nIn the 1980s, the relational model consolidated its position as the dominant\nDBMS paradigm, and database systems continued to gain widespread use. The\nSQL query language for relational databases, developed as part of IBM's Sys-\ntem R project, is now the standard query language.\nSQL was standardized\nin the late 1980s, and the current standard, SQL:1999, was adopted by the\nAmerican National Standards Institute (ANSI) and International Organization\nfor Standardization (ISO). Arguably, the most widely used form of concurrent\nprogramming is the concurrent execution of database programs (called trans-\nactions).\nUsers write programs a.\" if they are to be run by themselves, and\n\nOverview of Database Systems\nthe responsibility for running them concurrently is given to the DBl\\/IS. James\nGray won the 1999 Turing award for his contributions to database transaction\nmanagement.\nIn the late 1980s and the 1990s, advances were made in many areas of database\nsystems. Considerable research was carried out into more powerful query lan-\nguages and richer data models, with emphasis placed on supporting complex\nanalysis of data from all parts of an enterprise. Several vendors (e.g., IBM's\nDB2, Oracle 8, Informix2 UDS) extended their systems with the ability to store\nnew data types such as images and text, and to ask more complex queries. Spe-\ncialized systems have been developed by numerous vendors for creating data\nwarehouses, consolidating data from several databases, and for carrying out\nspecialized analysis.\nAn interesting phenomenon is the emergence of several enterprise resource\nplanning (ERP) and management resource planning (MRP) packages,\nwhich add a substantial layer of application-oriented features on top of a DBMS.\nWidely used. packages include systems from Baan, Oracle, PeopleSoft, SAP,\nand Siebel.\nThese packages identify a set of common tasks (e.g., inventory\nmanagement, human resources planning, financial analysis) encountered by a\nlarge number of organizations and provide a general application layer to carry\nout these ta.'3ks. The data is stored in a relational DBMS and the application\nlayer can be customized to different companies, leading to lower overall costs\nfor the companies, compared to the cost of building the application layer from\nscratch.\nMost significant, perhaps, DBMSs have entered the Internet Age. While the\nfirst generation of websites stored their data exclusively in operating systems\nfiles, the use of a DBMS to store data accessed through a Web browser is\nbecoming widespread.\nQueries are generated through Web-accessible forms\nand answers are formatted using a markup language such as HTML to be\neasily displayed in a browser. All the database vendors are adding features to\ntheir DBMS aimed at making it more suitable for deployment over the Internet.\nDatabclse management continues to gain importance as more and more data is\nbrought online and made ever more accessible through computer networking.\nToday the field is being driven by exciting visions such a'S multimedia databases,\ninteractive video, streaming data, digital libraries, a host of scientific projects\nsuch as the human genome mapping effort and NASA's Earth Observation Sys-\ntem project, and the desire of companies to consolidate their decision-making\nprocesses and mine their data repositories for useful information about their\nbusinesses. Commercially, database management systems represent one of the\n2Informix was recently acquired by IBM.\n\nlargest and most vigorous market segments. Thus the study of database sys-\ntems could prove to be richly rewarding in more ways than one!\n1.3\nFILE SYSTEMS VERSUS A DBMS\nTo understand the need for a\nDB:~,,1S, let us consider a motivating scenario: A\ncompany has a large collection (say, 500 GB3 ) of data on employees, depart-\nments, products, sales, and so on. This data is accessed concurrently by several\nemployees. Questions about the data must be answered quickly, changes made\nto the data by different users must be applied consistently, and access to certain\nparts of the data (e.g., salaries) must be restricted.\nWe can try to manage the data by storing it in operating system files.\nThis\napproach has many drawbacks, including the following:\n•\nWe probably do not have 500 GB of main memory to hold all the data.\nWe must therefore store data in a storage device such as a disk or tape and\nbring relevant parts into main memory for processing as needed.\n•\nEven if we have 500 GB of main memory, on computer systems with 32-bit\naddressing, we cannot refer directly to more than about 4 GB of data. We\nhave to program some method of identifying all data items.\n•\nWe have to write special programs to answer each question a user may want\nto ask about the data. These programs are likely to be complex because\nof the large volume of data to be searched.\n•\nWe must protect the data from inconsistent changes made by different users\naccessing the data concurrently. If applications must address the details of\nsuch concurrent access, this adds greatly to their complexity.\n•\nWe must ensure that data is restored to a consistent state if the system\ncrac;hes while changes are being made.\n•\nOperating systems provide only a password mechanism for security. This is\nnot sufficiently flexible to enforce security policies in which different users\nhave permission to access different subsets of the data.\nA DBMS is a piece of software designed to make the preceding tasks easier. By\nstoring data in.a DBNIS rather than as a collection of operating system files,\nwe can use the DBMS's features to manage the data in a robust and efficient\nrnanner.\nAs the volume of data and the number of users grow hundreds of\ngigabytes of data and thousands of users are common in current corporate\ndatabases DBMS support becomes indispensable.\n------,-\n.\n3 A kilobyte (KB) is 1024 bytes, a megabyte (MB) is 1024 KBs, a gigabyte (GB) is 1024 MBs, a\nterabyte ('1'B) is 1024 CBs, and a petabyte (PB) is 1024 terabytes.\n\nOverv'iew of Database Systems\n1.4\nADVANTAGES OF A DBMS\nUsing a DBMS to manage data h3..'3 many advantages:\nII\nData Independence: Application programs should not, ideally, be ex-\nposed to details of data representation and storage, The DBJVIS provides\nan abstract view of the data that hides such details.\nII\nEfficient Data Access: A DBMS utilizes a variety of sophisticated tech-\nniques to store and retrieve data efficiently. This feature is especially im-\npOl'tant if the data is stored on external storage devices.\nII\nData Integrity and Security: If data is always accessed through the\nDBMS, the DBMS can enforce integrity constraints. For example, before\ninserting salary information for an employee, the DBMS can check that\nthe department budget is not exceeded. Also, it can enforce access contmls\nthat govern what data is visible to different classes of users.\nII\nData Administration: When several users share the data, centralizing\nthe administration of data can offer sig11ificant improvements. Experienced\nprofessionals who understand the nature of the data being managed, and\nhow different groups of users use it, can be responsible for organizing the\ndata representation to minimize redundancy and for fine-tuning the storage\nof the data to make retrieval efficient.\nII\nConcurrent Access and Crash Recovery: A DBMS schedules concur-\nrent accesses to the data in such a manner that users can think of the data\nas being accessed by only one user at a time. Further, the DBMS protects\nusers from the effects of system failures.\nII\nReduced Application Development Time: Clearly, the DBMS sup-\nports important functions that are common to many applications accessing\ndata in the DBMS. This, in conjunction with the high-level interface to the\ndata, facilitates quick application development.\nDBMS applications are\nalso likely to be more robust than similar stand-alone applications because\nmany important tasks are handled by the DBMS (and do not have to be\ndebugged and tested in the application).\nGiven all these advantages, is there ever a reason not to use a DBMS? Some-\ntimes, yes. A DBMS is a complex piece of software, optimized for certain kinds\nof workloads (e.g., answering complex queries or handling many concurrent\nrequests), and its performance may not be adequate for certain specialized ap-\nplications.\nExamples include applications with tight real-time constraints or\njust a few well-defined critical operations for which efficient custom code must\nbe written. Another reason for not using a DBMS is that an application may\nneed to manipulate the data in ways not supported by the query language. In",
          "pages": [
            39,
            40,
            41,
            42,
            43,
            44
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER:l\nsuch a situation, the abstract view of the datet presented by the DBlVIS does\nnot match the application's needs and actually gets in the way. As an exam-\nple, relational databa.'3es do not support flexible analysis of text data (although\nvendors are now extending their products in this direction).\nIf specialized performance or data manipulation requirements are central to an\napplication, the application may choose not to use a DBMS, especially if the\nadded benefits of a DBMS (e.g., flexible querying, security, concurrent access,\nand crash recovery) are not required. In most situations calling for large-scale\ndata management, however, DBlVISs have become an indispensable tool.\n1.5\nDESCRIBING AND STORING DATA IN A DBMS\nThe user of a DBMS is ultimately concerned with some real-world enterprise,\nand the data to be stored describes various aspects of this enterprise.\nFor\nexample, there are students, faculty, and courses in a university, and the data\nin a university database describes these entities and their relationships.\nA data model is a collection of high-level data description constructs that hide\nmany low-level storage details. A DBMS allows a user to define the data to be\nstored in terms of a data model. Most database management systems today\nare based on the relational data model, which we focus on in this book.\nWhile the data model of the DBMS hides many details, it is nonetheless closer\nto how the DBMS stores data than to how a user thinks about the underlying\nenterprise. A semantic data model is a more abstract, high-level data model\nthat makes it easier for a user to come up with a good initial description of\nthe data in an enterprise. These models contain a wide variety of constructs\nthat help describe a real application scenario.\nA DBMS is not intended to\nsupport all these constructs directly; it is typically built around a data model\nwith just a few bi:1Sic constructs, such as the relational model.\nA databa.se\ndesign in terms of a semantic model serves as a useful starting point and is\nsubsequently translated into a database design in terms of the data model the\nDBMS actually supports.\nA widely used semantic data model called the entity-relationship (ER) model\nallows us to pictorially denote entities and the relationships among them. vVe\ncover the ER model in Chapter 2.\n\nOverview of Database Systc'lns\nJ\nAn Example of Poor Design: The relational schema for Students il-\nlustrates a poor design choice; you should neVCT create a field such as age,\nwhose value is constantly changing.\nA better choice would be DOB (for\ndate of birth); age can be computed from this. \\Ve continue to use age in\nour examples, however, because it makes them easier to read.\n1.5.1\nThe Relational Model\nIn this section we provide a brief introduction to the relational model.\nThe\ncentral data description construct in this model is a relation, which can be\nthought of as a set of records.\nA description of data in terms of a data model is called a schema.\nIn the\nrelational model, the schema for a relation specifies its name, the name of each\nfield (or attribute or column), and the type of each field. As an example,\nstudent information in a university database may be stored in a relation with\nthe following schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nThe preceding schema says that each record in the Students relation has five\nfields, with field names and types as indicated.\nAn example instance of the\nStudents relation appears in Figure 1.1.\nI sid\n[ name\nIZogin\nJones\njones@cs\n3.4\nSmith\nsmith@ee\n3.2\nSmith\nsmith@math\n3.8\nMadayan\nmadayan(gmusic\n1.8\nGuldu\nguldui:Qhnusic\n2.0\nAn Instance of the Students Relation\nEach row in the Students relation is a record that describes a student. The\ndescription is rlOt completeo----for example, the student's height is not included---\nbut is presumably adequate for the intended applications in the university\ndatabase. Every row follows the schema of the Students relation. The schema\ncall therefore be regarded as a template for describing a student.\nvVe can make the description of a collection of students more precise by specify-\ning integrity constraints, which are conditions that the records in a relation\n\nCHAPTER? 1\nmust satisfy. for example, we could specify that every student has a unique\nsid value. Observe that we cannot capture this information by simply adding\nanother field to the Students schema. Thus, the ability to specify uniqueness\nof the values in a field increases the accuracy with which we can describe our\ndata.\nThe expressiveness of the constructs available for specifying integrity\nconstraints is an important ar;;pect of a data model.\nOther Data Models\nIn addition to the relational data model (which is used in numerous systems,\nincluding IBM's DB2, Informix, Oracle, Sybase, Microsoft's Access, FoxBase,\nParadox, Tandem, and Teradata), other important data models include the\nhierarchical model (e.g., used in IBM's IMS DBMS), the network model (e.g.,\nused in IDS and IDMS), the object-oriented model (e.g., used in Objectstore\nand Versant), and the object-relational model (e.g., used in DBMS products\nfrom IBM, Informix, ObjectStore, Oracle, Versant, and others). While many\ndatabases use the hierarchical and network models and systems based on the\nobject-oriented and object-relational models are gaining acceptance in the mar-\nketplace, the dominant model today is the relational model.\nIn this book, we focus on the relational model because of its wide use and im-\nportance. Indeed, the object-relational model, which is gaining in popularity, is\nan effort to combine the best features of the relational and object-oriented mod-\nels, and a good grasp of the relational model is necessary to understand object-\nrelational concepts. (We discuss the object-oriented and object-relational mod-\nels in Chapter 23.)\n1.5.2\nLevels of Abstraction in a DBMS\nThe data in a DBMS is described at three levels of abstraction, ar;; illustrated\nin Figure 1.2. The database description consists of a schema at each of these\nthree levels of abstraction: the conceptual, physical, and external.\nA data definition language (DDL) is used to define the external and coneep-\ntual schemas. \\;Ye discuss the DDL facilities of the Inost wid(~ly used database\nlanguage, SQL, in Chapter 3. All DBMS vendors also support SQL commands\nto describe aspects of the physical schema, but these commands are not part of\nthe SQL language standard. Information about the conceptual, external, and\nphysical schemas is stored in the system catalogs (Section 12.1). vVe discuss\nthe three levels of abstraction in the rest of this section.",
          "pages": [
            45,
            46,
            47
          ],
          "relevance": {
            "score": 0.54,
            "sql_score": 0.8,
            "concept_score": 0.8,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "An ER diagram is a visual representation of entities, their attributes, and relationships between them. It's crucial for database design as it helps in understanding and modeling real-world data structures.",
        "explanation": "ER diagrams are essential for database design because they provide a clear, graphical way to represent the structure of a database. They help in identifying entities (like students, courses), their attributes (like student ID, course name), and how these entities relate to each other (like which students are enrolled in which courses). By using ER diagrams, we can ensure that our database is well-structured and meets the needs of the application it supports.",
        "key_points": [
          "Key point 1: Entities represent real-world objects or concepts.",
          "Key point 2: Attributes define the properties of entities.",
          "Key point 3: Relationships show how entities are connected.",
          "Key point 4: ER diagrams help in designing a logical database schema.",
          "Key point 5: They facilitate communication between database designers and users."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Define an entity CREATE TABLE students ( student_id INT PRIMARY KEY, name VARCHAR(100) ); -- Define a relationship CREATE TABLE enrollments ( enrollment_id INT PRIMARY KEY, student_id INT, course_id INT, FOREIGN KEY (student_id) REFERENCES students(student_id) );",
            "explanation": "This example shows how to define entities and their relationships in SQL. The 'students' table represents the entity, while the 'enrollments' table represents the relationship between students and courses.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Query using ER diagram\nSELECT s.student_id, s.name, c.course_name\nFROM students s\nJOIN enrollments e ON s.student_id = e.student_id\nJOIN courses c ON e.course_id = c.course_id;",
            "explanation": "This practical example demonstrates how to use an ER diagram to query a database. It joins the 'students', 'enrollments', and 'courses' tables to retrieve information about students enrolled in specific courses."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to define primary keys",
            "incorrect_code": "-- Incorrect SQL CREATE TABLE students ( name VARCHAR(100) );",
            "correct_code": "-- Correct SQL CREATE TABLE students ( student_id INT PRIMARY KEY, name VARCHAR(100) );",
            "explanation": "Primary keys are essential for uniquely identifying each record in a table. Forgetting to define one can lead to duplicate entries and other issues."
          }
        ],
        "practice": {
          "question": "Draw an ER diagram for a library system that includes entities like 'Books', 'Authors', and 'Members'. Show relationships between these entities.",
          "solution": "The solution involves creating three tables: 'Books' with attributes like book_id, title, author_id; 'Authors' with attributes like author_id, name; and 'Members' with attributes like member_id, name. Relationships would include a foreign key in the 'Books' table linking to the 'Authors' table and another linking to the 'Members' table for borrowings."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "xx.,xii\nDATABASE l\\IANAGEl'vIENT SYSTEMS\nAcknowledgments for the Third Edition\n\\rYe thank Raghav Kaushik for his contribution to the discussion of XML, and\nAlex Thomasian for his contribution to the c",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "cardinality": {
      "id": "cardinality",
      "title": "Cardinality Constraints",
      "definition": "Specifying how many instances of one entity relate to instances of another",
      "difficulty": "intermediate",
      "page_references": [
        45,
        46,
        47,
        48,
        49,
        50,
        51,
        52,
        53,
        54,
        55,
        56,
        57,
        58,
        59,
        60
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER:l\nsuch a situation, the abstract view of the datet presented by the DBlVIS does\nnot match the application's needs and actually gets in the way. As an exam-\nple, relational databa.'3es do not support flexible analysis of text data (although\nvendors are now extending their products in this direction).\nIf specialized performance or data manipulation requirements are central to an\napplication, the application may choose not to use a DBMS, especially if the\nadded benefits of a DBMS (e.g., flexible querying, security, concurrent access,\nand crash recovery) are not required. In most situations calling for large-scale\ndata management, however, DBlVISs have become an indispensable tool.\n1.5\nDESCRIBING AND STORING DATA IN A DBMS\nThe user of a DBMS is ultimately concerned with some real-world enterprise,\nand the data to be stored describes various aspects of this enterprise.\nFor\nexample, there are students, faculty, and courses in a university, and the data\nin a university database describes these entities and their relationships.\nA data model is a collection of high-level data description constructs that hide\nmany low-level storage details. A DBMS allows a user to define the data to be\nstored in terms of a data model. Most database management systems today\nare based on the relational data model, which we focus on in this book.\nWhile the data model of the DBMS hides many details, it is nonetheless closer\nto how the DBMS stores data than to how a user thinks about the underlying\nenterprise. A semantic data model is a more abstract, high-level data model\nthat makes it easier for a user to come up with a good initial description of\nthe data in an enterprise. These models contain a wide variety of constructs\nthat help describe a real application scenario.\nA DBMS is not intended to\nsupport all these constructs directly; it is typically built around a data model\nwith just a few bi:1Sic constructs, such as the relational model.\nA databa.se\ndesign in terms of a semantic model serves as a useful starting point and is\nsubsequently translated into a database design in terms of the data model the\nDBMS actually supports.\nA widely used semantic data model called the entity-relationship (ER) model\nallows us to pictorially denote entities and the relationships among them. vVe\ncover the ER model in Chapter 2.\n\nOverview of Database Systc'lns\nJ\nAn Example of Poor Design: The relational schema for Students il-\nlustrates a poor design choice; you should neVCT create a field such as age,\nwhose value is constantly changing.\nA better choice would be DOB (for\ndate of birth); age can be computed from this. \\Ve continue to use age in\nour examples, however, because it makes them easier to read.\n1.5.1\nThe Relational Model\nIn this section we provide a brief introduction to the relational model.\nThe\ncentral data description construct in this model is a relation, which can be\nthought of as a set of records.\nA description of data in terms of a data model is called a schema.\nIn the\nrelational model, the schema for a relation specifies its name, the name of each\nfield (or attribute or column), and the type of each field. As an example,\nstudent information in a university database may be stored in a relation with\nthe following schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nThe preceding schema says that each record in the Students relation has five\nfields, with field names and types as indicated.\nAn example instance of the\nStudents relation appears in Figure 1.1.\nI sid\n[ name\nIZogin\nJones\njones@cs\n3.4\nSmith\nsmith@ee\n3.2\nSmith\nsmith@math\n3.8\nMadayan\nmadayan(gmusic\n1.8\nGuldu\nguldui:Qhnusic\n2.0\nAn Instance of the Students Relation\nEach row in the Students relation is a record that describes a student. The\ndescription is rlOt completeo----for example, the student's height is not included---\nbut is presumably adequate for the intended applications in the university\ndatabase. Every row follows the schema of the Students relation. The schema\ncall therefore be regarded as a template for describing a student.\nvVe can make the description of a collection of students more precise by specify-\ning integrity constraints, which are conditions that the records in a relation\n\nCHAPTER? 1\nmust satisfy. for example, we could specify that every student has a unique\nsid value. Observe that we cannot capture this information by simply adding\nanother field to the Students schema. Thus, the ability to specify uniqueness\nof the values in a field increases the accuracy with which we can describe our\ndata.\nThe expressiveness of the constructs available for specifying integrity\nconstraints is an important ar;;pect of a data model.\nOther Data Models\nIn addition to the relational data model (which is used in numerous systems,\nincluding IBM's DB2, Informix, Oracle, Sybase, Microsoft's Access, FoxBase,\nParadox, Tandem, and Teradata), other important data models include the\nhierarchical model (e.g., used in IBM's IMS DBMS), the network model (e.g.,\nused in IDS and IDMS), the object-oriented model (e.g., used in Objectstore\nand Versant), and the object-relational model (e.g., used in DBMS products\nfrom IBM, Informix, ObjectStore, Oracle, Versant, and others). While many\ndatabases use the hierarchical and network models and systems based on the\nobject-oriented and object-relational models are gaining acceptance in the mar-\nketplace, the dominant model today is the relational model.\nIn this book, we focus on the relational model because of its wide use and im-\nportance. Indeed, the object-relational model, which is gaining in popularity, is\nan effort to combine the best features of the relational and object-oriented mod-\nels, and a good grasp of the relational model is necessary to understand object-\nrelational concepts. (We discuss the object-oriented and object-relational mod-\nels in Chapter 23.)\n1.5.2\nLevels of Abstraction in a DBMS\nThe data in a DBMS is described at three levels of abstraction, ar;; illustrated\nin Figure 1.2. The database description consists of a schema at each of these\nthree levels of abstraction: the conceptual, physical, and external.\nA data definition language (DDL) is used to define the external and coneep-\ntual schemas. \\;Ye discuss the DDL facilities of the Inost wid(~ly used database\nlanguage, SQL, in Chapter 3. All DBMS vendors also support SQL commands\nto describe aspects of the physical schema, but these commands are not part of\nthe SQL language standard. Information about the conceptual, external, and\nphysical schemas is stored in the system catalogs (Section 12.1). vVe discuss\nthe three levels of abstraction in the rest of this section.\n\nOucTlJ'iew of Database SyslcTns\nExternal Schema 1\nExternal Schema 2\nExternal Schema 3\nLevels of Abstraction in a DBMS\nConceptual Schema\nThe conceptual schema (sometimes called the logical schema) describes the\nstored data in terms of the data model of the DBMS. In a relational DBMS,\nthe conceptual schema describes all relations that are stored in the database.\nIn our sample university databa..'3e, these relations contain information about\nentities, such as students and faculty, and about relationships, such as students'\nenrollment in courses.\nAll student entities can be described using records in\na Students relation, as we saw earlier. In fact, each collection of entities and\neach collection of relationships can be described as a relation, leading to the\nfollowing conceptual schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nFaculty(fid: string, fname: string, sal: real)\nCourses( cid: string, cname: string, credits: integer)\nRooms(nw: integer, address: string, capacity: integer)\nEnrolled(sid: string, cid: string, grade: string)\nTeaches(fid: string, cid: string)\nMeets_In( cid: string, rno: integer, ti'fne: string)\nThe choice of relations, and the choice of fields for each relation, is not always\nobvious, and the process of arriving at a good conceptual schema is called\nconceptual database design.\nvVe discuss conceptual databa..se design in\nChapters 2 and 19.",
          "pages": [
            45,
            46,
            47,
            48
          ],
          "relevance": {
            "score": 0.31,
            "sql_score": 0.8,
            "concept_score": 0.33,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Physical Schema\nCHAPTER»1\nThe physical schema specifies additional storage details.\nEssentially, the\nphysical schema summarizes how the relations described in the conceptual\nschema are actually stored on secondary storage devices such as disks and\ntapes.\nWe must decide what file organizations to use to store the relations and create\nauxiliary data structures, called indexes, to speed up data retrieval operations.\nA sample physical schema for the university database follows:\n•\nStore all relations as unsorted files of records. (A file in a DBMS is either\na collection of records or a collection of pages, rather than a string of\ncharacters as in an operating system.)\n•\nCreate indexes on the first column of the Students, Faculty, and Courses\nrelations, the sal column of Faculty, and the capacity column of Rooms.\nDecisions about the physical schema are based on an understanding of how the\ndata is typically accessed. The process of arriving at a good physical schema\nis called physical database design. We discuss physical database design in\nChapter 20.\nExternal Schema\nExternal schemas, which usually are also in terms of the data model of\nthe DBMS, allow data access to be customized (and authorized) at the level\nof individual users or groups of users.\nAny given database has exactly one\nconceptual schema and one physical schema because it has just one set of\nstored relations, but it may have several external schemas, each tailored to a\nparticular group of users. Each external schema consists of a collection of one or\nmore views and relations from the conceptual schema. A view is conceptually\na relation, but the records in a view are not stored in the DBMS. Rather, they\nare computed using a definition for the view, in terms of relations stored in the\nDBMS. \\iVe discuss views in more detail in Chapters 3 and 25.\nThe external schema design is guided by end user requirements. For exalnple,\nwe might want to allow students to find out the names of faculty members\nteaching courses as well as course enrollments. This can be done by defining\nthe following view:\nCourseinfo( rid: string, fname: string, enTollment: integer)\nA user can treat a view just like a relation and ask questions about the records\nin the view.\nEven though the records in the view are not stored explicitly,\n\nOverview of Database Systems\n).5\nthey are computed as needed. vVe did not include Courseinfo in the conceptual\nschema because we can compute Courseinfo from the relations in the conceptual\nschema, and to store it in addition would be redundant. Such redundancy, in\naddition to the wasted space, could lead to inconsistencies.\nFor example, a\ntuple may be inserted into the Enrolled relation, indicating that a particular\nstudent has enrolled in some course, without incrementing the value in the\nenrollment field of the corresponding record of Courseinfo (if the latter also is\npart of the conceptual schema and its tuples are stored in the DBMS).\nL5.3\nData Independence\nA very important advantage of using a DBMS is that it offers data indepen-\ndence. That is, application programs are insulated from changes in the way\nthe data is structured and stored. Data independence is achieved through use\nof the three levels of data abstraction; in particular, the conceptual schema and\nthe external schema provide distinct benefits in this area.\nRelations in the external schema (view relations) are in principle generated\non demand from the relations corresponding to the conceptual schema.4\nIf\nthe underlying data is reorganized, that is, the conceptual schema is changed,\nthe definition of a view relation can be modified so that the same relation is\ncomputed as before.\nFor example, suppose that the Faculty relation in our\nuniversity database is replaced by the following two relations:\nFaculty_public(fid: string, fname: string, office: integer)\nFaculty_private(J£d: string, sal: real)\nIntuitively, some confidential information about faculty has been placed in a\nseparate relation and information about offices has been added. The Courseinfo\nview relation can be redefined in terms of Faculty_public and Faculty_private,\nwhich together contain all the information in Faculty, so that a user who queries\nCourseinfo will get the same answers as before.\nThus, users can be shielded from changes in the logical structure of the data, or\nchanges in the choice of relations to be stored. This property is called logical\ndata independence.\nIn turn, the conceptual schema insulates users from changes in physical storage\ndetails. This property is referred to as physical data independence. The\nconceptual schema hides details such as how the data is actually laid out on\ndisk, the file structure, and the choice of indexes. As long as the conceptual\n4In practice, they could be precomputed and stored to speed up queries on view relations, but the\ncomputed view relations must be updated whenever the underlying relations are updated.\n\nCHAPTE~ 1\nschema remains the same, we can change these storage details without altering\napplications. (Of course, performance might be affected by such changes.)\n1.6\nQUERIES IN A DBMS\nThe ease \\vith which information can be obtained from a database often de-\ntermines its value to a user. In contrast to older database systems, relational\ndatabase systems allow a rich class of questions to be posed easily; this feature\nhas contributed greatly to their popularity.\nConsider the sample university\ndatabase in Section 1.5.2. Here are some questions a user might ask:\n1. What is the name of the student with student ID 1234567\n2. What is the average salary of professors who teach course CS5647\n3. How many students are enrolled in CS5647\n4. What fraction of students in CS564 received a grade better than B7\n5. Is any student with a CPA less than 3.0 enrolled in CS5647\nSuch questions involving the data stored in a DBMS are called queries.\nA\nDBMS provides a specialized language, called the query language, in which\nqueries can be posed.\nA very attractive feature of the relational model is\nthat it supports powerful query languages. Relational calculus is a formal\nquery language based on mathematical logic, and queries in this language have\nan intuitive, precise meaning.\nRelational algebra is another formal query\nlanguage, based on a collection of operators for manipulating relations, which\nis equivalent in power to the calculus.\nA DBMS takes great care to evaluate queries as efficiently as possible.\nvVe\ndiscuss query optimization and evaluation in Chapters 12, Vl, and 15.\nOf\ncourse, the efficiency of query evaluation is determined to a large extent by\nhow the data is stored physically.\nIndexes can be used to speed up many\nqueries----in fact, a good choice of indexes for the underlying relations can speed\nup each query in the preceding list. \\Ve discuss data storage and indexing in\nChapters 8, 9, 10, and 11.\nA DBMS enables users to create, modify, and query data through a data\nmanipulation language (DML). Thus, the query language is only one part\nof the Dl\\ilL, which also provides constructs to insert, delete, and modify data,.\nvVe will discuss the DML features of SQL in Chapter 5. The DML and DDL\nare collectively referred to cl.s the data sublanguage when embedded within\na host language (e.g., C or COBOL).\n\nOverview of Database Systems\n1.7\nTRANSACTION MANAGEMENT\nConsider a database that holds information about airline reservations. At any\ngiven instant, it is possible (and likely) that several travel agents are look-\ning up information about available seats OIl various flights and making new\nseat reservations. When several users access (and possibly modify) a database\nconcurrently, the DBMS must order their requests carefully to avoid conflicts.\nFor example, when one travel agent looks up Flight 100 on some given day\nand finds an empty seat, another travel agent may simultaneously be making\na reservation for that seat, thereby making the information seen by the first\nagent obsolete.\nAnother example of concurrent use is a bank's database.\nWhile one user's\napplication program is computing the total deposits, another application may\ntransfer money from an account that the first application has just 'seen' to an\naccount that has not yet been seen, thereby causing the total to appear larger\nthan it should be.\nClearly, such anomalies should not be allowed to occur.\nHowever, disallowing concurrent access can degrade performance.\nFurther, the DBMS must protect users from the effects of system failures by\nensuring that all data (and the status of active applications) is restored to a\nconsistent state when the system is restarted after a crash. For example, if a\ntravel agent asks for a reservation to be made, and the DBMS responds saying\nthat the reservation has been made, the reservation should not be lost if the\nsystem crashes.\nOn the other hand, if the DBMS has not yet responded to\nthe request, but is making the necessary changes to the data when the crash\noccurs, the partial changes should be undone when the system comes back up.\nA transaction is anyone execution of a user program in a DBMS. (Executing\nthe same program several times will generate several transactions.) This is the\nbasic unit of change as seen by the DBMS: Partial transactions are not allowed,\nand the effect of a group of transactions is equivalent to some serial execution\nof all transactions.\nvVe briefly outline how these properties are guaranteed,\ndeferring a detailed discussion to later chapters.\n1.7.1\nConcurrent Execution of Transactions\nAn important task of a DBMS is to schedule concurrent accesses to data so\nthat each user can safely ignore the fact that others are accessing the data\nconcurrently. The importance of this ta.sk cannot be underestimated because\na database is typically shared by a large number of users, who submit their\nrequests to the DBMS independently and simply cannot be expected to deal\nwith arbitrary changes being made concurrently by other users.\nA DBMS\n\nCHAPTER a.\nallows users to think of their programs &'3 if they were executing in isolation,\none after the other in some order chosen by the DBJ\\;:IS. For example, if a\nprogTam that deposits cash into an account is submitted to the DBMS at the\nsame time as another program that debits money from the same account, either\nof these programs could be run first by the DBMS, but their steps will not be\ninterleaved in such a way that they interfere with each other.\nA locking protocol is a set of rules to be followed by each transaction (and en-\nforced by the DBMS) to ensure that, even though actions of several transactions\nmight be interleaved, the net effect is identical to executing all transactions in\nsome serial order. A lock is a mechanism used to control access to database\nobjects.\nTwo kinds of locks are commonly supported by a DBMS: shared\nlocks on an object can be held by two different transactions at the same time,\nbut an exclusive lock on an object ensures that no other transactions hold\nany lock on this object.\nSuppose that the following locking protocol is followed: Every transaction be-\ngins by obtaining a shared lock on each data object that it needs to read and an\nexclusive lock on each data object that it needs to\nrnod~fy, then releases all its\nlocks after completing all actions. Consider two transactions T1 and T2 such\nthat T1 wants to modify a data object and T2 wants to read the same object.\nIntuitively, if T1's request for an exclusive lock on the object is granted first,\nT2 cannot proceed until T1 relea..':les this lock, because T2's request for a shared\nlock will not be granted by the DBMS until then. Thus, all of T1's actions will\nbe completed before any of T2's actions are initiated. We consider locking in\nmore detail in Chapters 16 and 17.\n1.7.2\nIncomplete Transactions and System Crashes\nTransactions can be interrupted before running to completion for a va,riety of\nreasons, e.g., a system crash. A DBMS must ensure that the changes made by\nsuch incomplete transactions are removed from the database. For example, if\nthe DBMS is in the middle of transferring money from account A to account\nB and has debited the first account but not yet credited the second when the\ncrash occurs, the money debited from account A must be restored when the\nsystem comes back up after the crash.\nTo do so, the DBMS maintains a log of all writes to the database. A crucial\nproperty of the log is that each write action must be recorded in the log (on disk)\nbefore the corresponding change is reflected in the database itself--otherwise, if\nthe system crcLShes just after making the change in the datab(Lse but before the\nchange is recorded in the log, the DBIVIS would be unable to detect and undo\nthis change. This property is called Write-Ahead Log, or WAL. To ensure\n\nOverview of Database By.stems\nthis property, the DBMS must be able to selectively force a page in memory to\ndisk.\nThe log is also used to ensure that the changes made by a successfully com-\npleted transaction are not lost due to a system crash, as explained in Chapter\n18.\nBringing the database to a consistent state after a system crash can be\na slow process, since the DBMS must ensure that the effects of all transac-\ntions that completed prior to the crash are restored, and that the effects of\nincomplete transactions are undone. The time required to recover from a crash\ncan be reduced by periodically forcing some information to disk; this periodic\noperation is called a checkpoint.\n1.7.3\nPoints to Note\nIn summary, there are three points to remember with respect to DBMS support\nfor concurrency control and recovery:\n1. Every object that is read or written by a transaction is first locked in shared\nor exclusive mode, respectively.\nPlacing a lock on an object restricts its\navailability to other transactions and thereby affects performance.\n2. For efficient log maintenance, the DBMS must be able to selectively force\na collection of pages in main memory to disk. Operating system support\nfor this operation is not always satisfactory.\n3. Periodic checkpointing can reduce the time needed to recover from a crash.\nOf course, this must be balanced against the fact that checkpointing too\noften slows down normal execution.\n1.8\nSTRUCTURE OF A DBMS\nbased on the relational data model.\nThe DBMS accepts SQL comma,nels generated from a variety of user interfaces,\nproduces query evaluation plans, executes these plans against the databc4'le, and\nreturns the answers. (This is a simplification: SQL commands can be embedded\nin host-language application programs, e.g., Java or COBOL programs.\nvVe\nignore these issues to concentrate on the core DBl\\ilS functionality.)\nvVhen a user issues a query, the parsed query is presented to a query opti-\nmizer, which uses information about how the data is stored to produce an\nefficient execution plan for evaluating the query.\nAn execution plan is a\n\nCHAPTER 1\nUnsophisticated users (customers, travel agents, etc.)\nSophisticated users. application\nprogrammers, DB administrators\nDBMS\nshows references\nDATABASE\nRecovery\nManager\nshov.':$ command now\nSQL Interla<:e\n\\\nSystem Catalog\nData Files .--/\nPlan Executor\nOperator Evaluator\n[C-l\nInd,\"'I~---\"\"\n'----------,-~-~~-~ - --- ---,.--------.--\nQuery\nEvaluation\nL::::=======~=========::',J'Engine\nArchitecture of a DBMS\nblueprint for evaluating a query, usually represented as a tree of relational op-\nerators (with annotations that contain additional detailed information about\nwhich access methods to use, etc.). We discuss query optimization in Chapters\n12 and 15.\nRelational operators serve as the building blocks for evaluating\nqueries posed against the data. The implementation of these operators is dis-\ncussed in Chapters 12 and 14.\nThe code that implements relational operators sits on top of the file and access\nmethods layer. This layer supports the concept of a file, which, in a DBMS, is a\ncollection of pages or a collection of records. Heap files, or files of unordered\npages, a:s well as indexes are supported.\nIn addition to keeping track of the\npages in a file, this layer organizes the information within a page.\nFile and\npage level storage issues are considered in Chapter 9. File organizations and\nindexes are cQIlsidered in Chapter 8.\nThe files and access methods layer code sits on top of the buffer manager,\nwhich brings pages in from disk to main memory ct.\" needed in response to read\nrequests. Buffer management is discussed in Chapter 9.",
          "pages": [
            49,
            50,
            51,
            52,
            53,
            54,
            55
          ],
          "relevance": {
            "score": 0.38,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Ove1'Fie'll} of Database SY.'3te'171S\n2).\nThe lowest layer of the DBMS software deals with management of space on\ndisk, where the data is stored.\nHigher layers allocate, deallocate, read, and\nwrite pages through (routines provided by) this layer, called the disk space\nmanager. This layer is discussed in Chapter 9.\nThe DBMS supports concurrency and crash recovery by carefully scheduling\nuser requests and maintaining a log of all changes to the database. DBNIS com-\nponents associated with concurrency control and recovery include the trans-\naction manager, which ensures that transactions request and release locks\naccording to a suitable locking protocol and schedules the execution transac-\ntions; the lock manager, which keeps track of requests for locks and grants\nlocks on database objects when they become available; and the recovery man-\nager, which is responsible for maintaining a log and restoring the system to a\nconsistent state after a crash. The disk space manager, buffer manager, and\nfile and access method layers must interact with these components. We discuss\nconcurrency control and recovery in detail in Chapter 16.\n1.9\nPEOPLE WHO WORK WITH DATABASES\nQuite a variety of people are associated with the creation and use of databases.\nObviously, there are database implementors, who build DBMS software,\nand end users who wish to store and use data in a DBMS. Dat,abase imple-\nmentors work for vendors such as IBM or Oracle. End users come from a diverse\nand increasing number of fields. As data grows in complexity ant(volume, and\nis increasingly recognized as a major asset, the importance of maintaining it\nprofessionally in a DBMS is being widely accepted. Many end user.s simply use\napplications written by database application programmers (see below) and so\nrequire little technical knowledge about DBMS software. Of course, sophisti-\ncated users who make more extensive use of a DBMS, such as writing their own\nqueries, require a deeper understanding of its features.\nIn addition to end users and implementors, two other cla.'3ses of people are\nassociated with a DBMS: application programmer-s and database administrators.\nDatabase application programmers develop packages that facilitate data\naccess for end users, who are usually not computer professionals, using the\nhost or data languages and software tools that DBMS vendors provide. (Such\ntools include report writers, spreadsheets, statistical packages, and the like.)\nApplication programs should ideally access data through the external schema.\nIt is possible to write applications that access data at a lower level, but such\napplications would comprornise data independence.\n\nCHAPTEI~ 1\nA personal databa'3e is typically maintained by the individual who owns it and\nuses it. However, corporate or enterprise-wide databases are typically impor-\ntant enough and complex enough that the task of designing and maintaining the\ndatabase is entrusted to a professional, called the database administrator\n(DBA). The DBA is responsible for many critical tasks:\nIII\nDesign of the Conceptual and Physical Schemas: The DBA is re-\nsponsible for interacting with the users of the system to understand what\ndata is to be stored in the DBMS and how it is likely to be used. Based on\nthis knowledge, the DBA must design the conceptual schema (decide what\nrelations to store) and the physical schema (decide how to store them).\nThe DBA may also design widely used portions of the external schema, al-\nthough users probably augment this schema by creating additional views.\nIII\nSecurity and Authorization: The DBA is responsible for ensuring that\nunauthorized data access is not permitted. In general, not everyone should\nbe able to access all the data. In a relational DBMS, users can be granted\npermission to access only certain views and relations.\nFor example, al-\nthough you might allow students to find out course enrollments and who\nteaches a given course, you would not want students to see faculty salaries\nor each other's grade information.\nThe DBA can enforce this policy by\ngiving students permission to read only the Courseinfo view.\nIII\nData Availability and Recovery from Failures: The DBA must take\nsteps to ensure that if the system fails, users can continue to access as much\nof the uncorrupted data as possible. The DBA must also work to restore\nthe data to a consistent state. The DB.I\\!IS provides software support for\nthese functions, but the DBA is responsible for implementing procedures\nto back up the data periodically and maintain logs of system activity (to\nfacilitate recovery from a crash).\nl'il\nDatabase Tuning: Users' needs are likely to evolve with time. The DBA\nis responsible for modifying the database, in particular the conceptual and\nphysical schemas, to ensure adequate performance as requirements change.\n1.10\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\nvVhat are the main benefits of using a DBMS to manage data in applica-\ntions involving extensive data access? (Sections 1.1, 1.4)\nIII\nvVhen would you store data in a DBMS instead of in operating system files\nand vice-versa? (Section 1.3)\n\nOver-view of Database Systems\n•\nWhat is a data model? \\Vhat is the relational data model? What is data\nindependence and how does a DBNIS support it? (Section 1.5)\n•\nExplain the advantages of using a query language instead of custom pro-\ngrams to process data. (Section 1.6)\n•\nWhat is a transaction? \\Vhat guarantees does a DBMS offer with respect\nto transactions? (Section 1.7)\n•\nWhat are locks in a DBMS, and why are they used? What is write-ahead\nlogging, and why is it used? What is checkpointing and why is it used?\n(Section 1.7)\n•\nIdentify the main components in a DBMS and briefly explain what they\ndo. (Section 1.8)\n•\nExplain the different roles of database administrators, application program-\nmers, and end users of a database. Who needs to know the most about\ndatabase systems? (Section 1.9)\nEXERCISES\nExercise 1.1 Why would you choose a database system instead of simply storing data in\noperating system files? When would it make sense not to use a database system?\nExercise 1.2 What is logical data independence and why is it important?\nExercise 1.3 Explain the difference between logical and physical data independence.\nExercise 1.4 Explain the difference between external, internal, and conceptual schemas.\nHow are these different schema layers related to the concepts of logical and physical data\nindependence?\nExercise 1.5 What are the responsibilities of a DBA? If we assume that the DBA is never\ninterested in running his or her own queries, does the DBA still need to understand query\noptimization? Why?\nExercise 1.6 Scrooge McNugget wants to store information (names, addresses, descriptions\nof embarrassing moments, etc.) about the many ducks on his payroll. Not surprisingly, the\nvolume of data compels him to buy a database system. To save money, he wants to buy one\nwith the fewest possible features, and he plans to run it as a stand-alone application on his\nPC clone. Of course, Scrooge does not plan to share his list with anyone. Indicate which of\nthe following DBMS features Scrooge should pay for; in each case, also indicate why Scrooge\nshould (or should not) pay for that feature in the system he buys.\n1. A security facility.\n2. Concurrency control.\n3. Crash recovery.\n4. A view mechanism.",
          "pages": [
            56,
            57,
            58
          ],
          "relevance": {
            "score": 0.28,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Cardinality constraints are rules that define the number of relationships between entities in a database schema. They ensure data integrity and consistency by specifying how many instances of one entity can be associated with another.",
        "explanation": "In a relational database, cardinality constraints help maintain the accuracy and reliability of the data. These constraints specify the maximum and minimum number of related records that can exist between two tables. For example, in a university database, a student can enroll in multiple courses (many-to-many relationship), but each course can only have one instructor (one-to-one or one-to-many relationship). Understanding cardinality is crucial for designing efficient and effective databases.",
        "key_points": [
          "Cardinality constraints ensure data integrity by limiting the number of relationships between entities.",
          "They are essential for creating accurate and reliable database schemas.",
          "Common types include one-to-one, one-to-many, and many-to-many relationships."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Define a TABLE with a one-to-many relationship CREATE TABLE Students ( sid INT PRIMARY KEY, name VARCHAR(100) ); CREATE TABLE Enrolled ( sid INT, cid INT, grade CHAR(2), PRIMARY KEY (sid, cid), FOREIGN KEY (sid) REFERENCES Students(sid) );",
            "explanation": "This example demonstrates how to define a one-to-many relationship between students and courses. Each student can be enrolled in multiple courses, but each course has only one student.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Query to find all students enrolled in a specific course\nSELECT s.sid, s.name\nFROM Students s\nJOIN Enrolled e ON s.sid = e.sid\nWHERE e.cid = 'CS101';",
            "explanation": "This practical example shows how to use the one-to-many relationship to query data. It retrieves all students enrolled in a specific course."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to define foreign keys for relationships.",
            "incorrect_code": "-- Incorrect: No foreign key defined CREATE TABLE Enrolled ( sid INT, cid INT, grade CHAR(2) );",
            "correct_code": "-- Correct: Foreign key defined CREATE TABLE Enrolled ( sid INT, cid INT, grade CHAR(2), PRIMARY KEY (sid, cid), FOREIGN KEY (sid) REFERENCES Students(sid) );",
            "explanation": "Defining foreign keys ensures referential integrity and helps maintain the accuracy of the database."
          }
        ],
        "practice": {
          "question": "Design a database schema for a library system. Define tables for Books, Authors, and Borrowers with appropriate cardinality constraints.",
          "solution": "-- Solution: Define tables with correct cardinality constraints\nCREATE TABLE Authors (\n  aid INT PRIMARY KEY,\n  name VARCHAR(100)\n);\n\nCREATE TABLE Books (\n  bid INT PRIMARY KEY,\n  title VARCHAR(255),\n  author_id INT,\n  FOREIGN KEY (author_id) REFERENCES Authors(aid)\n);\n\nCREATE TABLE Borrowers (\n  bid INT PRIMARY KEY,\n  name VARCHAR(100)\n);\n\nCREATE TABLE Borrows (\n  bid INT,\n  book_id INT,\n  borrower_id INT,\n  borrow_date DATE,\n  return_date DATE,\n  PRIMARY KEY (bid, book_id),\n  FOREIGN KEY (book_id) REFERENCES Books(bid),\n  FOREIGN KEY (borrower_id) REFERENCES Borrowers(bid)\n);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER:l\nsuch a situation, the abstract view of the datet presented by the DBlVIS does\nnot match the application's needs and actually gets in the way. As an exam-\nple, relational databa.'3es do not s",
      "content_relevance": {
        "score": 0.35,
        "sql_score": 1.0,
        "concept_score": 0.5,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "relational-algebra": {
      "id": "relational-algebra",
      "title": "Relational Algebra",
      "definition": "Formal language for manipulating relations using operations like selection, projection, and join",
      "difficulty": "intermediate",
      "page_references": [
        75,
        76,
        77,
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89,
        90,
        91,
        92,
        93,
        94,
        95,
        96,
        97,
        98,
        99,
        100
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER 2\nMonitors /~\nI\n._.\n_.\n-\n-\n.-\n-\n-\n-\n-\n~\n-~\n-\n-\n-\n-\n-\n-\n-\n-\n-\n~\n-\n-\n-\n--\n-\n-\n-\n~\n-\n-\n-\n-\n--\n-- -;\nI\nI\n~:\nI\nDepartments\nSponsors\nI\nI\nI\nI\nI\n----------------------------------------\nI\n------~~~~~-------\nAggregation\nWhen should we use aggregation? Intuitively, we use it when we need to ex-\npress a relationship among relationships. But can we not express relationships\ninvolving other relationships without using aggregation? In our example, why\nnot make Sponsors a ternary relationship? The answer is that there are really\ntwo distinct relationships, Sponsors and Monitors, each possibly with attributes\nof its own. For instance, the Monitors relationship has an attribute 1tntil that\nrecords the date until when the employee is appointed as the sponsorship mon-\nitor. Compare this attribute with the attribute since of Sponsors, which is the\ndate when the sponsorship took effect. The use of aggregation versus a ternary\nrelationship may also be guided by certain integrity constraints, as explained\nin Section 2.5.4.\n2.5\nCONCEPTUAL DESIGN WITH THE ER MODEL\nDeveloping an ER diagram presents several choices, including the following:\n..\nShould a concept be modeled as an entity or an attribute?\n..\nShould a concept be modeled &'3 an entity or a relationship?\nII\n\"Vhat arc the relationship sets and their participating entity sets? Should\nwe use binary or ternary relationships?\nII\nShould we use aggregation?\n\nIntrod'lLct'ion to Database Design\n\\Ve now discuss the issues involved in making these choices.\n2.5.1\nEntity versus Attribute\n\\Vhile identifying the attributes of an entity set, it is sometimes not clear\nwhether a property should be modeled as an attribute or as an entity set (and\nrelated to the first entity set using a relationship set). For example, consider\nadding address information to the Employees entity set. One option is to use\nan attribute address.\nThis option is appropriate if we need to record only\none address per employee, and it suffices to think of an address as a string. An\nalternative is to create an entity set called Addresses and to record associations\nbetween employees and addresses using a relationship (say, Has_Address). This\nmore complex alternative is necessary in two situations:\n•\nWe have to record more than one address for an employee.\n•\nWe want to capture the structure of an address in our ER diagram. For\nexample, we might break down an address into city, state, country, and\nZip code, in addition to a string for street information. By representing an\naddress as an entity with these attributes, we can support queries such as\n\"Find all employees with an address in Madison, WI.\"\nFor another example of when to model a concept as an entity set rather than\nan attribute, consider the relationship set (called WorksJ:n4) shown in Figure\n2.14.\nThe \\Vorks_In4 Relationship Set\nIt differs from the \\Vorks_In relationship set of Figure 2.2 only in that it has\nattributes JTOtn and to, instead of since.\nIntuitively, it records the interval\nduring which an employee works for a department.\nNow suppose that it is\npossible for an employee to work in a given department over more than one\nperiod.\nThis possibility is ruled out by the ER diagram's semantics, because a rela-\ntionship is uniquely identified by the participating entities (recall from Section\n\nCHAPTER' 2\n2.3). The problem is that we want to record several values for the descriptive\nattributes for each instance of the vVorks-ln2 relationship.\n(This situation is\nanalogous to wanting to record several addresses for each employee.) vVe can\naddress this problem by introducing an entity set called, say, Duration, with\nattributes from and to, as shown in Figure 2.15.\n(~~-T:~~~)\nI\nEmployees\nI\nWorksJn4\nDepartments\nfrom\nto\nThe Works-ln4 Relationship Set\nIn some versions of the' ER model, attributes are allowed to take on sets as\nvalues. Given this feature, we could make Duration an attribute of Works_In,\nrather than an entity set; associated with each Works_In relationship, we would\nhave a set of intervals. This approach is perhaps more intuitive than model-\ning Duration as an entity set.\nNonetheless, when such set-valued attributes\nare translated into the relational model, which does not support set-valued\nattributes, the resulting relational schema is very similar to what we get by\nregarding Duration as an entity set.\n2.5.2\nEntity versus Relationship\nConsider the relationship set called Manages in Figure 2.6. Suppose that each\ndepartment manager is given a discretionary budget (dbudget) , as shown in\nEntity versus Relationship\n\nIntroduction to Database Design\nGiven a department, we know the manager, as well &'3 the manager's starting\ndate and budget for that department. This approach is natural if we t'l\"ssume\nthat a manager receives a separate discretionary budget for each department\nthat he or she manages.\nBut what if the discretionary budget is a sum that covers all departments\nmanaged by that employee?\nIn this case, each Manages2 relationship that\ninvolves a given employee will have the same value in the db1Ldget field, leading\nto redundant storage of the same information.\nAnother problem with this\ndesign is that it is misleading; it suggests that the budget is associated with\nthe relationship, when it is actually associated with the manager.\nWe can address these problems by introducing a new entity set called Managers\n(which can be placed below Employees in an ISA hierarchy, to show that every\nmanager is also an employee). The attributes since and dbudget now describe\na manager entity, as intended.\nAs a variation, while every manager has a\nbudget, each manager may have a different starting date (as manager) for each\ndepartment. In this case dbudget is an attribute of Managers, but since is an\nattribute of the relationship set between managers and departments.\nThe imprecise nature of ER modeling can thus make it difficult to recognize\nunderlying entities, and we might associate attributes with relationships rather\nthan the appropriate entities.\nIn general, such mistakes lead to redundant\nstorage of the same information and can cause many problems.\nWe discuss\nredundancy and its attendant problems in Chapter 19, and present a technique\ncalled normalization to eliminate redundancies from tables.\n2.5.3\nBinary versus Ternary Relationships\nConsider the ER diagram shown in Figure 2.17. It models a situation in which\nan employee can own several policies, each policy can be owned by several\nemployees, and each dependent can be covered by several policies.\nSuppose that we have the following additional requirements:\nIII\nA policy cannot be owned jointly by two or more employees.\n11II\nEvery policy must be owned by some employee.\nlIII\nDependents is a weak entity set, and each dependent entity is uniquely\nidentified by taking pname in conjunction with the policyid of a policy\nentity (which, intuitively, covers the given dependent).\nThe first requirement suggests that we impose a key constraint on Policies with\nrespect to Covers, but this constraint has the unintended side effect that a\n\nC~~C~T~\n~I\nI\nEmployees\nlot\nCovers\nPolicies as an Entity Set\nCHAPTERf 2\npolicy can cover only one dependent. The second requirement suggests that we\nimpose a total participation constraint on Policies. This solution is acceptable\nif each policy covers at least one dependent. The third requirement forces us\nto introduce an identifying relationship that is binary (in our version of ER\ndiagrams, although there are versions in which this is not the case).\nEven ignoring the third requirement, the best way to model this situation is to\nuse two binary relationships, as shown in Figure 2.18.\nPolicy Revisited\n\nIntTod'U(~t\"ion to Database Des'ign\n45.\nThis example really has two relationships involving Policies, and our attempt\nto use a single ternary relationship (Figure 2.17) is inappropriate. There are\nsituations, however, \"vhere a relationship inherently a.'3sociates more than two\nentities. vVe have seen such an example in Figures 2,4 and 2.15.\nAs a typical example of a ternary relationship, consider entity sets Parts, Sup-\npliers, and Departments, and a relationship set Contracts (with descriptive\nattribute qty) that involves all of them. A contract specifies that a supplier will\nsupply (some quantity of) a part to a department.\nThis relationship cannot\nbe adequately captured by a collection of binary relationships (without the use\nof aggregation). With binary relationships, we can denote that a supplier 'can\nsupply' certain parts, that a department 'needs' some parts, or that a depart-\nment 'deals with' a certain supplier.\nNo combination of these relationships\nexpresses the meaning of a contract adequately, for at least two reasons:\n•\nThe facts that supplier S can supply part P, that department D needs part\nP, and that D will buy from S do not necessarily imply that department D\nindeed buys part P from supplier S!\n•\nWe cannot represent the qty attribute of a contract cleanly.\n2.5.4\nAggregation versus Ternary Relationships\nAs we noted in Section 2.4.5, the choice between using aggregation or a ternary\nrelationship is mainly determined by the existence of a relationship that relates\na relationship set to an entity set (or second relationship set). The choice may\nalso be guided by certain integrity constraints that we want to express.\nFor\nexample, consider the ER diagram shown in Figure 2.13. According to this dia-\ngram, a project can be sponsored by any number of departments, a department\ncan sponsor one or more projects, and each sponsorship is monitored by one\nor more employees. If we don't need to record the unt-il attribute of Monitors,\nthen we might reasonably use a ternal'Y relationship, say, Sponsors2, as shown\nin Figure 2.19.\nConsider the constraint that each sponsorship (of a project by a department)\nbe monitored by at most one employee.\nVVe cannot express this constraint\nin terms of the Sponsors2 relationship set. On the other hand, we can easily\nexpress the cOnstraint by drawing an arrow from the aggregated relationship\nSponsors to the relationship Monitors in Figure 2.13.\nThus, the presence of\nsuch a constraint serves &s another reason for using aggregation rather than a\nternary relationship set.",
          "pages": [
            75,
            76,
            77,
            78,
            79,
            80
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTERt2\nEmployees\nstarted_on\nG:(:P\nProjects\nSponsors2\ndname\nG:(?\n>-------11\nDepartment<\nUsing a Ternary Relationship instead of Aggregation\n2.6\nCONCEPTUAL DESIGN FOR LARGE ENTERPRISES\nWe have thus far concentrated on the constructs available in the ER model\nfor describing various application concepts and relationships. The process of\nconceptual design consists of more than just describing small fragments of the\napplication in terms of ER diagrams. For a large enterprise, the design may re-\nquire the efforts of more than one designer and span data and application code\nused by a number of user groups.\nUsing a high-level, semantic data model,\nsuch as ER diagrams, for conceptual design in such an environment offers the\nadditional advantage that the high-level design can be diagrammatically rep-\nresented and easily understood by the many people who must provide input to\nthe design process.\nAn important aspect of the design process is the methodology used to structure\nthe development of the overall design and ensure that the design takes into\naccount all user requirements and is consistent. The usual approach is that the\nrequirements of various user groups are considered, any conflicting requirements\nare somehow resolved, and a single set of global requirements is generated at\nthe end of the.requirements analysis phase. Generating a single set of global\nrequirements is a difficult task, but it allows the conceptual design phase to\nproceed with the development of a logical schema that spans all the data and\napplications throughout the enterprise.\nAn alternative approach is to develop separate conceptual scherna.'-l for different\nuser groups and then integTate these conceptual schemas. To integrate\nmulti~\n\nIntmduction to Database De.s'ign\npIe conceptual schemas, we must €'Btablish correspondences between entities,\nrelationships, and attributes, and we must resolve numerous kinds of conflicts\n(e.g., naming conflicts, domain mismatches, differences in measurement units).\nThis task is difficult in its own right. In some situations, schema integration\ncannot be avoided; for example, when one organization merges with another,\nexisting databases may have to be integrated. Schema integration is also in-\ncreasing in importance as users demand access to heterogeneous data sources,\noften maintained by different organizations.\n2.7\nTHE UNIFIED MODELING LANGUAGE\nThere are many approaches to end-to-end software system design, covering all\nthe steps from identifying the business requirements to the final specifications\nfor a complete application, including workflow, user interfaces, and many as-\npects of software systems that go well beyond databases and the data stored in\nthem. In this section, we briefly discuss an approach that is becoming popular,\ncalled the unified modeling language (UML) approach.\nUML, like the ER model, has the attractive feature that its constructs can be\ndrawn as diagrams. It encompasses a broader spectrum of the software design\nprocess than the ER model:\nIII\nBusiness Modeling: In this phase, the goal is to describe the business\nprocesses involved in the software application being developed.\nIII\nSystem Modeling: The understanding of business processes is used to\nidentify the requirements for the software application.\nOne part of the\nrequirements is the database requirements.\nIII\nConceptual Database Modeling: This step corresponds to the creation\nof the ER design for the database. For this purpose, UML provides many\nconstructs that parallel the ER constructs.\nIII\nPhysical Database Modeling: Ul\\IL also provides pictorial represen-\ntations for physical database design choices, such &'3 the creation of table\nspaces and indexes. (\\\\1e discuss physical databa\"se design in later chapters,\nbut not the corresponding UML constructs.)\nIII\nHardware System Modeling: UML diagrams can be used to describe\nthe hardware configuration used for the application.\nTh(~re are many kinds of diagrams in UML. Use case diagrams describe the\nactions performed by the system in response to user requests, and the people\ninvolved in these actions.\nThese diagrams specify the external functionality\n<-hat the system is expected to support.\n\nCHAPTER;2\nActivity diagrams 8hmv the flow of actions in a business process. Statechart\ndiagrams describe dynamic interactions between system objects.\nThese dia-\ngrams, used in busine.c;s and systern modeling, describe how the external func-\ntionality is to be implemented, consistent with the business rules and processes\nof the enterprise.\nClass diagrams are similar to ER diagrams, although they are more general\nin that they are intended to model application entities (intuitively, important\nprogram components) and their logical relationships in addition to data entities\nand their relationships.\nBoth entity sets and relationship sets can be represented as classes in UML,\ntogether with key constraints, weak entities, and class hierarchies. The term\nrelationship is used slightly differently in UML, and UML's relationships are\nbinary.\nThis sometimes leads to confusion over whether relationship sets in\nan ER diagram involving three or more entity sets can be directly represented\nin UML. The confusion disappears once we understand that all relationship\nsets (in the ER sense) are represented as classes in UML; the binary UML\n'relationships' are essentially just the links shown in ER diagrams between\nentity sets and relationship sets.\nRelationship sets with key constraints are usually omitted from UML diagrams,\nand the relationship is indicated by directly linking the entity sets involved.\nFor example, consider Figure 2.6. A UML representation of this ER diagram\nwould have a class for Employees, a class for Departments, and the relationship\nManages is shown by linking these two classes. The link can be labeled with\na name and cardinality information to show that a department can have only\none manager.\nAs we will see in Chapter 3, ER diagrams are translated into the relational\nmodel by mapping each entity set into a table and each relationship set into\na table. FUrther, as we will see in Section 3.5.3, the table corresponding to a\none-to-many relationship set is typically omitted by including some additional\ninformation about the relationship in the table for one of the entity sets in-\nvolved. Thus, UML class diagrams correspond closely to the tables created by\nmapping an ER diagram.\nIndeed, every class in a U1I1L class diagram is mapped into a table in the cor-\nresponding U]\\'1L database diagram.\nUML's database diagrams show how\nclasses are represented in the database and contain additional details about\nthe structure of the database such as integrity constraints and indexes. Links\n(UML's 'relationships') between UML classes lead to various integrity con-\nstraints between the corresponding tables.\nMany details specific to the re-\nlational model (e.g., views, fOTe'ign keys, null-allowed fields) and that reflect\n\nIntroduction to Dutaba8C Design\nphysical design choices (e.g., indexed fields) can be modeled ill UN[L database\ndiagrams.\nUML's component diagrams describe storage aspects of the database, such\nas tablespaces and database pa,titions) , as well as interfaces to applications\nthat access the database. Finally, deployment diagrams show the hardware\naspects of the system.\nOur objective in this book is to concentrate on the data stored in a database\nand the related design issues.\nTo this end, we deliberately take a simplified\nview of the other steps involved in software design and development. Beyond\nthe specific discussion of UlIIL, the material in this section is intended to place\nthe design issues that we cover within the context of the larger software design\nprocess. \\Ve hope that this will assist readers interested in a more comprehen-\nsive discussion of software design to complement our discussion by referring to\nother material on their preferred approach to overall system design.\n2.8\nCASE STUDY: THE INTERNET SHOP\nWe now introduce an illustrative, 'cradle-to-grave' design case study that we\nuse as a running example throughout this book. DBDudes Inc., a well-known\ndatabase consulting firm, has been called in to help Barns and Nobble (B&N)\nwith its database design and implementation. B&N is a large bookstore special-\nizing in books on horse racing, and it has decided to go online. DBDudes first\nverifies that B&N is willing and able to pay its steep fees and then schedules a\nlunch meeting--billed to B&N, naturally~to do requirements analysis.\n2.8.1\nRequirements Analysis\nThe owner of B&N, unlike many people who need a database, has thought\nextensively about what he wants and offers a concise summary:\n\"I would like my customers to be able to browse my catalog of books and\nplace orders over the Internet. Currently, I take orders over the phone. I have\nmostly corporate customers who call me and give me the ISBN number of a\nbook and a quantity; they often pay by credit card. I then prepare a shipment\nthat contains the books they ordered. If I don't have enough copies in stock,\nI order additional copies and delay the shipment until the new copies arrive;\nI want to ship a customer's entire order together. My catalog includes all the\nbooks I sell. For each book, the catalog contains its ISBN number, title, author,\npurcha.se price, sales price, and the year the book was published. Most of my\nsustomers are regulars, and I have records with their names and addresses.\n\nyear-published\nOrders\nER Diagram of the Initial Design\nCHAPTER¢2\nNew customers have to call me first and establish an account before they can\nuse my website.\nOn my new website, customers should first identify themselves by their unique\ncustomer identification number. Then they should be able to browse my catalog\nand to place orders online.\"\nDBDudes's consultants are a little surprised by how quickly the requirements\nphase is completed--it usually takes weeks of discussions (and many lunches\nand dinners) to get this done~~but return to their offices to analyze this infor-\nmation.\n2.8.2\nConceptual Design\nIn the conceptual design step, DBDudes develops a high level description of\nthe data in terms of the ER model.\nThe initial design is shown in Figure\n2.20. Books and customers are modeled as entities and related through orders\nthat customers place.\nOrders is a relationship set connecting the Books and\nCustomers entity sets.\nFor each order, the following attributes are stored:\nquantity, order date, and ship date. As soon as an order is shipped, the ship\ndate is set; until then the ship date is set to null, indicating that this order has\nnot been shipped yet.\nDBDudes has an internal design review at this point, and several questions are\nraised.\nTo protect their identities, we will refer to the design team leader as\nDude 1 and the design reviewer as Dude 2.\nDude 2: \\\\That if a. customer places two orders for the same book in one day?\nDude 1: The first order is ha,ndlecl by crea.ting a new Orders relationship and\n\nIntroduct'ion to Database Design\n51,\nthe second order is handled by updating the value of the quantity attribute in\nthis relationship.\nDude 2: \\\\That if a customer places two orders for different books in one day?\nDude 1: No problem. Each instance of the Orders relationship set relates the\ncustomer to a different book.\nDude 2: Ah, but what if a customer places two orders for the same book on\ndifferent days?\nDude 1: \\Ve can use the attribute order date of the orders relationship to\ndistinguish the two orders.\nDude 2: Oh no you can't. The attributes of Customers and Books must jointly\ncontain a key for Orders.\nSo this design does not allow a customer to place\norders for the same book on different days.\nDude 1: Yikes, you're right. Oh well, B&N probably won't care; we'll see.\nDBDudes decides to proceed with the next phase, logical database design; we\nrejoin them in Section 3.8.\n2.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nName the main steps in database design. What is the goal of each step?\nIn which step is the ER model mainly used? (Section 2.1)\n•\nDefine these terms: entity, entity set, attribute, key. (Section 2.2)\n•\nDefine these terms:\nrelationship, relationship set, descriptive attributes.\n(Section 2.3)\n•\nDefine the following kinds of constraints, and give an example of each: key\nconstraint, participation constraint. What is a weak entity? What are class\nhierarchies'? What is aggregation?\nGive an example scenario motivating\nthe use of each of these ER model design constructs. (Section 2.4)\n•\nWhat guidelines would you use for each of these choices when doing ER\ndesign: \\Vhether to use an attribute or an entity set, an entity or a relation-\nship set, a binary or ternary relationship, or aggregation. (Section 2.5)\nIII'l\nWhy is designing a database for a large enterprise especially hard? (Sec-\ntion 2.6)\n•\nWhat is UML? How does databa\"se design fit into the overall design of\na data-intensive software system? How is UML related to ER diagrams?\n(Section 2.7)\n\nEXERCISES\nCHAPTERJ2\nExercise 2.1 Explain the following terms briefly:\nattribute, domain, entity, relationship,.\nentity set, relationship set, one-to-many relat'ionship, many-to-many 1'elationship. pan·tcipa-\ntion constmint. overlap constraint, covering constraint, weak entity set,. aggregat'ion, and role\nindicator.\nExercise 2.2 A university database contains information about professors (identified by so-\ncial security number, or SSN) and courses (identified by courseid). Professors teach courses;\neach of the following situations concerns the Teaches relationship set.\nFor each situation,\ndraw an ER diagram that describes it (assuming no further constraints hold).\n1. Professors can teach the same course in several semesters, and each offering must be\nrecorded.\n2. Professors can teach the same course in several semesters, and only the most recent\nsuch offering needs to be recorded.\n(Assume this condition applies in all subsequent\nquestions.)\n3. Every professor must teach some course.\n4. Every professor teaches exactly one course (no more, no less).\n5. Every professor teaches exactly one course (no more, no less), and every course must be\ntaught by some professor.\n6. Now suppose that certain courses can be taught by a team of professors jointly, but it\nis possible that no one professor in a team can teach the course. Model this situation,\nintroducing additional entity sets and relationship sets if necessary.\nExercise 2.3 Consider the following information about a university database:\nII\nProfessors have an SSN, a name, an age, a rank, and a research specialty.\nII\nProjects have a project number, a sponsor name (e.g., NSF), a starting date, an ending\ndate, and a budget.\nII\nGraduate students have an SSN, a name, an age, and a degree program (e.g., M.S. or\nPh.D.).\nII\nEach project is managed by one professor (known as the project's principal investigator).\nII\nEach project is worked on by one or more professors (known as the project's co-investigators).\nIII\nProfessors can manage and/or work on multiple projects.\nII\nEach project is worked on by one or more graduate students (known as the project's\nresearch assistants).\nII\nWhen graduate students >'lark on a project, a professor must supervise their work on the\nproject. Graduate students can work on multiple projects, in which case they will have\na (potentially different) supervisor for each one.\nII\nDepartments have a department number, a department name, and a main office.\nII\nDepartments have a professor (known as the chairman) who runs the department.\nII\nProfessors work in one or more departments, and for each department that they work\nin, a time percentage is associated with their job.\nII\nGraduate students have one major department in which they are working OIl their degree.\n\nIntroduction to Database Design\n~3\nIII\nEach graduate student has another, more senior graduate student (known as a student\nadvisor) who advises him or her OIl what courses to take.\nDesign and dra\\v an ER diagram that captures the information about the university. Use only\nthe basic ER model here; that is, entities, relationships, and attributes. Be sure to indicate\nany key and participation constraints.\nExercise 2.4 A company database needs to store information about employees (identified\nby ssn, with salary and phone as attributes), departments (identified by dna, with dname and\nbudget as attributes), and children of employees (with name and age as attributes). Employees\nwork in departments; each department is managed by an employee; a child must be identified\nuniquely by name when the parent (who is an employee; assume that only one parent works\nfor the company) is known.\nWe are not interested in information about a child once the\nparent leaves the company.\nDraw an ER diagram that captures this information.\nExercise 2.5 Notown Records has decided to store information about musicians who perform\non its albums (as well as other company data) in a database. The company has wisely chosen\nto hire you as a database designer (at your usual consulting fee of $2500jday).\nIII\nEach musician that records at Notown has an SSN, a name, an address, and a phone\nnumber. Poorly paid musicians often share the same address, and no address has more\nthan one phone.\nIII\nEach instrument used in songs recorded at Notown has a name (e.g., guitar, synthesizer,\nflute) and a musical key (e.g., C, B-flat, E-flat).\nIII\nEach album recorded on the Notown label has a title, a copyright date, a format (e.g.,\nCD or MC), and an album identifier.\nIII\nEach song recorded at Notown has a title and an author.\nIII\nEach musician may play several instruments, and a given instrument may be played by\nseveral musicians.\nIII\nEach album has a number of songs on it, but no song may appear on more than one\nalbum.\nIII\nEach song is performed by one or more musicians, and a musician may perform a number\nof songs.\nIII\nEach album has exactly one musician who acts as its producer. A musician may produce\nseveral albums, of course.\nDesign' a conceptual schema for Notown and draw an ER diagram for your schema.\nThe\npreceding information describes the situation that the Notown database must model. Be sure\nto indicate all key and cardinality constraints and any assumptions you make. Identify any\nconstraints you are unable to capture in the ER diagram and briefly explain why you could\nnot express them.\nExercise 2.6 Computer Sciences Department frequent fliers have been complaining to Dane\nCounty Airport officials about the poor organization at the airport. As a result, the officials\ndecided that all information related to the airport should be organized using a DBMS, and\nyou have been hired to design the database.\nYour first task is to organize the information\nabout all the airplanes stationed and maintainecl at the airport. The relevant information is\nas follows:\n\n•\nEvery airplane has a registration number, and each airplane is of a specific model.\n•\nThe airport accommodates a number of airplane models, and each model is identified by\na model number (e.g., DC-lO) and has a capacity and a weight.\n•\nA number of technicians work at the airport. You need to store the name, SSN, address,\nphone number, and salary of each technician.\n•\nEach technician is an expert on one or more plane model(s), and his or her expertise may\noverlap with that of other technicians. This information about technicians must also be\nrecorded.\n•\nTraffic controllers must have an annual medical examination. For each traffic controller,\nyou must store the date of the most recent exam.\n•\nAll airport employees (including technicians) belong to a union.\nYou must store the\nunion membership number of each employee.\nYou can assume that each employee is\nuniquely identified by a social security number.\n•\nThe airport has a number of tests that are used periodically to ensure that airplanes are\nstill airworthy. Each test has a Federal Aviation Administration (FAA) test number, a\nname, and a maximum possible score.\n•\nThe FAA requires the airport to keep track of each time a given airplane is tested by a\ngiven technician using a given test. For each testing event, the information needed is the\ndate, the number of hours the technician spent doing the test, and the score the airplane\nreceived on the test.\n1. Draw an ER diagram for the airport database. Be sure to indicate the various attributes\nof each entity and relationship set; also specify the key and participation constraints for\neach relationship set. Specify any necessary overlap and covering constraints a.s well (in\nEnglish).\n2. The FAA passes a regulation that tests on a plane must be conducted by a technician\nwho is an expert on that model.\nHow would you express this constraint in the ER\ndiagram? If you cannot express it, explain briefly.\nExercise 2.7 The Prescriptions-R-X chain of pharmacies ha.s offered to give you a free life-\ntime supply of medicine if you design its database. Given the rising cost of health care, you\nagree. Here's the information that you gather:\n11II\nPatients are identified by an SSN, and their names, addresses, and ages must be recorded.\n11II\nDoctors are identified by an SSN. For each doctor, the name, specialty, and years of\nexperience must be recorded.\nIII\nEach pharmaceutical company is identified by name and has a phone number.\nIII\nFor each drug, the trade name and formula must be recorded.\nEach drug is sold by\na given pharmaceutical company, and the trade name identifies a drug uniquely from\namong the pJ;oducts of that company. If a pharmaceutical company is deleted, you need\nnot keep track of its products any longer.\nIII\nEach pharmacy has a name, address, and phone number.\nIII\nEvery patient has a primary physician. Every doctor has at least one patient.\n•\nEach pharmacy sells several drugs and has a price for each.\nA drug could be sold at\nseveral pharmacies, and the price could vary from one pharmacy to another.\n\nIntToduction to DatabaBe Design\n•\nDoctors prescribe drugs for patients.\nA doctor could prescribe one or more drugs for\nseveral patients, and a patient could obtain prescriptions from several doctors.\nEach\nprescription has a date and a quantity associated with it.\nYou can assume that, if a\ndoctor prescribes the same drug for the same patient more than once, only the last such\nprescription needs to be stored.\n•\nPharmaceutical companies have long-term contracts with pharmacies. A pharmaceutical\ncompany can contract with several pharmacies, and a pharmacy can contract with several\npharmaceutical companies. For each contract, you have to store a start date, an end date,\nand the text of the contract.\n•\nPharmacies appoint a supervisor for each contract. There must always be a supervisor\nfor each contract, but the contract supervisor can change over the lifetime of the contract.\n1. Draw an ER diagram that captures the preceding information. Identify any constraints\nnot captured by the ER diagram.\n2. How would your design change if each drug must be sold at a fixed price by all pharma-\ncies?\n3. How would your design change if the design requirements change as follows: If a doctor\nprescribes the same drug for the same patient more than once, several such prescriptions\nmay have to be stored.\nExercise 2.8 Although you always wanted to be an artist, you ended up being an expert on\ndatabases because you love to cook data and you somehow confused database with data baste.\nYour old love is still there, however, so you set up a database company, ArtBase, that builds a\nproduct for art galleries. The core of this product is a database with a schema that captures\nall the information that galleries need to maintain. Galleries keep information about artists,\ntheir names (which are unique), birthplaces, age, and style of art. For each piece of artwork,\nthe artist, the year it was made, its unique title, its type of art (e.g., painting, lithograph,\nsculpture, photograph), and its price must be stored. Pieces of artwork are also classified into\ngroups of various kinds, for example, portraits, still lifes, works by Picasso, or works of the\n19th century; a given piece may belong to more than one group. Each group is identified by\na name (like those just given) that describes the group. Finally, galleries keep information\nabout customers. For each customer, galleries keep that person's unique name, address, total\namount of dollars spent in the gallery (very important!), and the artists and groups of art\nthat the customer tends to like.\nDraw the ER diagram for the database.\nExercise 2.9 Answer the following questions.\n•\nExplain the following terms briefly: UML, use case diagrams, statechart diagrams, class\ndiagrams, database diagrams, component diagrams, and deployment diagrams.\n•\nExplain the relationship between ER diagrams and UML.\nBffiLIOGRAPHIC NOTES\nSeveral books provide a good treatment of conceptual design; these include [63J (which also\ncontains a survey of commercial database design tools) and [730J.\nThe ER model wa..<; proposed by Chen [172], and extensions have been proposed in a number\nof subsequent papers. Generalization and aggregation were introduced in [693]. [390, 589]\n\nCHAPTER ,;2\ncontain good surveys of semantic data models. Dynamic and temporal aspects of semantic\ndata models are discussed in [749].\n[731] discusses a design methodology based on developing an ER diagram and then translating\nit to the relational model. Markowitz considers referential integrity in the context of ER to\nrelational mapping and discusses the support provided in some commercial systems (a..<; of\nthat date) in [513, 514].\nThe entity-relationship conference proceedings contain numerous papers on conceptual design,\nwith an emphasis on the ER model; for example, [698].\nThe OMG home page (www. omg. org) contains the specification for UML and related modeling\nstandards.\nNumerous good books discuss UML; for example [105, 278, 640] and there is a\nyearly conference dedicated to the advancement of UML, the International Conference on the\nUnified Modeling Language.\nView integration is discussed in several papers, including [97, 139, 184, 244, 535, 551, 550,\n685, 697, 748]. [64] is a survey of several integration approaches.",
          "pages": [
            81,
            82,
            83,
            84,
            85,
            86,
            87,
            88,
            89,
            90,
            91
          ],
          "relevance": {
            "score": 0.6,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "THE RELATIONAL MODEL\n....\nHow is data represented in the relational model?\n....\nWhat integrity constraints can be expressed?\n....\nHow can data be created and modified?\n....\nHow can data be manipulated and queried?\n....\nHow can we create, modify, and query tables using SQL?\n....\nHow do we obtain a relational database design from an ER diagram?\n....\nWhat are views and why are they used?\n..\nKey concepts:\nrelation, schema, instance, tuple, field, domain,\ndegree,\ncardinality;\nSQL DDL,\nCREATE TABLE,\nINSERT, DELETE,\nUPDATE; integrity constraints, domain constraints, key constraints,\nPRIMARY KEY, UNIQUE, foreign key constraints, FOREIGN KEY; refer-\nential integrity maintenance, deferred and immediate constraints; re-\nlational queries; logical database design, translating ER diagrams to\nrelations, expressing ER constraints using SQL; views, views and log:-\nical independence, security; creating views in SQL, updating views,\nquerying views, dropping views\nTABLE: An arrangement of words, numbers, or signs, or combinations of them,\n&s in parallel columns, to exhibit a set of facts or relations in a definite, compact,\nand comprehensive form; a synopsis or scheme.\n-----vVebster's Dictionary of the English Language\nCodd proposed the relational data model in 1970. At that time, most databa,,'Se\nsystems were based on one of two older data models (the hierarchical model\n\nCHAPTER ~\nSQL. Originally developed as the query language of the pioneering\nSystem-R relational DBl\\1S at IBIYl, structured query language (SQL)\nhas become the most widely used language for creating, manipulating,\nand querying relational DBMSs. Since many vendors offer SQL products,\nthere IS a need for a standard that defines \\official SQL.' The existence of\na standard allows users to measure a given vendor's version of SQL for\ncompleteness. It also allows users to distinguish SQLfeatures specific to\none product from those that are standard; an application that relies on\nnonstandard features is less portable.\nThe first SQL standard was developed in 1986 by the American National\nStandards Institute (ANSI) and was called SQL-86. There was a minor\nrevision in 1989 called SQL-89 and a major revision in 1992 called SQL-\n92.\nThe International Standards Organization (ISO) collaborated with\nANSI to develop SQL-92. Most commercial DBMSs currently support (the\ncore subset of) SQL-92 and are working to support the recently adopted\nSQL:1999 version of the standard, a major extension of SQL-92.\nOur\ncoverage of SQL is based on SQL:1999, but is applicable to SQL-92 as\nwell; features unique to SQL:1999 are explicitly noted.\nand the network model); the relational model revolutionized the database field\nand largely supplanted these earlier models.\nPrototype relational databa.'3e\nmanagement systems were developed in pioneering research projects at IBM\nand DC-Berkeley by the mid-197Gs, and several vendors were offering relational\ndatabase products shortly thereafter.\nToday, the relational model is by far\nthe dominant data model and the foundation for the leading DBMS products,\nincluding IBM's DB2 family, Informix, Oracle, Sybase, Microsoft's Access and\nSQLServer, FoxBase, and Paradox. Relational database systems are ubiquitous\nin the marketplace and represent a multibillion dollar industry.\nThe relational model is very simple and elegant: a database is a collection of\none or more relations, where each relation is a table with rows and columns.\nThis simple tabular representation enables even novice users to understand the\ncontents of a database, and it permits the use of simple, high-level languages\nto query the data. The major advantages of the relational model over the older\ndata models are its simple data representation and the ease with which even\ncomplex queries can be expressed.\n\\Vhile we concentrate on the underlying concepts, we also introduce the Data\nDefinition Language (DDL) features of SQL, the standard language for\ncreating, manipulating, and querying data in a relational DBMS. This allows\nus to ground the discussion firmly in terms of real databa.se systems.\n\nThe Relational 1\\1odd\nvVe discuss the concept of a relation in Section\n~t1 and show how to create\nrelations using the SQL language. An important component of a data model is\nthe set of constructs it provides for specifying conditions that must be satisfied\nby the data.\nSuch conditions, called 'integrity constraints (lGs), enable the\nDBIviS to reject operations that might corrupt the data. We present integrity\nconstraints in the relational model in Section 3.2, along with a discussion of\nSQL support for les. \\Ve discuss how a DBMS enforces integrity constraints\nin Section 3.3.\nIn Section 3.4, we turn to the mechanism for accessing and retrieving data\nfrom the database, query languages, and introduce the querying features of\nSQL, which we examine in greater detail in a later chapter.\nWe then discuss converting an ER diagram into a relational database schema\nin Section 3.5. We introduce views, or tables defined using queries, in Section\n3.6. Views can be used to define the external schema for a database and thus\nprovide the support for logical data independence in the relational model. In\nFinally, in Section 3.8 we extend our design case study, the Internet shop in-\ntroduced in Section 2.8, by showing how the ER diagram for its conceptual\nschema can be mapped to the relational model, and how the use of views can\nhelp in this design.\n3.1\nINTRODUCTION TO THE RELATIONAL MODEL\nThe main construct for representing data in the relational model is a relation.\nA relation consists of a relation schema and a relation instance.\nThe\nrelation instance is a table, and the relation schema describes the column heads\nfor the table.\nWe first describe the relation schema and then the relation\ninstance. The schema specifies the relation's name, the name of each field (or\ncolumn, or attribute), and the domain of each field. A domain is referred to\nin a relation schema by the domain name and has a set of associated values.\n\\Ve use the example of student information in a university database from Chap-\nter 1 to illustrate the parts of a relation schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nThis says, for instance, that the field named sid has a domain named string.\nThe set of values associated with domain string is the set of all character\nstrings.\n\nCHAPTER 3\nWe now turn to the instances of a relation. An instance of a relation is a set\nof tuples, also called records, in which each tuple has the same number of\nfields as the relation schema. A relation instance can be thought of as a table\nin which each tuple is a row, and all rows have the same number of fields. (The\nterm relation instance is often abbreviated to just relation, when there is no\nconfusion with other aspects of a relation such as its schema.)\nAn instance of the Students relation appears in Figure 3.1.\nThe instance 81\nField names\nTUPLES\n(RECORDS,\nROWS)\nFIELDS (ATTRIBUTES, COLUMNS)\n-~\nname\nI---/o'-gz-'n--\nDave\ndave@cs\n3.3\nJones\njones@cs\n3.4\nSmith\nsmith@ee\n3.2\nSmith\nsmith@math\n3.8\n\"'\\ 53831\nMadayan\nmadayan@music\n1.8\nGuldu\nguldu@music\n2.0\nAn Instance 81 of the Students Relation\ncontains six tuples and has, as we expect from the schema, five fields. Note that\nno two rows are identical. This is a requirement of the relational model-each\nrelation is defined to be a set of unique tuples or rows.\nIn practice, commercial systems allow tables to have duplicate rows, but we\nassume that a relation is indeed a set of tuples unless otherwise noted. The\norder in which the rows are listed is not important. Figure 3.2 shows the same\nrelation instance. If the fields are named, as in our schema definitions and\nI s'id\nI name\n[.login\nMadayan\nmadayan@music\n1.8\nGuldu\ngllldll@music\n2.0\nSmith\nsmith@ee\n3.2\nSmith\nsmith@math\n3.8\nJOI;es\njones@cs\n3.4\nDave\ndave@cs\n3.3\nAn Alternative Representation of Instance 81 of Students\nfigures depicting relation instances, the order of fields does not matter either.\nHowever, an alternative convention is to list fields in a specific order and refer\n\nThe Relat'ional lvfodel\n61,\nto a field by its position.\nThus, s'id is field 1 of Students, login is field\n:~,\nand so on. If this convention is used, the order of fields is significant. Most\ndatabase systems use a combination of these conventions. For example, in SQL,\nthe named fields convention is used in statements that retrieve tuples and the\nordered fields convention is commonly used when inserting tuples.\nA relation schema specifies the domain of each field or column in the relation\ninstance.\nThese domain constraints in the schema specify an important\ncondition that we want each instance of the relation to satisfy: The values\nthat appear in a column must be drawn from the domain associated with that\ncolumn.\nThus, the domain of a field is essentially the type of that field, in\nprogramming language terms, and restricts the values that can appear in the\nfield.\nMore formally, let R(fI:Dl, ..., In:Dn) be a relation schema, and for each Ii,\n1 :::; i :::; n, let Dami be the set of values associated with the domain named Di.\n.An instance of R that satisfies the domain constraints in the schema is a set of\ntuples with n fields:\n{ (fI : dl ,\n,In: dn)\nI dl E Daml' ... ,dn E Damn}\nThe angular brackets (\n) identify the fields of a tuple. Using this notation,\nthe first Students tuple shown in Figure 3.1 is written as (sid: 50000, name:\nDave, login: dave@cs, age: 19, gpa: 3.3). The curly brackets {...} denote a set\n(of tuples, in this definition). The vertical bar I should be read 'such that,' the\nsymbol E should be read 'in,' and the expression to the right of the vertical\nbar is a condition that must be satisfied by the field values of each tuple in the\nset. Therefore, an instance of R is defined as a set of tuples. The fields of each\ntuple must correspond to the fields in the relation schema.\nDomain constraints are so fundamental in the relational model that we hence-\nforth consider only relation instances that satisfy them; therefore, relation\ninstance means relation instance that satisfies the domain constraints in the\nrelation schema.\nThe degree, also called arity, of a relation is the number of fields. The car-\ndinality of a relation instance is the number of tuples in it. In Figure 3.1, the\ndegree of the relation (the number of columns) is five, and the cardinality of\nthis instance is six.\nA relational database is a collection of relations with distinct relation names.\nThe relational database schema is the collection of schemas for the relations\nin the database. 'For example, in Chapter 1, we discllssed a university database\nwith relations called Students, Faculty, Courses, Rooms, Enrolled, Teaches,\nand Meets~In. An instance of a relational databa..'3e is a collection of relation",
          "pages": [
            92,
            93,
            94,
            95,
            96
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Relational Algebra is a formal system for manipulating relations using operations like selection, projection, union, and join. It helps database designers understand how to construct queries that retrieve specific data from databases.",
        "explanation": "Relational Algebra provides a powerful yet simple way to express complex database queries. It uses a set of operations on relations (tables) to derive new relations. The key operations include:\n\n1. **Selection**: Filters rows based on conditions.\n2. **Projection**: Selects specific columns from the table.\n3. **Union**: Combines two or more tables, removing duplicates.\n4. **Join**: Combines rows from two tables based on related columns.\n\nThese operations are crucial for database design as they help in creating efficient and accurate queries. Understanding Relational Algebra helps in designing databases that can handle complex data retrieval tasks effectively.",
        "key_points": [
          "Relational Algebra is a formal system for manipulating relations using specific operations.",
          "Key operations include selection, projection, union, and join.",
          "It helps in creating efficient and accurate database queries by providing a structured approach to data manipulation."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- SELECT all employees FROM the 'Employees' TABLE SELECT * FROM Employees;",
            "explanation": "This example demonstrates how to select all columns from the 'Employees' table. It's a basic usage of projection.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Find all departments that have at least one employee with a salary greater than 5000\nSELECT D.department_name FROM Departments AS D JOIN Employees AS E ON D.department_id = E.department_id WHERE E.salary > 5000;",
            "explanation": "This practical example shows how to use join and where clauses to find departments based on employee salaries. It demonstrates the power of relational algebra in complex query construction."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to include a WHERE clause",
            "incorrect_code": "-- SELECT all employees FROM 'Employees' SELECT * FROM Employees;",
            "correct_code": "-- Correctly SELECT all employees with a salary greater than 5000 SELECT * FROM Employees WHERE salary > 5000;",
            "explanation": "This mistake happens when trying to filter data without specifying conditions. Always include a WHERE clause if you need to filter rows based on specific criteria."
          }
        ],
        "practice": {
          "question": "Create a query that selects all employees who work in the 'Sales' department and have a salary greater than 4000.",
          "solution": "-- Correct solution\nSELECT E.employee_name FROM Employees AS E JOIN Departments AS D ON E.department_id = D.department_id WHERE D.department_name = 'Sales' AND E.salary > 4000;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER 2\nMonitors /~\nI\n._.\n_.\n-\n-\n.-\n-\n-\n-\n-\n~\n-~\n-\n-\n-\n-\n-\n-\n-\n-\n-\n~\n-\n-\n-\n--\n-\n-\n-\n~\n-\n-\n-\n-\n--\n-- -;\nI\nI\n~:\nI\nDepartments\nSponsors\nI\nI\nI\nI\nI\n----------------------------------------\nI\n------~~~~~-",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "selection-projection": {
      "id": "selection-projection",
      "title": "Selection and Projection",
      "definition": "Basic relational algebra operations for filtering rows and selecting columns",
      "difficulty": "beginner",
      "page_references": [
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89
      ],
      "sections": {
        "definition": {
          "text": "Introduction to Database Design\nGiven a department, we know the manager, as well &'3 the manager's starting\ndate and budget for that department. This approach is natural if we t'l\"ssume\nthat a manager receives a separate discretionary budget for each department\nthat he or she manages.\nBut what if the discretionary budget is a sum that covers all departments\nmanaged by that employee?\nIn this case, each Manages2 relationship that\ninvolves a given employee will have the same value in the db1Ldget field, leading\nto redundant storage of the same information.\nAnother problem with this\ndesign is that it is misleading; it suggests that the budget is associated with\nthe relationship, when it is actually associated with the manager.\nWe can address these problems by introducing a new entity set called Managers\n(which can be placed below Employees in an ISA hierarchy, to show that every\nmanager is also an employee). The attributes since and dbudget now describe\na manager entity, as intended.\nAs a variation, while every manager has a\nbudget, each manager may have a different starting date (as manager) for each\ndepartment. In this case dbudget is an attribute of Managers, but since is an\nattribute of the relationship set between managers and departments.\nThe imprecise nature of ER modeling can thus make it difficult to recognize\nunderlying entities, and we might associate attributes with relationships rather\nthan the appropriate entities.\nIn general, such mistakes lead to redundant\nstorage of the same information and can cause many problems.\nWe discuss\nredundancy and its attendant problems in Chapter 19, and present a technique\ncalled normalization to eliminate redundancies from tables.\n2.5.3\nBinary versus Ternary Relationships\nConsider the ER diagram shown in Figure 2.17. It models a situation in which\nan employee can own several policies, each policy can be owned by several\nemployees, and each dependent can be covered by several policies.\nSuppose that we have the following additional requirements:\nIII\nA policy cannot be owned jointly by two or more employees.\n11II\nEvery policy must be owned by some employee.\nlIII\nDependents is a weak entity set, and each dependent entity is uniquely\nidentified by taking pname in conjunction with the policyid of a policy\nentity (which, intuitively, covers the given dependent).\nThe first requirement suggests that we impose a key constraint on Policies with\nrespect to Covers, but this constraint has the unintended side effect that a\n\nC~~C~T~\n~I\nI\nEmployees\nlot\nCovers\nPolicies as an Entity Set\nCHAPTERf 2\npolicy can cover only one dependent. The second requirement suggests that we\nimpose a total participation constraint on Policies. This solution is acceptable\nif each policy covers at least one dependent. The third requirement forces us\nto introduce an identifying relationship that is binary (in our version of ER\ndiagrams, although there are versions in which this is not the case).\nEven ignoring the third requirement, the best way to model this situation is to\nuse two binary relationships, as shown in Figure 2.18.\nPolicy Revisited\n\nIntTod'U(~t\"ion to Database Des'ign\n45.\nThis example really has two relationships involving Policies, and our attempt\nto use a single ternary relationship (Figure 2.17) is inappropriate. There are\nsituations, however, \"vhere a relationship inherently a.'3sociates more than two\nentities. vVe have seen such an example in Figures 2,4 and 2.15.\nAs a typical example of a ternary relationship, consider entity sets Parts, Sup-\npliers, and Departments, and a relationship set Contracts (with descriptive\nattribute qty) that involves all of them. A contract specifies that a supplier will\nsupply (some quantity of) a part to a department.\nThis relationship cannot\nbe adequately captured by a collection of binary relationships (without the use\nof aggregation). With binary relationships, we can denote that a supplier 'can\nsupply' certain parts, that a department 'needs' some parts, or that a depart-\nment 'deals with' a certain supplier.\nNo combination of these relationships\nexpresses the meaning of a contract adequately, for at least two reasons:\n•\nThe facts that supplier S can supply part P, that department D needs part\nP, and that D will buy from S do not necessarily imply that department D\nindeed buys part P from supplier S!\n•\nWe cannot represent the qty attribute of a contract cleanly.\n2.5.4\nAggregation versus Ternary Relationships\nAs we noted in Section 2.4.5, the choice between using aggregation or a ternary\nrelationship is mainly determined by the existence of a relationship that relates\na relationship set to an entity set (or second relationship set). The choice may\nalso be guided by certain integrity constraints that we want to express.\nFor\nexample, consider the ER diagram shown in Figure 2.13. According to this dia-\ngram, a project can be sponsored by any number of departments, a department\ncan sponsor one or more projects, and each sponsorship is monitored by one\nor more employees. If we don't need to record the unt-il attribute of Monitors,\nthen we might reasonably use a ternal'Y relationship, say, Sponsors2, as shown\nin Figure 2.19.\nConsider the constraint that each sponsorship (of a project by a department)\nbe monitored by at most one employee.\nVVe cannot express this constraint\nin terms of the Sponsors2 relationship set. On the other hand, we can easily\nexpress the cOnstraint by drawing an arrow from the aggregated relationship\nSponsors to the relationship Monitors in Figure 2.13.\nThus, the presence of\nsuch a constraint serves &s another reason for using aggregation rather than a\nternary relationship set.\n\nCHAPTERt2\nEmployees\nstarted_on\nG:(:P\nProjects\nSponsors2\ndname\nG:(?\n>-------11\nDepartment<\nUsing a Ternary Relationship instead of Aggregation\n2.6\nCONCEPTUAL DESIGN FOR LARGE ENTERPRISES\nWe have thus far concentrated on the constructs available in the ER model\nfor describing various application concepts and relationships. The process of\nconceptual design consists of more than just describing small fragments of the\napplication in terms of ER diagrams. For a large enterprise, the design may re-\nquire the efforts of more than one designer and span data and application code\nused by a number of user groups.\nUsing a high-level, semantic data model,\nsuch as ER diagrams, for conceptual design in such an environment offers the\nadditional advantage that the high-level design can be diagrammatically rep-\nresented and easily understood by the many people who must provide input to\nthe design process.\nAn important aspect of the design process is the methodology used to structure\nthe development of the overall design and ensure that the design takes into\naccount all user requirements and is consistent. The usual approach is that the\nrequirements of various user groups are considered, any conflicting requirements\nare somehow resolved, and a single set of global requirements is generated at\nthe end of the.requirements analysis phase. Generating a single set of global\nrequirements is a difficult task, but it allows the conceptual design phase to\nproceed with the development of a logical schema that spans all the data and\napplications throughout the enterprise.\nAn alternative approach is to develop separate conceptual scherna.'-l for different\nuser groups and then integTate these conceptual schemas. To integrate\nmulti~",
          "pages": [
            78,
            79,
            80,
            81
          ],
          "relevance": {
            "score": 0.28,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Intmduction to Database De.s'ign\npIe conceptual schemas, we must €'Btablish correspondences between entities,\nrelationships, and attributes, and we must resolve numerous kinds of conflicts\n(e.g., naming conflicts, domain mismatches, differences in measurement units).\nThis task is difficult in its own right. In some situations, schema integration\ncannot be avoided; for example, when one organization merges with another,\nexisting databases may have to be integrated. Schema integration is also in-\ncreasing in importance as users demand access to heterogeneous data sources,\noften maintained by different organizations.\n2.7\nTHE UNIFIED MODELING LANGUAGE\nThere are many approaches to end-to-end software system design, covering all\nthe steps from identifying the business requirements to the final specifications\nfor a complete application, including workflow, user interfaces, and many as-\npects of software systems that go well beyond databases and the data stored in\nthem. In this section, we briefly discuss an approach that is becoming popular,\ncalled the unified modeling language (UML) approach.\nUML, like the ER model, has the attractive feature that its constructs can be\ndrawn as diagrams. It encompasses a broader spectrum of the software design\nprocess than the ER model:\nIII\nBusiness Modeling: In this phase, the goal is to describe the business\nprocesses involved in the software application being developed.\nIII\nSystem Modeling: The understanding of business processes is used to\nidentify the requirements for the software application.\nOne part of the\nrequirements is the database requirements.\nIII\nConceptual Database Modeling: This step corresponds to the creation\nof the ER design for the database. For this purpose, UML provides many\nconstructs that parallel the ER constructs.\nIII\nPhysical Database Modeling: Ul\\IL also provides pictorial represen-\ntations for physical database design choices, such &'3 the creation of table\nspaces and indexes. (\\\\1e discuss physical databa\"se design in later chapters,\nbut not the corresponding UML constructs.)\nIII\nHardware System Modeling: UML diagrams can be used to describe\nthe hardware configuration used for the application.\nTh(~re are many kinds of diagrams in UML. Use case diagrams describe the\nactions performed by the system in response to user requests, and the people\ninvolved in these actions.\nThese diagrams specify the external functionality\n<-hat the system is expected to support.\n\nCHAPTER;2\nActivity diagrams 8hmv the flow of actions in a business process. Statechart\ndiagrams describe dynamic interactions between system objects.\nThese dia-\ngrams, used in busine.c;s and systern modeling, describe how the external func-\ntionality is to be implemented, consistent with the business rules and processes\nof the enterprise.\nClass diagrams are similar to ER diagrams, although they are more general\nin that they are intended to model application entities (intuitively, important\nprogram components) and their logical relationships in addition to data entities\nand their relationships.\nBoth entity sets and relationship sets can be represented as classes in UML,\ntogether with key constraints, weak entities, and class hierarchies. The term\nrelationship is used slightly differently in UML, and UML's relationships are\nbinary.\nThis sometimes leads to confusion over whether relationship sets in\nan ER diagram involving three or more entity sets can be directly represented\nin UML. The confusion disappears once we understand that all relationship\nsets (in the ER sense) are represented as classes in UML; the binary UML\n'relationships' are essentially just the links shown in ER diagrams between\nentity sets and relationship sets.\nRelationship sets with key constraints are usually omitted from UML diagrams,\nand the relationship is indicated by directly linking the entity sets involved.\nFor example, consider Figure 2.6. A UML representation of this ER diagram\nwould have a class for Employees, a class for Departments, and the relationship\nManages is shown by linking these two classes. The link can be labeled with\na name and cardinality information to show that a department can have only\none manager.\nAs we will see in Chapter 3, ER diagrams are translated into the relational\nmodel by mapping each entity set into a table and each relationship set into\na table. FUrther, as we will see in Section 3.5.3, the table corresponding to a\none-to-many relationship set is typically omitted by including some additional\ninformation about the relationship in the table for one of the entity sets in-\nvolved. Thus, UML class diagrams correspond closely to the tables created by\nmapping an ER diagram.\nIndeed, every class in a U1I1L class diagram is mapped into a table in the cor-\nresponding U]\\'1L database diagram.\nUML's database diagrams show how\nclasses are represented in the database and contain additional details about\nthe structure of the database such as integrity constraints and indexes. Links\n(UML's 'relationships') between UML classes lead to various integrity con-\nstraints between the corresponding tables.\nMany details specific to the re-\nlational model (e.g., views, fOTe'ign keys, null-allowed fields) and that reflect\n\nIntroduction to Dutaba8C Design\nphysical design choices (e.g., indexed fields) can be modeled ill UN[L database\ndiagrams.\nUML's component diagrams describe storage aspects of the database, such\nas tablespaces and database pa,titions) , as well as interfaces to applications\nthat access the database. Finally, deployment diagrams show the hardware\naspects of the system.\nOur objective in this book is to concentrate on the data stored in a database\nand the related design issues.\nTo this end, we deliberately take a simplified\nview of the other steps involved in software design and development. Beyond\nthe specific discussion of UlIIL, the material in this section is intended to place\nthe design issues that we cover within the context of the larger software design\nprocess. \\Ve hope that this will assist readers interested in a more comprehen-\nsive discussion of software design to complement our discussion by referring to\nother material on their preferred approach to overall system design.\n2.8\nCASE STUDY: THE INTERNET SHOP\nWe now introduce an illustrative, 'cradle-to-grave' design case study that we\nuse as a running example throughout this book. DBDudes Inc., a well-known\ndatabase consulting firm, has been called in to help Barns and Nobble (B&N)\nwith its database design and implementation. B&N is a large bookstore special-\nizing in books on horse racing, and it has decided to go online. DBDudes first\nverifies that B&N is willing and able to pay its steep fees and then schedules a\nlunch meeting--billed to B&N, naturally~to do requirements analysis.\n2.8.1\nRequirements Analysis\nThe owner of B&N, unlike many people who need a database, has thought\nextensively about what he wants and offers a concise summary:\n\"I would like my customers to be able to browse my catalog of books and\nplace orders over the Internet. Currently, I take orders over the phone. I have\nmostly corporate customers who call me and give me the ISBN number of a\nbook and a quantity; they often pay by credit card. I then prepare a shipment\nthat contains the books they ordered. If I don't have enough copies in stock,\nI order additional copies and delay the shipment until the new copies arrive;\nI want to ship a customer's entire order together. My catalog includes all the\nbooks I sell. For each book, the catalog contains its ISBN number, title, author,\npurcha.se price, sales price, and the year the book was published. Most of my\nsustomers are regulars, and I have records with their names and addresses.\n\nyear-published\nOrders\nER Diagram of the Initial Design\nCHAPTER¢2\nNew customers have to call me first and establish an account before they can\nuse my website.\nOn my new website, customers should first identify themselves by their unique\ncustomer identification number. Then they should be able to browse my catalog\nand to place orders online.\"\nDBDudes's consultants are a little surprised by how quickly the requirements\nphase is completed--it usually takes weeks of discussions (and many lunches\nand dinners) to get this done~~but return to their offices to analyze this infor-\nmation.\n2.8.2\nConceptual Design\nIn the conceptual design step, DBDudes develops a high level description of\nthe data in terms of the ER model.\nThe initial design is shown in Figure\n2.20. Books and customers are modeled as entities and related through orders\nthat customers place.\nOrders is a relationship set connecting the Books and\nCustomers entity sets.\nFor each order, the following attributes are stored:\nquantity, order date, and ship date. As soon as an order is shipped, the ship\ndate is set; until then the ship date is set to null, indicating that this order has\nnot been shipped yet.\nDBDudes has an internal design review at this point, and several questions are\nraised.\nTo protect their identities, we will refer to the design team leader as\nDude 1 and the design reviewer as Dude 2.\nDude 2: \\\\That if a. customer places two orders for the same book in one day?\nDude 1: The first order is ha,ndlecl by crea.ting a new Orders relationship and\n\nIntroduct'ion to Database Design\n51,\nthe second order is handled by updating the value of the quantity attribute in\nthis relationship.\nDude 2: \\\\That if a customer places two orders for different books in one day?\nDude 1: No problem. Each instance of the Orders relationship set relates the\ncustomer to a different book.\nDude 2: Ah, but what if a customer places two orders for the same book on\ndifferent days?\nDude 1: \\Ve can use the attribute order date of the orders relationship to\ndistinguish the two orders.\nDude 2: Oh no you can't. The attributes of Customers and Books must jointly\ncontain a key for Orders.\nSo this design does not allow a customer to place\norders for the same book on different days.\nDude 1: Yikes, you're right. Oh well, B&N probably won't care; we'll see.\nDBDudes decides to proceed with the next phase, logical database design; we\nrejoin them in Section 3.8.\n2.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nName the main steps in database design. What is the goal of each step?\nIn which step is the ER model mainly used? (Section 2.1)\n•\nDefine these terms: entity, entity set, attribute, key. (Section 2.2)\n•\nDefine these terms:\nrelationship, relationship set, descriptive attributes.\n(Section 2.3)\n•\nDefine the following kinds of constraints, and give an example of each: key\nconstraint, participation constraint. What is a weak entity? What are class\nhierarchies'? What is aggregation?\nGive an example scenario motivating\nthe use of each of these ER model design constructs. (Section 2.4)\n•\nWhat guidelines would you use for each of these choices when doing ER\ndesign: \\Vhether to use an attribute or an entity set, an entity or a relation-\nship set, a binary or ternary relationship, or aggregation. (Section 2.5)\nIII'l\nWhy is designing a database for a large enterprise especially hard? (Sec-\ntion 2.6)\n•\nWhat is UML? How does databa\"se design fit into the overall design of\na data-intensive software system? How is UML related to ER diagrams?\n(Section 2.7)\n\nEXERCISES\nCHAPTERJ2\nExercise 2.1 Explain the following terms briefly:\nattribute, domain, entity, relationship,.\nentity set, relationship set, one-to-many relat'ionship, many-to-many 1'elationship. pan·tcipa-\ntion constmint. overlap constraint, covering constraint, weak entity set,. aggregat'ion, and role\nindicator.\nExercise 2.2 A university database contains information about professors (identified by so-\ncial security number, or SSN) and courses (identified by courseid). Professors teach courses;\neach of the following situations concerns the Teaches relationship set.\nFor each situation,\ndraw an ER diagram that describes it (assuming no further constraints hold).\n1. Professors can teach the same course in several semesters, and each offering must be\nrecorded.\n2. Professors can teach the same course in several semesters, and only the most recent\nsuch offering needs to be recorded.\n(Assume this condition applies in all subsequent\nquestions.)\n3. Every professor must teach some course.\n4. Every professor teaches exactly one course (no more, no less).\n5. Every professor teaches exactly one course (no more, no less), and every course must be\ntaught by some professor.\n6. Now suppose that certain courses can be taught by a team of professors jointly, but it\nis possible that no one professor in a team can teach the course. Model this situation,\nintroducing additional entity sets and relationship sets if necessary.\nExercise 2.3 Consider the following information about a university database:\nII\nProfessors have an SSN, a name, an age, a rank, and a research specialty.\nII\nProjects have a project number, a sponsor name (e.g., NSF), a starting date, an ending\ndate, and a budget.\nII\nGraduate students have an SSN, a name, an age, and a degree program (e.g., M.S. or\nPh.D.).\nII\nEach project is managed by one professor (known as the project's principal investigator).\nII\nEach project is worked on by one or more professors (known as the project's co-investigators).\nIII\nProfessors can manage and/or work on multiple projects.\nII\nEach project is worked on by one or more graduate students (known as the project's\nresearch assistants).\nII\nWhen graduate students >'lark on a project, a professor must supervise their work on the\nproject. Graduate students can work on multiple projects, in which case they will have\na (potentially different) supervisor for each one.\nII\nDepartments have a department number, a department name, and a main office.\nII\nDepartments have a professor (known as the chairman) who runs the department.\nII\nProfessors work in one or more departments, and for each department that they work\nin, a time percentage is associated with their job.\nII\nGraduate students have one major department in which they are working OIl their degree.",
          "pages": [
            82,
            83,
            84,
            85,
            86,
            87
          ],
          "relevance": {
            "score": 0.18,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Introduction to Database Design\n~3\nIII\nEach graduate student has another, more senior graduate student (known as a student\nadvisor) who advises him or her OIl what courses to take.\nDesign and dra\\v an ER diagram that captures the information about the university. Use only\nthe basic ER model here; that is, entities, relationships, and attributes. Be sure to indicate\nany key and participation constraints.\nExercise 2.4 A company database needs to store information about employees (identified\nby ssn, with salary and phone as attributes), departments (identified by dna, with dname and\nbudget as attributes), and children of employees (with name and age as attributes). Employees\nwork in departments; each department is managed by an employee; a child must be identified\nuniquely by name when the parent (who is an employee; assume that only one parent works\nfor the company) is known.\nWe are not interested in information about a child once the\nparent leaves the company.\nDraw an ER diagram that captures this information.\nExercise 2.5 Notown Records has decided to store information about musicians who perform\non its albums (as well as other company data) in a database. The company has wisely chosen\nto hire you as a database designer (at your usual consulting fee of $2500jday).\nIII\nEach musician that records at Notown has an SSN, a name, an address, and a phone\nnumber. Poorly paid musicians often share the same address, and no address has more\nthan one phone.\nIII\nEach instrument used in songs recorded at Notown has a name (e.g., guitar, synthesizer,\nflute) and a musical key (e.g., C, B-flat, E-flat).\nIII\nEach album recorded on the Notown label has a title, a copyright date, a format (e.g.,\nCD or MC), and an album identifier.\nIII\nEach song recorded at Notown has a title and an author.\nIII\nEach musician may play several instruments, and a given instrument may be played by\nseveral musicians.\nIII\nEach album has a number of songs on it, but no song may appear on more than one\nalbum.\nIII\nEach song is performed by one or more musicians, and a musician may perform a number\nof songs.\nIII\nEach album has exactly one musician who acts as its producer. A musician may produce\nseveral albums, of course.\nDesign' a conceptual schema for Notown and draw an ER diagram for your schema.\nThe\npreceding information describes the situation that the Notown database must model. Be sure\nto indicate all key and cardinality constraints and any assumptions you make. Identify any\nconstraints you are unable to capture in the ER diagram and briefly explain why you could\nnot express them.\nExercise 2.6 Computer Sciences Department frequent fliers have been complaining to Dane\nCounty Airport officials about the poor organization at the airport. As a result, the officials\ndecided that all information related to the airport should be organized using a DBMS, and\nyou have been hired to design the database.\nYour first task is to organize the information\nabout all the airplanes stationed and maintainecl at the airport. The relevant information is\nas follows:\n\n•\nEvery airplane has a registration number, and each airplane is of a specific model.\n•\nThe airport accommodates a number of airplane models, and each model is identified by\na model number (e.g., DC-lO) and has a capacity and a weight.\n•\nA number of technicians work at the airport. You need to store the name, SSN, address,\nphone number, and salary of each technician.\n•\nEach technician is an expert on one or more plane model(s), and his or her expertise may\noverlap with that of other technicians. This information about technicians must also be\nrecorded.\n•\nTraffic controllers must have an annual medical examination. For each traffic controller,\nyou must store the date of the most recent exam.\n•\nAll airport employees (including technicians) belong to a union.\nYou must store the\nunion membership number of each employee.\nYou can assume that each employee is\nuniquely identified by a social security number.\n•\nThe airport has a number of tests that are used periodically to ensure that airplanes are\nstill airworthy. Each test has a Federal Aviation Administration (FAA) test number, a\nname, and a maximum possible score.\n•\nThe FAA requires the airport to keep track of each time a given airplane is tested by a\ngiven technician using a given test. For each testing event, the information needed is the\ndate, the number of hours the technician spent doing the test, and the score the airplane\nreceived on the test.\n1. Draw an ER diagram for the airport database. Be sure to indicate the various attributes\nof each entity and relationship set; also specify the key and participation constraints for\neach relationship set. Specify any necessary overlap and covering constraints a.s well (in\nEnglish).\n2. The FAA passes a regulation that tests on a plane must be conducted by a technician\nwho is an expert on that model.\nHow would you express this constraint in the ER\ndiagram? If you cannot express it, explain briefly.\nExercise 2.7 The Prescriptions-R-X chain of pharmacies ha.s offered to give you a free life-\ntime supply of medicine if you design its database. Given the rising cost of health care, you\nagree. Here's the information that you gather:\n11II\nPatients are identified by an SSN, and their names, addresses, and ages must be recorded.\n11II\nDoctors are identified by an SSN. For each doctor, the name, specialty, and years of\nexperience must be recorded.\nIII\nEach pharmaceutical company is identified by name and has a phone number.\nIII\nFor each drug, the trade name and formula must be recorded.\nEach drug is sold by\na given pharmaceutical company, and the trade name identifies a drug uniquely from\namong the pJ;oducts of that company. If a pharmaceutical company is deleted, you need\nnot keep track of its products any longer.\nIII\nEach pharmacy has a name, address, and phone number.\nIII\nEvery patient has a primary physician. Every doctor has at least one patient.\n•\nEach pharmacy sells several drugs and has a price for each.\nA drug could be sold at\nseveral pharmacies, and the price could vary from one pharmacy to another.",
          "pages": [
            88,
            89
          ],
          "relevance": {
            "score": 0.28,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Selection and projection are fundamental operations in database management that allow you to filter data (selection) and specify which columns to retrieve (projection). These operations help in managing and analyzing data efficiently.",
        "explanation": "Imagine you have a large library with many books. Selection is like choosing specific books based on certain criteria, such as genre or author. Projection is like deciding which pages of those books to read, focusing only on the information that's relevant to you. Both operations are crucial for organizing and accessing data effectively in databases.",
        "key_points": [
          "Key point 1: Selection filters rows based on a condition, while projection selects specific columns from the table.",
          "Key point 2: Use WHERE clause for selection and SELECT statement for projection.",
          "Key point 3: Common mistakes include forgetting to use the correct syntax or not understanding how conditions affect the result set.",
          "Key point 4: Always test your queries with a small subset of data before running them on large databases.",
          "Key point 5: Both selection and projection are essential for efficient data retrieval and analysis."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- SELECT all employees FROM the 'Sales' department SELECT * FROM Employees WHERE Department = 'Sales'; -- SELECT only the employee ID AND name FROM the 'Employees' TABLE SELECT EmployeeID, Name FROM Employees;",
            "explanation": "These examples demonstrate how to use selection and projection to filter and retrieve specific data.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Find all customers who have made a purchase over $1000 in the last month\nSELECT CustomerID, Name FROM Customers WHERE PurchaseAmount > 1000 AND Date >= DATE_SUB(CURDATE(), INTERVAL 1 MONTH);",
            "explanation": "This practical example shows how to combine selection and projection with conditions to retrieve meaningful data."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting the WHERE clause in selection.",
            "incorrect_code": "-- Incorrect: SELECT all employees FROM 'Sales' SELECT * FROM Employees;",
            "correct_code": "-- Correct: SELECT all employees FROM 'Sales' SELECT * FROM Employees WHERE Department = 'Sales';",
            "explanation": "Always include a condition in the WHERE clause to filter data correctly. Without it, you'll retrieve all rows."
          },
          {
            "mistake": "Selecting all columns when only specific ones are needed.",
            "incorrect_code": "-- Incorrect: SELECT all columns FROM 'Employees' SELECT * FROM Employees;",
            "correct_code": "-- Correct: SELECT only the required columns SELECT EmployeeID, Name, Position FROM Employees;",
            "explanation": "Only select the columns you need to avoid unnecessary data and improve performance."
          }
        ],
        "practice": {
          "question": "Write a SQL query that selects all customers from the 'North America' region who have made more than 5 purchases in the last year.",
          "solution": "-- Solution: Select required columns from Customers table\nSELECT CustomerID, Name FROM Customers WHERE Region = 'North America' AND (SELECT COUNT(*) FROM Purchases WHERE CustomerID = Customers.CustomerID AND Date >= DATE_SUB(CURDATE(), INTERVAL 1 YEAR)) > 5;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Introduction to Database Design\nGiven a department, we know the manager, as well &'3 the manager's starting\ndate and budget for that department. This approach is natural if we t'l\"ssume\nthat a manager",
      "content_relevance": {
        "score": 0.18,
        "sql_score": 1.0,
        "concept_score": 0.17,
        "non_sql_penalty": 0.2,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "set-operations": {
      "id": "set-operations",
      "title": "Set Operations in SQL",
      "definition": "UNION, INTERSECT, and EXCEPT operations in relational algebra and SQL",
      "difficulty": "intermediate",
      "page_references": [
        93,
        94,
        95,
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103,
        104,
        105,
        106
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER ~\nSQL. Originally developed as the query language of the pioneering\nSystem-R relational DBl\\1S at IBIYl, structured query language (SQL)\nhas become the most widely used language for creating, manipulating,\nand querying relational DBMSs. Since many vendors offer SQL products,\nthere IS a need for a standard that defines \\official SQL.' The existence of\na standard allows users to measure a given vendor's version of SQL for\ncompleteness. It also allows users to distinguish SQLfeatures specific to\none product from those that are standard; an application that relies on\nnonstandard features is less portable.\nThe first SQL standard was developed in 1986 by the American National\nStandards Institute (ANSI) and was called SQL-86. There was a minor\nrevision in 1989 called SQL-89 and a major revision in 1992 called SQL-\n92.\nThe International Standards Organization (ISO) collaborated with\nANSI to develop SQL-92. Most commercial DBMSs currently support (the\ncore subset of) SQL-92 and are working to support the recently adopted\nSQL:1999 version of the standard, a major extension of SQL-92.\nOur\ncoverage of SQL is based on SQL:1999, but is applicable to SQL-92 as\nwell; features unique to SQL:1999 are explicitly noted.\nand the network model); the relational model revolutionized the database field\nand largely supplanted these earlier models.\nPrototype relational databa.'3e\nmanagement systems were developed in pioneering research projects at IBM\nand DC-Berkeley by the mid-197Gs, and several vendors were offering relational\ndatabase products shortly thereafter.\nToday, the relational model is by far\nthe dominant data model and the foundation for the leading DBMS products,\nincluding IBM's DB2 family, Informix, Oracle, Sybase, Microsoft's Access and\nSQLServer, FoxBase, and Paradox. Relational database systems are ubiquitous\nin the marketplace and represent a multibillion dollar industry.\nThe relational model is very simple and elegant: a database is a collection of\none or more relations, where each relation is a table with rows and columns.\nThis simple tabular representation enables even novice users to understand the\ncontents of a database, and it permits the use of simple, high-level languages\nto query the data. The major advantages of the relational model over the older\ndata models are its simple data representation and the ease with which even\ncomplex queries can be expressed.\n\\Vhile we concentrate on the underlying concepts, we also introduce the Data\nDefinition Language (DDL) features of SQL, the standard language for\ncreating, manipulating, and querying data in a relational DBMS. This allows\nus to ground the discussion firmly in terms of real databa.se systems.\n\nThe Relational 1\\1odd\nvVe discuss the concept of a relation in Section\n~t1 and show how to create\nrelations using the SQL language. An important component of a data model is\nthe set of constructs it provides for specifying conditions that must be satisfied\nby the data.\nSuch conditions, called 'integrity constraints (lGs), enable the\nDBIviS to reject operations that might corrupt the data. We present integrity\nconstraints in the relational model in Section 3.2, along with a discussion of\nSQL support for les. \\Ve discuss how a DBMS enforces integrity constraints\nin Section 3.3.\nIn Section 3.4, we turn to the mechanism for accessing and retrieving data\nfrom the database, query languages, and introduce the querying features of\nSQL, which we examine in greater detail in a later chapter.\nWe then discuss converting an ER diagram into a relational database schema\nin Section 3.5. We introduce views, or tables defined using queries, in Section\n3.6. Views can be used to define the external schema for a database and thus\nprovide the support for logical data independence in the relational model. In\nFinally, in Section 3.8 we extend our design case study, the Internet shop in-\ntroduced in Section 2.8, by showing how the ER diagram for its conceptual\nschema can be mapped to the relational model, and how the use of views can\nhelp in this design.\n3.1\nINTRODUCTION TO THE RELATIONAL MODEL\nThe main construct for representing data in the relational model is a relation.\nA relation consists of a relation schema and a relation instance.\nThe\nrelation instance is a table, and the relation schema describes the column heads\nfor the table.\nWe first describe the relation schema and then the relation\ninstance. The schema specifies the relation's name, the name of each field (or\ncolumn, or attribute), and the domain of each field. A domain is referred to\nin a relation schema by the domain name and has a set of associated values.\n\\Ve use the example of student information in a university database from Chap-\nter 1 to illustrate the parts of a relation schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nThis says, for instance, that the field named sid has a domain named string.\nThe set of values associated with domain string is the set of all character\nstrings.\n\nCHAPTER 3\nWe now turn to the instances of a relation. An instance of a relation is a set\nof tuples, also called records, in which each tuple has the same number of\nfields as the relation schema. A relation instance can be thought of as a table\nin which each tuple is a row, and all rows have the same number of fields. (The\nterm relation instance is often abbreviated to just relation, when there is no\nconfusion with other aspects of a relation such as its schema.)\nAn instance of the Students relation appears in Figure 3.1.\nThe instance 81\nField names\nTUPLES\n(RECORDS,\nROWS)\nFIELDS (ATTRIBUTES, COLUMNS)\n-~\nname\nI---/o'-gz-'n--\nDave\ndave@cs\n3.3\nJones\njones@cs\n3.4\nSmith\nsmith@ee\n3.2\nSmith\nsmith@math\n3.8\n\"'\\ 53831\nMadayan\nmadayan@music\n1.8\nGuldu\nguldu@music\n2.0\nAn Instance 81 of the Students Relation\ncontains six tuples and has, as we expect from the schema, five fields. Note that\nno two rows are identical. This is a requirement of the relational model-each\nrelation is defined to be a set of unique tuples or rows.\nIn practice, commercial systems allow tables to have duplicate rows, but we\nassume that a relation is indeed a set of tuples unless otherwise noted. The\norder in which the rows are listed is not important. Figure 3.2 shows the same\nrelation instance. If the fields are named, as in our schema definitions and\nI s'id\nI name\n[.login\nMadayan\nmadayan@music\n1.8\nGuldu\ngllldll@music\n2.0\nSmith\nsmith@ee\n3.2\nSmith\nsmith@math\n3.8\nJOI;es\njones@cs\n3.4\nDave\ndave@cs\n3.3\nAn Alternative Representation of Instance 81 of Students\nfigures depicting relation instances, the order of fields does not matter either.\nHowever, an alternative convention is to list fields in a specific order and refer\n\nThe Relat'ional lvfodel\n61,\nto a field by its position.\nThus, s'id is field 1 of Students, login is field\n:~,\nand so on. If this convention is used, the order of fields is significant. Most\ndatabase systems use a combination of these conventions. For example, in SQL,\nthe named fields convention is used in statements that retrieve tuples and the\nordered fields convention is commonly used when inserting tuples.\nA relation schema specifies the domain of each field or column in the relation\ninstance.\nThese domain constraints in the schema specify an important\ncondition that we want each instance of the relation to satisfy: The values\nthat appear in a column must be drawn from the domain associated with that\ncolumn.\nThus, the domain of a field is essentially the type of that field, in\nprogramming language terms, and restricts the values that can appear in the\nfield.\nMore formally, let R(fI:Dl, ..., In:Dn) be a relation schema, and for each Ii,\n1 :::; i :::; n, let Dami be the set of values associated with the domain named Di.\n.An instance of R that satisfies the domain constraints in the schema is a set of\ntuples with n fields:\n{ (fI : dl ,\n,In: dn)\nI dl E Daml' ... ,dn E Damn}\nThe angular brackets (\n) identify the fields of a tuple. Using this notation,\nthe first Students tuple shown in Figure 3.1 is written as (sid: 50000, name:\nDave, login: dave@cs, age: 19, gpa: 3.3). The curly brackets {...} denote a set\n(of tuples, in this definition). The vertical bar I should be read 'such that,' the\nsymbol E should be read 'in,' and the expression to the right of the vertical\nbar is a condition that must be satisfied by the field values of each tuple in the\nset. Therefore, an instance of R is defined as a set of tuples. The fields of each\ntuple must correspond to the fields in the relation schema.\nDomain constraints are so fundamental in the relational model that we hence-\nforth consider only relation instances that satisfy them; therefore, relation\ninstance means relation instance that satisfies the domain constraints in the\nrelation schema.\nThe degree, also called arity, of a relation is the number of fields. The car-\ndinality of a relation instance is the number of tuples in it. In Figure 3.1, the\ndegree of the relation (the number of columns) is five, and the cardinality of\nthis instance is six.\nA relational database is a collection of relations with distinct relation names.\nThe relational database schema is the collection of schemas for the relations\nin the database. 'For example, in Chapter 1, we discllssed a university database\nwith relations called Students, Faculty, Courses, Rooms, Enrolled, Teaches,\nand Meets~In. An instance of a relational databa..'3e is a collection of relation",
          "pages": [
            93,
            94,
            95,
            96
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER~3\ninstances, one per relation schema in the database schema; of course, each\nrelation instance must satisfy the domain constraints in its schema.\n3.1.1\nCreating and Modifying Relations Using SQL\nThe SQL language standard uses the word table to denote relation, and we often\nfollow this convention when discussing SQL. The subset of SQL that supports\nthe creation, deletion, and modification of tables is called the Data Definition\nLanguage (DDL). Further, while there is a command that lets users define new\ndomains, analogous to type definition commands in a programming language,\nwe postpone a discussion of domain definition until Section 5.7. For now, we\nonly consider domains that are built-in types, such as integer.\nThe CREATE TABLE statement is used to define a new table. 1\nTo create the\nStudents relation, we can use the following statement:\nCREATE TABLE Students ( sid\nname\nlogin\nage\ngpa\nCHAR(20) ,\nCHAR(30) ,\nCHAR(20) ,\nINTEGER,\nREAL)\nTuples are inserted ,using the INSERT command. We can insert a single tuple\ninto the Students table as follows:\nINSERT\nINTO\nStudents\n(sid, name, login, age, gpa)\nVALUES (53688, 'Smith', 'smith@ee', 18, 3.2)\nWe can optionally omit the list of column names in the INTO clause and list\nthe values in the appropriate order, but it is good style to be explicit about\ncolumn names.\nWe can delete tuples using the DELETE command. We can delete all Students\ntuples with name equal to Smith using the command:\nDELETE\nFROM\nWHERE\nStudents S\nS.name = 'Smith'\n1SQL also provides statements to destroy tables and to change the columns associated with a table;\nwe discuss these in Section 3.7.\n\nThe Relational Pr10del\nvVe can modify the column values in an existing row using the UPDATE com-\nmand. For example, we can increment the age and decrement the gpa of the\nstudent with sid 53688:\nUPDATE Students S\nSET\nS.age = S.age + 1, S.gpa = S.gpa -\nWHERE\nS.sid = 53688\nThese examples illustrate some important points. The WHERE clause is applied\nfirst and determines which rows are to be modified.\nThe SET clause then\ndetermines how these rows are to be modified. If the column being modified is\nalso used to determine the new value, the value used in the expression on the\nright side of equals (=) is the old value, that is, before the modification. To\nillustrate these points further, consider the following variation of the previous\nquery:\nUPDATE Students S\nSET\nS.gpa = S.gpa - 0.1\nWHERE\nS.gpa >= 3.3\nIf this query is applied on the instance 81 of Students shown in Figure 3.1, we\nobtain the instance shown in Figure 3.3.\nI sid\nI name\nI login\nDave\ndave@cs\n3.2\nJones\njones@cs\n3.3\nSmith\nsmith@ee\n3.2\nSmith\nsmith@math\n3.7\nMadayan\nmadayan@music\n1.8\nGuldu\nguldu@music\n2.0\nStudents Instance 81 after Update\n3.2\nINTEGRITY CONSTRAINTS OVER RELATIONS\nA database is only as good as the information stored in it, and a DBMS must\ntherefore help prevent the entry of incorrect information. An integrity con-\nstraint (Ie) is a condition specified on a database schema and restricts the\ndata that can be stored in an instance of the databa'3e. If a database instance\nsatisfies all the integrity constraints specified on the database schema, it is a\nlegal instance. A DBMS enforces integrity constraints, in that it permits only\nlegal instances to be stored in the database.\nIntegrity constraints are specified and enforced at different times:\n\nCHAPTER 3\n1. \\\\Then the DBA or end user defines a database schema, he or she specifies\nthe rcs that must hold on any instance of this database.\n2. \"Vhen a database application is run, the DBMS checks for violations and\ndisallows changes to the data that violate the specified ICs.\n(In some\nsituations, rather than disallow the change, the DBMS might make some\ncompensating changes to the data to ensure that the database instance\nsatisfies all ICs. In any case, changes to the database are not allowed to\ncreate an instance that violates any IC.) It is important to specify exactly\nwhen integrity constraints are checked relative to the statement that causes\nthe change in the data and the transaction that it is part of. We discuss\nthis aspect in Chapter 16, after presenting the transaction concept, which\nwe introduced in Chapter 1, in more detail.\nMany kinds of integrity constraints can be specified in the relational model.\nWe have already seen one example of an integrity constraint in the domain\nconstraints associated with a relation schema (Section 3.1). In general, other\nkinds of constraints can be specified as well; for example, no two students\nhave the same sid value. In this section we discuss the integrity constraints,\nother than domain constraints, that a DBA or user can specify in the relational\nmodel.\n3.2.1\nKey Constraints\nConsider the Students relation and the constraint that no two students have the\nsame student id. This IC is an example of a key constraint. A key constraint\nis a statement that a certain minimal subset of the fields of a relation is a\nunique identifier for a tuple.\nA set of fields that uniquely identifies a tuple\naccording to a key constraint is called a candidate key for the relation; we\noften abbreviate this to just key. In the case of the Students relation, the (set\nof fields containing just the) sid field is a candidate key.\nLet us take a closer look at the above definition of a (candidate) key. There\nare two parts to the definition: 2\n1. Two distinct tuples in a legal instance (an instance that satisfies all Ies,\nincluding the key constraint) cannot have identical values in all the fields\nof a key.\n2. No subset of the set of fields in a key is a unique identifier for a tuple.\n2The term key is rather overworked.\nIn the context of access methods, we speak of sear'ch keys.\nwhich are quite different.\n\nThe Relational ltdodel\nThe first part of the definition means that, in any legal instance, the values in\nthe key fields uniquely identify a tuple in the instance. \\Vhen specifying a key\nconstraint, the DBA or user must be sure that this constraint will not prevent\nthem from storing a 'correct' set of tuples. (A similar comment applies to the\nspecification of other kinds of les as well.)\nThe notion of •correctness' here\ndepends on the nature of the data being stored. For example, several students\nmay have the same name, although each student has a unique student id. If\nthe name field is declared to be a key, the DBMS will not allow the Students\nrelation to contain two tuples describing different students with the same name!\nThe second part of the definition means, for example, that the set of fields\n{sid, name} is not a key for Students, because this set properly contains the\nkey {sid}. The set {sid, name} is an example of a superkey, which is a set of\nfields that contains a key.\nLook again at the instance of the Students relation in Figure 3.1. Observe that\ntwo different rows always have different sid values; sid is a key and uniquely\nidentifies a tuple. However, this does not hold for nonkey fields. For example,\nthe relation contains two rows with Smith in the name field.\nNote that every relation is guaranteed to have a key. Since a relation is a set of\ntuples, the set of all fields is always a superkey. If other constraints hold, some\nsubset of the fields may form a key, but if not, the set of all fields is a key.\nA relation may have several candidate keys.\nFor example, the login and age\nfields of the Students relation may, taken together, also identify students uniquely.\nThat is, {login, age} is also a key. It may seem that login is a key, since no\ntwo rows in the example instance have the same login value. However, the key\nmust identify tuples uniquely in all possible legal instances of the relation. By\nstating that {login, age} is a key, the user is declaring that two students may\nhave the same login or age, but not both.\nOut of all the available candidate keys, a database designer can identify a\nprimary key.\nIntuitively, a tuple can be referred to from elsewhere in the\ndatabase by storing the values of its primary key fields. For example, we can\nrefer to a Students tuple by storing its sid value. As a consequence of referring\nto student tuples in this manner, tuples are frequently accessed by specifying\ntheir sid value.\nIn principle, we can use any key, not just the primary key,\nto refer to a tuple.\nHowever, using the primary key is preferable because it\nis what the DBMS expects this is the significance of designating a particular\ncandidate key as a primary key\nand optimizes for. For example, the DBMS\nmay create an index with the primary key fields 8..'3 the search key, to make the\nretrieval of a tuple given its primary key value efficient. The idea of referring\nto a tuple is developed further in the next section.\n\nSpecifying Key Constraints in SQL\nCHAPTER,3\nIn SQL, we can declare that a subset of the columns of a table constitute a key\nby using the UNIQUE constraint. At most one of these candidate keys can be\ndeclared to be a primary key, using the PRIMARY KEY constraint.\n(SQL does\nnot require that such constraints be declared for a table.)\nLet us revisit our example table definition and specify key information:\nCREATE TABLE Students ( sid\nCHAR(20) ,\nname CHAR (30) ,\nlogin\nCHAR(20) ,\nage\nINTEGER,\ngpa\nREAL,\nUNIQUE (name, age),\nCONSTRAINT StudentsKey PRIMARY KEY (sid) )\nThis definition says that sid is the primary key and the combination of name\nand age is also a key.\nThe definition of the primary key also illustrates how\nwe can name a constraint by preceding it with CONSTRAINT constraint-name.\nIf the constraint is violated, the constraint name is returned and can be used\nto identify the error.\n3.2.2\nForeign Key Constraints\nSometimes the information stored in a relation is linked to the information\nstored in another relation. If one of the relations is modified, the other must be\nchecked, and perhaps modified, to keep the data consistent. An IC involving\nboth relations must be specified if a DBMS is to make such checks. The most\ncommon IC involving two relations is a foreign key constraint.\nSuppose that, in addition to Students, we have a second relation:\nEnrolled(studid: string, cid: string, gTade: string)\nTo ensure that only bona fide students can enroll in courses, any value that\nappears in the studid field of an instance of the Enrolled relation should also\nappear in the sid field of some tuple in the Students relation. The st'udid field\nof Enrolled is called a foreign key and refers to Students. The foreign key in\nt~l~referencil1grel~tio~~(Enrolled,inour. exalIlpl~)!nll~tl~latcht~le~)l:lirl~l~y~key:_:-- -\n()f -the JCferC11(;ed relation (Students); that jS,-jtIn~lstJUly(iU\"lhe'-s<lnie-~l~~i;;~_\n_.. ,'--\"\n.\n\".\n...•.. ....\n........••.....\n....-._--....\n--~-----~-\nof columns and cornpatible dCita types,\naltl1()u~h the column\nnanl(~S can be\nalffei'cii£.'\n.----\n'~'.\n_N~\n~_-~",
          "pages": [
            97,
            98,
            99,
            100,
            101
          ],
          "relevance": {
            "score": 0.6,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "The Relational A10del\nThis constraint is illustrated in Figure 3.4. As the figure shows, there may well\nbe some Students tuples that are not referenced from Enrolled (e.g., the student\nwith sid=50000). However, every studid value that appears in the instance of\nthe Enrolled table appears in the primary key column of a row in the Students\ntable.\nForeign key\nr---I\nPrimary key\n~\ncid\ngrade studid ~ -- sid\nname\nlogin\nage\ngpa\n~===:====:==~\n~==i====*==========*====:::*~\nCarnatic101\nC\n53831,\nDave\ndave@cs\nI---~~~~~+-~--t\nReggae203\nB\n53832, '-\n,-f\nJones\njones@cs\nI---..::::::...~~~~+-~--t\nTopology112\nA\n5365(}-~' ,\\'\nSmith\nsmith@ee\n1---~-=~~~+-~---1 ,'\\- ,\nHistory105\nB\n53666\"\n\\'\"\",~\nSmith\nsmith@math\n~~-=--~-'----~...L-~-J\n\\\"'\\\nMadayan\nmadayan@music\n\"'\\\nGuldu\ngu1du@music\n3.3\n3.4\n3.2\n3.8\n1.8\n2.0\nEnrolled (Referencing relation)\nStudents (Referenced relation)\nReferential Integrity\nIf we try to insert the tuple (55555, Artl04, A) into E1, the Ie is violated be-\ncause there is no tuple in 51 with sid 55555; the database system should reject\nsuch an insertion. Similarly, if we delete the tuple (53666, Jones, jones@cs, 18,\n3.4) from 51, we violate the foreign key constraint because the tuple (53666,\nHistoryl05, B) in El contains studid value 53666, the sid of the deleted Stu-\ndents tuple. The DBMS should disallow the deletion or, perhaps, also delete\nthe Enrolled tuple that refers to the deleted Students tuple. We discuss foreign\nkey constraints and their impact on updates in Section 3.3.\nFinally, we note that a foreign key could refer to the same relation. For example,\nwe could extend the Students relation with a column called partner and declare\nthis column to be a foreign key referring to Students. Intuitively, every student\ncould then have a partner, and the partner field contains the partner's sid. The\nobservant reader will no doubt ask, \"y\\That if a student does not (yet) have\na partnerT' This situation is handled in SQL by using a special value called\nnull. The use of nun in a field of a tuple rneans that value in that field is either\nunknown or not applicable (e.g., we do not know the partner yet or there is\nno partner). The appearanC(~ of null in a foreign key field does not violate the\nforeign key constraint.\nHowever, null values are not allowed to appear in a\nprimary key field (because the primary key fields are used to identify a tuple\nuniquely). \\Ve discuss null values further in Chapter 5.\n\nSpecifying Foreign Key Constraints in SQL\nCHAPTERp3\nLet us define Enrolled(studid: string, cid: string, grade: string):\nCREATE TABLE Enrolled ( studid CHAR(20) ,\ncid\nCHAR(20),\ngrade CHAR(10),\nPRIMARY KEY (studid, cid),\nFOREIGN KEY (studid) REFERENCES Students)\nThe foreign key constraint states that every st'udid value in Enrolled must also\nappear in Students, that is, studid in Enrolled is a foreign key referencing Stu-\ndents. Specifically, every studid value in Enrolled must appear as the value in\nthe primary key field, sid, of Students. Incidentally, the primary key constraint\nfor Enrolled states that a student has exactly one grade for each course he or\nshe is enrolled in. If we want to record more than one grade per student per\ncourse, we should change the primary key constraint.\n3.2.3\nGeneral Constraints\nDomain, primary key, and foreign key constraints are considered to be a fun-\ndamental part of the relational data model and are given special attention in\nmost commercial systems. Sometimes, however, it is necessary to specify more\ngeneral constraints.\nFor example, we may require that student ages be within a certain range of\nvalues; given such an IC specification, the DBMS rejects inserts and updates\nthat violate the constraint. This is very useful in preventing data entry errors.\nIf we specify that all students must be at least 16 years old, the instance of\nStudents shown in Figure 3.1 is illegal because two students are underage. If\nwe disallow the insertion of these two tuples, we have a legal instance, as shown\nin Figure 3.5.\ns'id\nI na'me\nlogin\nI age I gpa I\nJones\njones@cs\n3.4\nSmith\nsmithCQ)ee\n~).2\nI\nSmith\nsmith@math\n3.8 I\n. -\nAn Instance 82 of the Students Relation\nThe IC that students must be older than 16 can be thought of as an extended\ndomain constraint, since we are essentially defining the set of permissible age\n\nThe Relational lV/ode!\nvalues more stringently than is possible by simply using a standard domain\nsuch :'1.S integer. In general, however, constraints that go well beyond domain,\nkey, or foreign key constraints can be specified. For example, we could require\nthat every student whose age is greater than 18 must have a gpa greater than\n3.\nCurrent relational database systems support such general constraints in the\nform of table constraints and assertions. Table constraints are associated with a\nsingle table and checked whenever that table is modified. In contrast, assertions\ninvolve several tables and are checked whenever any of these tables is modified.\nBoth table constraints and assertions can use the full power of SQL queries to\nspecify the desired restriction.\nWe discuss SQL support for table constraints\nand assertions in Section 5.7 because a full appreciation of their power requires\na good grasp of SQL's query capabilities.\n3.3\nENFORCING INTEGRITY CONSTRAINTS\nAs we observed earlier, ICs are specified when a relation is created and enforced\nwhen a relation is modified. The impact of domain, PRIMARY KEY, and UNIQUE\nconstraints is straightforward: If an insert, delete, or update command causes\na violation, it is rejected. Every potential Ie violation is generally checked at\nthe end of each SQL statement execution, although it can be deferred until the\nend of the transaction executing the statement, as we will see in Section 3.3.1.\nConsider the instance 51 of Students shown in Figure 3.1. The following inser-\ntion violates the primary key constraint because there is already a tuple with\nthe s'id 53688, and it will be rejected by the DBMS:\nINSERT\nINTO\nStudents\n(sid, name, login, age, gpa)\nVALUES (53688, 'Mike', 'mike@ee', 17,3.4)\nThe following insertion violates the constraint that the primary key cannot\ncontain null:\nINSERT\nINTO\nStudents\n(sid, name, login, age, gpa)\nVALUES (null, 'Mike', 'mike@ee', 17,3.4)\nOf course, a similar problem arises whenever we try to insert a tuple with a\nvalue in a field that is not in the domain associated with that field, that is,\nwhenever we violate a domain constraint. Deletion does not cause a violation\nof clornain, primary key or unique constraints. However, an update can cause\nviolations, sirnilar to an insertion:",
          "pages": [
            102,
            103,
            104
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Set operations in SQL allow you to combine the results of two or more SELECT statements into a single result set. They are essential for performing complex queries and data analysis.",
        "explanation": "Set operations include UNION, INTERSECT, and EXCEPT. Each serves a different purpose:\n\n1. **UNION**: Combines rows from two SELECT statements. It removes duplicate rows unless you use UNION ALL.\n2. **INTERSECT**: Returns only the rows that are common to both SELECT statements.\n3. **EXCEPT**: Returns rows that are in the first SELECT statement but not in the second.\n\nThese operations are particularly useful when you need to compare data across different tables or conditions, allowing for powerful data analysis and reporting.",
        "key_points": [
          "Key point 1: UNION combines results from multiple queries, removing duplicates unless specified with UNION ALL.",
          "Key point 2: INTERSECT finds common rows between two queries, useful for identifying shared data.",
          "Key point 3: EXCEPT identifies unique rows in the first query that are not present in the second, ideal for finding differences.",
          "Key point 4: Always ensure column counts and types match across SELECT statements when using set operations.",
          "Key point 5: Set operations can be nested to perform more complex queries."
        ],
        "examples": [
          {
            "title": "Basic UNION Example",
            "code": "-- Selecting students FROM two different departments SELECT name FROM students WHERE department = 'CS' UNION SELECT name FROM students WHERE department = 'EE';",
            "explanation": "This query combines the names of students from Computer Science and Electrical Engineering departments, removing any duplicates.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical INTERSECT Example",
            "code": "-- Finding common courses between two professors SELECT course_id FROM professor_courses WHERE professor_id = 101 INTERSECT SELECT course_id FROM professor_courses WHERE professor_id = 102;",
            "explanation": "This practical example helps identify which courses are taught by both Professor 101 and Professor 102.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to match column counts and types in set operations",
            "incorrect_code": "-- Incorrect query due to mismatched columns SELECT name FROM students UNION SELECT id, name FROM employees;",
            "correct_code": "-- Correct query with matching columns SELECT name FROM students UNION SELECT name AS name FROM employees;",
            "explanation": "Ensure that each SELECT statement in a set operation has the same number of columns and compatible data types."
          }
        ],
        "practice": {
          "question": "Write a SQL query using UNION to find all customers who have either made a purchase or subscribed to a service.",
          "solution": "-- Solution\nSELECT customer_id FROM purchases\nUNION\nSELECT customer_id FROM subscriptions;",
          "explanation": "This question tests the ability to combine results from two different tables into one result set, ensuring no duplicates."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER ~\nSQL. Originally developed as the query language of the pioneering\nSystem-R relational DBl\\1S at IBIYl, structured query language (SQL)\nhas become the most widely used language for creating, ",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "sql-intro": {
      "id": "sql-intro",
      "title": "Introduction to SQL",
      "definition": "Structured Query Language for managing relational databases",
      "difficulty": "beginner",
      "page_references": [
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129,
        130
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER'3\nManages and WorksJn\nCREATE TABLE DepLMgr ( did\nINTEGER,\ndname\nCHAR(20) ,\nbudget\nREAL,\nssn\nCHAR(11) NOT NULL,\nsince\nDATE,\nPRIMARY KEY (did),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE NO ACTION)\nIt also captures the participation constraint that every department must have\na manager: Because ssn cannot take on null values, each tuple of DepLMgr\nidentifies a tuple in Employees (who is the manager). The NO ACTION specifi-\ncation, which is the default and need not be explicitly specified, ensures that\nan Employees tuple cannot be deleted while it is pointed to by a Dept-Mgr\ntuple. If we wish to delete such an Employees tuple, we must first change the\nDepLMgr tuple to have a new employee &'3 manager. (vVe could have specified\nCASCADE instead of NO ACTION, but deleting all information about a department\njust because its manager has been fired seems a bit extreme!)\nThe constraint that every department must have a manager cannot be cap-\ntured using the first translation approach discussed in Section 3.5.3.\n(Look\nat the definition of lVIanages and think about what effect it would have if we\nadded NOT NULL constraints to the ssn and did fields.\nHint: The constraint\nwould prevent the firing of a manager, but does not ensure that a manager is\ninitially appointed for each department!) This situation is a strong argument\n\nThe Relational lvfodel\n8~\nin favor of using the second approach for one-to-many relationships such as\nManages, especially when the entity set with the key constraint also has a total\nparticipation constraint.\nUnfortunately, there are many participation constraints that we cannot capture\nusing SQL, short of using table constraints or assertions. Table constraints and\nassertions can be specified using the full power of the SQL query language\n(as discussed in Section 5.7) and are very expressive but also very expensive to\ncheck and enforce. For example, we cannot enforce the participation constraints\non the \\iVorks_In relation without using these general constraints. To see why,\nconsider the Works-ln relation obtained by translating the ER diagram into·\nrelations.\nIt contains fields ssn and did, which are foreign keys referring to\nEmployees and Departments. To ensure total participation of Departments in\nWorks_In, we have to guarantee that every did value in Departments appears\nin a tuple of Works_In. We could try to guarantee this condition by declaring\nthat did in Departments is a foreign key referring to Works_In, but this is not\na valid foreign key constraint because did is not a candidate key for Works_In.\nTo ensure total participation of Departments in Works_In using SQL, we need\nan assertion. We have to guarantee that every did value in Departments appears\nin a tuple of Works_In; further, this tuple of Works_In must also have non-null\nvalues in the fields that are foreign keys referencing other entity sets involved in\nthe relationship (in this example, the ssn field). We can ensure the second part\nof this constraint by imposing the stronger requirement that ssn in Works-ln\ncannot contain null values.\n(Ensuring that the participation of Employees in\nWorks_In is total is symmetric.)\nAnother constraint that requires assertions to express in SQL is the requirement\nthat each Employees entity (in the context of the Manages relationship set)\nmust manage at least one department.\nIn fact, the Manages relationship set exemplifies most of the participation con-\nstraints that we can capture using key and foreign key constraints. Manages is\na binary relationship set in which exactly one of the entity sets (Departments)\nhas a key constraint, and the total participation constraint is expressed on that\nentity set.\n\\Ve can also capture participation constraints using key and foreign key con-\nstraints in one other special situation: a relationship set in which all participat-\ning entity sets have key constraints and total participation. The best translation\napproach in this case is to map all the entities &'3 well as the relationship into\na single table; the details are straightforward.\n\nCHAPTER~3\n3.5.5\nTranslating Weak Entity Sets\nA weak entity set always participates in a one-to-many binary relationship and\nhas a key constraint and total participation. The second translation approach\ndiscussed in Section 3.5.3 is ideal in this case, but we must take into account\nthat the weak entity has only a partial key.\nAlso, when an owner entity is\ndeleted, we want all owned weak entities to be deleted.\nConsider the Dependents weak entity set shown in Figure 3.14, with partial\nkey pname. A Dependents entity can be identified uniquely only if we take the\nkey of the owning Employees entity and the pname of the Dependents entity,\nand the Dependents entity must be deleted if the owning Employees entity is\ndeleted.\nEmployees\nThe Dependents Weak Entity Set\nWe can capture the desired semantics with the following definition of the\nDep_Policy relation:\nCREATE TABLE Dep_Policy (pname\nCHAR(20) ,\nage\nINTEGER,\ncost\nREAL,\nssn\nCHAR (11) ,\nPRIMARY KEY (pname, ssn),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE CASCADE )\nObserve that the primary key is (pna:me, ssn) , since Dependents is a weak\nentity. This constraint is a change with respect to the translation discussed in\n\\Ve have to ensure that every Dependents entity is associated\nwith an Employees entity (the owner), as per the total participation constraint\non Dependents. That is, ssn cannot be null.\nThis is ensured because\nSST/, is\npart of the primary key. The CASCADE option ensures that information about\nan employee's policy and dependents is deleted if the corresponding Employees\ntuple is deleted.\n\nThe Relational 1\"1,,1oriel\n3.5.6\nTranslating Class Hierarchies\nWe present the two basic approaches to handling ISA hierarchies by applying\nthem to the ER diagram shown in Figure 3.15:\nClass Hierarchy\n1. We can map each of the entity sets Employees, Hourly_Emps, and Con-\ntracLEmps to a distinct relation.\nThe Employees relation is created as\nin Section 2.2.\nWe discuss Hourly~mps here; ContracLEmps is han-\ndled similarly.\nThe relation for Hourly_Emps includes the hourly_wages\nand hours_worked attributes of Hourly_Emps. It also contains the key at-\ntributes of the superclass (ssn, in this example), which serve as the primary\nkey for Hourly_Emps, a.', well as a foreign key referencing the superclass\n(Employees).\nFor each Hourly_Emps entity, the value of the name and\nlot attributes are stored in the corresponding row of the supercla...,s (Em-\nployees). Note that if the superclass tuple is deleted, the delete must be\ncascaded to Hourly~mps.\n2. Alternatively, we can create just two relations, corresponding to Hourly_Emps\nand ContracLEmps.\nThe relation for Hourly~mps includes all the at-\ntributes of Hourly_Emps as well as all the attributes of Employees (i.e.,\nssn, name, lot, hO'l.1,rly_wages, hours_worked:).\nThe first approach is general and always applicable. Queries in which we want\nto (~xamine all employees and do not care about the attributes specific to the\nsubclasses are handled easily using the Employees relation. However, queries\nin which we want to examine, say, hourly employees, may require us to com-\nbine Hourly_Emps (or ContracLEmps, as the case may be) with Employees to\nretrieve name and lot.\n\nCHAPTER··~\nThe second approach is not applicable if we have employees who are neither\nhourly employees nor contract employees, since there is no way to store such\nemployees. Also, if an employee is both an Hourly-.Emps and a ContracLEmps\nentity, then the name and lot: values are stored twice. This duplication can lead\nto some of the anomalies that we discuss in Chapter 19. A query that needs to\nexamine all employees must now examine two relations. On the other hand, a\nquery that needs to examine only hourly employees can now do so by examining\njust one relation. The choice between these approaches clearly depends on the\nsemantics of the data and the frequency of common operations.\nIn general, overlap and covering constraints can be expressed in SQL only by\nusing assertions.\n3.5.7\nTranslating ER Diagrams with Aggregation\nConsider the ER diagram shown in Figure 3.16.\nThe Employees, Projects,\nManilars\nDepartments\nI\n_______did~ fT:C~'~~)\nAggregation\nand Departments entity sets and the Sponsors relationship set are mapped as\ndescribed in previous sections. For the Monitors relationship set, we create a\nrelation with the following attributes: the key attributes of Employees (88n), the\nkey attributes of Sponsors (d'id, p'id), and the descriptive attributes of Monitors\n('/.tnt:'il). This translation is essentially the standard mapping for a relationship\nset, as described in Section 3.5.2.",
          "pages": [
            115,
            116,
            117,
            118,
            119
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "The Relational A!odd\n~\nThere is a special case in which this translation can be refined by dropping the\nSponsors relation. Consicler the Sponsors relation. It has attributes pid, did,\nand since; and in general we need it (in addition to l\\rlonitors) for two reasons:\n1. \\Ve have to record the descriptive attributes (in our example, since) of the\nSponsors relationship.\n2. Not every sponsorship has a monitor, and thus some (p'id, did) pairs in the\nSponsors relation may not appear in the Monitors relation.\nHowever, if Sponsors has no descriptive attributes and has total participation\nin Monitors, every possible instance of the Sponsors relation can be obtained\nfrom the (pid, did) columns of Monitors; Sponsors can be dropped.\n3.5.8\nER to Relational: Additional Examples\nConsider the ER diagram shown in Figure 3.17. We can use the key constraints\nPolicy Revisited\nto combine Purchaser information with Policies and Beneficiary information\nwith Dependents, and translate it into the relational model as follows:\nCREATE TABLE Policies ( policyid INTEGER,\ncost\nREAL,\nssn\nCHAR (11) NOT NULL,\nPRIMARY KEY (policyid),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE CASCADE )\n\nCHAPTERB\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nage\nINTEGER,\npolicyid INTEGER,\nPRIMARY KEY (pname, policyid),\nFOREIGN KEY (policyid) REFERENCES Policies\nON DELETE CASCADE)\nNotice how the deletion of an employee leads to the deletion of all policies\nowned by the employee and all dependents who are beneficiaries of those poli-\ncies. Further, each dependent is required to have a covering policy-because\npolicyid is part of the primary key of Dependents, there is an implicit NOT NULL\nconstraint. This model accurately reflects the participation constraints in the\nER diagram and the intended actions when an employee entity is deleted.\nIn general, there could be a chain of identifying relationships for weak entity\nsets. For example, we assumed that policyid uniquely identifies a policy. Sup-\npose that policyid distinguishes only the policies owned by a given employee;\nthat is, policyid is only a partial key and Policies should be modeled as a weak\nentity set. This new assumption about policyid does not cause much to change\nin the preceding discussion.\nIn fact, the only changes are that the primary\nkey of Policies becomes (policyid, ssn) , and as a consequence, the definition of\nDependents changes-a field called ssn is added and becomes part of both the\nprimary key of Dependents and the foreign key referencing Policies:\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nssn\nCHAR (11) ,\nage\nINTEGER,\npolicyid INTEGER NOT NULL,\nPRIMARY KEY (pname, policyid, ssn),\nFOREIGN KEY (policyid, ssn) REFERENCES Policies\nON DELETE CASCADE )\n3.6\nINTRODUCTION TO VIEWS\nA view is a table whose rows are not explicitly stored in the database but\nare computed as needed from a view definition. Consider the Students and\nEnrolled relations. Suppose we are often interested in finding the names and\nstudent identifiers of students who got a grade of B in some course, together\nwith the course identifier. \\Ne can define a view for this purpose. Using SQL\nnotation:\nCREATE VIEW B-Students (name, sid, course)\nAS SELECT S.sname, S.sid, E.cid\n\nThe Relational 1I1odel\nFROM\nWHERE\nStudents S, Enrolled E\nS.sid = E.studid AND E.grade = 'B'\n$\nThe view B-Students has three fields called name, sid, and course with the\nsame domains as the fields sname and sid in Students and cid in Enrolled.\n(If the optional arguments name, sid, and course are omitted from the CREATE\nVIEW statement, the column names sname, sid, and cid are inherited.)\nThis view can be used just like a base table, or explicitly stored table, in\ndefining new queries or views. Given the instances of Enrolled and Students\nshown in Figure 3.4, B-Students contains the tuples shown in Figure 3.18.\nConceptually, whenever B-Students is used in a query, the view definition is\nfirst evaluated to obtain the corresponding instance of B-Students, then the rest\nof the query is evaluated treating B-Students like any other relation referred\nto in the query. (We discuss how queries on views are evaluated in practice in\nChapter 25.)\nsid\ncourse\nHistory105\nReggae203\nAn Instance of the B-Students View\n3.6.1\nViews, Data Independence, Security\nConsider the levels of abstraction we discussed in Section 1.5.2. The physical\nschema for a relational database describes how the relations in the conceptual\nschema are stored, in terms of the file organizations and indexes used.\nThe\nconceptual schema is the collection of schemas of the relations stored in the\ndatabase. While some relations in the conceptual schema can also be exposed to\napplications, that is, be part of the exte'mal schema of the database, additional\nrelations in the external schema can be defined using the view mechanism.\nThe view mechanism thus provides the support for logical data independence\nin the relational model.\nThat is, it can be used to define relations in the\nexternal schema that mask changes in the conceptual schema of the database\nfrom applications. For example, if the schema of a stored relation is changed,\nwe can define a view with the old schema and applications that expect to see\nthe old schema can now use this view.\nViews are also valuable in the context of security: We can define views that\ngive a group of users access to just the information they are allowed to see. For\nexample, we can define a view that allows students to see the other students'\n\nCHAPTER B\nname and age but not their gpa, and allows all students to access this view but\nnot the underlying Students table (see Chapter 21).\n3.6.2\nUpdates on Views\nThe motivation behind the view mechanism is to tailor how users see the data.\nUsers should not have to worry about the view versus base table distinction.\nThis goal is indeed achieved in the case of queries on views; a view can be used\njust like any other relation in defining a query. However, it is natural to want to\nspecify updates on views as well. Here, unfortunately, the distinction between\na view and a ba.se table must be kept in mind.\nThe SQL-92 standard allows updates to be specified only on views that are\ndefined on a single base table using just selection and projection, with no use of\naggregate operations.3 Such views are called updatable views. This definition\nis oversimplified, but it captures the spirit of the restrictions.\nAn update on\nsuch a restricted view can always be implemented by updating the underlying\nbase table in an unambiguous way. Consider the following view:\nCREATE VIEW GoodStudents (sid, gpa)\nAS SELECT S.sid, S.gpa\nFROM\nStudents S\nWHERE\nS.gpa> 3.0\nWe can implement a command to modify the gpa of a GoodStudents row by\nmodifying the corresponding row in Students. We can delete a GoodStudents\nrow by deleting the corresponding row from Students. (In general, if the view\ndid not include a key for the underlying table, several rows in the table could\n'correspond' to a single row in the view. This would be the case, for example,\nif we used S.sname instead of S.sid in the definition of GoodStudents. A com-\nmand that affects a row in the view then affects all corresponding rows in the\nunderlying table.)\nWe can insert a GoodStudents row by inserting a row into Students, using\nnull values in columns of Students that do not appear in GoodStudents (e.g.,\nsname, login). Note that primary key columns are not allowed to contain null\nvalues. Therefore, if we attempt to insert rows through a view that does not\ncontain the primary key of the underlying table, the insertions will be rejected.\nFor example, if GoodStudents contained snarne but not ,c;id, we could not insert\nrows into Students through insertions to GooclStudents.\n3There is also the restriction that the DISTINCT operator cannot be used in updatable vi(;w defi-\nnitions. By default, SQL does not eliminate duplicate copies of rows from the result of it query; the\nDISTINCT operator requires duplicate elimination. vVe discuss t.his point further in Chapt.er 5.\n\nThe Relational A10dd\n----------------~-\nUpdatable Views in SQL:1999 The Hew SQL standard has expanded\nthe class of view definitions that are\nupdatable~ taking primary . key\nconstraints into account.\nIn contra..')t·to SQL-92~ a· view definition that\ncontains more than OIle table in the FROM clause may be updatable under\nthe new definition.\nIntuitively~ we can update afield of a. view if it is\nobtained from exactly one of the underlying tables, and the primary key\nof that table is included in the fields of the view.\nSQL:1999 distinguishes between views whose rows can be modified (updat-\nable views) and views into which new rows can be inserted (insertable-\ninto views): Views defined using the SQL constructs UNION, INTERSECT,\nand EXCEPT (which we discuss in Chapter 5) cannot be inserted into, even\nif they are updatable. Intuitively, updatability ensures that an updated\ntuple in the view can be traced to exactly one tuple in one of the tables\nused to define the view. The updatability property, however, may still not\nenable us to decide into which table to insert a new tuple.\nAn important observation is that an INSERT or UPDATE may change the un-\nderlying base table so that the resulting (i.e., inserted or modified) row is not\nin the view! For example, if we try to insert a row (51234, 2.8) into the view,\nthis row can be (padded with null values in the other fields of Students and\nthen) added to the underlying Students table, but it will not appear in the\nGoodStudents view because it does not satisfy the view condition gpa > 3.0.\nThe SQL default action is to allow this insertion, but we can disallow it by\nadding the clause WITH CHECK OPTION to the definition of the view.\nIn this\ncase, only rows that will actually appear in the view are permissible insertions.\nWe caution the reader, that when a view is defined in terms of another view,\nthe interaction between these view definitions with respect to updates and the\nCHECK OPTION clause can be complex; we not go into the details.\nNeed to Restrict View Updates\nvVhile the SQL rules on updatable views are more stringent than necessary,\nthere are some fundamental problems with updates specified on views and good\nreason to limit the class of views that can be updated. Consider the Students\nrelation and a new relation called Clubs:\nClubs( cname: string, jyear: date, mnarne: string)\n\n~\nSailing\nDave\nHiking\nSmith\nRowing\nSmith\nDave\nJones\nSmith\nSmith\ndave(gcs\njones~~cs\nsmith@ee\nsmith@math\nCHAPTER 8\nAn Instance C of Clubs\nI name\n,. login\nAn Instance 53 of Students\nI dub\nsince\nDave\ndave@cs\nSailing\nSmith\nsmith@ee\nHiking\nSmith\nsmith@ee\nRowing\nSmith\nsmith@math\nHiking\nSmith\nsmith@math\nRowing\nInstance of ActiveStudents\nA tuple in Clubs denotes that the student called mname has been a member of\nthe club cname since the date jyear.4 Suppose that we are often interested in\nfinding the names and logins of students with a gpa greater than 3 who belong\nto at least one club, along with the club name and the date they joined the\nclub. We can define a view for this purpose:\nCREATE VIEW ActiveStudents (name, login, club, since)\nAS SELECT S.sname, S.login, C.cname, C.jyear\nFROM\nStudents S, Clubs C\nWHERE\nS.sname = C.mname AND S.gpa> 3\nConsider the instances of Students and Clubs shown in Figures 3.19 and 3.20.\nWhen evaluated using the instances C and S3, ActiveStudents contains the\nrows shown in Figure 3.21.\nNow suppose that we want to delete the row (Smith, smith@ee, Hiking, 1997)\nfrom ActiveStudents.\nHow are we to do this?\nActiveStudents rows are not\nstored explicitly but computed as needed from the Students and Clubs tables\nusing the view definition.\nSo we must change either Students or Clubs (or\nboth) in such a way that evaluating the view definition on the modified instance\ndoes not produce the row (Snrith, 8Tnith@ec, Hiking, 1997.) This ta.sk can be\nctccomplished in one of two ways:\nby either deleting the row (53688.. Sm'ith,\n8Tn'ith(iJ)ee, 18, ,g.2) from Students or deleting the row (Hiking, 1.997, 8m/ith)\nclvVe remark that Clubs has a poorly designed schema (chosen for the sake of our discussion of view\nupdates), since it identifies students by name, which is not a candidate key for Students.\n\nThe Relational tv!odel\n9J\nfrom Clubs. But neither solution is satisfactory. Removing the Students row\nhas the effect of also deleting the row (8m:ith, smith@ee, Rowing, 1998) from the\nview ActiveStudents. Removing the Clubs row h&'3 the effect of also deleting the\nrow (Smith, smith@math, Hiking, 1991) from the view ActiveStudents. Neither\nside effect is desirable. In fact, the only reasonable solution is to d'isallow such\nupdates on views.\nViews involving more than one base table can, in principle, be safely updated.\nThe B-Students view we introduced at the beginning of this section is an ex-\nample of such a view.\nConsider the instance of B-Students shown in Figure\n3.18 (with, of course, the corresponding instances of Students and Enrolled as\nin Figure 3.4). To insert a tuple, say (Dave, 50000, Reggae203) B-Students, we\ncan simply insert a tuple (Reggae203, B, 50000) into Enrolled since there is al-\nready a tuple for sid 50000 in Students. To insert (John, 55000, Reggae203), on\nthe other hand, we have to insert (Reggae203, B, 55000) into Enrolled and also\ninsert (55000, John, null, null, null) into Students. Observe how null values\nare used in fields of the inserted tuple whose value is not available. Fortunately,\nthe view schema contains the primary key fields of both underlying base tables;\notherwise, we would not be able to support insertions into this view. To delete\na tuple from the view B-Students, we can simply delete the corresponding tuple\nfrom Enrolled.\nAlthough this example illustrates that the SQL rules on updatable views are\nunnecessarily restrictive, it also brings out the complexity of handling view\nupdates in the general case. For practical reasons, the SQL standard has chosen\nto allow only updates on a very restricted class of views.\n3.7\nDESTROYING/ALTERING TABLES AND VIEWS\nIf we decide that we no longer need a base table and want to destroy it (i.e.,\ndelete all the rows and remove the table definition information), we can use\nthe DROP TABLE command. For example, DROP TABLE Students RESTRICT de-\nstroys the Students table unless some view or integrity constraint refers to\nStudents; if so, the command fails. If the keyword RESTRICT is replaced by\nCASCADE, Students is dropped and any referencing views or integrity constraints\nare (recursively) dropped as well; one of these t\\VO keyvlOrds must always be\nspecified. A vipw can be dropped using the DROP VIEW command, which is just\nlike DROP TABLE.\nALTER TABLE modifies the structure of an existing table.\nTo add a column\ncalled maiden-name to Students, for example, we would use the following com-\nmand:",
          "pages": [
            120,
            121,
            122,
            123,
            124,
            125,
            126
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "ALTER TABLE Students\nADD COLUMN maiden-name CHAR(10)\nCUAPTER·.'3\nThe definition of Students is modified to add this column, and all existing rows\nare padded with null values in this column.\nALTER TABLE can also be used\nto delete columns and add or drop integrity constraints on a table; we do not\ndiscuss these aspects of the command beyond remarking that dropping columns\nis treated very similarly to dropping tables or views.\n3.8\nCASE STUDY: THE INTERNET STORE\nThe next design step in our running example, continued from Section 2.8, is\nlogical database design. Using the standard approach discussed in Chapter 3,\nDBDudes maps the ER diagram shown in Figure 2.20 to the relational model,\ngenerating the following tables:\nCREATE TABLE Books ( isbn\nCHAR ( 10) ,\ntitle\nCHAR(80) ,\nauthor\nCHAR(80),\nqty_in-stock\nINTEGER,\nprice\nREAL,\nyeaLpublished\nINTEGER,\nPRIMARY KEY (isbn))\nCREATE TABLE Orders ( isbn\nCHAR (10) ,\nciel\nINTEGER,\ncarelnum\nCHAR (16) ,\nqty\nINTEGER,\norder_date\nDATE,\nship_date\nDATE,\nPRIMARY KEY (isbn,cid),\nFOREIGN KEY (isbn) REFERENCES Books,\nFOREIGN KEY (cid) REFERENCES Customers)\nCREATE TABLE Customers ( cid\nINTEGER,\ncname\nCHAR(80),\naddress\nCHAR(200),\nPRIMARY KEY (cid)\nThe design team leader, who is still brooding over the fact that the review\nexposed a flaw in the design, now has an inspiration. The Orders table contains\nthe field order_date and the key for the table contains only the fields isbn and\nc'id. Because of this, a customer cannot order the same book OIl different days,\n\nThe Relat'ional l1;lodel\n9~\na re.striction that was not intended. vVhy not add the order-date attribute to\nthe key for the Orders table? This would eliminate the unwanted restrietion:\nCREATE TABLE Orders (\nisbn\nCHAR(10) ,\nPRIMARY KEY (isbn,cid,ship_date),\n...)\nThe reviewer, Dude 2, is not entirely happy with this solution, which he calls\na 'hack'.\nHe points out that no natural ER diagram reflects this design and\nstresses the importance of the ER diagram\n&<; a design do·cument.\nDude 1\nargues that, while Dude 2 has a point, it is important to present B&N with\na preliminary design and get feedback; everyone agrees with this, and they go\nback to B&N.\nThe owner of B&N now brings up some additional requirements he did not\nmention during the initial discussions: \"Customers should be able to purchase\nseveral different books in a single order. For example, if a customer wants to\npurchase three copies of 'The English Teacher' and two copies of 'The Character\nof Physical Law,' the customer should be able to place a single order for both\nbooks.\"\nThe design team leader, Dude 1, asks how this affects the shippping policy.\nDoes B&N still want to ship all books in an order together?\nThe owner of\nB&N explains their shipping policy: \"As soon as we have have enough copies\nof an ordered book we ship it, even if an order contains several books. So it\ncould happen that the three copies of 'The English Teacher' are shipped today\nbecause we have five copies in stock, but that 'The Character of Physical Law'\nis shipped tomorrow, because we currently have only one copy in stock and\nanother copy arrives tomorrow.\nIn addition, my customers could place more\nthan one order per day, and they want to be able to identify the orders they\nplaced.\"\nThe DBDudes team thinks this over and identifies two new requirements: First,\nit must be possible to order several different books in a single order and sec-\nond, a customer must be able to distinguish between several orders placed the\nsame day. To accomodate these requirements, they introduce a new attribute\ninto the Orders table called ordernum, which uniquely identifies an order and\ntherefore the customer placing the order. However, since several books could be\npurchased in a single order, onleTnum and isbn are both needed to determine\nqt.y and ship_dat.e in the Orders table.\nOrders are assign(~d order numbers sequentially and orders that are placed later\nhave higher order numbers. If several orders are placed by the same customer\n\nCHAPTER03\non a single day, these orders have different order numbers and can thus be\ndistinguished. The SQL DDL statement to create the modified Orders table\nfollows:\nCREATE TABLE Orders ( ordernum\nINTEGER,\nisbn\nCHAR(10),\ndd\nINTEGER,\ncardnum\nCHAR (16) ,\nqty\nINTEGER,\nordeLdate\nDATE,\nship~date\nDATE,\nPRIMARY KEY (ordernum, isbn),\nFOREIGN KEY (isbn) REFERENCES Books\nFOREIGN KEY (dd) REFERENCES Customers)\nThe owner of B&N is quite happy with this design for Orders, but has realized\nsomething else. (DBDudes is not surprised; customers almost always come up\nwith several new requirements as the design progresses.) While he wants all\nhis employees to be able to look at the details of an order, so that they can\nrespond to customer enquiries, he wants customers' credit card information to\nbe secure. To address this concern, DBDudes creates the following view:\nCREATE VIEW OrderInfo (isbn, cid, qty, order-date, ship_date)\nAS SELECT O.cid, O.qty, O.ordeLdate, O.ship_date\nFROM\nOrders 0\nThe plan is to allow employees to see this table, but not Orders; the latter is\nrestricted to B&N's Accounting division. We'll see how this is accomplished in\n3.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhat is a relation? Differentiate between a relation schema and a relation\ninstance. Define the terms arity and degree of a relation. What are domain\nconstraints? (Section 3.1)\n•\nWhat SQL construct enables the definition of a relation? \\Vhat constructs\nallow modification of relation instances? (Section 3.1.1)\n•\n\\Vhat are integrity constraints? Define the terms primary key constTa'int\nand foreign key constraint. How are these constraints expressed in SQL?\nWhat other kinds of constraints can we express in SQL? (Section 3.2)",
          "pages": [
            127,
            128,
            129
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "SQL (Structured Query Language) is a programming language used for managing and manipulating relational databases. It allows users to create, retrieve, update, and delete data from databases efficiently.",
        "explanation": "SQL is essential for database management because it provides a standardized way to interact with databases. Here’s how it works and when to use it:\n\n1. **What problem does SQL solve?** SQL addresses the need for efficient data management by allowing users to perform complex operations on large datasets without needing to manually handle each record.\n\n2. **How does it work?** SQL uses a set of commands (like SELECT, INSERT, UPDATE, DELETE) to interact with databases. Each command is designed to perform a specific task, such as retrieving data that meets certain criteria or modifying existing data.\n\n3. **When to use it?** Use SQL whenever you need to manage a relational database. This includes creating new databases, adding or removing data, updating records, and querying data based on specific conditions.\n\n4. **Key things to remember:** Always ensure your SQL queries are well-structured and properly formatted. Common mistakes include forgetting to close parentheses or using incorrect syntax.",
        "key_points": [
          "SQL is used for managing relational databases",
          "It uses commands like SELECT, INSERT, UPDATE, DELETE",
          "Always ensure proper formatting and correct syntax",
          "Use SQL for complex data operations on large datasets"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- SELECT all employees FROM the Employees TABLE SELECT * FROM Employees;",
            "explanation": "This example demonstrates how to retrieve all records from a table.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Find all employees who work in the 'Sales' department SELECT name, ssn FROM Employees WHERE dept_id = (SELECT did FROM Departments WHERE dname = 'Sales');",
            "explanation": "This practical example shows how to use a subquery to filter data based on related tables.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to close parentheses",
            "incorrect_code": "-- Incorrect query SELECT name, ssn FROM Employees WHERE dept_id = (SELECT did FROM Departments WHERE dname = 'Sales';",
            "correct_code": "-- Corrected query\nSELECT name, ssn FROM Employees WHERE dept_id = (SELECT did FROM Departments WHERE dname = 'Sales')",
            "explanation": "This mistake can lead to syntax errors. Always ensure all parentheses are properly closed."
          }
        ],
        "practice": {
          "question": "Write a SQL query that selects the names and social security numbers of all employees who work in departments with a budget greater than $50,000.",
          "solution": "-- Solution\nSELECT e.name, e.ssn FROM Employees e JOIN DepLMgr d ON e.ssn = d.ssn WHERE d.budget > 50000;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER'3\nManages and WorksJn\nCREATE TABLE DepLMgr ( did\nINTEGER,\ndname\nCHAR(20) ,\nbudget\nREAL,\nssn\nCHAR(11) NOT NULL,\nsince\nDATE,\nPRIMARY KEY (did),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE N",
      "content_relevance": {
        "score": 0.7,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "select-basic": {
      "id": "select-basic",
      "title": "SELECT Statement Basics",
      "definition": "Retrieving data from one or more tables using the SELECT statement",
      "difficulty": "beginner",
      "page_references": [
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER'3\nManages and WorksJn\nCREATE TABLE DepLMgr ( did\nINTEGER,\ndname\nCHAR(20) ,\nbudget\nREAL,\nssn\nCHAR(11) NOT NULL,\nsince\nDATE,\nPRIMARY KEY (did),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE NO ACTION)\nIt also captures the participation constraint that every department must have\na manager: Because ssn cannot take on null values, each tuple of DepLMgr\nidentifies a tuple in Employees (who is the manager). The NO ACTION specifi-\ncation, which is the default and need not be explicitly specified, ensures that\nan Employees tuple cannot be deleted while it is pointed to by a Dept-Mgr\ntuple. If we wish to delete such an Employees tuple, we must first change the\nDepLMgr tuple to have a new employee &'3 manager. (vVe could have specified\nCASCADE instead of NO ACTION, but deleting all information about a department\njust because its manager has been fired seems a bit extreme!)\nThe constraint that every department must have a manager cannot be cap-\ntured using the first translation approach discussed in Section 3.5.3.\n(Look\nat the definition of lVIanages and think about what effect it would have if we\nadded NOT NULL constraints to the ssn and did fields.\nHint: The constraint\nwould prevent the firing of a manager, but does not ensure that a manager is\ninitially appointed for each department!) This situation is a strong argument\n\nThe Relational lvfodel\n8~\nin favor of using the second approach for one-to-many relationships such as\nManages, especially when the entity set with the key constraint also has a total\nparticipation constraint.\nUnfortunately, there are many participation constraints that we cannot capture\nusing SQL, short of using table constraints or assertions. Table constraints and\nassertions can be specified using the full power of the SQL query language\n(as discussed in Section 5.7) and are very expressive but also very expensive to\ncheck and enforce. For example, we cannot enforce the participation constraints\non the \\iVorks_In relation without using these general constraints. To see why,\nconsider the Works-ln relation obtained by translating the ER diagram into·\nrelations.\nIt contains fields ssn and did, which are foreign keys referring to\nEmployees and Departments. To ensure total participation of Departments in\nWorks_In, we have to guarantee that every did value in Departments appears\nin a tuple of Works_In. We could try to guarantee this condition by declaring\nthat did in Departments is a foreign key referring to Works_In, but this is not\na valid foreign key constraint because did is not a candidate key for Works_In.\nTo ensure total participation of Departments in Works_In using SQL, we need\nan assertion. We have to guarantee that every did value in Departments appears\nin a tuple of Works_In; further, this tuple of Works_In must also have non-null\nvalues in the fields that are foreign keys referencing other entity sets involved in\nthe relationship (in this example, the ssn field). We can ensure the second part\nof this constraint by imposing the stronger requirement that ssn in Works-ln\ncannot contain null values.\n(Ensuring that the participation of Employees in\nWorks_In is total is symmetric.)\nAnother constraint that requires assertions to express in SQL is the requirement\nthat each Employees entity (in the context of the Manages relationship set)\nmust manage at least one department.\nIn fact, the Manages relationship set exemplifies most of the participation con-\nstraints that we can capture using key and foreign key constraints. Manages is\na binary relationship set in which exactly one of the entity sets (Departments)\nhas a key constraint, and the total participation constraint is expressed on that\nentity set.\n\\Ve can also capture participation constraints using key and foreign key con-\nstraints in one other special situation: a relationship set in which all participat-\ning entity sets have key constraints and total participation. The best translation\napproach in this case is to map all the entities &'3 well as the relationship into\na single table; the details are straightforward.\n\nCHAPTER~3\n3.5.5\nTranslating Weak Entity Sets\nA weak entity set always participates in a one-to-many binary relationship and\nhas a key constraint and total participation. The second translation approach\ndiscussed in Section 3.5.3 is ideal in this case, but we must take into account\nthat the weak entity has only a partial key.\nAlso, when an owner entity is\ndeleted, we want all owned weak entities to be deleted.\nConsider the Dependents weak entity set shown in Figure 3.14, with partial\nkey pname. A Dependents entity can be identified uniquely only if we take the\nkey of the owning Employees entity and the pname of the Dependents entity,\nand the Dependents entity must be deleted if the owning Employees entity is\ndeleted.\nEmployees\nThe Dependents Weak Entity Set\nWe can capture the desired semantics with the following definition of the\nDep_Policy relation:\nCREATE TABLE Dep_Policy (pname\nCHAR(20) ,\nage\nINTEGER,\ncost\nREAL,\nssn\nCHAR (11) ,\nPRIMARY KEY (pname, ssn),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE CASCADE )\nObserve that the primary key is (pna:me, ssn) , since Dependents is a weak\nentity. This constraint is a change with respect to the translation discussed in\n\\Ve have to ensure that every Dependents entity is associated\nwith an Employees entity (the owner), as per the total participation constraint\non Dependents. That is, ssn cannot be null.\nThis is ensured because\nSST/, is\npart of the primary key. The CASCADE option ensures that information about\nan employee's policy and dependents is deleted if the corresponding Employees\ntuple is deleted.\n\nThe Relational 1\"1,,1oriel\n3.5.6\nTranslating Class Hierarchies\nWe present the two basic approaches to handling ISA hierarchies by applying\nthem to the ER diagram shown in Figure 3.15:\nClass Hierarchy\n1. We can map each of the entity sets Employees, Hourly_Emps, and Con-\ntracLEmps to a distinct relation.\nThe Employees relation is created as\nin Section 2.2.\nWe discuss Hourly~mps here; ContracLEmps is han-\ndled similarly.\nThe relation for Hourly_Emps includes the hourly_wages\nand hours_worked attributes of Hourly_Emps. It also contains the key at-\ntributes of the superclass (ssn, in this example), which serve as the primary\nkey for Hourly_Emps, a.', well as a foreign key referencing the superclass\n(Employees).\nFor each Hourly_Emps entity, the value of the name and\nlot attributes are stored in the corresponding row of the supercla...,s (Em-\nployees). Note that if the superclass tuple is deleted, the delete must be\ncascaded to Hourly~mps.\n2. Alternatively, we can create just two relations, corresponding to Hourly_Emps\nand ContracLEmps.\nThe relation for Hourly~mps includes all the at-\ntributes of Hourly_Emps as well as all the attributes of Employees (i.e.,\nssn, name, lot, hO'l.1,rly_wages, hours_worked:).\nThe first approach is general and always applicable. Queries in which we want\nto (~xamine all employees and do not care about the attributes specific to the\nsubclasses are handled easily using the Employees relation. However, queries\nin which we want to examine, say, hourly employees, may require us to com-\nbine Hourly_Emps (or ContracLEmps, as the case may be) with Employees to\nretrieve name and lot.",
          "pages": [
            115,
            116,
            117,
            118
          ],
          "relevance": {
            "score": 0.37,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER··~\nThe second approach is not applicable if we have employees who are neither\nhourly employees nor contract employees, since there is no way to store such\nemployees. Also, if an employee is both an Hourly-.Emps and a ContracLEmps\nentity, then the name and lot: values are stored twice. This duplication can lead\nto some of the anomalies that we discuss in Chapter 19. A query that needs to\nexamine all employees must now examine two relations. On the other hand, a\nquery that needs to examine only hourly employees can now do so by examining\njust one relation. The choice between these approaches clearly depends on the\nsemantics of the data and the frequency of common operations.\nIn general, overlap and covering constraints can be expressed in SQL only by\nusing assertions.\n3.5.7\nTranslating ER Diagrams with Aggregation\nConsider the ER diagram shown in Figure 3.16.\nThe Employees, Projects,\nManilars\nDepartments\nI\n_______did~ fT:C~'~~)\nAggregation\nand Departments entity sets and the Sponsors relationship set are mapped as\ndescribed in previous sections. For the Monitors relationship set, we create a\nrelation with the following attributes: the key attributes of Employees (88n), the\nkey attributes of Sponsors (d'id, p'id), and the descriptive attributes of Monitors\n('/.tnt:'il). This translation is essentially the standard mapping for a relationship\nset, as described in Section 3.5.2.\n\nThe Relational A!odd\n~\nThere is a special case in which this translation can be refined by dropping the\nSponsors relation. Consicler the Sponsors relation. It has attributes pid, did,\nand since; and in general we need it (in addition to l\\rlonitors) for two reasons:\n1. \\Ve have to record the descriptive attributes (in our example, since) of the\nSponsors relationship.\n2. Not every sponsorship has a monitor, and thus some (p'id, did) pairs in the\nSponsors relation may not appear in the Monitors relation.\nHowever, if Sponsors has no descriptive attributes and has total participation\nin Monitors, every possible instance of the Sponsors relation can be obtained\nfrom the (pid, did) columns of Monitors; Sponsors can be dropped.\n3.5.8\nER to Relational: Additional Examples\nConsider the ER diagram shown in Figure 3.17. We can use the key constraints\nPolicy Revisited\nto combine Purchaser information with Policies and Beneficiary information\nwith Dependents, and translate it into the relational model as follows:\nCREATE TABLE Policies ( policyid INTEGER,\ncost\nREAL,\nssn\nCHAR (11) NOT NULL,\nPRIMARY KEY (policyid),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE CASCADE )\n\nCHAPTERB\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nage\nINTEGER,\npolicyid INTEGER,\nPRIMARY KEY (pname, policyid),\nFOREIGN KEY (policyid) REFERENCES Policies\nON DELETE CASCADE)\nNotice how the deletion of an employee leads to the deletion of all policies\nowned by the employee and all dependents who are beneficiaries of those poli-\ncies. Further, each dependent is required to have a covering policy-because\npolicyid is part of the primary key of Dependents, there is an implicit NOT NULL\nconstraint. This model accurately reflects the participation constraints in the\nER diagram and the intended actions when an employee entity is deleted.\nIn general, there could be a chain of identifying relationships for weak entity\nsets. For example, we assumed that policyid uniquely identifies a policy. Sup-\npose that policyid distinguishes only the policies owned by a given employee;\nthat is, policyid is only a partial key and Policies should be modeled as a weak\nentity set. This new assumption about policyid does not cause much to change\nin the preceding discussion.\nIn fact, the only changes are that the primary\nkey of Policies becomes (policyid, ssn) , and as a consequence, the definition of\nDependents changes-a field called ssn is added and becomes part of both the\nprimary key of Dependents and the foreign key referencing Policies:\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nssn\nCHAR (11) ,\nage\nINTEGER,\npolicyid INTEGER NOT NULL,\nPRIMARY KEY (pname, policyid, ssn),\nFOREIGN KEY (policyid, ssn) REFERENCES Policies\nON DELETE CASCADE )\n3.6\nINTRODUCTION TO VIEWS\nA view is a table whose rows are not explicitly stored in the database but\nare computed as needed from a view definition. Consider the Students and\nEnrolled relations. Suppose we are often interested in finding the names and\nstudent identifiers of students who got a grade of B in some course, together\nwith the course identifier. \\Ne can define a view for this purpose. Using SQL\nnotation:\nCREATE VIEW B-Students (name, sid, course)\nAS SELECT S.sname, S.sid, E.cid\n\nThe Relational 1I1odel\nFROM\nWHERE\nStudents S, Enrolled E\nS.sid = E.studid AND E.grade = 'B'\n$\nThe view B-Students has three fields called name, sid, and course with the\nsame domains as the fields sname and sid in Students and cid in Enrolled.\n(If the optional arguments name, sid, and course are omitted from the CREATE\nVIEW statement, the column names sname, sid, and cid are inherited.)\nThis view can be used just like a base table, or explicitly stored table, in\ndefining new queries or views. Given the instances of Enrolled and Students\nshown in Figure 3.4, B-Students contains the tuples shown in Figure 3.18.\nConceptually, whenever B-Students is used in a query, the view definition is\nfirst evaluated to obtain the corresponding instance of B-Students, then the rest\nof the query is evaluated treating B-Students like any other relation referred\nto in the query. (We discuss how queries on views are evaluated in practice in\nChapter 25.)\nsid\ncourse\nHistory105\nReggae203\nAn Instance of the B-Students View\n3.6.1\nViews, Data Independence, Security\nConsider the levels of abstraction we discussed in Section 1.5.2. The physical\nschema for a relational database describes how the relations in the conceptual\nschema are stored, in terms of the file organizations and indexes used.\nThe\nconceptual schema is the collection of schemas of the relations stored in the\ndatabase. While some relations in the conceptual schema can also be exposed to\napplications, that is, be part of the exte'mal schema of the database, additional\nrelations in the external schema can be defined using the view mechanism.\nThe view mechanism thus provides the support for logical data independence\nin the relational model.\nThat is, it can be used to define relations in the\nexternal schema that mask changes in the conceptual schema of the database\nfrom applications. For example, if the schema of a stored relation is changed,\nwe can define a view with the old schema and applications that expect to see\nthe old schema can now use this view.\nViews are also valuable in the context of security: We can define views that\ngive a group of users access to just the information they are allowed to see. For\nexample, we can define a view that allows students to see the other students'\n\nCHAPTER B\nname and age but not their gpa, and allows all students to access this view but\nnot the underlying Students table (see Chapter 21).\n3.6.2\nUpdates on Views\nThe motivation behind the view mechanism is to tailor how users see the data.\nUsers should not have to worry about the view versus base table distinction.\nThis goal is indeed achieved in the case of queries on views; a view can be used\njust like any other relation in defining a query. However, it is natural to want to\nspecify updates on views as well. Here, unfortunately, the distinction between\na view and a ba.se table must be kept in mind.\nThe SQL-92 standard allows updates to be specified only on views that are\ndefined on a single base table using just selection and projection, with no use of\naggregate operations.3 Such views are called updatable views. This definition\nis oversimplified, but it captures the spirit of the restrictions.\nAn update on\nsuch a restricted view can always be implemented by updating the underlying\nbase table in an unambiguous way. Consider the following view:\nCREATE VIEW GoodStudents (sid, gpa)\nAS SELECT S.sid, S.gpa\nFROM\nStudents S\nWHERE\nS.gpa> 3.0\nWe can implement a command to modify the gpa of a GoodStudents row by\nmodifying the corresponding row in Students. We can delete a GoodStudents\nrow by deleting the corresponding row from Students. (In general, if the view\ndid not include a key for the underlying table, several rows in the table could\n'correspond' to a single row in the view. This would be the case, for example,\nif we used S.sname instead of S.sid in the definition of GoodStudents. A com-\nmand that affects a row in the view then affects all corresponding rows in the\nunderlying table.)\nWe can insert a GoodStudents row by inserting a row into Students, using\nnull values in columns of Students that do not appear in GoodStudents (e.g.,\nsname, login). Note that primary key columns are not allowed to contain null\nvalues. Therefore, if we attempt to insert rows through a view that does not\ncontain the primary key of the underlying table, the insertions will be rejected.\nFor example, if GoodStudents contained snarne but not ,c;id, we could not insert\nrows into Students through insertions to GooclStudents.\n3There is also the restriction that the DISTINCT operator cannot be used in updatable vi(;w defi-\nnitions. By default, SQL does not eliminate duplicate copies of rows from the result of it query; the\nDISTINCT operator requires duplicate elimination. vVe discuss t.his point further in Chapt.er 5.\n\nThe Relational A10dd\n----------------~-\nUpdatable Views in SQL:1999 The Hew SQL standard has expanded\nthe class of view definitions that are\nupdatable~ taking primary . key\nconstraints into account.\nIn contra..')t·to SQL-92~ a· view definition that\ncontains more than OIle table in the FROM clause may be updatable under\nthe new definition.\nIntuitively~ we can update afield of a. view if it is\nobtained from exactly one of the underlying tables, and the primary key\nof that table is included in the fields of the view.\nSQL:1999 distinguishes between views whose rows can be modified (updat-\nable views) and views into which new rows can be inserted (insertable-\ninto views): Views defined using the SQL constructs UNION, INTERSECT,\nand EXCEPT (which we discuss in Chapter 5) cannot be inserted into, even\nif they are updatable. Intuitively, updatability ensures that an updated\ntuple in the view can be traced to exactly one tuple in one of the tables\nused to define the view. The updatability property, however, may still not\nenable us to decide into which table to insert a new tuple.\nAn important observation is that an INSERT or UPDATE may change the un-\nderlying base table so that the resulting (i.e., inserted or modified) row is not\nin the view! For example, if we try to insert a row (51234, 2.8) into the view,\nthis row can be (padded with null values in the other fields of Students and\nthen) added to the underlying Students table, but it will not appear in the\nGoodStudents view because it does not satisfy the view condition gpa > 3.0.\nThe SQL default action is to allow this insertion, but we can disallow it by\nadding the clause WITH CHECK OPTION to the definition of the view.\nIn this\ncase, only rows that will actually appear in the view are permissible insertions.\nWe caution the reader, that when a view is defined in terms of another view,\nthe interaction between these view definitions with respect to updates and the\nCHECK OPTION clause can be complex; we not go into the details.\nNeed to Restrict View Updates\nvVhile the SQL rules on updatable views are more stringent than necessary,\nthere are some fundamental problems with updates specified on views and good\nreason to limit the class of views that can be updated. Consider the Students\nrelation and a new relation called Clubs:\nClubs( cname: string, jyear: date, mnarne: string)\n\n~\nSailing\nDave\nHiking\nSmith\nRowing\nSmith\nDave\nJones\nSmith\nSmith\ndave(gcs\njones~~cs\nsmith@ee\nsmith@math\nCHAPTER 8\nAn Instance C of Clubs\nI name\n,. login\nAn Instance 53 of Students\nI dub\nsince\nDave\ndave@cs\nSailing\nSmith\nsmith@ee\nHiking\nSmith\nsmith@ee\nRowing\nSmith\nsmith@math\nHiking\nSmith\nsmith@math\nRowing\nInstance of ActiveStudents\nA tuple in Clubs denotes that the student called mname has been a member of\nthe club cname since the date jyear.4 Suppose that we are often interested in\nfinding the names and logins of students with a gpa greater than 3 who belong\nto at least one club, along with the club name and the date they joined the\nclub. We can define a view for this purpose:\nCREATE VIEW ActiveStudents (name, login, club, since)\nAS SELECT S.sname, S.login, C.cname, C.jyear\nFROM\nStudents S, Clubs C\nWHERE\nS.sname = C.mname AND S.gpa> 3\nConsider the instances of Students and Clubs shown in Figures 3.19 and 3.20.\nWhen evaluated using the instances C and S3, ActiveStudents contains the\nrows shown in Figure 3.21.\nNow suppose that we want to delete the row (Smith, smith@ee, Hiking, 1997)\nfrom ActiveStudents.\nHow are we to do this?\nActiveStudents rows are not\nstored explicitly but computed as needed from the Students and Clubs tables\nusing the view definition.\nSo we must change either Students or Clubs (or\nboth) in such a way that evaluating the view definition on the modified instance\ndoes not produce the row (Snrith, 8Tnith@ec, Hiking, 1997.) This ta.sk can be\nctccomplished in one of two ways:\nby either deleting the row (53688.. Sm'ith,\n8Tn'ith(iJ)ee, 18, ,g.2) from Students or deleting the row (Hiking, 1.997, 8m/ith)\nclvVe remark that Clubs has a poorly designed schema (chosen for the sake of our discussion of view\nupdates), since it identifies students by name, which is not a candidate key for Students.",
          "pages": [
            119,
            120,
            121,
            122,
            123,
            124,
            125
          ],
          "relevance": {
            "score": 0.62,
            "sql_score": 1.0,
            "concept_score": 0.83,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "The Relational tv!odel\n9J\nfrom Clubs. But neither solution is satisfactory. Removing the Students row\nhas the effect of also deleting the row (8m:ith, smith@ee, Rowing, 1998) from the\nview ActiveStudents. Removing the Clubs row h&'3 the effect of also deleting the\nrow (Smith, smith@math, Hiking, 1991) from the view ActiveStudents. Neither\nside effect is desirable. In fact, the only reasonable solution is to d'isallow such\nupdates on views.\nViews involving more than one base table can, in principle, be safely updated.\nThe B-Students view we introduced at the beginning of this section is an ex-\nample of such a view.\nConsider the instance of B-Students shown in Figure\n3.18 (with, of course, the corresponding instances of Students and Enrolled as\nin Figure 3.4). To insert a tuple, say (Dave, 50000, Reggae203) B-Students, we\ncan simply insert a tuple (Reggae203, B, 50000) into Enrolled since there is al-\nready a tuple for sid 50000 in Students. To insert (John, 55000, Reggae203), on\nthe other hand, we have to insert (Reggae203, B, 55000) into Enrolled and also\ninsert (55000, John, null, null, null) into Students. Observe how null values\nare used in fields of the inserted tuple whose value is not available. Fortunately,\nthe view schema contains the primary key fields of both underlying base tables;\notherwise, we would not be able to support insertions into this view. To delete\na tuple from the view B-Students, we can simply delete the corresponding tuple\nfrom Enrolled.\nAlthough this example illustrates that the SQL rules on updatable views are\nunnecessarily restrictive, it also brings out the complexity of handling view\nupdates in the general case. For practical reasons, the SQL standard has chosen\nto allow only updates on a very restricted class of views.\n3.7\nDESTROYING/ALTERING TABLES AND VIEWS\nIf we decide that we no longer need a base table and want to destroy it (i.e.,\ndelete all the rows and remove the table definition information), we can use\nthe DROP TABLE command. For example, DROP TABLE Students RESTRICT de-\nstroys the Students table unless some view or integrity constraint refers to\nStudents; if so, the command fails. If the keyword RESTRICT is replaced by\nCASCADE, Students is dropped and any referencing views or integrity constraints\nare (recursively) dropped as well; one of these t\\VO keyvlOrds must always be\nspecified. A vipw can be dropped using the DROP VIEW command, which is just\nlike DROP TABLE.\nALTER TABLE modifies the structure of an existing table.\nTo add a column\ncalled maiden-name to Students, for example, we would use the following com-\nmand:\n\nALTER TABLE Students\nADD COLUMN maiden-name CHAR(10)\nCUAPTER·.'3\nThe definition of Students is modified to add this column, and all existing rows\nare padded with null values in this column.\nALTER TABLE can also be used\nto delete columns and add or drop integrity constraints on a table; we do not\ndiscuss these aspects of the command beyond remarking that dropping columns\nis treated very similarly to dropping tables or views.\n3.8\nCASE STUDY: THE INTERNET STORE\nThe next design step in our running example, continued from Section 2.8, is\nlogical database design. Using the standard approach discussed in Chapter 3,\nDBDudes maps the ER diagram shown in Figure 2.20 to the relational model,\ngenerating the following tables:\nCREATE TABLE Books ( isbn\nCHAR ( 10) ,\ntitle\nCHAR(80) ,\nauthor\nCHAR(80),\nqty_in-stock\nINTEGER,\nprice\nREAL,\nyeaLpublished\nINTEGER,\nPRIMARY KEY (isbn))\nCREATE TABLE Orders ( isbn\nCHAR (10) ,\nciel\nINTEGER,\ncarelnum\nCHAR (16) ,\nqty\nINTEGER,\norder_date\nDATE,\nship_date\nDATE,\nPRIMARY KEY (isbn,cid),\nFOREIGN KEY (isbn) REFERENCES Books,\nFOREIGN KEY (cid) REFERENCES Customers)\nCREATE TABLE Customers ( cid\nINTEGER,\ncname\nCHAR(80),\naddress\nCHAR(200),\nPRIMARY KEY (cid)\nThe design team leader, who is still brooding over the fact that the review\nexposed a flaw in the design, now has an inspiration. The Orders table contains\nthe field order_date and the key for the table contains only the fields isbn and\nc'id. Because of this, a customer cannot order the same book OIl different days,",
          "pages": [
            126,
            127
          ],
          "relevance": {
            "score": 0.37,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The SELECT statement is used to retrieve data from a database. It allows you to specify which columns and rows of data you want to see.",
        "explanation": "The SELECT statement is one of the most fundamental parts of SQL. It enables you to filter, sort, and organize data in your database. Here's how it works:\n1. **Specify Columns**: You can choose specific columns from a table or use an asterisk (*) to select all columns.\n2. **Filter Data**: Use WHERE clause to filter rows based on conditions.\n3. **Sort Data**: Use ORDER BY to sort the results in ascending or descending order.\n4. **Limit Results**: Use LIMIT to restrict the number of rows returned.\n5. **Group and Aggregate**: Use GROUP BY and aggregate functions like COUNT, SUM, AVG to perform calculations on groups of data.\nWhen to use it: Whenever you need to access specific information from your database. It's used in almost every query you write.\nKey things to remember:\n- Always specify columns instead of using * for better performance.\n- Use WHERE clause carefully to avoid unnecessary data retrieval.\n- ORDER BY is useful for presenting data in a readable format.\nCommon pitfall to avoid: Not understanding the difference between SELECT and UPDATE. Selecting data doesn't change it, but updating does.\nBest practice or tip: Always test your queries with LIMIT 10 first to ensure they're working as expected before running them on large datasets.",
        "key_points": [
          "Always specify columns instead of using * for better performance",
          "Use WHERE clause carefully to avoid unnecessary data retrieval",
          "ORDER BY is useful for presenting data in a readable format",
          "Not understanding the difference between SELECT and UPDATE",
          "Always test your queries with LIMIT 10 first"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- SELECT all columns FROM employees SELECT * FROM Employees;",
            "explanation": "This example retrieves all data from the 'Employees' table. It's useful for getting an overview of your data.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- SELECT specific columns AND filter by age SELECT name, salary FROM Employees WHERE age > 30;",
            "explanation": "This practical example retrieves the names and salaries of employees who are older than 30. It demonstrates how to specify columns and use a WHERE clause.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using * instead of specific column names",
            "incorrect_code": "-- Incorrect way\nSELECT * FROM Employees;",
            "correct_code": "-- Correct way\nSELECT name, salary FROM Employees;",
            "explanation": "Selecting all columns can be inefficient if you only need a few. It's better to specify the columns you need."
          }
        ],
        "practice": {
          "question": "Create a query that selects the names and average salaries of employees in each department, sorted by average salary in descending order.",
          "solution": "-- Solution\nSELECT department, AVG(salary) AS avg_salary FROM Employees GROUP BY department ORDER BY avg_salary DESC;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER'3\nManages and WorksJn\nCREATE TABLE DepLMgr ( did\nINTEGER,\ndname\nCHAR(20) ,\nbudget\nREAL,\nssn\nCHAR(11) NOT NULL,\nsince\nDATE,\nPRIMARY KEY (did),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE N",
      "content_relevance": {
        "score": 0.7,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "where-clause": {
      "id": "where-clause",
      "title": "WHERE Clause and Filtering",
      "definition": "Filtering rows using conditions with WHERE, comparison operators, and logical operators",
      "difficulty": "beginner",
      "page_references": [
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129,
        130,
        131,
        132,
        133,
        134,
        135
      ],
      "sections": {
        "definition": {
          "text": "The Relational A!odd\n~\nThere is a special case in which this translation can be refined by dropping the\nSponsors relation. Consicler the Sponsors relation. It has attributes pid, did,\nand since; and in general we need it (in addition to l\\rlonitors) for two reasons:\n1. \\Ve have to record the descriptive attributes (in our example, since) of the\nSponsors relationship.\n2. Not every sponsorship has a monitor, and thus some (p'id, did) pairs in the\nSponsors relation may not appear in the Monitors relation.\nHowever, if Sponsors has no descriptive attributes and has total participation\nin Monitors, every possible instance of the Sponsors relation can be obtained\nfrom the (pid, did) columns of Monitors; Sponsors can be dropped.\n3.5.8\nER to Relational: Additional Examples\nConsider the ER diagram shown in Figure 3.17. We can use the key constraints\nPolicy Revisited\nto combine Purchaser information with Policies and Beneficiary information\nwith Dependents, and translate it into the relational model as follows:\nCREATE TABLE Policies ( policyid INTEGER,\ncost\nREAL,\nssn\nCHAR (11) NOT NULL,\nPRIMARY KEY (policyid),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE CASCADE )\n\nCHAPTERB\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nage\nINTEGER,\npolicyid INTEGER,\nPRIMARY KEY (pname, policyid),\nFOREIGN KEY (policyid) REFERENCES Policies\nON DELETE CASCADE)\nNotice how the deletion of an employee leads to the deletion of all policies\nowned by the employee and all dependents who are beneficiaries of those poli-\ncies. Further, each dependent is required to have a covering policy-because\npolicyid is part of the primary key of Dependents, there is an implicit NOT NULL\nconstraint. This model accurately reflects the participation constraints in the\nER diagram and the intended actions when an employee entity is deleted.\nIn general, there could be a chain of identifying relationships for weak entity\nsets. For example, we assumed that policyid uniquely identifies a policy. Sup-\npose that policyid distinguishes only the policies owned by a given employee;\nthat is, policyid is only a partial key and Policies should be modeled as a weak\nentity set. This new assumption about policyid does not cause much to change\nin the preceding discussion.\nIn fact, the only changes are that the primary\nkey of Policies becomes (policyid, ssn) , and as a consequence, the definition of\nDependents changes-a field called ssn is added and becomes part of both the\nprimary key of Dependents and the foreign key referencing Policies:\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nssn\nCHAR (11) ,\nage\nINTEGER,\npolicyid INTEGER NOT NULL,\nPRIMARY KEY (pname, policyid, ssn),\nFOREIGN KEY (policyid, ssn) REFERENCES Policies\nON DELETE CASCADE )\n3.6\nINTRODUCTION TO VIEWS\nA view is a table whose rows are not explicitly stored in the database but\nare computed as needed from a view definition. Consider the Students and\nEnrolled relations. Suppose we are often interested in finding the names and\nstudent identifiers of students who got a grade of B in some course, together\nwith the course identifier. \\Ne can define a view for this purpose. Using SQL\nnotation:\nCREATE VIEW B-Students (name, sid, course)\nAS SELECT S.sname, S.sid, E.cid\n\nThe Relational 1I1odel\nFROM\nWHERE\nStudents S, Enrolled E\nS.sid = E.studid AND E.grade = 'B'\n$\nThe view B-Students has three fields called name, sid, and course with the\nsame domains as the fields sname and sid in Students and cid in Enrolled.\n(If the optional arguments name, sid, and course are omitted from the CREATE\nVIEW statement, the column names sname, sid, and cid are inherited.)\nThis view can be used just like a base table, or explicitly stored table, in\ndefining new queries or views. Given the instances of Enrolled and Students\nshown in Figure 3.4, B-Students contains the tuples shown in Figure 3.18.\nConceptually, whenever B-Students is used in a query, the view definition is\nfirst evaluated to obtain the corresponding instance of B-Students, then the rest\nof the query is evaluated treating B-Students like any other relation referred\nto in the query. (We discuss how queries on views are evaluated in practice in\nChapter 25.)\nsid\ncourse\nHistory105\nReggae203\nAn Instance of the B-Students View\n3.6.1\nViews, Data Independence, Security\nConsider the levels of abstraction we discussed in Section 1.5.2. The physical\nschema for a relational database describes how the relations in the conceptual\nschema are stored, in terms of the file organizations and indexes used.\nThe\nconceptual schema is the collection of schemas of the relations stored in the\ndatabase. While some relations in the conceptual schema can also be exposed to\napplications, that is, be part of the exte'mal schema of the database, additional\nrelations in the external schema can be defined using the view mechanism.\nThe view mechanism thus provides the support for logical data independence\nin the relational model.\nThat is, it can be used to define relations in the\nexternal schema that mask changes in the conceptual schema of the database\nfrom applications. For example, if the schema of a stored relation is changed,\nwe can define a view with the old schema and applications that expect to see\nthe old schema can now use this view.\nViews are also valuable in the context of security: We can define views that\ngive a group of users access to just the information they are allowed to see. For\nexample, we can define a view that allows students to see the other students'\n\nCHAPTER B\nname and age but not their gpa, and allows all students to access this view but\nnot the underlying Students table (see Chapter 21).\n3.6.2\nUpdates on Views\nThe motivation behind the view mechanism is to tailor how users see the data.\nUsers should not have to worry about the view versus base table distinction.\nThis goal is indeed achieved in the case of queries on views; a view can be used\njust like any other relation in defining a query. However, it is natural to want to\nspecify updates on views as well. Here, unfortunately, the distinction between\na view and a ba.se table must be kept in mind.\nThe SQL-92 standard allows updates to be specified only on views that are\ndefined on a single base table using just selection and projection, with no use of\naggregate operations.3 Such views are called updatable views. This definition\nis oversimplified, but it captures the spirit of the restrictions.\nAn update on\nsuch a restricted view can always be implemented by updating the underlying\nbase table in an unambiguous way. Consider the following view:\nCREATE VIEW GoodStudents (sid, gpa)\nAS SELECT S.sid, S.gpa\nFROM\nStudents S\nWHERE\nS.gpa> 3.0\nWe can implement a command to modify the gpa of a GoodStudents row by\nmodifying the corresponding row in Students. We can delete a GoodStudents\nrow by deleting the corresponding row from Students. (In general, if the view\ndid not include a key for the underlying table, several rows in the table could\n'correspond' to a single row in the view. This would be the case, for example,\nif we used S.sname instead of S.sid in the definition of GoodStudents. A com-\nmand that affects a row in the view then affects all corresponding rows in the\nunderlying table.)\nWe can insert a GoodStudents row by inserting a row into Students, using\nnull values in columns of Students that do not appear in GoodStudents (e.g.,\nsname, login). Note that primary key columns are not allowed to contain null\nvalues. Therefore, if we attempt to insert rows through a view that does not\ncontain the primary key of the underlying table, the insertions will be rejected.\nFor example, if GoodStudents contained snarne but not ,c;id, we could not insert\nrows into Students through insertions to GooclStudents.\n3There is also the restriction that the DISTINCT operator cannot be used in updatable vi(;w defi-\nnitions. By default, SQL does not eliminate duplicate copies of rows from the result of it query; the\nDISTINCT operator requires duplicate elimination. vVe discuss t.his point further in Chapt.er 5.",
          "pages": [
            120,
            121,
            122,
            123
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "The Relational A10dd\n----------------~-\nUpdatable Views in SQL:1999 The Hew SQL standard has expanded\nthe class of view definitions that are\nupdatable~ taking primary . key\nconstraints into account.\nIn contra..')t·to SQL-92~ a· view definition that\ncontains more than OIle table in the FROM clause may be updatable under\nthe new definition.\nIntuitively~ we can update afield of a. view if it is\nobtained from exactly one of the underlying tables, and the primary key\nof that table is included in the fields of the view.\nSQL:1999 distinguishes between views whose rows can be modified (updat-\nable views) and views into which new rows can be inserted (insertable-\ninto views): Views defined using the SQL constructs UNION, INTERSECT,\nand EXCEPT (which we discuss in Chapter 5) cannot be inserted into, even\nif they are updatable. Intuitively, updatability ensures that an updated\ntuple in the view can be traced to exactly one tuple in one of the tables\nused to define the view. The updatability property, however, may still not\nenable us to decide into which table to insert a new tuple.\nAn important observation is that an INSERT or UPDATE may change the un-\nderlying base table so that the resulting (i.e., inserted or modified) row is not\nin the view! For example, if we try to insert a row (51234, 2.8) into the view,\nthis row can be (padded with null values in the other fields of Students and\nthen) added to the underlying Students table, but it will not appear in the\nGoodStudents view because it does not satisfy the view condition gpa > 3.0.\nThe SQL default action is to allow this insertion, but we can disallow it by\nadding the clause WITH CHECK OPTION to the definition of the view.\nIn this\ncase, only rows that will actually appear in the view are permissible insertions.\nWe caution the reader, that when a view is defined in terms of another view,\nthe interaction between these view definitions with respect to updates and the\nCHECK OPTION clause can be complex; we not go into the details.\nNeed to Restrict View Updates\nvVhile the SQL rules on updatable views are more stringent than necessary,\nthere are some fundamental problems with updates specified on views and good\nreason to limit the class of views that can be updated. Consider the Students\nrelation and a new relation called Clubs:\nClubs( cname: string, jyear: date, mnarne: string)\n\n~\nSailing\nDave\nHiking\nSmith\nRowing\nSmith\nDave\nJones\nSmith\nSmith\ndave(gcs\njones~~cs\nsmith@ee\nsmith@math\nCHAPTER 8\nAn Instance C of Clubs\nI name\n,. login\nAn Instance 53 of Students\nI dub\nsince\nDave\ndave@cs\nSailing\nSmith\nsmith@ee\nHiking\nSmith\nsmith@ee\nRowing\nSmith\nsmith@math\nHiking\nSmith\nsmith@math\nRowing\nInstance of ActiveStudents\nA tuple in Clubs denotes that the student called mname has been a member of\nthe club cname since the date jyear.4 Suppose that we are often interested in\nfinding the names and logins of students with a gpa greater than 3 who belong\nto at least one club, along with the club name and the date they joined the\nclub. We can define a view for this purpose:\nCREATE VIEW ActiveStudents (name, login, club, since)\nAS SELECT S.sname, S.login, C.cname, C.jyear\nFROM\nStudents S, Clubs C\nWHERE\nS.sname = C.mname AND S.gpa> 3\nConsider the instances of Students and Clubs shown in Figures 3.19 and 3.20.\nWhen evaluated using the instances C and S3, ActiveStudents contains the\nrows shown in Figure 3.21.\nNow suppose that we want to delete the row (Smith, smith@ee, Hiking, 1997)\nfrom ActiveStudents.\nHow are we to do this?\nActiveStudents rows are not\nstored explicitly but computed as needed from the Students and Clubs tables\nusing the view definition.\nSo we must change either Students or Clubs (or\nboth) in such a way that evaluating the view definition on the modified instance\ndoes not produce the row (Snrith, 8Tnith@ec, Hiking, 1997.) This ta.sk can be\nctccomplished in one of two ways:\nby either deleting the row (53688.. Sm'ith,\n8Tn'ith(iJ)ee, 18, ,g.2) from Students or deleting the row (Hiking, 1.997, 8m/ith)\nclvVe remark that Clubs has a poorly designed schema (chosen for the sake of our discussion of view\nupdates), since it identifies students by name, which is not a candidate key for Students.\n\nThe Relational tv!odel\n9J\nfrom Clubs. But neither solution is satisfactory. Removing the Students row\nhas the effect of also deleting the row (8m:ith, smith@ee, Rowing, 1998) from the\nview ActiveStudents. Removing the Clubs row h&'3 the effect of also deleting the\nrow (Smith, smith@math, Hiking, 1991) from the view ActiveStudents. Neither\nside effect is desirable. In fact, the only reasonable solution is to d'isallow such\nupdates on views.\nViews involving more than one base table can, in principle, be safely updated.\nThe B-Students view we introduced at the beginning of this section is an ex-\nample of such a view.\nConsider the instance of B-Students shown in Figure\n3.18 (with, of course, the corresponding instances of Students and Enrolled as\nin Figure 3.4). To insert a tuple, say (Dave, 50000, Reggae203) B-Students, we\ncan simply insert a tuple (Reggae203, B, 50000) into Enrolled since there is al-\nready a tuple for sid 50000 in Students. To insert (John, 55000, Reggae203), on\nthe other hand, we have to insert (Reggae203, B, 55000) into Enrolled and also\ninsert (55000, John, null, null, null) into Students. Observe how null values\nare used in fields of the inserted tuple whose value is not available. Fortunately,\nthe view schema contains the primary key fields of both underlying base tables;\notherwise, we would not be able to support insertions into this view. To delete\na tuple from the view B-Students, we can simply delete the corresponding tuple\nfrom Enrolled.\nAlthough this example illustrates that the SQL rules on updatable views are\nunnecessarily restrictive, it also brings out the complexity of handling view\nupdates in the general case. For practical reasons, the SQL standard has chosen\nto allow only updates on a very restricted class of views.\n3.7\nDESTROYING/ALTERING TABLES AND VIEWS\nIf we decide that we no longer need a base table and want to destroy it (i.e.,\ndelete all the rows and remove the table definition information), we can use\nthe DROP TABLE command. For example, DROP TABLE Students RESTRICT de-\nstroys the Students table unless some view or integrity constraint refers to\nStudents; if so, the command fails. If the keyword RESTRICT is replaced by\nCASCADE, Students is dropped and any referencing views or integrity constraints\nare (recursively) dropped as well; one of these t\\VO keyvlOrds must always be\nspecified. A vipw can be dropped using the DROP VIEW command, which is just\nlike DROP TABLE.\nALTER TABLE modifies the structure of an existing table.\nTo add a column\ncalled maiden-name to Students, for example, we would use the following com-\nmand:\n\nALTER TABLE Students\nADD COLUMN maiden-name CHAR(10)\nCUAPTER·.'3\nThe definition of Students is modified to add this column, and all existing rows\nare padded with null values in this column.\nALTER TABLE can also be used\nto delete columns and add or drop integrity constraints on a table; we do not\ndiscuss these aspects of the command beyond remarking that dropping columns\nis treated very similarly to dropping tables or views.\n3.8\nCASE STUDY: THE INTERNET STORE\nThe next design step in our running example, continued from Section 2.8, is\nlogical database design. Using the standard approach discussed in Chapter 3,\nDBDudes maps the ER diagram shown in Figure 2.20 to the relational model,\ngenerating the following tables:\nCREATE TABLE Books ( isbn\nCHAR ( 10) ,\ntitle\nCHAR(80) ,\nauthor\nCHAR(80),\nqty_in-stock\nINTEGER,\nprice\nREAL,\nyeaLpublished\nINTEGER,\nPRIMARY KEY (isbn))\nCREATE TABLE Orders ( isbn\nCHAR (10) ,\nciel\nINTEGER,\ncarelnum\nCHAR (16) ,\nqty\nINTEGER,\norder_date\nDATE,\nship_date\nDATE,\nPRIMARY KEY (isbn,cid),\nFOREIGN KEY (isbn) REFERENCES Books,\nFOREIGN KEY (cid) REFERENCES Customers)\nCREATE TABLE Customers ( cid\nINTEGER,\ncname\nCHAR(80),\naddress\nCHAR(200),\nPRIMARY KEY (cid)\nThe design team leader, who is still brooding over the fact that the review\nexposed a flaw in the design, now has an inspiration. The Orders table contains\nthe field order_date and the key for the table contains only the fields isbn and\nc'id. Because of this, a customer cannot order the same book OIl different days,\n\nThe Relat'ional l1;lodel\n9~\na re.striction that was not intended. vVhy not add the order-date attribute to\nthe key for the Orders table? This would eliminate the unwanted restrietion:\nCREATE TABLE Orders (\nisbn\nCHAR(10) ,\nPRIMARY KEY (isbn,cid,ship_date),\n...)\nThe reviewer, Dude 2, is not entirely happy with this solution, which he calls\na 'hack'.\nHe points out that no natural ER diagram reflects this design and\nstresses the importance of the ER diagram\n&<; a design do·cument.\nDude 1\nargues that, while Dude 2 has a point, it is important to present B&N with\na preliminary design and get feedback; everyone agrees with this, and they go\nback to B&N.\nThe owner of B&N now brings up some additional requirements he did not\nmention during the initial discussions: \"Customers should be able to purchase\nseveral different books in a single order. For example, if a customer wants to\npurchase three copies of 'The English Teacher' and two copies of 'The Character\nof Physical Law,' the customer should be able to place a single order for both\nbooks.\"\nThe design team leader, Dude 1, asks how this affects the shippping policy.\nDoes B&N still want to ship all books in an order together?\nThe owner of\nB&N explains their shipping policy: \"As soon as we have have enough copies\nof an ordered book we ship it, even if an order contains several books. So it\ncould happen that the three copies of 'The English Teacher' are shipped today\nbecause we have five copies in stock, but that 'The Character of Physical Law'\nis shipped tomorrow, because we currently have only one copy in stock and\nanother copy arrives tomorrow.\nIn addition, my customers could place more\nthan one order per day, and they want to be able to identify the orders they\nplaced.\"\nThe DBDudes team thinks this over and identifies two new requirements: First,\nit must be possible to order several different books in a single order and sec-\nond, a customer must be able to distinguish between several orders placed the\nsame day. To accomodate these requirements, they introduce a new attribute\ninto the Orders table called ordernum, which uniquely identifies an order and\ntherefore the customer placing the order. However, since several books could be\npurchased in a single order, onleTnum and isbn are both needed to determine\nqt.y and ship_dat.e in the Orders table.\nOrders are assign(~d order numbers sequentially and orders that are placed later\nhave higher order numbers. If several orders are placed by the same customer\n\nCHAPTER03\non a single day, these orders have different order numbers and can thus be\ndistinguished. The SQL DDL statement to create the modified Orders table\nfollows:\nCREATE TABLE Orders ( ordernum\nINTEGER,\nisbn\nCHAR(10),\ndd\nINTEGER,\ncardnum\nCHAR (16) ,\nqty\nINTEGER,\nordeLdate\nDATE,\nship~date\nDATE,\nPRIMARY KEY (ordernum, isbn),\nFOREIGN KEY (isbn) REFERENCES Books\nFOREIGN KEY (dd) REFERENCES Customers)\nThe owner of B&N is quite happy with this design for Orders, but has realized\nsomething else. (DBDudes is not surprised; customers almost always come up\nwith several new requirements as the design progresses.) While he wants all\nhis employees to be able to look at the details of an order, so that they can\nrespond to customer enquiries, he wants customers' credit card information to\nbe secure. To address this concern, DBDudes creates the following view:\nCREATE VIEW OrderInfo (isbn, cid, qty, order-date, ship_date)\nAS SELECT O.cid, O.qty, O.ordeLdate, O.ship_date\nFROM\nOrders 0\nThe plan is to allow employees to see this table, but not Orders; the latter is\nrestricted to B&N's Accounting division. We'll see how this is accomplished in\n3.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhat is a relation? Differentiate between a relation schema and a relation\ninstance. Define the terms arity and degree of a relation. What are domain\nconstraints? (Section 3.1)\n•\nWhat SQL construct enables the definition of a relation? \\Vhat constructs\nallow modification of relation instances? (Section 3.1.1)\n•\n\\Vhat are integrity constraints? Define the terms primary key constTa'int\nand foreign key constraint. How are these constraints expressed in SQL?\nWhat other kinds of constraints can we express in SQL? (Section 3.2)\n\nThe Relational Alodel\n•\n\\Vhat does the DBMS do when constraints are violated? What is referen-\ntial 'integr-ity? \\Vhat options does SQL give application programmers for\ndealing with violations of referential integrity? (Section 3.3)\n•\nWhen are integrity constraints enforced by a DBMS? How can an appli-\ncation programmer control the time that constraint violations are checked\nduring transaction execution? (Section 3.3.1)\n•\nWhat is a relational database query? (Section 3.4)\n•\nHow can we translate an ER diagram into SQL statements to create ta-\nbles?\nHow are entity sets mapped into relations?\nHow are relationship\nsets mapped? How are constraints in the ER model, weak entity sets, class\nhierarchies, and aggregation handled? (Section 3.5)\n•\nWhat is a view? How do views support logical data independence? How\nare views used for security?\nHow are queries on views evaluated? Why\ndoes SQL restrict the class of views that can be updated? (Section 3.6)\n•\nWhat are the SQL constructs to modify the structure of tables and de--\nstray tables and views? Discuss what happens when we destroy a view.\n(Section 3.7)\nEXERCISES\nExercise 3.1 Define the following terms: relation schema, relational database schema, do-\nmain, relation instance, relation cardinality, and relation degree.\nExercise 3.2 How many distinct tuples are in a relation instance with cardinality 22?\nExercise 3.3 Does the relational model, as seen by an SQL query writer, provide physical\nand logical data independence? Explain.\nExercise 3.4 \\\\That is the difference between a candidate key and the primary key for a given\nrelation? What is a superkey?\nExercise 3.5 Consider the instance of the Students relation shown in Figure 3.1.\n1. Give an example of an attribute (or set of attributes) that you can deduce is not a\ncandidate key, based on this instance being legaL\n2. Is there any example of an attribute (or set of attributes) that you can deduce is a\ncandidate key, based on this instance being legal?\nExercise 3.6 What is a foreign key constraint? Why are such constraints important? What\nis referential integrity?\nExercise 3.7 Consider the relations Students, Faculty, Courses, Rooms, Enrolled, Teaches,\n'Lnd Meets_In defined in Section 1.5.2.\n\nCHAPTERr3\n1. List all the foreign key constraints among these relations.\n2. Give an example of a (plausible) constraint involving one or more of these relations that\nis not a primary key or foreign key constraint.\nExercise 3.8 Answer each of the following questions briefly. The questions are b&'>ed OIl the\nfollowing relational schema:\nEmp( eid: integer, ename: string, age: integer, sala1l1: real)\nWorks(eid: integer, did: integer, peLtime: integer)\nDept(did: integer, dname: string, budget: real, managerid: integer)\n1. Give an example of a foreign key constraint that involves the Dept relation. What are\nthe options for enforcing this constraint when a user attempts to delete a Dept tuple?\n2. Write the SQL statements required to create the preceding relations, including appro-\npriate versions of all primary and foreign key integrity constraints.\n3. Define the Dept relation in SQL so that every department is guaranteed to have a\nmanager.\n4. Write an SQL statement to add John Doe as an employee with eid = 101, age = 32 and\nsalary = 15,000.\n5. Write an SQL statement to give every employee a 10 percent raise.\n6. Write an SQL statement to delete the Toy department. Given the referential integrity\nconstraints you chose for this schema, explain what happens when this statement is\nexecuted.\nExercise 3.9 Consider the SQL query whose answer is shown in Figure 3.6.\n1. Modify this query so that only the login column is included in the answer.\n2. If the clause WHERE S.gpa >= 2 is added to the original query, what is the set of tuples\nin the answer?\nExercise 3.10 Explain why the addition of NOT NULL constraints to the SQL definition of\nthe Manages relation (in Section 3.5.3) would not enforce the constraint that each department\nmust have a manager. What, if anything, is achieved by requiring that the S8n field of Manages\nbe non-null?\nExercise 3.11 Suppose that we have a ternary relationship R between entity sets A, B,\nand C such that A has a key constraint and total participation and B has a key constraint;\nthese are the only constraints.\nA has attributes al and a2, with al being the key; Band\nC are similar.\nR has no descriptive attributes.\nWrite SQL statements that create tables\ncorresponding to this information so &s to capture as many of the constraints as possible. If\nyou cannot capt,).ue some constraint, explain why.\nExercise 3.12 Consider the scenario from Exercise 2.2, where you designed an ER diagram\nfor a university database. \\Vrite SQL staternents to create the corresponding relations and\ncapture as many of the constraints as possible. If you cannot: capture some constraints, explain\nwhy.\nExercise 3.13 Consider the university database from Exercise 2.:3 and the ER diagram you\ndesigned. Write SQL statements to create the corresponding relations and capture &'> many\nof the constraints as possible. If you cannot capture some constraints, explain why.",
          "pages": [
            124,
            125,
            126,
            127,
            128,
            129,
            130,
            131
          ],
          "relevance": {
            "score": 0.6,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "The RelatioTwl A10del\nt\nExercise 3.14 Consider the scenario from Exercise 2.4, where you designed an ER diagram\nfor a company databa,c;e.\n\\~Trite SQL statements to create the corresponding relations and\ncapture as many of the constraints as possible.\nIf you cannot capture some constraints,\nexplain why.\nExercise 3.15 Consider the Notown database from Exercise 2.5. You have decided to rec-\nommend that Notown use a relational database system to store company data.\nShow the\nSQL statements for creating relations corresponding to the entity sets and relationship sets\nin your design. Identify any constraints in the ER diagram that you are unable to capture in\nthe SQL statements and briefly explain why you could not express them.\nExercise 3.16 Thanslate your ER diagram from Exercise 2.6 into a relational schema, and\nshow the SQL statements needed to create the relations, using only key and null constraints.\nIf your translation cannot capture any constraints in the ER diagram, explain why.\nIn Exercise 2.6, you also modified the ER diagram to include the constraint that tests on a\nplane must be conducted by a technician who is an expert on that model. Can you modify\nthe SQL statements defining the relations obtained by mapping the ER diagram to check this\nconstraint?\nExercise 3.17 Consider the ER diagram that you designed for the Prescriptions-R-X chain of\npharmacies in Exercise 2.7. Define relations corresponding to the entity sets and relationship\nsets in your design using SQL.\nExercise 3.18 Write SQL statements to create the corresponding relations to the ER dia-\ngram you designed for Exercise 2.8. If your translation cannot capture any constraints in the\nER diagram, explain why.\nExercise 3.19 Briefly answer the following questions based on this schema:\nEmp(e'id: integer, ename: string, age: integer, salary: real)\nWorks(eid: integer, did: integer, peLtime: integer)\nDept(did: integer, budget: real, managerid: integer)\n1. Suppose you have a view SeniorEmp defined as follows:\nCREATE VIEW SeniorEmp (sname, sage, salary)\nAS SELECT E.ename, Kage, E.salary\nFROM\nEmp E\nWHERE\nKage > 50\nExplain what the system will do to process the following query:\nSELECT S.sname\nFROM\nSeniorEmp S\nWHERE\nS.salary > 100,000\n2. Give an example of a view on Emp that could be automatically updated by updating\nEmp.\n3. Give an example of a view on Emp that would be impossible to update (automatically)\nand explain why your example presents the update problem that it does.\nExercise 3.20 C::onsider the following schema:\n\nSuppliers(sid: integer, sname: string, address: string)\nParts(pid: integer, pname: string, color: string)\nCatalog(sid: integer, pid: integer, cost: real)\nCHAPTER,. 3\nThe Catalog relation lists the prices charged for parts by Suppliers.\nAnswer the following\nquestions:\n•\nGive an example of an updatable view involving one relation.\n•\nGive an example of an updatable view involving two relations.\n•\nGive an example of an insertable-into view that is updatable.\n•\nPROJECT-BASED EXERCISES\nExercise 3.21 Create the relations Students, Faculty, Courses, Rooms, Enrolled, Teaches,\nand Meets_In in Minibase.\nExercise 3.22 Insert the tuples shown in Figures 3.1 and 3.4 into the relations Students and\nEnrolled. Create reasonable instances of the other relations.\nExercise 3.23 What integrity constraints are enforced by Minibase?\nExercise 3.24 Run the SQL queries presented in this chapter.\nBIBLIOGRAPHIC NOTES\nThe relational model was proposed in a seminal paper by Codd [187]. Childs [176] and Kuhns\n[454] foreshadowed some of these developments.\nGallaire and :WIinker's book [296] contains\nseveral papers on the use of logic in the context of relational databases. A system based on a\nvariation of the relational model in which the entire database is regarded abstractly as a single\nrelation, called the universal relation, is described in [746]. Extensions of the relational model\nto incorporate null values, which indicate an unknown or missing field value, are discussed by\nseveral authors; for example, [329, 396, 622, 754, 790].\nPioneering projects include System R [40, 150] at IBM San Jose Research Laboratory (now\nIBM Almaden Research Center), Ingres [717] at the University of California at Berkeley,\nPRTV [737] at the IBNI UK Scientific Center in Peterlee, and QBE [801] at IBM T. J.\nWatson Research Center.\nA rich theory underpins the field of relational databases. Texts devoted to theoretical aspects\ninclude those by··Atzeni and DeAntonellis [45]; Maier [501]; and Abiteboul, Hull, and Vianu\n[:3]. [415] is an excellent survey article.\nIntegrity constraints in relational databases have been discussed at length.\n[190] addresses\nsemantic extensions to the relational model, and integrity, in particular referential integrity.\nU~60] discusses semantic integrity constraints.\n[2()~3] contains papers that address various\naspects of integrity constraints, including in particular a detailed discussion of referential\nintegrity. A vast literature deals \\vith enforcing integrity constraints. [51] compares the cost\n\nThe Relational AIodel\n.~\nof enforcing integrity constraints via compile-time, run-time, and post-execution checks. [145]\npresents an SQL-based language for specifying integrity constraints and identifies conditions\nunder which integrity rules specified in this language can be violated.\n[713] discusses the\ntechnique of integrity constraint checking by query modification.\n[180] discusses real-time\nintegrity constraints.\nOther papers on checking integrity constraints in databases include\n[82, 122, 138,517]. [681] considers the approach of verifying the correctness of programs that\naccess the database instead of run-time checks. Note that this list of references is far fTom\ncomplete; in fact, it does not include any of the many papers on checking recursively specified\nintegrity constraints. Some early papers in this widely studied area can be found in [296] and\n[295].\nFor references on SQL, see the bibliographic notes for Chapter 5. This book does not discuss\nspecific products based on the relational model, but many fine books discuss each of the major\ncommercial systems; for example, Chamberlin's book on DB2 [149], Date and McGoveran's\nbook on Sybase [206], and Koch and Loney's book on Oracle [443].\nSeveral papers consider the problem of translaping updates specified on views into updates\non the underlying table [59, 208, 422, 468, 778].\n[292] is a good survey on this topic.\nSee\nthe bibliographic notes for Chapter 25 for references to work querying views and maintaining\nmaterialized views.\n[731] discusses a design methodology based on developing an ER diagram and then translating\nto the relational model.\nMarkowitz considers referential integrity in the context of ER to\nrelational mapping and discusses the support provided in some commercial systems (as of\nthat date) in [513, 514].",
          "pages": [
            132,
            133,
            134
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The WHERE clause is used to filter records in a database table based on specified conditions. It helps in retrieving only those rows that meet certain criteria.",
        "explanation": "The WHERE clause is essential for filtering data efficiently. Imagine you have a large library with thousands of books, and you want to find all the books about science fiction. The WHERE clause allows you to specify this condition (genre = 'Science Fiction') so that only those books are returned. This makes your search more efficient and relevant.",
        "key_points": [
          "Key point 1: The WHERE clause filters records based on conditions.",
          "Key point 2: It is used in SELECT statements to narrow down the data retrieved.",
          "Key point 3: Common operators include =, <, >, <=, >=, and LIKE for pattern matching."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- SELECT all students who are 18 years old SELECT name FROM Students WHERE age = 18;",
            "explanation": "This example shows how to use the WHERE clause to filter records where the age is exactly 18.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Find all policies that cost more than $500\nSELECT policyid, cost FROM Policies WHERE cost > 500;",
            "explanation": "This practical example demonstrates filtering records based on a numerical condition to find expensive policies."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using the wrong operator",
            "incorrect_code": "-- Incorrectly using = instead of LIKE\nSELECT name FROM Students WHERE name = 'John%';",
            "correct_code": "-- Correctly using LIKE for pattern matching\nSELECT name FROM Students WHERE name LIKE 'John%';",
            "explanation": "This mistake occurs when the wrong operator is used. The correct operator should be LIKE for pattern matching."
          }
        ],
        "practice": {
          "question": "Create a query to find all employees who have been with the company for more than 5 years, given that their hire date is stored in the 'hire_date' column.",
          "solution": "-- Correct solution\nSELECT name FROM Employees WHERE hire_date <= DATE_SUB(CURDATE(), INTERVAL 5 YEAR);",
          "explanation": "This question tests understanding of using the WHERE clause with date comparisons. The correct solution uses a date function to calculate the date 5 years ago and compares it with the 'hire_date' column."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "The Relational A!odd\n~\nThere is a special case in which this translation can be refined by dropping the\nSponsors relation. Consicler the Sponsors relation. It has attributes pid, did,\nand since; and i",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "joins": {
      "id": "joins",
      "title": "SQL Joins",
      "definition": "Combining data from multiple tables using INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN",
      "difficulty": "intermediate",
      "page_references": [
        140,
        141,
        142,
        143,
        144,
        145,
        146,
        147,
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160
      ],
      "sections": {
        "definition": {
          "text": "Relational Algebra and CalC'ul1L8\nH15\n•\nSet-difference: R- 8 returns a relation instance containing all tuples that\noccur in R but not in 8. The relations Rand 8 must be union-compatible,\nand the schema of the result is defined to be identical to the schema of R.\n•\nCross-product: R x 8 returns a relation instance whose schema contains\nall the fields of R (in the same order as they appear in R) followed by all\nthe fields of 8 (in the same order as they appear in 8). The result of R x 8\ncontains OIle tuple (1', s) (the concatenation of tuples rand s) for each pair\nof tuples l' E R,\nS E 8. The cross-product opertion is sometimes called\nCartesian product.\n\\\\Te use the convention that the fields of R x 8 inherit names from the\ncorresponding fields of Rand 8. It is possible for both Rand 8 to contain\none or more fields having the same name; this situation creates a naming\nconfi'ict. The corresponding fields in R x 8 are unnamed and are referred\nto solely by position.\nIn the preceding definitions, note that each operator can be applied to relation\ninstances that are computed using a relational algebra (sub)expression.\nWe now illustrate these definitions through several examples. The union of 81\nand 82 is shown in Figure 4.8. Fields are listed in order; field names are also\ninherited from 81. 82 has the same field names, of course, since it is also an\ninstance of Sailors. In general, fields of 82 may have different names; recall that\nwe require only domains to match. Note that the result is a set of tuples. TUples\nthat appear in both 81 and 82 appear only once in 81 U 82. Also, 81 uRI is\nnot a valid operation because the two relations are not union-compatible. The\nintersection of 81 and 82 is shown in Figure 4.9, and the set-difference 81- 82\nis shown in Figure 4.10.\nDustin\n45.0\nLubber\n55.5\nRusty\n35.0\nyuppy\n35.0\nguppy\n35.0\n31 u 52\nThe result of the cross-product 81 x Rl is shown in Figure 4.11. Because Rl\nand 81 both have a field named sid, by our convention on field names, the\ncorresponding two fields in 81 x Rl are unnamed, and referred to solely by the\nposition in which they appear in Figure 4.11. The fields in 81 x Rl have the\nsame domains as the corresponding fields in Rl and 5'1. In Figure 4.11, sid is\n\n.isifi\n\"\"\".h.~\"\nLubber\n55.5\nRusty\n35.0\n81 n 82\nGHAPTER f4\nli~·iiJB1ff/fj,me\nIt,{4t~rf1l1f:4f1ei I\nI 22 I Dustin I 7\n[3fOJ\n81 - 82\nlisted in parentheses to emphasize that it is not an inherited field name; only\nthe corresponding domain is inherited.\n(sid!)\nbid\naay\nDustin\n45.0\n10/10/96\nDustin\n45.0\n11/12/96\nLubber\n55.5\n10/10/96\nLubber\n55.5\n11/12/96\nRusty\n35.0\n10/10/96\nRusty\n35.0\n11/12/96\n81 x R1\n4.2.3\nRenaming\nWe have been careful to adopt field name conventions that ensure that the result\nof a relational algebra expression inherits field names from its argument (input)\nrelation instances in a natural way whenever possible. However, name conflicts\ncan arise in some cases; for example, in 81 x Rl. It is therefore convenient\nto be able to give names explicitly to the fields of a relation instance that is\ndefined by a relational algebra expression. In fact, it is often convenient to give\nthe instance itself a name so that we can break a large algebra expression into\nsmaller pieces by giving names to the results of subexpressions.\nvVe introduce a renaming operator p for this purpose. The expression p(R(F), E)\ntakes an arbitrary relational algebra expression E and returns an instance of\na (new) relation called R. R contains the same tuples as the result of E and\nhas the same schema as E, but some fields are renamed. The field names in\nrelation R are the sarne as in E, except for fields renamed in the Tenaming list\nF, which is a list of terms having the form oldname ~, newnarne or position ~\nrW1llTlJLrne. For p to be well-defined, references to fields (in the form of oldnarnes\nor posit.ions in the renaming list) may be unarnbiguous and no two fields in the\nresult may have the same name. Sometimes we want to only renarne fields or\n(re)name the relation; we therefore treat both Rand F as optional in the use\nof p. (Of course, it is meaningless to omit both.)\n\nRelational AlgebTa and Calc\"Uh18\nFor example, the expression p(C(l\n----7 s'id1,5\n----7 sid2), 81 x R1) returns a\nrelation that contains the tuples shown in Figure 4.11 and has the following\nschema: C(sidl: integer, ,marrw: string, mt'ing: integer, age: real, sid2:\ninteger, bid: integer, day: dates).\nIt is customary to include some additional operators in the algebra, but all of\nthem can be defined in terms of the operators we have defined thus far.\n(In\nfact, the renaming operator is needed only for syntactic convenience, and even\nthe n operator is redundant; R n 8 can be defined as R - (R - 8).) We consider\nthese additional operators and their definition in terms of the basic operators\nin the next two subsections.\n4.2.4\nJoins\nThe join operation is one of the most useful operations in relational algebra\nand the most commonly used way to combine information from two or more\nrelations. Although a join can be defined as a cross-product followed by selec-\ntions and projections, joins arise much more frequently in practice than plain\ncross-products. Further, the result of a cross-product is typically much larger\nthan the result of a join, and it is very important to recognize joins and imple-\nment them without materializing the underlying cross-product (by applying the\nselections and projections 'on-the-fly'). For these reasons, joins have received\na lot of attention, and there are several variants of the join operation. 1\nCondition Joins\nThe most general version of the join operation accepts a join condition c and\na pair of relation instances as arguments and returns a relation instance. The\njoin cond'it-ion is identical to a selection condition in form.\nThe operation is\ndefined as follows:\nR [:X)e S =\nO\"e(R X S)\nThus [:X) is defined to be a cross-product followed by a selection. Note that the\ncondition c can (and typically does) refer to attributes of both Rand S. The\nreference to an attribute of a relation, say, R, can be by positioll (of the form\nR.i) or by Ilame (of the form R.name).\nAs an example, the result of Sl\n[><JS1.8id<Rl.sid R1 is shown in Figure 4.12.\nBecause sid appears in both 81 and R1, the corresponding fields in the result\nof the cross-product 81 x R1 (and therefore in the result of 81 [:X)S1.sid<Rl.sid R1)\n1Several variants of joins are not discussed in this chapter.\nAn important c.la..'iS of joins, called\n01lter joins, is discussed in Chapter 5.\n\nCHAPTER ..t1\nare unnamed. Domains are inherited from the corresponding fields of 81 and\nRl.\nI (sid) I snarne I rating I age\nI···{si41:l bid\nda/lj\nI 22\nI Dustin\n45.0\nI 103 I 11/12/96\nI 31\nI Lubber\n.- 55.5\nI 103 I 11/12/96\n51 NSl. s id<R1.sid R1\nEquijoin\nA common special case of the join operation R [>(] 8 is when the join condition\nconsists solely of equalities (connected by 1\\) of the form R.name1 = 8.name2,\nthat is, equalities between two fields in Rand S. In this case, obviously, there is\nsome redundancy in retaining both attributes in the result. For join conditions\nthat contain only such equalities, the join operation is refined by doing an\nadditional projection in which 8.name2 is dropped. The join operation with\nthis refinement is called equijoin.\nThe schema of the result of an equijoin contains the fields of R (with the same\nnames and domains as in R) followed by the fields of 8 that do not appear\nin the join conditions. If this set of fields in the result relation includes two\nfields that inherit the same name from Rand 8, they are unnamed in the result\nrelation.\nWe illustrate 81l:<JR.sid=5.sid Rl in Figure 4.13. Note that only one field called\nsid appears in the result.\n~ ,marneI rating I age I· bid·1 day =tJ\nI 22\nDustIn I 7\nI 45.0\n101 I 10/10/96 I\nI 58\nRust}~ I 10\nI ~5.0\n103 I 11/12/96 I\n81 MR ..,H1=S'.\"id HI\nNatural Join\nA further special ca.'3e of the join operation R\n[>(] S is an eqUlJom in which\nequalities arc specified on all fields having the same name in Rand S.\nIn\nthis case, we can simply omit the join condition; the default is that the join\ncondition is a collection of equalities on all common fields. We call this special\ncase a natumJ jo'in, and it has the nice property that the result is guaranteed\nnot to have two fields with the saIne name.\n\nRelai'ional Algelrra and Calculus\nlQ9\nThe equijoin expression 81 D<m..sid=s.sid R1 is actually a natural join al1d can\nsimply be denoted as 81 [Xl R1, since the only common field is sid. If the two\nrelations have no attributes in common, 81 [Xl Rl is simply the cross-product.\n4.2.5\nDivision\nThe division operator is useful for expressing certain kinds of queries for exam-\nple, \"Find the names of sailors who have reserved all boats.\"\nUnderstanding\nhow to use the basic operators of the algebra to define division is a useful exer-\ncise. However, the division operator does not have the same importance as the\nother operators-it is not needed as often, and database systems do not try to\nexploit the semantics of division by implementing it as a distinct operator (as,\nfor example, is done with the join operator).\nWe discuss division through an example.\nConsider two relation instances A\nand B in which A has (exactly) two fields x and y and B has just one field y,\nwith the same domain as in A. We define the division operation AlB as the\nset of all x values (in the form of unary tuples) such that for every y value in\n(a tuple of) B, there is a tuple (x,y) in A.\nAnother way to understand division is as follows. For each x value in (the first\ncolumn of) A, consider the set of y values that appear in (the second field of)\ntuples of A with that x value. If this set contains (all y values in) B, the x\nvalue is in the result of AlB.\nAn analogy with integer division may also help to understand division.\nFor\nintegers A and B, AlB is the largest integer Q such that Q * B\n::::; A.\n:For\nrelation instances A and B, AlB is the largest relation instance Q such that\nQ x B S:::A.\nDivision is illustrated in Figure 4.14. It helps to think of A as a relation listing\nthe parts supplied by suppliers and of the B relations as listing parts. AIB'i\ncomputes suppliers who supply all parts listed in rdation instance Bi.\nExpressing AlBin terms of the ba...sic algebra operators is an interesting ex-\nercise, and the reader should try to do this before reading further. The basic\nidea is to compute all :r values in A that are not disqualified.\nAn x value is\ndisqualified if lJy attaching a y value from B, we obtain a tuple (x,y) that is not\nin A. We can compute disqualified tuples using the algebra expression\nThus, we can define AlBa.....,",
          "pages": [
            140,
            141,
            142,
            143,
            144
          ],
          "relevance": {
            "score": 0.42,
            "sql_score": 1.0,
            "concept_score": 0.44,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "A\nI sno i pno I\n\\\npI I\nsl\np2\n~._~~\ni\np4\n\\\npI\n~_P2\n-~\n'\np2\np2\ns4\np4\nCHAPTER$4\nBl\n[P.!!~J\nAlBl\n~;~J\n,....---,\n~\nLEU\nI 82 I\nB2\nI pno ]\nBE\np4\n~\nAlB2\nB3 ~\nBE\nB1J\np2\n~\np4\nAlB3 [ill\nExamples Illustrating Division\nTo understand the division operation in full generality, we have to consider the\ncase when both x and yare replaced by a set of attributes. The generalization is\nstraightforward and left as an exercise for the reader. We discuss two additional\nexamples illustrating division (Queries Q9 and Q10) later in this section.\n4.2.6\nMore Examples of Algebra Queries\nWe now present several examples to illustrate how to write queries in relational\nalgebra. We use the Sailors, Reserves, and Boats schema for all our examples\nin this section. We use parentheses as needed to make our algebra expressions\nunambiguous.\nNote that all the example queries in this chapter are given\na unique query number. The query numbers are kept unique across both this\nchapter and the SQL query chapter (Chapter 5). This numbering makes it easy\nto identify a query when it is revisited in the context of relational calculus and\nSQL and to compare different ways of writing the same query. (All references\nto a query can be found in the subject index.)\nIn the rest of this chapter (and in Chapter 5), we illustrate queries using the\ninstances 83 of Sailors, R2 of Reserves, and B1 of Boats, shown in Figures\n4.15, 4.16, and 4.17, respectively.\n(Q1) Find the names of sailors who have rcscT'ucd boat lOS.\nThis query can be written as follows:\n\".marne (( (Jbid=1O:~Re.5erve8)[XJ 8ailoT.5)\n\nRelational Algebra and Calcul1ls\n~l\n»>\n.....\"\"\"'.+A.\".,'\"\nhAh\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nHoratio\n35.0\nArt\n25.5\nBob\n63.5\nAn Instance 83 of Sailors\n111,\n10/10/98\n10/10/98\n10/8/98\n10/7/98\n11/10/98\n11/6/98\n11/12/98\n9/5/98\n9/8/98\n9/8/98\nAn Instance R2 of Reserves\nWe first compute the set of tuples in Reserves with bid = 103 and then take the\nnatural join of this set with Sailors. This expression can be evaluated on in-\nstances of Reserves and Sailors. Evaluated on the instances R2 and S3, it yields\na relation that contains just one field, called sname, and three tuples (Dustin),\n(Horatio), and (Lubber). (Observe that two sailors are called Horatio and only\none of them has reserved a red boat.)\n[~]bname\nI color· I\nInterlake\nblue\nInterlake\nred\nClipper\ngreen\nMarine\nred\nAn Instance HI of Boats\nWe can break this query into smaller pieces llsing the renaming operator p:\np(Temp1, IJbir1=103 ReseTves)\np(Temp2, Temp11XJ Sailor's)\n1Tsname(Temp2)\nNotice that because we are only llsing p to give names to intermediate relations,\nthe renaming list is optional and is omitted. TempI denotes an intermediate\nrelation that identifies reservations of boat 103. Temp2 is another intermediate\nrelation, and it denotes sailors who have mad(~ a reservation in the set Templ.\nThe instances of these relations when evaluating this query on the instances R2\nand S3 are illustrated in Figures 4.18 and 4.19. Finally, we extract the sname\ncolumn from Temp2.\n\n10~~\n10/8/98\n11/6/98\n9/8/98\nDustin\nLubber\nHoratio\nCHAPTER;l\n10/8/98\n11/6/98--\n9/8/98\nInstance of TempI\nInstance of Temp2\nThe version of the query using p is essentially the same as the original query;\nthe use of p is just syntactic sugar. However, there are indeed several distinct\nways to write a query in relational algebra. Here is another way to write this\nquery:\nJrsname(CJbid=103(Reserves IX! Sailors))\nIn this version we first compute the natural join of Reserves and Sailors and\nthen apply the selection and the projection.\nThis example offers a glimpse of the role played by algebra in a relational\nDBMS. Queries are expressed by users in a language such as SQL. The DBMS\ntranslates an SQL query into (an extended form of) relational algebra and\nthen looks for other algebra expressions that produce the same answers but are\ncheaper to evaluate. If the user's query is first translated into the expression\n7fsname (CJbid=103 (Reserves IX! Sailors))\na good query optimizer will find the equivalent expression\n7rsname ((CJb·id=103Reserves) IX! Sailors)\nFurther, the optimizer will recognize that the second expression is likely to\nbe less expensive to compute because the sizes of intermediate relations are\nsmaller, thanks to the early use of selection.\n(Q2) Find the names of sailors who ha've reserved a red boat.\n7f.marne ((CJcolor='red'Boats) IX! Reserves !><J SailoI's)\nThis query involves a series of two joins. First, we choose (tuples describing)\nred boats.\nThen, we join this set with Reserves (natural join, with equality\nspecified on thE) bid column) to identify reservations of red boats.\nNext, we\njoin the resulting intermediate relation with Sailors (natural join, with equality\nspecified on the sid column) to retrieve the names of sailors who have rnade\nreservations for red boats. Finally, we project the sailors' names. The answer,\nwhen evaluated on the instances B1, R2, and S3, contains the names Dustin,\nHoratio, and Lubber.\n\nRelational Algebra and Calculus\nAn equivalent expression is:\nB.3\nThe reader is invited to rewrite both of these queries by using p to make the\nintermediate relations explicit and compare the schema.<=; of the intermediate\nrelations.\nThe second expression generates intermediate relations with fewer\nfields (and is therefore likely to result in intermediate relation instances with\nfewer tuples as well). A relational query optimizer would try to arrive at the\nsecond expression if it is given the first.\n(Q3) Find the colors of boats reserved by Lubber.\nJrcolor((asname='Lubber,Sa'ilors) [XJ Reserves [XJ Boats)\nThis query is very similar to the query we used to compute sailors who reserved\nred boats. On instances Bl, R2, and S3, the query returns the colors green\nand red.\n(Q4) Find the names of sailors who have reserved at least one boat.\nJrsname(Sailors [XJ Reserves)\nThe join of Sailors and Reserves creates an intermediate relation in which tuples\nconsist of a Sailors tuple 'attached to' a Reserves tuple. A Sailors tuple appears\nin (some tuple of) this intermediate relation only if at least one Reserves tuple\nhas the same sid value, that is, the sailor has made some reservation.\nThe\nanswer, when evaluated on the instances Bl, R2 and S3, contains the three\ntuples (Dustin), (HoTatio) , and (LubbeT).\nEven though two sailors called\nHoratio have reserved a boat, the answer contains only one copy of the tuple\n(HoTatio) , because the answer is a relation, that is, a set of tuples, with no\nduplicates.\nAt this point it is worth remarking on how frequently the natural join operation\nis used in our examples. This frequency is more than just a coincidence based\non the set of queries we have chosen to discuss; the natural join is a very\nnatural, widely used operation. In particular, natural join is frequently used\nwhen joining two tables on a foreign key field. In Query Q4, for exalnple, the\njoin equates the sid fields of Sailors and Reserves, and the sid field of Reserves\nis a foreign key that refers to the sid field of Sailors.\n(Q5) Find the narnes of sailors who have reserved a Ted OT a gTeen boat.\np(Tempboats, (acoloT='rcd' Boats) U (acolor='green' Boats))\nJrsna·me(Tempboats [XJ ReseTves [XJ Sailors)\n\nCHAPTER $4\nvVe identify the set of all boats that are either red or green (Tempboats, which\ncontains boats \\vith the bids 102, 103, and 104 on instances E1, R2, and S3).\nThen we join with Reserves to identify sid.., of sailors who have reserved OIle of\nthese boats; this gives us sids 22, 31, 64, and 74 over our example instances.\nFinally, we join (an intermediate relation containing this set of sids) with Sailors\nto find the names of Sailors with these sids. This gives us the names Dustin,\nHoratio, and Lubber on the instances E1, R2, and S3.\nAnother equivalent\ndefinition is the following:\np(Tempboats, (acolor='red'Vcolor='green' Boats))\n7fsname(Tempboats [><] Reserves [><] Sailors)\nLet us now consider a very similar query.\n(Q6) Find the names of sailors who have reserved a red and a green boat. It\nis tempting to try to do this by simply replacing U by n in the definition of\nTempboats:\np(Tempboats2, (acolor='red,Eoats) n (O\"color='green,Boats))\n'7fsname(Tempboats2 [><] Reserves [><] Sailors)\nHowever, this solution is incorrect-it instead tries to compute sailors who have\nreserved a boat that is both red and green. (Since bid is a key for Boats, a boat\ncan be only one color; this query will always return an empty answer set.) The\ncorrect approach is to find sailors who have reserved a red boat, then sailors\nwho have reserved a green boat, and then take the intersection of these two\nsets:\np(Tempred, '7fsid((acolor='red' Eoats) [><] Reserves))\np(Tempgreen, '7fsid((O\"color='green,Boats) [><] Reserves))\n'7f,marne((Ternpred n Tempgreen)\n[><] Sailors)\nThe two temporary relations compute the sids of sailors, and their intersection\nidentifies sailors who have reserved both red and green boats.\nOn instances\nBI, R2, and 53, the sids of sailors who have reserved a red boat are 22, 31,\nand 64. The s'icLs of sailors who have reserved a green boat are 22, 31, and 74.\nThus, sailors 22 and 31 have reserved both a red boat and a green boat; their\nnames are Dustin and Lubber.\nThis formulation of Query Q6 can easily be adapted to find sailors \\vho have\nreserved red or green boats (Query Q5); just replace n by U:\np(TempTed, '7fsid( (O\"color=lrcd' Boats) [)<] Reserves))\np(Tempgreen, '7fsid((O\"color='green' Boats) [)<] Reserves))\n'7fsTwme((Tempred U Tempgreen) [)<] Sailors)\n\nRelatiorwl Algebra and Galcurus\nIn the formulations of Queries Q5 and Q6, the fact that sid (the field over\nwhich we compute union or intersection) is a key for Sailors is very important.\nConsider the following attempt to answer Query Q6:\np(Tempred, Jrsname((CJcolor='red,Boats) [><] Reserves [><] Sailors))\np(Tempgreen,Jrsname((CJcoloT='gTeenlBoats) [><] Reserves [><] Sailors))\nTempred n Tempgreen\nThis attempt is incorrect for a rather subtle reason. Two distinct sailors with\nthe same name, such as Horatio in our example instances, may have reserved\nred and green boats, respectively. In this case, the name Horatio (incorrectly)\nis included in the answer even though no one individual called Horatio has\nreserved a red boat and a green boat. The cause of this error is that sname\nis used to identify sailors (while doing the intersection) in this version of the\nquery, but sname is not a key.\n(Q7) Find the names of sailors who have reser-ved at least two boats.\np(Reser-vations, Jrsid,sname,bid (Sailors [><] Reserves))\np(Reservationpairs(l\n---'? sid1, 2 ---'? sname1, 3 ---'? bid1, 4 ---'? sid2,\n5 ---'? sname2, 6 ---'? bid2), Reservations x Reservations)\nJrsname1 CJ(sidl=sid2)I\\(bidl=1-bid2) Reservationpair-s\nFirst, we compute tuples of the form (sid,sname, bid) , where sailor sid has made\na reservation for boat bid; this set of tuples is the temporary relation Reserva-\ntions. Next we find all pairs of Reservations tuples where the same sailor has\nmade both reservations and the boats involved are distinct. Here is the central\nidea: To show that a sailor has reserved two boats, we must find two Reserva-\ntions tuples involving the same sailor but distinct boats. Over instances El,\nR2, and S3, each of the sailors with sids 22, 31, and 64 have reserved at least\ntwo boats. Finally, we project the names of such sailors to obtain the answer,\ncontaining the names Dustin, Horatio, and Lubber.\nNotice that we included sid in Reservations because it is the key field identifying\nsailors, and we need it to check that two Reservations tuples involve the same\nsailor. As noted in the previous example, we cannot use sname for this purpose.\n(Q8) Find the sids of sailors w'ith age over 20 who have not TeseTved a Ted boat.\nJrsid(CJage>20Sa'ilors) -\n7rsid((CJco[oT='red,Boats) [><] Reserves [><] Sa'ilors)\nThis query illustrates the use of the set-difference operator. Again, we use the\nfact that sid is the key for Sailors. vVe first identify sailors aged over 20 (over\n\ninstances B1, R2, and S3, .'!'ids 22, 29, 31, 32, 58, 64, 74, 85, and 95) and then\ndiscard those who have reserved a red boat (sid.c; 22, 31, and 64), to obtain the\nanswer (sids 29, 32, 58, 74, 85, and 95). If we want to compute the names of\nsuch sailors, \\ve must first compute their sids (as shown earlier) and then join\nwith Sailors and project the sname values.\n(Q9) Find the names of sailors 'Who have rese'rved all boats.\nThe use of the word all (or every) is a good indication that the division operation\nmight be applicable:\np(Tempsids, (7l\"sid,bidReserves) / (7l\"bidBoats))\n7l\"sname(Tempsids N Sailors)\nThe intermediate relation Tempsids is defined using division and computes the\nset of sids of sailors who have reserved every boat (over instances Bl, R2, and\nS3, this is just sid 22). Note how we define the two relations that the division\noperator (/) is applied to·-·--the first relation has the schema (sid,bid) and the\nsecond has the schema (b'id). Division then returns all sids such that there is a\ntuple (sid,bid) in the first relation for each bid in the second. Joining Tempsids\nwith Sailors is necessary to associate names with the selected sids; for sailor\n22, the name is Dustin.\n(Q10) Find the names of sailors 'Who have reserved all boats called Interlake.\np(Tempsids, (7l\".5'id,bidReserves) / (7l\"bid((Jbname='Interlake' Boats)))\n7l\"sname(Tempsids [Xl Sailors)\nThe only difference with respect to the previous query is that now we apply a\nselection to Boats, to ensure that we compute bids only of boats named Interlake\nin defining the second argument to the division operator. Over instances El,\nR2, and S3, Tempsids evaluates to sids 22 and 64, and the answer contains\ntheir names, Dustin and Horatio.\nRELATIONAL CALCULUS\nRelational calculus is an alternative to relational algebra. In contra.':;t to the\nalgebra, which is procedural, the calculus is nonprocedural, or declarative, in\nthat it allows us to describe the set of answers without being explicit about\nhow they should be computed. Relational calculus has had a big influence on\nthe design of commercial query languages such a,s SQL and, especially, Query-\nby-Example (QBE).\nThe variant of the calculus we present in detail is called the tuple relational\ncalculus (TRC). Variables in TRC take on tuples as values. In another vari-\n\nRelatiO'Tul,l Algebra and Calculus\nant, called the domain relational calculus (DRC), the variables range over\nfield values. TRC has had more of an influence on SQL, \\vhile DRC has strongly\ninfluenced QBE. vVe discuss DRC in Section 4.3.2.2\n4$3.1\nTuple Relational Calculus\nA tuple variable is a variable that takes on tuples of a particular relation\nschema as values. That is, every value assigned to a given tuple variable has\nthe same number and type of fields. A tuple relational calculus query has the\nform { T I p(T) }, where T is a tuple variable and p(T) denotes a formula that\ndescribes T; we will shortly define formulas and queries rigorously. The result\nof this query is the set of all tuples t for which the formula p(T) evaluates to\ntrue with T = t. The language for writing formulas p(T) is thus at the heart of\nTRC and essentially a simple subset of first-order logic. As a simple example,\nconsider the following query.\n(Q11) Find all sailors with a rating above 7.\n{S I S E Sailors 1\\ S.rating > 7}\nWhen this query is evaluated on an instance of the Sailors relation, the tuple\nvariable S is instantiated successively with each tuple, and the test S. rat'ing> 7\nis applied. The answer contains those instances of S that pass this test. On\ninstance S3 of Sailors, the answer contains Sailors tuples with sid 31, 32, 58,\n71, and 74.\nSyntax of TRC Queries\nWe now define these concepts formally, beginning with the notion of a formula.\nLet Rel be a relation name, Rand S be tuple variables, a be an attribute of\nR, and b be an attribute of S. Let op denote an operator in the set {<, >, =\n, :S;, 2:, =I-}. An atomic formula is one of the following:\nIII\nR E Ref\nlIII\nR.a op S.b\nIIlI\nR.a op constant, or constant op R.a\nA formula is recursively defined to be one of the following, where p and q\nare themselves formula.s and p(R) denotes a formula in which the variable R\nappears:\n.~-----------\n2The material on DRC is referred to in the (online) chapter OIl QBE; with the exception of this\nchapter, the material on DRC and TRe can be omitted without loss of continuity.\n\nCHAPTER .,4\n•\nany atomic formula\n•\n-'p, P /\\ q, P V q, or p :::} q\n•\n3R(p(R)), where R is a tuple variable\n•\n'ifR(p(R)) , where R is a tuple variable\nIn the last two clauses, the quantifiers :3 and 'if are said to bind the variable R.\nA variable is said to be free in a formula or subformuia (a formula contained\nin a larger formula) if the (sub)formula does not contain an occurrence of a\nquantifier that binds it.3\nWe observe that every variable in a TRC formula appears in a subformula\nthat is atomic, and every relation schema specifies a domain for each field; this\nobservation ensures that each variable in a TRC formula has a well-defined\ndomain from which values for the variable are drawn. That is, each variable\nhas a well-defined type, in the programming language sense.\nInformally, an\natomic formula R E Rei gives R the type of tuples in ReI, and comparisons\nsuch as R.a op S.b and R.a op constant induce type restrictions on the field\nR.a. If a variable R does not appear in an atomic formula of the form R E Rei\n(Le., it appears only in atomic formulas that are comparisons), we follow the\nconvention that the type of R is a tuple whose fields include all (and only) fields\nof R that appear in the formula.\nWe do not define types of variables formally, but the type of a variable should\nbe clear in most cases, and the important point to note is that comparisons of\nvalues having different types should always fail.\n(In discussions of relational\ncalculus, the simplifying assumption is often made that there is a single domain\nof constants and this is the domain associated with each field of each relation.)\nA TRC query is defined to be expression of the form {T I p(T)}, where T is\nthe only free variable in the formula p.\nSemantics of TRC Queries\nWhat does a TRC query mean? More precisely, what is the set of answer tuples\nfor a given TRC query? The answer to a TRC query {T I p(T)}, as noted\nearlier, is the set of all tuples t for which the formula peT) evaluates to true\nwith variable T &'3signed the tuple value t:. To complete this definition, we must\nstate which assignments of tuple values to the free variables in a formula make\nthe formula evaluate to true.\n3vVe make the assumption that each variable in a formula is either free or bound by exactly one\noccurrence of a quantifier, to avoid worrying about details such a.'l nested occurrences of quantifiers\nthat bind some, but not all, occurrences of variables.\n\nRelational Algebra and Calcuhl8\nA query is evaluated on a given instance of the database. Let each free variable\nin a formula F be bound to a tuple value. For the given assignment of tuples\nto variables, with respect to the given database instance, F evaluates to (or\nsimply 'is') true if one of the following holds:\n•\nF is an atomic formula R E Rel, and R is assigned a tuple in the instance\nof relation Rel.\n•\nF is a comparison R.a op S.b, R.a op constant, or constant op R.a, and\nthe tuples assigned to Rand S have field values R.a and S.b that make the\ncomparison true.\n•\nF is of the form ---,p and p is not true, or of the form p 1\\ q, and both p and\nq are true, or of the form p V q and one of them is true, or of the form\np =} q and q is true whenever4 p is true.\n•\nF is of the form 3R(p(R)), and there is some assignment of tuples to the\nfree variables in p(R), including the variable R,5 that makes the formula\np(R) true.\n•\nF is of the form VR(p(R)), and there is some assignment of tuples to the\nfree variables in p(R) that makes the formula p(R) true no matter what\ntuple is assigned to R.\nExamples of TRC Queries\nWe now illustrate the calculus through several examples, using the instances\nB1 of Boats, R2 of Reserves, and S3 of Sailors shown in Figures 4.15, 4.16,\nand 4.17. We use parentheses as needed to make our formulas unambiguous.\nOften, a formula p(R) includes a condition R E Rel, and the meaning of the\nphrases some tuple R and for all tuples R is intuitive.\nWe use the notation\n3R E Rel(p(R)) for 3R(R E Rel 1\\ p(R)).\nSimilarly, we use the notation\nVR E Rel(p(R)) for VR(R E Rel =} p(R)).\n(Q12) Find the names and ages of sailors with a rating above 7.\n{P I 3S E Sailors(S.rating > 7 1\\ Pname = S.sname 1\\ Page = S.age)}\nThis query illustrates a useful convention: P is considered to be a tuple variable\nwith exactly two fields, which are called name and age, because these are the\nonly fields of P mentioned and P does not range over any of the relations in\nthe query; that is, there is no subformula of the form P\nE Relname.\nThe\nresult of this query is a relation with two fields, name and age.\nThe atomic\n4 WheneveT should be read more precisely as 'for all assignments of tuples to the free variables.'\n5Note that some of the free variables in p(R) (e.g., the variable R itself) IIlay be bound in P.\n\nCHAPTER J4\nformulas P.name = S.sname and Page = S.age give values to the fields of an\nanswer tuple P. On instances E1, R2, and S3, the answer is the set of tuples\n(Lubber,55.5), (Andy, 25.5), (Rusty, ~~5.0), (Zorba, 16.0), ::lnd (Horatio, 35.0).\n(Q1S) Find the so;ilor name, boat'id, and reseT1}Q.tion date for each reservation.\n{P I 3R E ReseT\"ues 3S E Sailors\n(R.sid = 8.sid!\\ P.bid = R.bid!\\ P.day = R.day !\\ P.sname = S.sname)}\nFor each Reserves tuple, we look for a tuple in Sailors with the same sid. Given\na pair of such tuples, we construct an answer tuple P with fields sname, bid,\nand day by copying the corresponding fields from these two tuples. This query\nillustrates how we can combine values from different relations in each answer\ntuple. The answer to this query on instances E1, R2, and 83 is shown in Figure\n4.20.\nIsname ~..... day\nDustin\n10/10/98\nDustin\n10/10/98\nDustin\n10/8/98\nDustin\n10/7/98\nLubber\n11/10/98\nLubber\n11/6/98\nLubber\n11/12/98\nHoratio\n9/5/98\nHoratio\n9/8/98\nHoratio\n9/8/98\nAnswer to Query Q13\n(Q1) Find the names of sailors who have reserved boat lOS.\n{P I 35 E Sailors 3R E Reserves(R.s'id = S.sid!\\ R.b'id = 103\n!\\Psname = 8.snarne)}\nThis query can be read as follows:\n\"Retrieve all sailor tuples for which there\nexists a tuple ,in Reserves having the same value in the s,id field and with\nb'id = 103.\" That is, for each sailor tuple, we look for a tuple in Reserves that\nshows that this sailor ha\" reserved boat\n10~~. The answer tuple P contains just\none field, sname.\n((22) Find the narnes of sailors who have reserved a n:.d boat.\n{P I :38 E Sailors\n:3R E Reserves(R.sid = 5.sid !\\ P.sname = S.8name",
          "pages": [
            145,
            146,
            147,
            148,
            149,
            150,
            151,
            152,
            153,
            154,
            155
          ],
          "relevance": {
            "score": 0.53,
            "sql_score": 1.0,
            "concept_score": 0.67,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Relational Algebra (nul Calculus\n)\n1\\3B E Boats(B.llid = R.md 1\\ B.color ='red'))}\nThis query can be read as follows:\n\"Retrieve all sailor tuples S for which\nthere exist tuples R in Reserves and B in Boats such that S.sid =\nR.sid,\nR.bid = B.b'id, and B.coior ='red'.\"\nAnother way to write this query, which\ncorresponds more closely to this reading, is as follows:\n{P I 3S E SailoTs 3R E Reserves 3B E Boats\n(Rsid = S.sid 1\\ B.bid = R.bid 1\\ B.color ='red' 1\\ Psname = S.sname)}\n(Q7) Find the names of sailors who have reserved at least two boats.\n{P I 3S E Sailors 3Rl E Reserves 3R2 E Reserves\n(S.sid = R1.sid 1\\ R1.sid = R2.sid 1\\ R1.bid =I- R2.bid\nI\\Psname = S.sname)}\nContrast this query with the algebra version and see how much simpler the\ncalculus version is. In part, this difference is due to the cumbersome renaming\nof fields in the algebra version, but the calculus version really is simpler.\n(Q9) Find the narnes of sailors who have reserved all boats.\n{P I 3S E Sailors VB E Boats\n(3R E Reserves(S.sid = R.sid 1\\ R.bid = B.bid 1\\ Psname = S.sname))}\nThis query was expressed using the division operator in relational algebra. Note\nhow easily it is expressed in the calculus. The calculus query directly reflects\nhow we might express the query in English: \"Find sailors S such that for all\nboats B there is a Reserves tuple showing that sailor S has reserved boat B.\"\n(Q14) Find sailors who have reserved all red boats.\n{S I S E Sailor's 1\\ VB E Boats\n(B.color ='red' :::} (3R E Reserves(S.sid = R.sid 1\\ R.bid = B.bid)))}\nThis query can be read as follows: For each candidate (sailor), if a boat is red,\nthe sailor must have reserved it. That is, for a candidate sailor, a boat being\nred must imply that the sailor has reserved it. Observe that since we can return\nan entire sailor tuple as the ans\\ver instead of just the sailor's name, we avoided\nintroducing a new free variable (e.g., the variable P in the previous example)\nto hold the answer values. On instances Bl. R2, and S3, the answer contains\nthe Sailors tuples with sids 22 and 31.\nWe can write this query without using implication, by observing that an ex-\npression of the form p :::} q is logically equivalent to -'p V q:\n{S ! S E Sailors 1\\ VB E Boats\n\nCHAPTER\n.~\n(B.coioT i-'Ted' V (3R E ReSeTVeS(S.sid = R..':tid/\\ R.b'id = B.lJid)))}\nThis query should be read a.s follows: \"Find sailors S such that, for all boats B,\neither the boat is not red or a Reserves tuple shows that sailor S has reserved\nboat B.\"\n4.3.2\nDomain Relational Calculus\nA domain variable is a variable that ranges over the values in the domain\nof some attribute (e.g., the variable can be assigned an integer if it appears\nin an attribute whose domain is the set of integers).\nA DRC query has the\nform {(XI,X2, ... ,Xn ) I P((XI,X2, ... ,Xn ))}, where each Xi is either a domain\nvariable or a constant and p((Xl, X2, ... ,xn )) denotes a DRC formula whose\nonly free variables are the variables among the Xi, 1 Sis n. The result of this\nquery is the set of all tuples (Xl, X2, ... , x n ) for which the formula evaluates to\ntrue.\nA DRC formula is defined in a manner very similar to the definition of a TRC\nformula. The main difference is that the variables are now domain variables.\nLet op denote an operator in the set {<, >, =, S,~, i-} and let X and Y be\ndomain variables. An atomic formula in DRC is one of the following:\nII\n(Xl, X2, ... , Xn )\nE Rel, where Rei is a relation with n attributes; each\nXi, 1 SiS n is either a variable or a constant\nII\nX op Y\nII\nX op constant, or constant op X\nA formula is recursively defined to be one of the following, where P and q\nare themselves formulas and p(X) denotes a formula in which the variable X\nappears:\nII\nany atomic formula\nII\n--.p, P /\\ q, P V q, or p =} q\nII\n3X(p(X)), where X is a domain variable\nII\n\\/X(p(X)), where X is a domain variable\nThe reader is invited to compare this definition with the definition of TRC\nforrnulch'3 and see how closely these two definitions correspond.\n\\Ve will not\ndefine the semantics of DRC formula.s formally; this is left as an exercise for\nthe reader.\n\nRelat'ional Algebra and Calculus\nExamples of DRC Queries\nvVe now illustrate DRC through several examples.\nThe reader is invited to\ncompare these with the TRC versions.\n(Q11) Find all sa'ilors with a rating above 7.\n{(1, N, T, A) I (I, N, T, A) E Sa'ilors /\\ T > 7}\nThis differs from the TRC version in giving each attribute a (variable) name.\nThe condition (1, N, T, A) E Sailors ensures that the domain variables I, N,\nT, and A are restricted to be fields of the same tuple. In comparison with the\nTRC query, we can say T > 7 instead of S.rating > 7, but we must specify the\ntuple (I, N, T, A) in the result, rather than just S.\n(Q1) Find the names of sailors who have reserved boat 103.\n{(N) I 31, T, A( (1, N, T, A) E Sa'ilors\n/\\311', Br, D( (11', Br, D) E Reserves /\\ 11' = I /\\ Br = 103))}\nNote that only the sname field is retained in the answer and that only N\nis a free variable.\nWe use the notation 3Ir,Br,D(... ) as a shorthand for\n3Ir(3Br(?JD(.. .))).\nVery often, all the quantified variables appear in a sin-\ngle relation, as in this example. An even more compact notation in this case\nis 3(11', Br, D) E Reserves. With this notation, which we use henceforth, the\nquery would be as follows:\n{(N) I 31, T, A((I, N, T, A) E Sailors\n/\\3(11', Br, D) E Reserves(Ir = I /\\ Br = 103))}\nThe comparison with the corresponding TRC formula should now be straight-\nforward.\nThis query can also be written as follows; note the repetition of\nvariable I and the use of the constant 103:\n{(N) I 31, T, A((1, N, T, A) E Sailors\n/\\3D( (1,103, D) E Reserves))}\n(Q2) Find the names of sailors who have Teserved a red boat.\n/\\3(1, Br, D) E ReseTves /\\ 3(Br, BN,'Ted') E Boats)}\n(Q7) Find the names of sailoT.'! who have TeseTved at least two boat.s.\n{(N) I 31, T, A((1, N, T, A) E Sailors /\\\n?JBrl, BT2, Dl, D2( (1, Brl, DI) E Reserves\n/\\(1, Br2, D2) E Reserves /\\ Brl # Br2))}\n\nCHAPTER\n..fl\nNote how the repeated use of variable I ensures that the same sailor has reserved\nboth the boats in question.\n(Q9) Find the names of sailors who have Teserved all boat8.\n{(N) I ~I, T, A( (I, N, T, A) E Sailors!\\\nVB, BN,C(-,((B, BN,C) E Boats) V\n(::J(Ir, Br, D) E Reserves(I = IT!\\ BT = B))))}\nThis query can be read as follows: \"Find all values of N such that some tuple\n(I, N, T, A) in Sailors satisfies the following condition: For every (B, BN, C),\neither this is not a tuple in Boats or there is some tuple (IT, BT, D) in Reserves\nthat proves that Sailor I has reserved boat B.\"\nThe V quantifier allows the\ndomain variables B, BN, and C to range over all values in their respective\nattribute domains, and the pattern '-,( (B, BN, C) E Boats)V' is necessary to\nrestrict attention to those values that appear in tuples of Boats. This pattern\nis common in DRC formulas, and the notation V(B, BN, C) E Boats can be\nused as a shortcut instead. This is similar to the notation introduced earlier\nfor 3. With this notation, the query would be written as follows:\n{(N)\nI 31, T, A((I, N, T, A) E Sa'iloTs !\\ V(B, BN, C) E Boats\n(3(1'1', BT, D) E ReseTves(I = IT!\\ BT = B)))}\n(Q14) Find sailoTs who have TeseTved all Ted boats.\n{(I, N, T, A)\nI (I, N, T, A) E SailoTs!\\ V(B, BN, C) E Boats\n(C ='red' =? ?J(Ir, BT, D) E Reserves(I = IT!\\ Br = B))}\nHere, we find all sailors such that, for every red boat, there is a tuple in Reserves\nthat shows the sailor has reserved it.\n4.4\nEXPRESSIVE POWER OF ALGEBRA AND\nCALCULUS\n\\Ve presented two formal query languages for the relational model. Are they\nequivalent in power?\nCan every query that can be expressed in relational\nalgebra also be expressed in relational calculus?\nThe answer is yes, it can.\nin relational algebra?\nBefore we answer this question, we consider a major\nproblem with the calculus as we presented it.\nConsider the query {S I -,(S E Sailors)}. This query is syntactically correct.\nHowever, it asks for all tuples S such that S is not in (the given instance of)\n\nRelational Algebra an,d Calculu8\nSailors. The set of such S tuples is obviously infinite, in the context of infinite\ndomains such as the set of all integers.\nThis simple example illustrates an\nunsafe query. It is desirable to restrict relational calculus to disallow unsafe\nqueries.\nvVe now sketch how calculus queries are restricted to be safe. Consider a set I\nof relation instances, with one instance per relation that appears in the query\nQ.\nLet Dom(Q, 1) be the set of all constants that appear in these relation\ninstances I or in the formulation of the query Q itself.\nSince we allow only\nfinite instances I, Dom(Q, 1) is also finite.\nFor a calculus formula Q to be considered safe, at a minimum we want to\nensure that, for any given I, the set of answers for Q contains only values in\nDom(Q, 1). While this restriction is obviously required, it is not enough. Not\nonly do we want the set of answers to be composed of constants in Dom(Q, 1),\nwe wish to compnte the set of answers by examining only tuples that contain\nconstants in Dom(Q, 1)! This wish leads to a subtle point associated with the\nuse of quantifiers V and :::J: Given a TRC formula of the form :::JR(p(R)), we want\nto find all values for variable R that make this formula true by checking only\ntuples that contain constants in Dom(Q, 1). Similarly, given a TRC formula of\nthe form VR(p(R)), we want to find any values for variable R that make this\nformula false by checking only tuples that contain constants in Dom(Q, 1).\nWe therefore define a safe TRC formula Q to be a formula such that:\n1. For any given I, the set of answers for Q contains only values that are in\nDom(Q, 1).\n2. For each subexpression of the form :::JR(p(R)) in Q, if a tuple r (assigned\nto variable R) makes the formula true, then r contains only constants in\nDorn(Q,I).\n3. For each subexpression of the form VR(p(R)) in Q, if a tuple r (assigned\nto variable R) contains a constant that is not in Dom(Q, 1), then r must\nmake the formula true.\nNote that this definition is not constructive, that is, it does not tell us hmv to\ncheck if a query is safe.\nThe query Q\n=\n{S I -.(S E Sailors)} is unsafe by this definition. Dom(Q,1)\nis the set of all values that appear in (an instance I of) Sailors. Consider the\ninstance Sl shown in Figure 4.1. The answer to this query obviously includes\nvalues that do not appear in Dorn(Q,81).",
          "pages": [
            156,
            157,
            158,
            159,
            160
          ],
          "relevance": {
            "score": 0.16,
            "sql_score": 0.5,
            "concept_score": 0.22,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "SQL Joins are operations that allow us to combine rows from two or more tables based on a related column between them. They are essential for retrieving data that spans across different tables and are used extensively in database management.",
        "explanation": "Joins solve the problem of combining data from multiple tables into a single result set. Here’s how they work:\n1. **Cross-Product**: This is the most basic join, which combines every row from one table with every row from another table. It results in a Cartesian product, which can be very large and inefficient if not used carefully.\n2. **Equi-Join**: This type of join combines rows based on equal values in specified columns between tables. It is the most common form of join and is used when you want to match records from two tables where specific fields are identical.\n3. **Natural Join**: A natural join automatically joins tables using all columns with the same name, which can simplify queries but might lead to unexpected results if not carefully managed.\nJoins are crucial in SQL because they allow us to perform complex data analysis and reporting by combining data from multiple sources.",
        "key_points": [
          "Equi-joins are most commonly used when matching records based on equal values in specified columns.",
          "Natural joins automatically join tables using all columns with the same name, which can simplify queries but might lead to unexpected results if not carefully managed.",
          "Cross-products should be avoided unless absolutely necessary due to their potential for generating very large result sets."
        ],
        "examples": [
          {
            "title": "Basic Equi-Join Example",
            "code": "SELECT s.sid, s.sname, r.bid\nFROM sailors AS s\nJOIN reserves AS r ON s.sid = r.sid;",
            "explanation": "This example joins the 'sailors' and 'reserves' tables on the 'sid' column to retrieve the sailor ID and name along with the boat ID they have reserved."
          },
          {
            "title": "Practical Natural Join Example",
            "code": "SELECT s.sid, s.sname, r.bid\nFROM sailors AS s\nNATURAL JOIN reserves AS r;",
            "explanation": "This practical example demonstrates a natural join between the 'sailors' and 'reserves' tables. It automatically joins on all columns with the same name ('sid'), which in this case is just one column."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using cross-product instead of equi-join",
            "incorrect_code": "SELECT s.sid, s.sname, r.bid\nFROM sailors AS s, reserves AS r;",
            "correct_code": "SELECT s.sid, s.sname, r.bid\nFROM sailors AS s\nJOIN reserves AS r ON s.sid = r.sid;",
            "explanation": "Cross-products can generate very large result sets and are generally not necessary unless explicitly required. Always use equi-joins for matching records based on specific conditions."
          }
        ],
        "practice": {
          "question": "Write a SQL query that joins the 'employees' table with the 'departments' table to retrieve the employee ID, name, and department name where the employee's department ID matches the department ID in the departments table.",
          "solution": "SELECT e.emp_id, e.name, d.dept_name\nFROM employees AS e\nJOIN departments AS d ON e.dept_id = d.dept_id;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Relational Algebra and CalC'ul1L8\nH15\n•\nSet-difference: R- 8 returns a relation instance containing all tuples that\noccur in R but not in 8. The relations Rand 8 must be union-compatible,\nand the sche",
      "content_relevance": {
        "score": 0.53,
        "sql_score": 1.0,
        "concept_score": 0.67,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "inner-join": {
      "id": "inner-join",
      "title": "INNER JOIN",
      "definition": "Retrieving matching rows from two or more tables",
      "difficulty": "beginner",
      "page_references": [
        140,
        141,
        142,
        143,
        144,
        145,
        146,
        147,
        148,
        149,
        150
      ],
      "sections": {
        "definition": {
          "text": "Relational Algebra and CalC'ul1L8\nH15\n•\nSet-difference: R- 8 returns a relation instance containing all tuples that\noccur in R but not in 8. The relations Rand 8 must be union-compatible,\nand the schema of the result is defined to be identical to the schema of R.\n•\nCross-product: R x 8 returns a relation instance whose schema contains\nall the fields of R (in the same order as they appear in R) followed by all\nthe fields of 8 (in the same order as they appear in 8). The result of R x 8\ncontains OIle tuple (1', s) (the concatenation of tuples rand s) for each pair\nof tuples l' E R,\nS E 8. The cross-product opertion is sometimes called\nCartesian product.\n\\\\Te use the convention that the fields of R x 8 inherit names from the\ncorresponding fields of Rand 8. It is possible for both Rand 8 to contain\none or more fields having the same name; this situation creates a naming\nconfi'ict. The corresponding fields in R x 8 are unnamed and are referred\nto solely by position.\nIn the preceding definitions, note that each operator can be applied to relation\ninstances that are computed using a relational algebra (sub)expression.\nWe now illustrate these definitions through several examples. The union of 81\nand 82 is shown in Figure 4.8. Fields are listed in order; field names are also\ninherited from 81. 82 has the same field names, of course, since it is also an\ninstance of Sailors. In general, fields of 82 may have different names; recall that\nwe require only domains to match. Note that the result is a set of tuples. TUples\nthat appear in both 81 and 82 appear only once in 81 U 82. Also, 81 uRI is\nnot a valid operation because the two relations are not union-compatible. The\nintersection of 81 and 82 is shown in Figure 4.9, and the set-difference 81- 82\nis shown in Figure 4.10.\nDustin\n45.0\nLubber\n55.5\nRusty\n35.0\nyuppy\n35.0\nguppy\n35.0\n31 u 52\nThe result of the cross-product 81 x Rl is shown in Figure 4.11. Because Rl\nand 81 both have a field named sid, by our convention on field names, the\ncorresponding two fields in 81 x Rl are unnamed, and referred to solely by the\nposition in which they appear in Figure 4.11. The fields in 81 x Rl have the\nsame domains as the corresponding fields in Rl and 5'1. In Figure 4.11, sid is\n\n.isifi\n\"\"\".h.~\"\nLubber\n55.5\nRusty\n35.0\n81 n 82\nGHAPTER f4\nli~·iiJB1ff/fj,me\nIt,{4t~rf1l1f:4f1ei I\nI 22 I Dustin I 7\n[3fOJ\n81 - 82\nlisted in parentheses to emphasize that it is not an inherited field name; only\nthe corresponding domain is inherited.\n(sid!)\nbid\naay\nDustin\n45.0\n10/10/96\nDustin\n45.0\n11/12/96\nLubber\n55.5\n10/10/96\nLubber\n55.5\n11/12/96\nRusty\n35.0\n10/10/96\nRusty\n35.0\n11/12/96\n81 x R1\n4.2.3\nRenaming\nWe have been careful to adopt field name conventions that ensure that the result\nof a relational algebra expression inherits field names from its argument (input)\nrelation instances in a natural way whenever possible. However, name conflicts\ncan arise in some cases; for example, in 81 x Rl. It is therefore convenient\nto be able to give names explicitly to the fields of a relation instance that is\ndefined by a relational algebra expression. In fact, it is often convenient to give\nthe instance itself a name so that we can break a large algebra expression into\nsmaller pieces by giving names to the results of subexpressions.\nvVe introduce a renaming operator p for this purpose. The expression p(R(F), E)\ntakes an arbitrary relational algebra expression E and returns an instance of\na (new) relation called R. R contains the same tuples as the result of E and\nhas the same schema as E, but some fields are renamed. The field names in\nrelation R are the sarne as in E, except for fields renamed in the Tenaming list\nF, which is a list of terms having the form oldname ~, newnarne or position ~\nrW1llTlJLrne. For p to be well-defined, references to fields (in the form of oldnarnes\nor posit.ions in the renaming list) may be unarnbiguous and no two fields in the\nresult may have the same name. Sometimes we want to only renarne fields or\n(re)name the relation; we therefore treat both Rand F as optional in the use\nof p. (Of course, it is meaningless to omit both.)\n\nRelational AlgebTa and Calc\"Uh18\nFor example, the expression p(C(l\n----7 s'id1,5\n----7 sid2), 81 x R1) returns a\nrelation that contains the tuples shown in Figure 4.11 and has the following\nschema: C(sidl: integer, ,marrw: string, mt'ing: integer, age: real, sid2:\ninteger, bid: integer, day: dates).\nIt is customary to include some additional operators in the algebra, but all of\nthem can be defined in terms of the operators we have defined thus far.\n(In\nfact, the renaming operator is needed only for syntactic convenience, and even\nthe n operator is redundant; R n 8 can be defined as R - (R - 8).) We consider\nthese additional operators and their definition in terms of the basic operators\nin the next two subsections.\n4.2.4\nJoins\nThe join operation is one of the most useful operations in relational algebra\nand the most commonly used way to combine information from two or more\nrelations. Although a join can be defined as a cross-product followed by selec-\ntions and projections, joins arise much more frequently in practice than plain\ncross-products. Further, the result of a cross-product is typically much larger\nthan the result of a join, and it is very important to recognize joins and imple-\nment them without materializing the underlying cross-product (by applying the\nselections and projections 'on-the-fly'). For these reasons, joins have received\na lot of attention, and there are several variants of the join operation. 1\nCondition Joins\nThe most general version of the join operation accepts a join condition c and\na pair of relation instances as arguments and returns a relation instance. The\njoin cond'it-ion is identical to a selection condition in form.\nThe operation is\ndefined as follows:\nR [:X)e S =\nO\"e(R X S)\nThus [:X) is defined to be a cross-product followed by a selection. Note that the\ncondition c can (and typically does) refer to attributes of both Rand S. The\nreference to an attribute of a relation, say, R, can be by positioll (of the form\nR.i) or by Ilame (of the form R.name).\nAs an example, the result of Sl\n[><JS1.8id<Rl.sid R1 is shown in Figure 4.12.\nBecause sid appears in both 81 and R1, the corresponding fields in the result\nof the cross-product 81 x R1 (and therefore in the result of 81 [:X)S1.sid<Rl.sid R1)\n1Several variants of joins are not discussed in this chapter.\nAn important c.la..'iS of joins, called\n01lter joins, is discussed in Chapter 5.\n\nCHAPTER ..t1\nare unnamed. Domains are inherited from the corresponding fields of 81 and\nRl.\nI (sid) I snarne I rating I age\nI···{si41:l bid\nda/lj\nI 22\nI Dustin\n45.0\nI 103 I 11/12/96\nI 31\nI Lubber\n.- 55.5\nI 103 I 11/12/96\n51 NSl. s id<R1.sid R1\nEquijoin\nA common special case of the join operation R [>(] 8 is when the join condition\nconsists solely of equalities (connected by 1\\) of the form R.name1 = 8.name2,\nthat is, equalities between two fields in Rand S. In this case, obviously, there is\nsome redundancy in retaining both attributes in the result. For join conditions\nthat contain only such equalities, the join operation is refined by doing an\nadditional projection in which 8.name2 is dropped. The join operation with\nthis refinement is called equijoin.\nThe schema of the result of an equijoin contains the fields of R (with the same\nnames and domains as in R) followed by the fields of 8 that do not appear\nin the join conditions. If this set of fields in the result relation includes two\nfields that inherit the same name from Rand 8, they are unnamed in the result\nrelation.\nWe illustrate 81l:<JR.sid=5.sid Rl in Figure 4.13. Note that only one field called\nsid appears in the result.\n~ ,marneI rating I age I· bid·1 day =tJ\nI 22\nDustIn I 7\nI 45.0\n101 I 10/10/96 I\nI 58\nRust}~ I 10\nI ~5.0\n103 I 11/12/96 I\n81 MR ..,H1=S'.\"id HI\nNatural Join\nA further special ca.'3e of the join operation R\n[>(] S is an eqUlJom in which\nequalities arc specified on all fields having the same name in Rand S.\nIn\nthis case, we can simply omit the join condition; the default is that the join\ncondition is a collection of equalities on all common fields. We call this special\ncase a natumJ jo'in, and it has the nice property that the result is guaranteed\nnot to have two fields with the saIne name.",
          "pages": [
            140,
            141,
            142,
            143
          ],
          "relevance": {
            "score": 0.39,
            "sql_score": 0.9,
            "concept_score": 0.44,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Relai'ional Algelrra and Calculus\nlQ9\nThe equijoin expression 81 D<m..sid=s.sid R1 is actually a natural join al1d can\nsimply be denoted as 81 [Xl R1, since the only common field is sid. If the two\nrelations have no attributes in common, 81 [Xl Rl is simply the cross-product.\n4.2.5\nDivision\nThe division operator is useful for expressing certain kinds of queries for exam-\nple, \"Find the names of sailors who have reserved all boats.\"\nUnderstanding\nhow to use the basic operators of the algebra to define division is a useful exer-\ncise. However, the division operator does not have the same importance as the\nother operators-it is not needed as often, and database systems do not try to\nexploit the semantics of division by implementing it as a distinct operator (as,\nfor example, is done with the join operator).\nWe discuss division through an example.\nConsider two relation instances A\nand B in which A has (exactly) two fields x and y and B has just one field y,\nwith the same domain as in A. We define the division operation AlB as the\nset of all x values (in the form of unary tuples) such that for every y value in\n(a tuple of) B, there is a tuple (x,y) in A.\nAnother way to understand division is as follows. For each x value in (the first\ncolumn of) A, consider the set of y values that appear in (the second field of)\ntuples of A with that x value. If this set contains (all y values in) B, the x\nvalue is in the result of AlB.\nAn analogy with integer division may also help to understand division.\nFor\nintegers A and B, AlB is the largest integer Q such that Q * B\n::::; A.\n:For\nrelation instances A and B, AlB is the largest relation instance Q such that\nQ x B S:::A.\nDivision is illustrated in Figure 4.14. It helps to think of A as a relation listing\nthe parts supplied by suppliers and of the B relations as listing parts. AIB'i\ncomputes suppliers who supply all parts listed in rdation instance Bi.\nExpressing AlBin terms of the ba...sic algebra operators is an interesting ex-\nercise, and the reader should try to do this before reading further. The basic\nidea is to compute all :r values in A that are not disqualified.\nAn x value is\ndisqualified if lJy attaching a y value from B, we obtain a tuple (x,y) that is not\nin A. We can compute disqualified tuples using the algebra expression\nThus, we can define AlBa.....,\n\nA\nI sno i pno I\n\\\npI I\nsl\np2\n~._~~\ni\np4\n\\\npI\n~_P2\n-~\n'\np2\np2\ns4\np4\nCHAPTER$4\nBl\n[P.!!~J\nAlBl\n~;~J\n,....---,\n~\nLEU\nI 82 I\nB2\nI pno ]\nBE\np4\n~\nAlB2\nB3 ~\nBE\nB1J\np2\n~\np4\nAlB3 [ill\nExamples Illustrating Division\nTo understand the division operation in full generality, we have to consider the\ncase when both x and yare replaced by a set of attributes. The generalization is\nstraightforward and left as an exercise for the reader. We discuss two additional\nexamples illustrating division (Queries Q9 and Q10) later in this section.\n4.2.6\nMore Examples of Algebra Queries\nWe now present several examples to illustrate how to write queries in relational\nalgebra. We use the Sailors, Reserves, and Boats schema for all our examples\nin this section. We use parentheses as needed to make our algebra expressions\nunambiguous.\nNote that all the example queries in this chapter are given\na unique query number. The query numbers are kept unique across both this\nchapter and the SQL query chapter (Chapter 5). This numbering makes it easy\nto identify a query when it is revisited in the context of relational calculus and\nSQL and to compare different ways of writing the same query. (All references\nto a query can be found in the subject index.)\nIn the rest of this chapter (and in Chapter 5), we illustrate queries using the\ninstances 83 of Sailors, R2 of Reserves, and B1 of Boats, shown in Figures\n4.15, 4.16, and 4.17, respectively.\n(Q1) Find the names of sailors who have rcscT'ucd boat lOS.\nThis query can be written as follows:\n\".marne (( (Jbid=1O:~Re.5erve8)[XJ 8ailoT.5)\n\nRelational Algebra and Calcul1ls\n~l\n»>\n.....\"\"\"'.+A.\".,'\"\nhAh\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nHoratio\n35.0\nArt\n25.5\nBob\n63.5\nAn Instance 83 of Sailors\n111,\n10/10/98\n10/10/98\n10/8/98\n10/7/98\n11/10/98\n11/6/98\n11/12/98\n9/5/98\n9/8/98\n9/8/98\nAn Instance R2 of Reserves\nWe first compute the set of tuples in Reserves with bid = 103 and then take the\nnatural join of this set with Sailors. This expression can be evaluated on in-\nstances of Reserves and Sailors. Evaluated on the instances R2 and S3, it yields\na relation that contains just one field, called sname, and three tuples (Dustin),\n(Horatio), and (Lubber). (Observe that two sailors are called Horatio and only\none of them has reserved a red boat.)\n[~]bname\nI color· I\nInterlake\nblue\nInterlake\nred\nClipper\ngreen\nMarine\nred\nAn Instance HI of Boats\nWe can break this query into smaller pieces llsing the renaming operator p:\np(Temp1, IJbir1=103 ReseTves)\np(Temp2, Temp11XJ Sailor's)\n1Tsname(Temp2)\nNotice that because we are only llsing p to give names to intermediate relations,\nthe renaming list is optional and is omitted. TempI denotes an intermediate\nrelation that identifies reservations of boat 103. Temp2 is another intermediate\nrelation, and it denotes sailors who have mad(~ a reservation in the set Templ.\nThe instances of these relations when evaluating this query on the instances R2\nand S3 are illustrated in Figures 4.18 and 4.19. Finally, we extract the sname\ncolumn from Temp2.\n\n10~~\n10/8/98\n11/6/98\n9/8/98\nDustin\nLubber\nHoratio\nCHAPTER;l\n10/8/98\n11/6/98--\n9/8/98\nInstance of TempI\nInstance of Temp2\nThe version of the query using p is essentially the same as the original query;\nthe use of p is just syntactic sugar. However, there are indeed several distinct\nways to write a query in relational algebra. Here is another way to write this\nquery:\nJrsname(CJbid=103(Reserves IX! Sailors))\nIn this version we first compute the natural join of Reserves and Sailors and\nthen apply the selection and the projection.\nThis example offers a glimpse of the role played by algebra in a relational\nDBMS. Queries are expressed by users in a language such as SQL. The DBMS\ntranslates an SQL query into (an extended form of) relational algebra and\nthen looks for other algebra expressions that produce the same answers but are\ncheaper to evaluate. If the user's query is first translated into the expression\n7fsname (CJbid=103 (Reserves IX! Sailors))\na good query optimizer will find the equivalent expression\n7rsname ((CJb·id=103Reserves) IX! Sailors)\nFurther, the optimizer will recognize that the second expression is likely to\nbe less expensive to compute because the sizes of intermediate relations are\nsmaller, thanks to the early use of selection.\n(Q2) Find the names of sailors who ha've reserved a red boat.\n7f.marne ((CJcolor='red'Boats) IX! Reserves !><J SailoI's)\nThis query involves a series of two joins. First, we choose (tuples describing)\nred boats.\nThen, we join this set with Reserves (natural join, with equality\nspecified on thE) bid column) to identify reservations of red boats.\nNext, we\njoin the resulting intermediate relation with Sailors (natural join, with equality\nspecified on the sid column) to retrieve the names of sailors who have rnade\nreservations for red boats. Finally, we project the sailors' names. The answer,\nwhen evaluated on the instances B1, R2, and S3, contains the names Dustin,\nHoratio, and Lubber.\n\nRelational Algebra and Calculus\nAn equivalent expression is:\nB.3\nThe reader is invited to rewrite both of these queries by using p to make the\nintermediate relations explicit and compare the schema.<=; of the intermediate\nrelations.\nThe second expression generates intermediate relations with fewer\nfields (and is therefore likely to result in intermediate relation instances with\nfewer tuples as well). A relational query optimizer would try to arrive at the\nsecond expression if it is given the first.\n(Q3) Find the colors of boats reserved by Lubber.\nJrcolor((asname='Lubber,Sa'ilors) [XJ Reserves [XJ Boats)\nThis query is very similar to the query we used to compute sailors who reserved\nred boats. On instances Bl, R2, and S3, the query returns the colors green\nand red.\n(Q4) Find the names of sailors who have reserved at least one boat.\nJrsname(Sailors [XJ Reserves)\nThe join of Sailors and Reserves creates an intermediate relation in which tuples\nconsist of a Sailors tuple 'attached to' a Reserves tuple. A Sailors tuple appears\nin (some tuple of) this intermediate relation only if at least one Reserves tuple\nhas the same sid value, that is, the sailor has made some reservation.\nThe\nanswer, when evaluated on the instances Bl, R2 and S3, contains the three\ntuples (Dustin), (HoTatio) , and (LubbeT).\nEven though two sailors called\nHoratio have reserved a boat, the answer contains only one copy of the tuple\n(HoTatio) , because the answer is a relation, that is, a set of tuples, with no\nduplicates.\nAt this point it is worth remarking on how frequently the natural join operation\nis used in our examples. This frequency is more than just a coincidence based\non the set of queries we have chosen to discuss; the natural join is a very\nnatural, widely used operation. In particular, natural join is frequently used\nwhen joining two tables on a foreign key field. In Query Q4, for exalnple, the\njoin equates the sid fields of Sailors and Reserves, and the sid field of Reserves\nis a foreign key that refers to the sid field of Sailors.\n(Q5) Find the narnes of sailors who have reserved a Ted OT a gTeen boat.\np(Tempboats, (acoloT='rcd' Boats) U (acolor='green' Boats))\nJrsna·me(Tempboats [XJ ReseTves [XJ Sailors)\n\nCHAPTER $4\nvVe identify the set of all boats that are either red or green (Tempboats, which\ncontains boats \\vith the bids 102, 103, and 104 on instances E1, R2, and S3).\nThen we join with Reserves to identify sid.., of sailors who have reserved OIle of\nthese boats; this gives us sids 22, 31, 64, and 74 over our example instances.\nFinally, we join (an intermediate relation containing this set of sids) with Sailors\nto find the names of Sailors with these sids. This gives us the names Dustin,\nHoratio, and Lubber on the instances E1, R2, and S3.\nAnother equivalent\ndefinition is the following:\np(Tempboats, (acolor='red'Vcolor='green' Boats))\n7fsname(Tempboats [><] Reserves [><] Sailors)\nLet us now consider a very similar query.\n(Q6) Find the names of sailors who have reserved a red and a green boat. It\nis tempting to try to do this by simply replacing U by n in the definition of\nTempboats:\np(Tempboats2, (acolor='red,Eoats) n (O\"color='green,Boats))\n'7fsname(Tempboats2 [><] Reserves [><] Sailors)\nHowever, this solution is incorrect-it instead tries to compute sailors who have\nreserved a boat that is both red and green. (Since bid is a key for Boats, a boat\ncan be only one color; this query will always return an empty answer set.) The\ncorrect approach is to find sailors who have reserved a red boat, then sailors\nwho have reserved a green boat, and then take the intersection of these two\nsets:\np(Tempred, '7fsid((acolor='red' Eoats) [><] Reserves))\np(Tempgreen, '7fsid((O\"color='green,Boats) [><] Reserves))\n'7f,marne((Ternpred n Tempgreen)\n[><] Sailors)\nThe two temporary relations compute the sids of sailors, and their intersection\nidentifies sailors who have reserved both red and green boats.\nOn instances\nBI, R2, and 53, the sids of sailors who have reserved a red boat are 22, 31,\nand 64. The s'icLs of sailors who have reserved a green boat are 22, 31, and 74.\nThus, sailors 22 and 31 have reserved both a red boat and a green boat; their\nnames are Dustin and Lubber.\nThis formulation of Query Q6 can easily be adapted to find sailors \\vho have\nreserved red or green boats (Query Q5); just replace n by U:\np(TempTed, '7fsid( (O\"color=lrcd' Boats) [)<] Reserves))\np(Tempgreen, '7fsid((O\"color='green' Boats) [)<] Reserves))\n'7fsTwme((Tempred U Tempgreen) [)<] Sailors)",
          "pages": [
            144,
            145,
            146,
            147,
            148,
            149
          ],
          "relevance": {
            "score": 0.53,
            "sql_score": 1.0,
            "concept_score": 0.67,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Relatiorwl Algebra and Galcurus\nIn the formulations of Queries Q5 and Q6, the fact that sid (the field over\nwhich we compute union or intersection) is a key for Sailors is very important.\nConsider the following attempt to answer Query Q6:\np(Tempred, Jrsname((CJcolor='red,Boats) [><] Reserves [><] Sailors))\np(Tempgreen,Jrsname((CJcoloT='gTeenlBoats) [><] Reserves [><] Sailors))\nTempred n Tempgreen\nThis attempt is incorrect for a rather subtle reason. Two distinct sailors with\nthe same name, such as Horatio in our example instances, may have reserved\nred and green boats, respectively. In this case, the name Horatio (incorrectly)\nis included in the answer even though no one individual called Horatio has\nreserved a red boat and a green boat. The cause of this error is that sname\nis used to identify sailors (while doing the intersection) in this version of the\nquery, but sname is not a key.\n(Q7) Find the names of sailors who have reser-ved at least two boats.\np(Reser-vations, Jrsid,sname,bid (Sailors [><] Reserves))\np(Reservationpairs(l\n---'? sid1, 2 ---'? sname1, 3 ---'? bid1, 4 ---'? sid2,\n5 ---'? sname2, 6 ---'? bid2), Reservations x Reservations)\nJrsname1 CJ(sidl=sid2)I\\(bidl=1-bid2) Reservationpair-s\nFirst, we compute tuples of the form (sid,sname, bid) , where sailor sid has made\na reservation for boat bid; this set of tuples is the temporary relation Reserva-\ntions. Next we find all pairs of Reservations tuples where the same sailor has\nmade both reservations and the boats involved are distinct. Here is the central\nidea: To show that a sailor has reserved two boats, we must find two Reserva-\ntions tuples involving the same sailor but distinct boats. Over instances El,\nR2, and S3, each of the sailors with sids 22, 31, and 64 have reserved at least\ntwo boats. Finally, we project the names of such sailors to obtain the answer,\ncontaining the names Dustin, Horatio, and Lubber.\nNotice that we included sid in Reservations because it is the key field identifying\nsailors, and we need it to check that two Reservations tuples involve the same\nsailor. As noted in the previous example, we cannot use sname for this purpose.\n(Q8) Find the sids of sailors w'ith age over 20 who have not TeseTved a Ted boat.\nJrsid(CJage>20Sa'ilors) -\n7rsid((CJco[oT='red,Boats) [><] Reserves [><] Sa'ilors)\nThis query illustrates the use of the set-difference operator. Again, we use the\nfact that sid is the key for Sailors. vVe first identify sailors aged over 20 (over",
          "pages": [
            150
          ],
          "relevance": {
            "score": 0.18,
            "sql_score": 0.4,
            "concept_score": 0.11,
            "non_sql_penalty": 0.0,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "An INNER JOIN is a type of join operation that combines rows from two tables based on a related column between them, returning only the rows where there is a match.",
        "explanation": "INNER JOINs are used when you want to retrieve data from two or more tables based on a common attribute. Imagine you have two tables: one for 'Customers' and another for 'Orders'. You can use an INNER JOIN to find out which orders belong to each customer. The join condition is typically specified using the ON keyword, followed by the column names that match in both tables.",
        "key_points": [
          "Key point 1: Only returns rows where there is a match in both tables",
          "Key point 2: Commonly used when you need to combine data from two related tables",
          "Key point 3: Can be used with multiple join conditions if necessary",
          "Key point 4: Important to ensure the join condition accurately reflects the relationship between tables",
          "Key point 5: Connects directly to other relational algebra concepts like CROSS JOIN and OUTER JOIN"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "SELECT Customers.CustomerName, Orders.OrderID\nFROM Customers\nINNER JOIN Orders ON Customers.CustomerID = Orders.CustomerID;",
            "explanation": "This example retrieves the customer name and order ID for each order made by a customer. Only rows where there is a match in both tables (i.e., a customer has placed an order) are returned."
          },
          {
            "title": "Practical Example",
            "code": "SELECT Employees.EmployeeName, Departments.DepartmentName\nFROM Employees\nINNER JOIN Departments ON Employees.DepartmentID = Departments.DepartmentID;",
            "explanation": "In a real-world scenario, this query would return the name of each employee along with the name of their department. This helps in understanding the organizational structure and who works where."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to specify the join condition",
            "incorrect_code": "SELECT Employees.EmployeeName, Departments.DepartmentName\nFROM Employees\nINNER JOIN Departments;",
            "correct_code": "SELECT Employees.EmployeeName, Departments.DepartmentName\nFROM Employees\nINNER JOIN Departments ON Employees.DepartmentID = Departments.DepartmentID;",
            "explanation": "This mistake happens when you try to perform an INNER JOIN without specifying how the tables are related. Always include the ON keyword followed by the join condition."
          }
        ],
        "practice": {
          "question": "Given two tables, 'Employees' and 'Departments', where 'Employees' has columns 'EmployeeID' and 'DepartmentID', and 'Departments' has columns 'DepartmentID' and 'DepartmentName', write an INNER JOIN query to retrieve the employee name and department name.",
          "solution": "SELECT Employees.EmployeeName, Departments.DepartmentName\nFROM Employees\nINNER JOIN Departments ON Employees.DepartmentID = Departments.DepartmentID;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Relational Algebra and CalC'ul1L8\nH15\n•\nSet-difference: R- 8 returns a relation instance containing all tuples that\noccur in R but not in 8. The relations Rand 8 must be union-compatible,\nand the sche",
      "content_relevance": {
        "score": 0.53,
        "sql_score": 1.0,
        "concept_score": 0.67,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "outer-join": {
      "id": "outer-join",
      "title": "OUTER JOIN",
      "definition": "Retrieving all rows from one table and matching rows from another (LEFT, RIGHT, FULL)",
      "difficulty": "intermediate",
      "page_references": [
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160
      ],
      "sections": {
        "definition": {
          "text": "Relational Algebra and Calculus\nAn equivalent expression is:\nB.3\nThe reader is invited to rewrite both of these queries by using p to make the\nintermediate relations explicit and compare the schema.<=; of the intermediate\nrelations.\nThe second expression generates intermediate relations with fewer\nfields (and is therefore likely to result in intermediate relation instances with\nfewer tuples as well). A relational query optimizer would try to arrive at the\nsecond expression if it is given the first.\n(Q3) Find the colors of boats reserved by Lubber.\nJrcolor((asname='Lubber,Sa'ilors) [XJ Reserves [XJ Boats)\nThis query is very similar to the query we used to compute sailors who reserved\nred boats. On instances Bl, R2, and S3, the query returns the colors green\nand red.\n(Q4) Find the names of sailors who have reserved at least one boat.\nJrsname(Sailors [XJ Reserves)\nThe join of Sailors and Reserves creates an intermediate relation in which tuples\nconsist of a Sailors tuple 'attached to' a Reserves tuple. A Sailors tuple appears\nin (some tuple of) this intermediate relation only if at least one Reserves tuple\nhas the same sid value, that is, the sailor has made some reservation.\nThe\nanswer, when evaluated on the instances Bl, R2 and S3, contains the three\ntuples (Dustin), (HoTatio) , and (LubbeT).\nEven though two sailors called\nHoratio have reserved a boat, the answer contains only one copy of the tuple\n(HoTatio) , because the answer is a relation, that is, a set of tuples, with no\nduplicates.\nAt this point it is worth remarking on how frequently the natural join operation\nis used in our examples. This frequency is more than just a coincidence based\non the set of queries we have chosen to discuss; the natural join is a very\nnatural, widely used operation. In particular, natural join is frequently used\nwhen joining two tables on a foreign key field. In Query Q4, for exalnple, the\njoin equates the sid fields of Sailors and Reserves, and the sid field of Reserves\nis a foreign key that refers to the sid field of Sailors.\n(Q5) Find the narnes of sailors who have reserved a Ted OT a gTeen boat.\np(Tempboats, (acoloT='rcd' Boats) U (acolor='green' Boats))\nJrsna·me(Tempboats [XJ ReseTves [XJ Sailors)\n\nCHAPTER $4\nvVe identify the set of all boats that are either red or green (Tempboats, which\ncontains boats \\vith the bids 102, 103, and 104 on instances E1, R2, and S3).\nThen we join with Reserves to identify sid.., of sailors who have reserved OIle of\nthese boats; this gives us sids 22, 31, 64, and 74 over our example instances.\nFinally, we join (an intermediate relation containing this set of sids) with Sailors\nto find the names of Sailors with these sids. This gives us the names Dustin,\nHoratio, and Lubber on the instances E1, R2, and S3.\nAnother equivalent\ndefinition is the following:\np(Tempboats, (acolor='red'Vcolor='green' Boats))\n7fsname(Tempboats [><] Reserves [><] Sailors)\nLet us now consider a very similar query.\n(Q6) Find the names of sailors who have reserved a red and a green boat. It\nis tempting to try to do this by simply replacing U by n in the definition of\nTempboats:\np(Tempboats2, (acolor='red,Eoats) n (O\"color='green,Boats))\n'7fsname(Tempboats2 [><] Reserves [><] Sailors)\nHowever, this solution is incorrect-it instead tries to compute sailors who have\nreserved a boat that is both red and green. (Since bid is a key for Boats, a boat\ncan be only one color; this query will always return an empty answer set.) The\ncorrect approach is to find sailors who have reserved a red boat, then sailors\nwho have reserved a green boat, and then take the intersection of these two\nsets:\np(Tempred, '7fsid((acolor='red' Eoats) [><] Reserves))\np(Tempgreen, '7fsid((O\"color='green,Boats) [><] Reserves))\n'7f,marne((Ternpred n Tempgreen)\n[><] Sailors)\nThe two temporary relations compute the sids of sailors, and their intersection\nidentifies sailors who have reserved both red and green boats.\nOn instances\nBI, R2, and 53, the sids of sailors who have reserved a red boat are 22, 31,\nand 64. The s'icLs of sailors who have reserved a green boat are 22, 31, and 74.\nThus, sailors 22 and 31 have reserved both a red boat and a green boat; their\nnames are Dustin and Lubber.\nThis formulation of Query Q6 can easily be adapted to find sailors \\vho have\nreserved red or green boats (Query Q5); just replace n by U:\np(TempTed, '7fsid( (O\"color=lrcd' Boats) [)<] Reserves))\np(Tempgreen, '7fsid((O\"color='green' Boats) [)<] Reserves))\n'7fsTwme((Tempred U Tempgreen) [)<] Sailors)\n\nRelatiorwl Algebra and Galcurus\nIn the formulations of Queries Q5 and Q6, the fact that sid (the field over\nwhich we compute union or intersection) is a key for Sailors is very important.\nConsider the following attempt to answer Query Q6:\np(Tempred, Jrsname((CJcolor='red,Boats) [><] Reserves [><] Sailors))\np(Tempgreen,Jrsname((CJcoloT='gTeenlBoats) [><] Reserves [><] Sailors))\nTempred n Tempgreen\nThis attempt is incorrect for a rather subtle reason. Two distinct sailors with\nthe same name, such as Horatio in our example instances, may have reserved\nred and green boats, respectively. In this case, the name Horatio (incorrectly)\nis included in the answer even though no one individual called Horatio has\nreserved a red boat and a green boat. The cause of this error is that sname\nis used to identify sailors (while doing the intersection) in this version of the\nquery, but sname is not a key.\n(Q7) Find the names of sailors who have reser-ved at least two boats.\np(Reser-vations, Jrsid,sname,bid (Sailors [><] Reserves))\np(Reservationpairs(l\n---'? sid1, 2 ---'? sname1, 3 ---'? bid1, 4 ---'? sid2,\n5 ---'? sname2, 6 ---'? bid2), Reservations x Reservations)\nJrsname1 CJ(sidl=sid2)I\\(bidl=1-bid2) Reservationpair-s\nFirst, we compute tuples of the form (sid,sname, bid) , where sailor sid has made\na reservation for boat bid; this set of tuples is the temporary relation Reserva-\ntions. Next we find all pairs of Reservations tuples where the same sailor has\nmade both reservations and the boats involved are distinct. Here is the central\nidea: To show that a sailor has reserved two boats, we must find two Reserva-\ntions tuples involving the same sailor but distinct boats. Over instances El,\nR2, and S3, each of the sailors with sids 22, 31, and 64 have reserved at least\ntwo boats. Finally, we project the names of such sailors to obtain the answer,\ncontaining the names Dustin, Horatio, and Lubber.\nNotice that we included sid in Reservations because it is the key field identifying\nsailors, and we need it to check that two Reservations tuples involve the same\nsailor. As noted in the previous example, we cannot use sname for this purpose.\n(Q8) Find the sids of sailors w'ith age over 20 who have not TeseTved a Ted boat.\nJrsid(CJage>20Sa'ilors) -\n7rsid((CJco[oT='red,Boats) [><] Reserves [><] Sa'ilors)\nThis query illustrates the use of the set-difference operator. Again, we use the\nfact that sid is the key for Sailors. vVe first identify sailors aged over 20 (over\n\ninstances B1, R2, and S3, .'!'ids 22, 29, 31, 32, 58, 64, 74, 85, and 95) and then\ndiscard those who have reserved a red boat (sid.c; 22, 31, and 64), to obtain the\nanswer (sids 29, 32, 58, 74, 85, and 95). If we want to compute the names of\nsuch sailors, \\ve must first compute their sids (as shown earlier) and then join\nwith Sailors and project the sname values.\n(Q9) Find the names of sailors 'Who have rese'rved all boats.\nThe use of the word all (or every) is a good indication that the division operation\nmight be applicable:\np(Tempsids, (7l\"sid,bidReserves) / (7l\"bidBoats))\n7l\"sname(Tempsids N Sailors)\nThe intermediate relation Tempsids is defined using division and computes the\nset of sids of sailors who have reserved every boat (over instances Bl, R2, and\nS3, this is just sid 22). Note how we define the two relations that the division\noperator (/) is applied to·-·--the first relation has the schema (sid,bid) and the\nsecond has the schema (b'id). Division then returns all sids such that there is a\ntuple (sid,bid) in the first relation for each bid in the second. Joining Tempsids\nwith Sailors is necessary to associate names with the selected sids; for sailor\n22, the name is Dustin.\n(Q10) Find the names of sailors 'Who have reserved all boats called Interlake.\np(Tempsids, (7l\".5'id,bidReserves) / (7l\"bid((Jbname='Interlake' Boats)))\n7l\"sname(Tempsids [Xl Sailors)\nThe only difference with respect to the previous query is that now we apply a\nselection to Boats, to ensure that we compute bids only of boats named Interlake\nin defining the second argument to the division operator. Over instances El,\nR2, and S3, Tempsids evaluates to sids 22 and 64, and the answer contains\ntheir names, Dustin and Horatio.\nRELATIONAL CALCULUS\nRelational calculus is an alternative to relational algebra. In contra.':;t to the\nalgebra, which is procedural, the calculus is nonprocedural, or declarative, in\nthat it allows us to describe the set of answers without being explicit about\nhow they should be computed. Relational calculus has had a big influence on\nthe design of commercial query languages such a,s SQL and, especially, Query-\nby-Example (QBE).\nThe variant of the calculus we present in detail is called the tuple relational\ncalculus (TRC). Variables in TRC take on tuples as values. In another vari-\n\nRelatiO'Tul,l Algebra and Calculus\nant, called the domain relational calculus (DRC), the variables range over\nfield values. TRC has had more of an influence on SQL, \\vhile DRC has strongly\ninfluenced QBE. vVe discuss DRC in Section 4.3.2.2\n4$3.1\nTuple Relational Calculus\nA tuple variable is a variable that takes on tuples of a particular relation\nschema as values. That is, every value assigned to a given tuple variable has\nthe same number and type of fields. A tuple relational calculus query has the\nform { T I p(T) }, where T is a tuple variable and p(T) denotes a formula that\ndescribes T; we will shortly define formulas and queries rigorously. The result\nof this query is the set of all tuples t for which the formula p(T) evaluates to\ntrue with T = t. The language for writing formulas p(T) is thus at the heart of\nTRC and essentially a simple subset of first-order logic. As a simple example,\nconsider the following query.\n(Q11) Find all sailors with a rating above 7.\n{S I S E Sailors 1\\ S.rating > 7}\nWhen this query is evaluated on an instance of the Sailors relation, the tuple\nvariable S is instantiated successively with each tuple, and the test S. rat'ing> 7\nis applied. The answer contains those instances of S that pass this test. On\ninstance S3 of Sailors, the answer contains Sailors tuples with sid 31, 32, 58,\n71, and 74.\nSyntax of TRC Queries\nWe now define these concepts formally, beginning with the notion of a formula.\nLet Rel be a relation name, Rand S be tuple variables, a be an attribute of\nR, and b be an attribute of S. Let op denote an operator in the set {<, >, =\n, :S;, 2:, =I-}. An atomic formula is one of the following:\nIII\nR E Ref\nlIII\nR.a op S.b\nIIlI\nR.a op constant, or constant op R.a\nA formula is recursively defined to be one of the following, where p and q\nare themselves formula.s and p(R) denotes a formula in which the variable R\nappears:\n.~-----------\n2The material on DRC is referred to in the (online) chapter OIl QBE; with the exception of this\nchapter, the material on DRC and TRe can be omitted without loss of continuity.",
          "pages": [
            148,
            149,
            150,
            151,
            152
          ],
          "relevance": {
            "score": 0.52,
            "sql_score": 1.0,
            "concept_score": 0.44,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER .,4\n•\nany atomic formula\n•\n-'p, P /\\ q, P V q, or p :::} q\n•\n3R(p(R)), where R is a tuple variable\n•\n'ifR(p(R)) , where R is a tuple variable\nIn the last two clauses, the quantifiers :3 and 'if are said to bind the variable R.\nA variable is said to be free in a formula or subformuia (a formula contained\nin a larger formula) if the (sub)formula does not contain an occurrence of a\nquantifier that binds it.3\nWe observe that every variable in a TRC formula appears in a subformula\nthat is atomic, and every relation schema specifies a domain for each field; this\nobservation ensures that each variable in a TRC formula has a well-defined\ndomain from which values for the variable are drawn. That is, each variable\nhas a well-defined type, in the programming language sense.\nInformally, an\natomic formula R E Rei gives R the type of tuples in ReI, and comparisons\nsuch as R.a op S.b and R.a op constant induce type restrictions on the field\nR.a. If a variable R does not appear in an atomic formula of the form R E Rei\n(Le., it appears only in atomic formulas that are comparisons), we follow the\nconvention that the type of R is a tuple whose fields include all (and only) fields\nof R that appear in the formula.\nWe do not define types of variables formally, but the type of a variable should\nbe clear in most cases, and the important point to note is that comparisons of\nvalues having different types should always fail.\n(In discussions of relational\ncalculus, the simplifying assumption is often made that there is a single domain\nof constants and this is the domain associated with each field of each relation.)\nA TRC query is defined to be expression of the form {T I p(T)}, where T is\nthe only free variable in the formula p.\nSemantics of TRC Queries\nWhat does a TRC query mean? More precisely, what is the set of answer tuples\nfor a given TRC query? The answer to a TRC query {T I p(T)}, as noted\nearlier, is the set of all tuples t for which the formula peT) evaluates to true\nwith variable T &'3signed the tuple value t:. To complete this definition, we must\nstate which assignments of tuple values to the free variables in a formula make\nthe formula evaluate to true.\n3vVe make the assumption that each variable in a formula is either free or bound by exactly one\noccurrence of a quantifier, to avoid worrying about details such a.'l nested occurrences of quantifiers\nthat bind some, but not all, occurrences of variables.\n\nRelational Algebra and Calcuhl8\nA query is evaluated on a given instance of the database. Let each free variable\nin a formula F be bound to a tuple value. For the given assignment of tuples\nto variables, with respect to the given database instance, F evaluates to (or\nsimply 'is') true if one of the following holds:\n•\nF is an atomic formula R E Rel, and R is assigned a tuple in the instance\nof relation Rel.\n•\nF is a comparison R.a op S.b, R.a op constant, or constant op R.a, and\nthe tuples assigned to Rand S have field values R.a and S.b that make the\ncomparison true.\n•\nF is of the form ---,p and p is not true, or of the form p 1\\ q, and both p and\nq are true, or of the form p V q and one of them is true, or of the form\np =} q and q is true whenever4 p is true.\n•\nF is of the form 3R(p(R)), and there is some assignment of tuples to the\nfree variables in p(R), including the variable R,5 that makes the formula\np(R) true.\n•\nF is of the form VR(p(R)), and there is some assignment of tuples to the\nfree variables in p(R) that makes the formula p(R) true no matter what\ntuple is assigned to R.\nExamples of TRC Queries\nWe now illustrate the calculus through several examples, using the instances\nB1 of Boats, R2 of Reserves, and S3 of Sailors shown in Figures 4.15, 4.16,\nand 4.17. We use parentheses as needed to make our formulas unambiguous.\nOften, a formula p(R) includes a condition R E Rel, and the meaning of the\nphrases some tuple R and for all tuples R is intuitive.\nWe use the notation\n3R E Rel(p(R)) for 3R(R E Rel 1\\ p(R)).\nSimilarly, we use the notation\nVR E Rel(p(R)) for VR(R E Rel =} p(R)).\n(Q12) Find the names and ages of sailors with a rating above 7.\n{P I 3S E Sailors(S.rating > 7 1\\ Pname = S.sname 1\\ Page = S.age)}\nThis query illustrates a useful convention: P is considered to be a tuple variable\nwith exactly two fields, which are called name and age, because these are the\nonly fields of P mentioned and P does not range over any of the relations in\nthe query; that is, there is no subformula of the form P\nE Relname.\nThe\nresult of this query is a relation with two fields, name and age.\nThe atomic\n4 WheneveT should be read more precisely as 'for all assignments of tuples to the free variables.'\n5Note that some of the free variables in p(R) (e.g., the variable R itself) IIlay be bound in P.\n\nCHAPTER J4\nformulas P.name = S.sname and Page = S.age give values to the fields of an\nanswer tuple P. On instances E1, R2, and S3, the answer is the set of tuples\n(Lubber,55.5), (Andy, 25.5), (Rusty, ~~5.0), (Zorba, 16.0), ::lnd (Horatio, 35.0).\n(Q1S) Find the so;ilor name, boat'id, and reseT1}Q.tion date for each reservation.\n{P I 3R E ReseT\"ues 3S E Sailors\n(R.sid = 8.sid!\\ P.bid = R.bid!\\ P.day = R.day !\\ P.sname = S.sname)}\nFor each Reserves tuple, we look for a tuple in Sailors with the same sid. Given\na pair of such tuples, we construct an answer tuple P with fields sname, bid,\nand day by copying the corresponding fields from these two tuples. This query\nillustrates how we can combine values from different relations in each answer\ntuple. The answer to this query on instances E1, R2, and 83 is shown in Figure\n4.20.\nIsname ~..... day\nDustin\n10/10/98\nDustin\n10/10/98\nDustin\n10/8/98\nDustin\n10/7/98\nLubber\n11/10/98\nLubber\n11/6/98\nLubber\n11/12/98\nHoratio\n9/5/98\nHoratio\n9/8/98\nHoratio\n9/8/98\nAnswer to Query Q13\n(Q1) Find the names of sailors who have reserved boat lOS.\n{P I 35 E Sailors 3R E Reserves(R.s'id = S.sid!\\ R.b'id = 103\n!\\Psname = 8.snarne)}\nThis query can be read as follows:\n\"Retrieve all sailor tuples for which there\nexists a tuple ,in Reserves having the same value in the s,id field and with\nb'id = 103.\" That is, for each sailor tuple, we look for a tuple in Reserves that\nshows that this sailor ha\" reserved boat\n10~~. The answer tuple P contains just\none field, sname.\n((22) Find the narnes of sailors who have reserved a n:.d boat.\n{P I :38 E Sailors\n:3R E Reserves(R.sid = 5.sid !\\ P.sname = S.8name\n\nRelational Algebra (nul Calculus\n)\n1\\3B E Boats(B.llid = R.md 1\\ B.color ='red'))}\nThis query can be read as follows:\n\"Retrieve all sailor tuples S for which\nthere exist tuples R in Reserves and B in Boats such that S.sid =\nR.sid,\nR.bid = B.b'id, and B.coior ='red'.\"\nAnother way to write this query, which\ncorresponds more closely to this reading, is as follows:\n{P I 3S E SailoTs 3R E Reserves 3B E Boats\n(Rsid = S.sid 1\\ B.bid = R.bid 1\\ B.color ='red' 1\\ Psname = S.sname)}\n(Q7) Find the names of sailors who have reserved at least two boats.\n{P I 3S E Sailors 3Rl E Reserves 3R2 E Reserves\n(S.sid = R1.sid 1\\ R1.sid = R2.sid 1\\ R1.bid =I- R2.bid\nI\\Psname = S.sname)}\nContrast this query with the algebra version and see how much simpler the\ncalculus version is. In part, this difference is due to the cumbersome renaming\nof fields in the algebra version, but the calculus version really is simpler.\n(Q9) Find the narnes of sailors who have reserved all boats.\n{P I 3S E Sailors VB E Boats\n(3R E Reserves(S.sid = R.sid 1\\ R.bid = B.bid 1\\ Psname = S.sname))}\nThis query was expressed using the division operator in relational algebra. Note\nhow easily it is expressed in the calculus. The calculus query directly reflects\nhow we might express the query in English: \"Find sailors S such that for all\nboats B there is a Reserves tuple showing that sailor S has reserved boat B.\"\n(Q14) Find sailors who have reserved all red boats.\n{S I S E Sailor's 1\\ VB E Boats\n(B.color ='red' :::} (3R E Reserves(S.sid = R.sid 1\\ R.bid = B.bid)))}\nThis query can be read as follows: For each candidate (sailor), if a boat is red,\nthe sailor must have reserved it. That is, for a candidate sailor, a boat being\nred must imply that the sailor has reserved it. Observe that since we can return\nan entire sailor tuple as the ans\\ver instead of just the sailor's name, we avoided\nintroducing a new free variable (e.g., the variable P in the previous example)\nto hold the answer values. On instances Bl. R2, and S3, the answer contains\nthe Sailors tuples with sids 22 and 31.\nWe can write this query without using implication, by observing that an ex-\npression of the form p :::} q is logically equivalent to -'p V q:\n{S ! S E Sailors 1\\ VB E Boats\n\nCHAPTER\n.~\n(B.coioT i-'Ted' V (3R E ReSeTVeS(S.sid = R..':tid/\\ R.b'id = B.lJid)))}\nThis query should be read a.s follows: \"Find sailors S such that, for all boats B,\neither the boat is not red or a Reserves tuple shows that sailor S has reserved\nboat B.\"\n4.3.2\nDomain Relational Calculus\nA domain variable is a variable that ranges over the values in the domain\nof some attribute (e.g., the variable can be assigned an integer if it appears\nin an attribute whose domain is the set of integers).\nA DRC query has the\nform {(XI,X2, ... ,Xn ) I P((XI,X2, ... ,Xn ))}, where each Xi is either a domain\nvariable or a constant and p((Xl, X2, ... ,xn )) denotes a DRC formula whose\nonly free variables are the variables among the Xi, 1 Sis n. The result of this\nquery is the set of all tuples (Xl, X2, ... , x n ) for which the formula evaluates to\ntrue.\nA DRC formula is defined in a manner very similar to the definition of a TRC\nformula. The main difference is that the variables are now domain variables.\nLet op denote an operator in the set {<, >, =, S,~, i-} and let X and Y be\ndomain variables. An atomic formula in DRC is one of the following:\nII\n(Xl, X2, ... , Xn )\nE Rel, where Rei is a relation with n attributes; each\nXi, 1 SiS n is either a variable or a constant\nII\nX op Y\nII\nX op constant, or constant op X\nA formula is recursively defined to be one of the following, where P and q\nare themselves formulas and p(X) denotes a formula in which the variable X\nappears:\nII\nany atomic formula\nII\n--.p, P /\\ q, P V q, or p =} q\nII\n3X(p(X)), where X is a domain variable\nII\n\\/X(p(X)), where X is a domain variable\nThe reader is invited to compare this definition with the definition of TRC\nforrnulch'3 and see how closely these two definitions correspond.\n\\Ve will not\ndefine the semantics of DRC formula.s formally; this is left as an exercise for\nthe reader.\n\nRelat'ional Algebra and Calculus\nExamples of DRC Queries\nvVe now illustrate DRC through several examples.\nThe reader is invited to\ncompare these with the TRC versions.\n(Q11) Find all sa'ilors with a rating above 7.\n{(1, N, T, A) I (I, N, T, A) E Sa'ilors /\\ T > 7}\nThis differs from the TRC version in giving each attribute a (variable) name.\nThe condition (1, N, T, A) E Sailors ensures that the domain variables I, N,\nT, and A are restricted to be fields of the same tuple. In comparison with the\nTRC query, we can say T > 7 instead of S.rating > 7, but we must specify the\ntuple (I, N, T, A) in the result, rather than just S.\n(Q1) Find the names of sailors who have reserved boat 103.\n{(N) I 31, T, A( (1, N, T, A) E Sa'ilors\n/\\311', Br, D( (11', Br, D) E Reserves /\\ 11' = I /\\ Br = 103))}\nNote that only the sname field is retained in the answer and that only N\nis a free variable.\nWe use the notation 3Ir,Br,D(... ) as a shorthand for\n3Ir(3Br(?JD(.. .))).\nVery often, all the quantified variables appear in a sin-\ngle relation, as in this example. An even more compact notation in this case\nis 3(11', Br, D) E Reserves. With this notation, which we use henceforth, the\nquery would be as follows:\n{(N) I 31, T, A((I, N, T, A) E Sailors\n/\\3(11', Br, D) E Reserves(Ir = I /\\ Br = 103))}\nThe comparison with the corresponding TRC formula should now be straight-\nforward.\nThis query can also be written as follows; note the repetition of\nvariable I and the use of the constant 103:\n{(N) I 31, T, A((1, N, T, A) E Sailors\n/\\3D( (1,103, D) E Reserves))}\n(Q2) Find the names of sailors who have Teserved a red boat.\n/\\3(1, Br, D) E ReseTves /\\ 3(Br, BN,'Ted') E Boats)}\n(Q7) Find the names of sailoT.'! who have TeseTved at least two boat.s.\n{(N) I 31, T, A((1, N, T, A) E Sailors /\\\n?JBrl, BT2, Dl, D2( (1, Brl, DI) E Reserves\n/\\(1, Br2, D2) E Reserves /\\ Brl # Br2))}",
          "pages": [
            153,
            154,
            155,
            156,
            157,
            158
          ],
          "relevance": {
            "score": 0.28,
            "sql_score": 0.9,
            "concept_score": 0.22,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER\n..fl\nNote how the repeated use of variable I ensures that the same sailor has reserved\nboth the boats in question.\n(Q9) Find the names of sailors who have Teserved all boat8.\n{(N) I ~I, T, A( (I, N, T, A) E Sailors!\\\nVB, BN,C(-,((B, BN,C) E Boats) V\n(::J(Ir, Br, D) E Reserves(I = IT!\\ BT = B))))}\nThis query can be read as follows: \"Find all values of N such that some tuple\n(I, N, T, A) in Sailors satisfies the following condition: For every (B, BN, C),\neither this is not a tuple in Boats or there is some tuple (IT, BT, D) in Reserves\nthat proves that Sailor I has reserved boat B.\"\nThe V quantifier allows the\ndomain variables B, BN, and C to range over all values in their respective\nattribute domains, and the pattern '-,( (B, BN, C) E Boats)V' is necessary to\nrestrict attention to those values that appear in tuples of Boats. This pattern\nis common in DRC formulas, and the notation V(B, BN, C) E Boats can be\nused as a shortcut instead. This is similar to the notation introduced earlier\nfor 3. With this notation, the query would be written as follows:\n{(N)\nI 31, T, A((I, N, T, A) E Sa'iloTs !\\ V(B, BN, C) E Boats\n(3(1'1', BT, D) E ReseTves(I = IT!\\ BT = B)))}\n(Q14) Find sailoTs who have TeseTved all Ted boats.\n{(I, N, T, A)\nI (I, N, T, A) E SailoTs!\\ V(B, BN, C) E Boats\n(C ='red' =? ?J(Ir, BT, D) E Reserves(I = IT!\\ Br = B))}\nHere, we find all sailors such that, for every red boat, there is a tuple in Reserves\nthat shows the sailor has reserved it.\n4.4\nEXPRESSIVE POWER OF ALGEBRA AND\nCALCULUS\n\\Ve presented two formal query languages for the relational model. Are they\nequivalent in power?\nCan every query that can be expressed in relational\nalgebra also be expressed in relational calculus?\nThe answer is yes, it can.\nin relational algebra?\nBefore we answer this question, we consider a major\nproblem with the calculus as we presented it.\nConsider the query {S I -,(S E Sailors)}. This query is syntactically correct.\nHowever, it asks for all tuples S such that S is not in (the given instance of)\n\nRelational Algebra an,d Calculu8\nSailors. The set of such S tuples is obviously infinite, in the context of infinite\ndomains such as the set of all integers.\nThis simple example illustrates an\nunsafe query. It is desirable to restrict relational calculus to disallow unsafe\nqueries.\nvVe now sketch how calculus queries are restricted to be safe. Consider a set I\nof relation instances, with one instance per relation that appears in the query\nQ.\nLet Dom(Q, 1) be the set of all constants that appear in these relation\ninstances I or in the formulation of the query Q itself.\nSince we allow only\nfinite instances I, Dom(Q, 1) is also finite.\nFor a calculus formula Q to be considered safe, at a minimum we want to\nensure that, for any given I, the set of answers for Q contains only values in\nDom(Q, 1). While this restriction is obviously required, it is not enough. Not\nonly do we want the set of answers to be composed of constants in Dom(Q, 1),\nwe wish to compnte the set of answers by examining only tuples that contain\nconstants in Dom(Q, 1)! This wish leads to a subtle point associated with the\nuse of quantifiers V and :::J: Given a TRC formula of the form :::JR(p(R)), we want\nto find all values for variable R that make this formula true by checking only\ntuples that contain constants in Dom(Q, 1). Similarly, given a TRC formula of\nthe form VR(p(R)), we want to find any values for variable R that make this\nformula false by checking only tuples that contain constants in Dom(Q, 1).\nWe therefore define a safe TRC formula Q to be a formula such that:\n1. For any given I, the set of answers for Q contains only values that are in\nDom(Q, 1).\n2. For each subexpression of the form :::JR(p(R)) in Q, if a tuple r (assigned\nto variable R) makes the formula true, then r contains only constants in\nDorn(Q,I).\n3. For each subexpression of the form VR(p(R)) in Q, if a tuple r (assigned\nto variable R) contains a constant that is not in Dom(Q, 1), then r must\nmake the formula true.\nNote that this definition is not constructive, that is, it does not tell us hmv to\ncheck if a query is safe.\nThe query Q\n=\n{S I -.(S E Sailors)} is unsafe by this definition. Dom(Q,1)\nis the set of all values that appear in (an instance I of) Sailors. Consider the\ninstance Sl shown in Figure 4.1. The answer to this query obviously includes\nvalues that do not appear in Dorn(Q,81).",
          "pages": [
            159,
            160
          ],
          "relevance": {
            "score": 0.02,
            "sql_score": 0.2,
            "concept_score": 0.11,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "Non-SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "An OUTER JOIN is a type of join operation that returns all records from both tables, even if there are no matching records between them.",
        "explanation": "Imagine you have two sets of data - one for students and another for their favorite books. An OUTER JOIN would give you a list of all students, along with the book they like (if any). If a student doesn't have a favorite book listed, it will still show up in the result set with NULL values for the book details.",
        "key_points": [
          "Key point 1: It returns all records from both tables",
          "Key point 2: Uses LEFT JOIN, RIGHT JOIN, or FULL OUTER JOIN to specify which table's rows should be included",
          "Key point 3: Commonly used when you need data from both tables even if there are no matches"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "SELECT students.name, books.title\nFROM students\nLEFT JOIN books ON students.book_id = books.id;",
            "explanation": "This query will list all students and their favorite book. If a student doesn't have a favorite book, the book details will be NULL."
          },
          {
            "title": "Practical Example",
            "code": "SELECT employees.name, departments.department_name\nFROM employees\nRIGHT JOIN departments ON employees.department_id = departments.id;",
            "explanation": "This query shows all departments and their assigned employees. If a department has no employees, the employee details will be NULL."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to specify LEFT, RIGHT, or FULL",
            "incorrect_code": "SELECT * FROM table1 JOIN table2;",
            "correct_code": "SELECT * FROM table1 LEFT JOIN table2 ON condition;",
            "explanation": "Always specify the type of OUTER JOIN you need. A simple JOIN without a keyword will result in an INNER JOIN, not an OUTER JOIN."
          }
        ],
        "practice": {
          "question": "Write a query to find all customers and their orders, even if some customers haven't made any orders.",
          "solution": "SELECT customers.name, orders.order_id\nFROM customers\nLEFT JOIN orders ON customers.id = orders.customer_id;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Relational Algebra and Calculus\nAn equivalent expression is:\nB.3\nThe reader is invited to rewrite both of these queries by using p to make the\nintermediate relations explicit and compare the schema.<=",
      "content_relevance": {
        "score": 0.48,
        "sql_score": 1.0,
        "concept_score": 0.56,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "aggregate-functions": {
      "id": "aggregate-functions",
      "title": "Aggregate Functions",
      "definition": "Computing summary values with COUNT, SUM, AVG, MAX, MIN",
      "difficulty": "beginner",
      "page_references": [
        165,
        166,
        167,
        168,
        169,
        170,
        171,
        172,
        173,
        174,
        175,
        176,
        177,
        178,
        179,
        180,
        181,
        182
      ],
      "sections": {
        "definition": {
          "text": "SQL: QUERIES,\nCONSTRNNTS, TRIGGERS\n..\nWhat is included in the SQL language? What is SQL:1999?\n..\nHow are queries expressed in SQL? How is the meaning of a query\nspecified in the SQL standard?\n,..-\nHow does SQL build on and extend relational algebra and calculus?\nl\"-\n\\Vhat is grouping? How is it used with aggregate operations?\n...\nWhat are nested queries?\n..\nWhat are null values?\n...\nHow can we use queries in writing complex integrity constraints?\n...\nWhat are triggers, and why are they useful? How are they related to\nintegrity constraints?\nItt\nKey concepts:\nSQL queries, connection to relational algebra and\ncalculus; features beyond algebra, DISTINCT clause and multiset se-\nmantics, grouping and aggregation; nested queries, correlation; set-\ncomparison operators; null values, outer joins; integrity constraints\nspecified using queries; triggers and active databases, event-condition-\naction rules.\n-------_.__._---_._------------_..__._---------------_\n_\n_----_.._ .._---\n\\Vhat men or gods are these? \\\\1hat Inaiclens loth?\n\\Vhat mad pursuit? \\1\\7hat struggle to escape?\n\\Vhat pipes and tilubrels? \\Vhat wild ecstasy?\n.... John Keats, Odc on\n(L Gr'ccian Urn\nStructured Query Language (SQL) is the most widely used conunercial rela-\ntional database language. It wa.\", originally developed at IBlVI in the SEQUEL-\n\nSQL Standards Conformance: SQL:1999 ha.,;; a collection of features\ncalled Core SQL that a vendor must implement to claim conformance with\nthe SQL:1999 standard. It is estimated that all the major vendors can\ncomply with Core SQL with little effort. l\\IIany of the remaining features\nare organized into packages.\nFor example, packages address each of the following (with relevant chapters\nin parentheses): enhanced date and time, enhanced integrity management\nI\nand active databases (this chapter), external language 'interfaces (Chapter\nl\n:6), OLAP (Chapter 25), and object features (Chapter 23). The SQL/Ml\\JI\nstandard complements SQL:1999 by defining additional packages that sup-\nport data mining (Chapter 26), spatial data (Chapter 28) and text docu-\nments (Chapter 27). Support for XML data and queries is forthcoming.\nXRM and System-R projects (1974-1977). Almost immediately, other vendors\nintroduced DBMS products based on SQL, and it is now a de facto standard.\nSQL continues to evolve in response to changing needs in the database area.\nThe current ANSI/ISO standard for SQL is called SQL:1999.\nWhile not all\nDBMS products support the full SQL:1999 standard yet, vendors are working\ntoward this goal and most products already support the core features.\nThe\nSQL:1999 standard is very close to the previous standard, SQL-92, with re-\nspect to the features discussed in this chapter. Our presentation is consistent\nwith both SQL-92 and SQL:1999, and we explicitly note any aspects that differ\nin the two versions of the standard.\n5.1\nOVERVIEW\nThe SQL language has several aspects to it.\n..\nThe Data Manipulation Language (DML): This subset of SQL allows\nusers to pose queries and to insert, delete, and modify rows. Queries are\nthe main focus of this chapter.\nWe covered DML commands to insert,\ndelete, and modify rows in Chapter 3.\n..\nThe Data Definition Language (DDL): This subset of SQL supports\nthe creation, deletion, and modification of definitions for tables and views.\nIntegrity constraints can be defined on tables, either when the table is\ncreated or later. \\Ve cocvered the DDL features of SQL in Chapter 3. Al-\nthough the standard does not discuss indexes, commercial implementations\nalso provide commands for creating and deleting indexes.\n..\nTriggers and Advanced Integrity Constraints: The new SQL:1999\nstandard includes support for triggers, which are actions executed by the\n\nCHAPTER 5\nDBMS whenever changes to the databa..'3e meet conditions specified in the\ntrigger. vVe cover triggers in this chapter. SQL allows the use of queries\nto specify complex integrity constraint specifications. vVe also discuss such\nconstraints in this chapter.\n•\nEmbedded and Dynamic SQL: Embedded SQL features allow SQL\ncode to be called from a host language such as C or COBOL. Dynamic\nSQL features allow a query to be constructed (and executed) at run-time.\n\\Ve cover these features in Chapter 6.\n•\nClient-Server Execution and Remote Database Access: These com-\nmands control how a client application program can connect to an SQL\ndatabase server, or access data from a database over a network. We cover\nthese commands in Chapter 7.\n•\nTransaction Management: Various commands allow a user to explicitly\ncontrol aspects of how a tnmsaction is to be executed.\nWe cover these\ncommands in Chapter 21.\n•\nSecurity: SQL provides mechanisms to control users' access to data ob-\njects such as tables and views. We cover these in Chapter 2l.\n•\nAdvanced features:\nThe SQL:1999 standard includes object-oriented\nfeatures (Chapter 23), recursive queries (Chapter 24), decision support\nqueries (Chapter 25), and also addresses emerging areas such as data min-\ning (Chapter 26), spatial data (Chapter 28), and text and XML data man-\nagement (Chapter 27).\n5.1.1\nChapter Organization\nThe rest of this chapter is organized as follows. We present basic SQL queries\nin Section 5.2 and introduce SQL's set operators in Section 5.3.\nWe discuss\nnested queries, in which a relation referred to in the query is itself defined\nwithin the query, in Section 5.4. vVe cover aggregate operators, which allow us\nto write SQL queries that are not expressible in relational algebra, in Section\n5.5. \\Ve discuss null values, which are special values used to indicate unknown\nor nonexistent field values, in Section 5.6. We discuss complex integrity con-\nstraints that can be specified using the SQL DDL in Section 5.7, extending the\nSQL DDL discussion from Chapter 3; the new constraint specifications allow\nus to fully utilize the query language capabilities of SQL.\nFinally, we discuss the concept of an active databa8e in Sections 5.8 and 5.9.\nAn active database h&'3 a collection of triggers, which are specified by the\nDBA. A trigger describes actions to be taken when certain situations arise. The\nDBMS lllonitors the database, detects these situations, and invokes the trigger.\n\nSqL: QueT'ies. ConstTairLts, Triggf::T\"s\nThe SQL:1999 standard requires support for triggers, and several relational\nDB.rvIS products already support some form of triggers.\nAbout the Examples\n~Te will present a number of sample queries using the following table definitions:\nSailors(sid: integer, sname: string, rating: integer, age: real)\nBoats( bid: integer, bname: string, color: string)\nReserves(sid: integer, bid: integer, day: date)\nWe give each query a unique number, continuing with the numbering scheme\nused in Chapter 4. The first new query in this chapter has number Q15. Queries\nQ1 through Q14 were introduced in Chapter 4.1 We illustrate queries using the\ninstances 83 of Sailors, R2 of Reserves, and B1 of Boats introduced in Chapter\n4, which we reproduce in Figures 5.1, 5.2, and 5.3, respectively.\nAll the example tables and queries that appear in this chapter are available\nonline on the book's webpage at\nhttp://www.cs.wisc.edu/-dbbook\nThe online material includes instructions on how to set up Orade, IBM DB2,\nMicrosoft SQL Server, and MySQL, and scripts for creating the example tables\nand queries.\n5.2\nTHE FORM OF A BASIC SQL QUERY\nThis section presents the syntax of a simple SQL query and explains its meaning\nthrough a conceptual Evaluation strategy. A conceptual evaluation strategy is\na way to evaluate the query that is intended to be easy to understand rather\nthan efficient. A DBMS would typically execute a query in a different and more\nefficient way.\nThe basic form of an SQL query is &'3 follows:\nSELECT [DISTINCT] select-list\nFROM\nfrom-list\nWHERE\nqualification\n1All references to a query can be found in the subject index for the book.\n\nI sid I sname·1 rating I age I\nCHAPTER 9\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nHoratio\n35.0\nArt\n25.5\nBob\n63.5\nAn Instance 53 of Sailors\n10/10/98\n10/10/98\n10/8/98\n10/7/98\n11/10/98\n11/6/98\n11/12/98\n9/5/98\n9/8/98\n9/8/98\nAn Instance R2 of Reserves\n~ bname\nI color ··1\nInterlake\nblue\nInterlake\nred\nClipper\ngreen\nMarine\nred\nAn Instance Bl of Boats\nEvery query must have a SELECT clause, which specifies columns to be retained\nin the result, and a FROM clause, which specifies a cross-product of tables. The\noptional WHERE clause specifies selection conditions on the tables mentioned in\nthe FROM clause.\nSuch a query intuitively corresponds to a relational algebra expression involving\nselections, projections, and cross-products. The close relationship between SQL\nand relational algebra is the basis for query optimization in a relational DBMS,\nas we will see in Chapters 12 and 15. Indeed, execution plans for SQL queries\nare represented using a variation of relational algebra expressions (Section 15.1).\nLet us consider a simple example.\n(Q15) Find the' names and ages of all sailors.\nSELECT DISTINCT S.sname, S.age\nFROM\nSailors S\nThe answer is a set of rows, each of which is a pair (sname, age). If two or\nmore sailors have the same name and age, the answer still contains just one pair",
          "pages": [
            165,
            166,
            167,
            168,
            169
          ],
          "relevance": {
            "score": 0.42,
            "sql_score": 1.0,
            "concept_score": 0.25,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "SQL:\nQ1Le7~ies. Con8tnrint8, TriggeT8\n~\nwith that name and age. This query is equivalent to applying the projection\noperator of relational algebra.\nIf we omit the keyword DISTINCT, we would get a copy of the row (s,a) for\neach sailor with name s and age a; the answer would be a rnultiset of rows. A\nmultiset is similar to a set in that it is an unordered collection of elements,\nbut there could be several copies of each element, and the number of copies is\nsignificant-two multisets could have the same elements and yet be different\nbecause the number of copies is different for some elements. For example, {a,\nb, b} and {b, a, b} denote the same multiset, and differ from the multiset {a,\na, b}.\nThe answer to this query with and without the keyword DISTINCT on instance\n53 of Sailors is shown in Figures 5.4 and 5.5. The only difference is that the\ntuple for Horatio appears twice if DISTINCT is omitted; this is because there\nare two sailors called Horatio and age 35.\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nHoratio\n35.0\nArt\n25.5\nBob\n63.5\nI sname\nI age I\n'-----\"--\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nArt\n25.5\nBob\n63.5\nI.snarne\nI age I\nAnswer to Q15\nAnswer to Q15 without DISTINCT\nOur next query is equivalent to an application of the selection operator of\nrelational algebra.\n(Q11) Find all sailors with a rating above 7.\nSELECT S.sid, S.sname, S.rating, S.age\nFROM\nSailors AS S\nWHERE\nS.rating > 7\nThis query uses the optional keyword AS to introduce a range variable. Inci-\ndentally, when we want to retrieve all columns, as in this query, SQL provides a\n\nCHAPTER fj\nconvenient shorthand: \\eVe can simply write SELECT *. This notation is useful\nfor interactive querying, but it is poor style for queries that are intended to be\nreused and maintained because the schema of the result is not clear from the\nquery itself; we have to refer to the schema of the underlying Sailors ta.ble.\nAs these two examples illustrate, the SELECT clause is actually used to do pm-\njection, whereas selections in the relational algebra sense are expressed using\nthe WHERE clause! This mismatch between the naming of the selection and pro-\njection operators in relational algebra and the syntax of SQL is an unfortunate\nhistorical accident.\nWe now consider the syntax of a basic SQL query in more detail.\n•\nThe from-list in the FROM clause is a list of table names. A table name\ncan be followed by a range variable; a range variable is particularly useful\nwhen the same table name appears more than once in the from-list.\n•\nThe select-list is a list of (expressions involving) column names of tables\nnamed in the from-list. Column names can be prefixed by a range variable.\n•\nThe qualification in the WHERE clause is a boolean combination (i.e., an\nexpression using the logical connectives AND, OR, and NOT) of conditions\nof the form expression op expression, where op is one of the comparison\noperators {<, <=, =, <>, >=, >}.2\nAn expression is a column name, a\nconstant, or an (arithmetic or string) expression.\n•\nThe DISTINCT keyword is optional. It indicates that the table computed\nas an answer to this query should not contain duplicates, that is, two copies\nof the same row. The default is that duplicates are not eliminated.\nAlthough the preceding rules describe (informally) the syntax of a basic SQL\nquery, they do not tell us the meaning of a query. The answer to a query is\nitself a relation\nwhich is a rnultisef of rows in SQL!--whose contents can be\nunderstood by considering the following conceptual evaluation strategy:\n1. Cmnpute the cross-product of the tables in the from-list.\n2. Delete rows in the cross-product that fail the qualification conditions.\n3. Delete all columns that do not appear in the select-list.\n4. If DISTINCT is specified, eliminate duplicate rows.\n2ExpressiollS with NOT can always be replaced by equivalent expressions without NOT given the set\nof comparison operators just listed.\n\nSCJL: Queries, ConstTaints, TriggeTs\n~\nThis straightforward conceptual evaluation strategy makes explicit the rows\nthat must be present in the answer to the query.\nHowever, it is likely to be\nquite inefficient. We will consider how a DB:MS actually evaluates queries in\nlater chapters; for now, our purpose is simply to explain the meaning of a query.\n\\Ve illustrate the conceptual evaluation strategy using the following query':\n(Q1) Find the names of sailors 'Who have reseTved boat number 103.\nIt can be expressed in SQL as follows.\nSELECT S.sname\nFROM\nSailors S, Reserves R\nWHERE\nS.sid = R.sid AND R.bid=103\nLet us compute the answer to this query on the instances R3 of Reserves and\n84 of Sailors shown in Figures 5.6 and 5.7, since the computation on our usual\nexample instances (R2 and 83) would be unnecessarily tedious.\n~day\nI 22 I 101\n10/10/96\nI 58\nI 103\n11/12/96\nInstance R3 of Reserves\n~ sname I Tating I age I\ndustin\n45.0\nlubber\n55.5\nrusty\n35.0\nInstance 54 of Sailors\nThe first step is to construct the cross-product 84 x R3, which is shown in\n~ sname·j Tating·I···age~day\ndustin\n45.0\n10/10/96\ndustin\n45.0\n11/12/96\nlubber\n55.5\n10/10/96\nlubber\n---\n55.5\n11/12/96\nrusty\n3.5.0\n10/10/96\nrusty\n35.0\n11/12/96\n84 x RS\nThe second step is to apply the qualification S./rid = R.sid AND R.bid=103.\n(Note that the first part of this qualification requires a join operation.) This\nstep eliminates all but the last row from the instance shown in Figure 5.8. The\nthird step is to eliminate unwanted columns; only sname appears in the SELECT\nclause. This step leaves us with the result shown in Figure .5.9, which is a table\nwith a single column and, a.c; it happens, just one row.\n\n! sna'me!\n[I1lStL]\nAnswer to Query Ql 011 R:l and 84\n5.2.1\nExamples of Basic SQL Queries\nvVe now present several example queries, many of which were expressed earlier\nin relational algebra and calculus (Chapter 4).\nOur first example illustrates\nthat the use of range variables is optional, unless they are needed to resolve an\nambiguity. Query Ql, which we discussed in the previous section, can also be\nexpressed as follows:\nSELECT sname\nFROM\nSailors 5, Reserves R\nWHERE\nS.sid = R.sid AND bid=103\nOnly the occurrences of sid have to be qualified, since this column appears in\nboth the Sailors and Reserves tables. An equivalent way to write this query is:\nSELECT SHame\nFROM\nSailors, Reserves\nWHERE\nSailors.sid = Reserves.sid AND bid=103\nThis query shows that table names can be used implicitly as row variables.\nRange variables need to be introduced explicitly only when the FROM clause\ncontains more than one occurrence of a relation. 3\nHowever, we recommend\nthe explicit use of range variables and full qualification of all occurrences of\ncolumns with a range variable to improve the readability of your queries. We\nwill follow this convention in all our examples.\n(Q16) Find the sids of sa'iloTs who have TeseTved a Ted boat.\nSELECT\nFROM\nWHERE\nR.sid\nBoats B, Reserves R\nB.bid = R.bid AND 8.color = 'red'\nThis query contains a join of two tables, followed by a selection on the color\nof boats. vVe can think of 13 and R\n&<; rows in the corresponding tables that\n:~The table name cannot be used aii an implicit. range variable once a range variable is introduced\nfor t.he relation.\n\nSQL: QUEeries, Constraints, Triggers\n:prove' that a sailor with sid R.sid reserved a reel boat B.bid. On our example\ninstances R2 and 83 (Figures 5.1 and 5.2), the answer consists of the Bids 22,\n31, and 64. If we want the names of sailors in the result, we must also consider\nthe Sailors relation, since Reserves does not contain this information, as the\nnext example illustrates.\n(Q2) Find the names of sailors 'Who have TeseTved a Ted boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats 13\nS.sid = R.sid AND R.bid = 13.bid AND B.color = 'red'\nThis query contains a join of three tables followed by a selection on the color\nof boats. The join with Sailors allows us to find the name of the sailor who,\naccording to Reserves tuple R, has reserved a red boat described by tuple 13.\n(QS) Find the coloTS of boats reseTved by LubbeT.\nSELECT 13.color\nFROM\nWHERE\nS.sid = R.sid AND R.bid = B.bid AND S.sname = 'Lubber'\nThis query is very similar to the previous one. Note that in general there may\nbe more than one sailor called Lubber (since sname is not a key for Sailors);\nthis query is still correct in that it will return the colors of boats reserved by\nsome Lubber, if there are several sailors called Lubber.\n(Q4) Find the names of sa'iloTs who have Teserved at least one boat.\nSELECT S.sname\nFROM\nSailors S, Reserves R\nWHERE\nS.sid = R.sid\nThe join of Sailors and Reserves ensures that for each selected sname, the\nsailor has made some reservation. (If a sailor has not made a reservation, the\nsecond step in the conceptual evaluation strategy would eliminate all rows in\nthe cross-product that involve this sailor.)\n5.2.2\nExpressions and Strings in the SELECT Command\nSQL supports a more general version of the select-list than just a list of\ncolu1nn8.\nEach item in a select-list can be of the form e:l:pTcssion AS col-\n'wnrLno:me, where c:rprcs.sion is any arithmetic or string expression over column\n\nCHAPTERf)\nnames (possibly prefixed by range variables) and constants, and colurnnswrne\nis a ne\"v name for this column in the output of the query. It can also contain\naggregates such as smn and count, which we will discuss in Section 5.5. The\nSQL standard also includes expressions over date and time values, which we will\nnot discuss. Although not part of the SQL standard, many implementations\nalso support the use of built-in functions such as sqrt, sin, and rnod.\n(Q17) Compute increments for the mtings of peTsons who have sailed two dif-\nferent boats on the same day.\nSELECT\nFROM\nWHERE\nS.sname, S.rating+1 AS rating\nSailors S, Reserves R1, Reserves R2\nS.sid = R1.sid AND S.sid = R2.sid\nAND R1.day = R2.day AND R1.bid <> R2.bid\nAlso, each item in a qualification can be as general as expTession1 = expression2.\nSELECT S1.sname AS name1, S2.sname AS name2\nFROM\nSailors Sl, Sailors S2\nWHERE\n2*S1.rating = S2.rating-1\nFor string comparisons, we can use the comparison operators (=, <, >, etc.)\nwith the ordering of strings determined alphabetically as usual.\nIf we need\nto sort strings by an order other than alphabetical (e.g., sort strings denoting\nmonth names in the calendar order January, February, March, etc.), SQL sup-\nports a general concept of a collation, or sort order, for a character set.\nA\ncollation allows the user to specify which characters are 'less than' which others\nand provides great flexibility in string manipulation.\nIn addition, SQL provides support for pattern matching through the LIKE op-\nerator, along with the use of the wild-card symbols % (which stands for zero\nor more arbitrary characters) and\n~ (which stands for exactly one, arbitrary,\ncharacter).\nThus, '_AB%' denotes a pattern matching every string that con-\ntains at lea.'3t three characters, with the second and third characters being A\nand B respectively.\nNote that unlike the other comparison operators, blanks\ncan be significant for the LIKE operator (depending on the collation for the\nunderlying character set). Thus, 'Jeff' = 'Jeff' is true while 'Jeff'LIKE 'Jeff\n, is false. An example of the use of LIKE in a query is given below.\n(Q18) Find the ages of sailors wh08e name begins and ends with B and has at\nleast three chamcters.\nSELECT S.age\n\nSQL: Q'Il,e'rie8, Constraints, TTiggeTs\n$\nr---'-~~-~:~;- Expre~~~'~-:~'-'~:' '~Q'~'~\"\"~,~flecting the incr~~~~~~~mpo~~:l~ceof I\nI\ntext data, SQL:1999 includes a more powerful version of theLIKE operator\ni\ni\ncalled SIMILAR. This operator allows a rich set of regular expressions to be\nI\nI\nused as patterns while searching text. The regular expressions are similar t~\nI\nthose sUPPo.rted by the Unix operating systenifor string searches, although'\nthe syntax is a little different.\n-- -\n.\n••••._m.\n.-.-.-.-.-....•--..-----........\n.-••••••••••••.•.•••••••....-- ..-\n----.--\n-----..-- ..-\n------------ ---------- -.••..•.---•••••.••.•.•••••••••..•.•.••••••.••- ••••.....•- ••••.'.-\"\"-'-.\nJ '\n.\nRelational Algebra and SQL: The set operations of SQL are available in\nrelational algebra. The main difference, of course, is that they are multiset\noperations in SQL, since tables are multisets of tuples.\nFROM\nWHERE\nSailors S\nS.sname LIKE 'B.%B'\nThe only such sailor is Bob, and his age is 63.5.\n5.3\nUNION, INTERSECT, AND EXCEPT\nSQL provides three set-manipulation constructs that extend the basic query\nform presented earlier. Since the answer to a query is a multiset of rows, it is\nnatural to consider the use of operations such as union, intersection, and differ-\nence. SQL supports these operations under the names UNION, INTERSECT, and\nEXCEPT. 4 SQL also provides other set operations: IN (to check if an element\nis in a given set), op ANY, op ALL (to compare a value with the elements in\na given set, using comparison operator op), and EXISTS (to check if a set is\nempty). IN and EXISTS can be prefixed by NOT, with the obvious modification\nto their meaning.\nWe cover UNION,\nINTERSECT, and EXCEPT in this section,\nand the other operations in Section 5.4.\nConsider the following query:\n(Q5) Find the names of sailors who have reserved a red 01' a green boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sid = R.sid AND R.bid = B.bid\nAND (B.color = 'red' OR B.color = 'green')\n.. _----\n4Note that although the SQL standard includes these operations, many systems currently support\nonly UNION. Also. many systems recognize the keyword MINUS for EXCEPT.\n\nCHAPTERD\nThis query is easily expressed using the OR connective in the WHERE clause.\nHovvever, the following query, which is identical except for the use of 'and'\nrather than 'or' in the English version, turns out to be much more difficult:\n(Q6) Find the names of sailor's who have rescr'ved both a red and a green boat.\nIf we were to just replace the use of OR in the previous query by AND, in analogy\nto the English statements of the two queries, we would retrieve the names of\nsailors who have reserved a boat that is both red and green.\nThe integrity\nconstraint that bid is a key for Boats tells us that the same boat cannot have\ntwo colors, and so the variant of the previous query with AND in place of OR will\nalways return an empty answer set.\nA correct statement of Query Q6 using\nAND is the following:\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves RI, Boats BI, Reserves R2, Boats B2\nS.sid = Rl.sid AND R1.bid = Bl.bid\nAND S.sid = R2.sid AND R2.bid = B2.bid\nAND B1.color='red' AND B2.color = 'green'\nWe can think of RI and BI as rows that prove that sailor S.sid has reserved a\nred boat. R2 and B2 similarly prove that the same sailor has reserved a green\nboat. S.sname is not included in the result unless five such rows S, RI, BI, R2,\nand B2 are found.\nThe previous query is difficult to understand (and also quite inefficient to ex-\necute, as it turns out). In particular, the similarity to the previous OR query\n(Query Q5) is completely lost. A better solution for these two queries is to use\nUNION and INTERSECT.\nThe OR query (Query Q5) can be rewritten as follows:\nSELECT\nFROM\nWHERE\nUNION\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sicl = R.sid AND R.bid = B.bid AND B.color = 'red'\nS2.sname\nSailors S2, Boats B2, Reserves H2\nS2.sid = H2.sid AND R2.bid = B2.bicl AND B2.color = 'green'\nThis query sa,)'s that we want the union of the set of sailors who have reserved\nred boats and the set of sailors who have reserved green boats.\nIn complete\nsymmetry, the AND query (Query Q6) can be rewritten a.s follovvs:\nSELECT S.snarne",
          "pages": [
            170,
            171,
            172,
            173,
            174,
            175,
            176,
            177
          ],
          "relevance": {
            "score": 0.39,
            "sql_score": 1.0,
            "concept_score": 0.38,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "SqL: Q'lleries 7 Constraints, Triggers\nFROM\nSailors S, Reserves R, Boats B\nWHERE\nS.sid = R.sid AND R.bid = B.bid AND B.color = 'red'\nINTERSECT\nSELECT S2.sname\nFROM\nSailors S2, Boats B2, Reserves R2\nWHERE\nS2.sid = R2.sid AND R2.bid = B2.bid AND B2.color = 'green'\nThis query actually contains a subtle bug-if there are two sailors such as\nHoratio in our example instances B1, R2, and 83, one of whom has reserved a\nred boat and the other has reserved a green boat, the name Horatio is returned\neven though no one individual called Horatio has reserved both a red and a\ngreen boat. Thus, the query actually computes sailor names such that some\nsailor with this name has reserved a red boat and some sailor with the same\nname (perhaps a different sailor) has reserved a green boat.\nAs we observed in Chapter 4, the problem arises because we are using sname\nto identify sailors, and sname is not a key for Sailors! If we select sid instead of\nsname in the previous query, we would compute the set of sids of sailors who\nhave reserved both red and green boats. (To compute the names of such sailors\nrequires a nested query; we will return to this example in Section 5.4.4.)\nOur next query illustrates the set-difference operation in SQL.\n(Q19) Find the sids of all sailor's who have reserved red boats but not green\nboats.\nSELECT\nFROM\nWHERE\nEXCEPT\nSELECT\nFROM\nWHERE\nS.sid\nSailors S, Reserves R, Boats B\nS2.sid\nSailors S2, Reserves R2, Boats B2\nSailors 22, 64, and 31 have reserved red boats.\nSailors 22, 74, and 31 have\nreserved green boats. Hence, the answer contains just the sid 64.\nIndeed, since the Reserves relation contains sid information, there is no need\nto look at the Sailors relation, and we can use the following simpler query:\nSELECT\nFROM\nWHERE\nEXCEPT\nH..sid\nBoats B, Reserves R\nR.bicl = B.bid AND B.color = 'red'\n\nSELECT R2.sid\nFROM\nBoats B2, Reserves R2\nWHERE\nR2.bicl = B2.bid AND B2.color = :green'\nCHAPTER~5\nObserve that this query relies on referential integrity; that is, there are no\nreservations for nonexisting sailors. Note that UNION, INTERSECT, and EXCEPT\ncan be used on any two tables that are union-compatible, that is, have the same\nnumber of columns and the columns, taken in order, have the same types. For\nexample, we can write the following query:\n(Q20) Find all sids of sailors who have a rating of 10 or reserved boat 104.\nSELECT\nFROM\nWHERE\nUNION\nSELECT\nFROM\nWHERE\nS.sid\nSailors S\nS.rating = 10\nR.sid\nReserves R\nR.bid = 104\nThe first part of the union returns the sids 58 and 71. The second part returns\n22 and 31.\nThe answer is, therefore, the set of sids 22, 31, 58, and 71.\nA\nfinal point to note about UNION, INTERSECT, and EXCEPT follows. In contrast\nto the default that duplicates are not eliminated unless DISTINCT is specified\nin the basic query form, the default for UNION queries is that duplicates are\neliminated! To retain duplicates, UNION ALL must be used; if so, the number\nof copies of a row in the result is always m + n, where m and n are the num-\nbers of times that the row appears in the two parts of the union.\nSimilarly,\nINTERSECT ALL retains cluplicates--the number of copies of a row in the result\nis min(m, n )-~ancl EXCEPT ALL also retains duplicates~thenumber of copies\nof a row in the result is m - n, where 'm corresponds to the first relation.\n5.4\nNESTED QUERIES\nOne of the most powerful features of SQL is nested queries. A nested query\nis a query that has another query embedded within it; the embedded query\nis called a suhquery. The embedded query can of course be a nested query\nitself; thus queries that have very deeply nested structures are possible. When\nwriting a query, we sornetimes need to express a condition that refers to a table\nthat must itself be computed. The query used to compute this subsidiary table\nis a subquery and appears as part of the main query.\nA subquery typically\nappears within the WHERE clause of a query. Subqueries can sometimes appear\nin the FROM clause or the HAVING clause (which we present in Section 5.5).\n\nSQL:\nQueT~ie8] Constraints] Triggers\n$\nr-~'-'-'~\n-.-------\n-~---\n,.,.._,---..--\n, ,-, ,-_\n,\n. ---------\nI Relational Algebra and SQL: Nesting of queries is a feature that is not\nI\navailable in relational algebra, but nested queries can be translated into\ni\nalgebra, as we will see in Chapter 15. Nesting in SQL is inspired more by\n,\nrelational calculus than algebra. In conjunction with some of SQL's other\nfeatures, such as (multi)set operators and aggregation, nesting is a very\nexpressive construct.\nThis section discusses only subqueries that appear in the WHERE clause. The\ntreatment of subqueries appearing elsewhere is quite similar. Some examples of\nsubqueries that appear in the FROM clause are discussed later in Section 5.5.1.\n5.4.1\nIntroduction to Nested Queries\nAs an example, let us rewrite the following query, which we discussed earlier,\nusing a nested subquery:\n(Ql) Find the names of sailors who have reserved boat 103.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.sid IN ( SELECT\nFROM\nWHERE\nR.sid\nReserves R\nR.bid = 103 )\nThe nested subquery computes the (multi)set of sids for sailors who have re-\nserved boat 103 (the set contains 22,31, and 74 on instances R2 and 83), and\nthe top-level query retrieves the names of sailors whose sid is in this set. The\nIN operator allows us to test whether a value is in a given set of elements; an\nSQL query is used to generate the set to be tested. Note that it is very easy to\nmodify this query to find all sailors who have not reserved boat 103-we can\njust replace IN by NOT IN!\nThe best way to understand a nested query is to think of it in terms of a con-\nceptual evaluation strategy. In our example, the strategy consists of examining\nrows in Sailors and, for each such row, evaluating the subquery over Reserves.\nIn general, the conceptual evaluation strategy that we presented for defining\nthe semantics of a query can be extended to cover nested queries as follows:\nConstruct the cross-product of the tables in the FROM clause of the top-level\nquery as hefore. For each row in the cross-product, while testing the qllalifica-\n\ntion in the WHERE clause, (re)compute the subquery.5 Of course, the subquery\nmight itself contain another nested subquery, in which case we apply the same\nidea one more time, leading to an evaluation strategy \\vith several levels of\nnested loops.\nAs an example of a multiply nested query, let us rewrite the following query.\n(Q2) Find the names of sailors 'who ha'ue reserved a red boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.sid IN ( SELECT R.sid\nFROM\nReserves R\nWHERE\nR.bid IN (SELECT B.bid\nFROM\nBoats B\nWHERE\nB.color = 'red'\nThe innermost subquery finds the set of bids of red boats (102 and 104 on\ninstance E1). The subquery one level above finds the set of sids of sailors who\nhave reserved one of these boats. On instances E1, R2, and 83, this set of sids\ncontains 22, 31, and 64. The top-level query finds the names of sailors whose\nsid is in this set of sids; we get Dustin, Lubber, and Horatio.\nTo find the names of sailors who have not reserved a red boat, wc replace the\noutermost occurrence of IN by NOT IN, as illustrated in the next query.\n(Q21) Find the names of sailors who have not reserved a red boat.\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.sid NOT IN ( SELECT\nFROM\nWHERE\nR.sid\nReserves R\nR.bid IN ( SELECT B.bid\nFROM\nBoats B\nWHERE\nB.color = 'red' )\nThis qucry computes the names of sailors whose sid is not in the set 22, 31,\nand 64.\nIn contrast to Query Q21, we can modify the previous query (the nested version\nof Q2) by replacing the inner occurrence (rather than the outer occurence) of\n5Since the inner subquery in our example does not depend on the 'current' row from the outer\nquery ill any way, you rnight wonder why we have to recompute the subquery for each outer row. For\nan answer, see Section 5.4.2.",
          "pages": [
            178,
            179,
            180,
            181
          ],
          "relevance": {
            "score": 0.42,
            "sql_score": 1.0,
            "concept_score": 0.25,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Aggregate functions in SQL allow you to perform calculations on a set of values and return a single value. They are essential for summarizing data and extracting meaningful insights from large datasets.",
        "explanation": "Aggregate functions are used when you need to compute a single output from multiple rows of data. Common examples include SUM, COUNT, AVG, MAX, and MIN. These functions operate on a column of data and return a result based on the operation applied. For instance, SUM adds up all the values in a column, while COUNT returns the number of non-null entries.",
        "key_points": [
          "Key point 1: Aggregate functions are used to perform calculations on multiple rows and return a single value.",
          "Key point 2: Common aggregate functions include SUM, COUNT, AVG, MAX, and MIN.",
          "Key point 3: Always specify the column you want to apply the function to within parentheses, e.g., SUM(column_name).",
          "Key point 4: Aggregate functions can be used with WHERE clauses to filter data before performing calculations.",
          "Key point 5: Understanding how aggregate functions work is crucial for writing efficient and effective SQL queries."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Calculate the total number of sailors\nSELECT COUNT(*) FROM Sailors;",
            "explanation": "This example demonstrates how to use the COUNT function to find out how many rows are in the 'Sailors' table."
          },
          {
            "title": "Practical Example",
            "code": "-- Find the average rating of all sailors\nSELECT AVG(rating) FROM Sailors;",
            "explanation": "This practical example shows how to use the AVG function to calculate the average value in a column, providing useful information about the dataset."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting parentheses around the column name",
            "incorrect_code": "-- Incorrect usage\nSELECT SUM rating FROM Sailors;",
            "correct_code": "-- Correct usage\nSELECT SUM(rating) FROM Sailors;",
            "explanation": "This mistake happens when students forget to put parentheses around the column name after the function. It results in a syntax error."
          }
        ],
        "practice": {
          "question": "Write an SQL query that calculates the total number of boats with a color of 'red' from the 'Boats' table.",
          "solution": "-- Correct solution\nSELECT COUNT(*) FROM Boats WHERE color = 'red';\n-- Explanation: This query counts all rows in the 'Boats' table where the 'color' column is equal to 'red'."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "SQL: QUERIES,\nCONSTRNNTS, TRIGGERS\n..\nWhat is included in the SQL language? What is SQL:1999?\n..\nHow are queries expressed in SQL? How is the meaning of a query\nspecified in the SQL standard?\n,..-\nHow",
      "content_relevance": {
        "score": 0.45,
        "sql_score": 1.0,
        "concept_score": 0.5,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "group-by": {
      "id": "group-by",
      "title": "GROUP BY Clause",
      "definition": "Grouping rows with common values for aggregate calculations",
      "difficulty": "intermediate",
      "page_references": [
        175,
        176,
        177,
        178,
        179,
        180,
        181,
        182,
        183,
        184,
        185,
        186,
        187,
        188,
        189,
        190,
        191
      ],
      "sections": {
        "definition": {
          "text": "CHAPTERf)\nnames (possibly prefixed by range variables) and constants, and colurnnswrne\nis a ne\"v name for this column in the output of the query. It can also contain\naggregates such as smn and count, which we will discuss in Section 5.5. The\nSQL standard also includes expressions over date and time values, which we will\nnot discuss. Although not part of the SQL standard, many implementations\nalso support the use of built-in functions such as sqrt, sin, and rnod.\n(Q17) Compute increments for the mtings of peTsons who have sailed two dif-\nferent boats on the same day.\nSELECT\nFROM\nWHERE\nS.sname, S.rating+1 AS rating\nSailors S, Reserves R1, Reserves R2\nS.sid = R1.sid AND S.sid = R2.sid\nAND R1.day = R2.day AND R1.bid <> R2.bid\nAlso, each item in a qualification can be as general as expTession1 = expression2.\nSELECT S1.sname AS name1, S2.sname AS name2\nFROM\nSailors Sl, Sailors S2\nWHERE\n2*S1.rating = S2.rating-1\nFor string comparisons, we can use the comparison operators (=, <, >, etc.)\nwith the ordering of strings determined alphabetically as usual.\nIf we need\nto sort strings by an order other than alphabetical (e.g., sort strings denoting\nmonth names in the calendar order January, February, March, etc.), SQL sup-\nports a general concept of a collation, or sort order, for a character set.\nA\ncollation allows the user to specify which characters are 'less than' which others\nand provides great flexibility in string manipulation.\nIn addition, SQL provides support for pattern matching through the LIKE op-\nerator, along with the use of the wild-card symbols % (which stands for zero\nor more arbitrary characters) and\n~ (which stands for exactly one, arbitrary,\ncharacter).\nThus, '_AB%' denotes a pattern matching every string that con-\ntains at lea.'3t three characters, with the second and third characters being A\nand B respectively.\nNote that unlike the other comparison operators, blanks\ncan be significant for the LIKE operator (depending on the collation for the\nunderlying character set). Thus, 'Jeff' = 'Jeff' is true while 'Jeff'LIKE 'Jeff\n, is false. An example of the use of LIKE in a query is given below.\n(Q18) Find the ages of sailors wh08e name begins and ends with B and has at\nleast three chamcters.\nSELECT S.age\n\nSQL: Q'Il,e'rie8, Constraints, TTiggeTs\n$\nr---'-~~-~:~;- Expre~~~'~-:~'-'~:' '~Q'~'~\"\"~,~flecting the incr~~~~~~~mpo~~:l~ceof I\nI\ntext data, SQL:1999 includes a more powerful version of theLIKE operator\ni\ni\ncalled SIMILAR. This operator allows a rich set of regular expressions to be\nI\nI\nused as patterns while searching text. The regular expressions are similar t~\nI\nthose sUPPo.rted by the Unix operating systenifor string searches, although'\nthe syntax is a little different.\n-- -\n.\n••••._m.\n.-.-.-.-.-....•--..-----........\n.-••••••••••••.•.•••••••....-- ..-\n----.--\n-----..-- ..-\n------------ ---------- -.••..•.---•••••.••.•.•••••••••..•.•.••••••.••- ••••.....•- ••••.'.-\"\"-'-.\nJ '\n.\nRelational Algebra and SQL: The set operations of SQL are available in\nrelational algebra. The main difference, of course, is that they are multiset\noperations in SQL, since tables are multisets of tuples.\nFROM\nWHERE\nSailors S\nS.sname LIKE 'B.%B'\nThe only such sailor is Bob, and his age is 63.5.\n5.3\nUNION, INTERSECT, AND EXCEPT\nSQL provides three set-manipulation constructs that extend the basic query\nform presented earlier. Since the answer to a query is a multiset of rows, it is\nnatural to consider the use of operations such as union, intersection, and differ-\nence. SQL supports these operations under the names UNION, INTERSECT, and\nEXCEPT. 4 SQL also provides other set operations: IN (to check if an element\nis in a given set), op ANY, op ALL (to compare a value with the elements in\na given set, using comparison operator op), and EXISTS (to check if a set is\nempty). IN and EXISTS can be prefixed by NOT, with the obvious modification\nto their meaning.\nWe cover UNION,\nINTERSECT, and EXCEPT in this section,\nand the other operations in Section 5.4.\nConsider the following query:\n(Q5) Find the names of sailors who have reserved a red 01' a green boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sid = R.sid AND R.bid = B.bid\nAND (B.color = 'red' OR B.color = 'green')\n.. _----\n4Note that although the SQL standard includes these operations, many systems currently support\nonly UNION. Also. many systems recognize the keyword MINUS for EXCEPT.\n\nCHAPTERD\nThis query is easily expressed using the OR connective in the WHERE clause.\nHovvever, the following query, which is identical except for the use of 'and'\nrather than 'or' in the English version, turns out to be much more difficult:\n(Q6) Find the names of sailor's who have rescr'ved both a red and a green boat.\nIf we were to just replace the use of OR in the previous query by AND, in analogy\nto the English statements of the two queries, we would retrieve the names of\nsailors who have reserved a boat that is both red and green.\nThe integrity\nconstraint that bid is a key for Boats tells us that the same boat cannot have\ntwo colors, and so the variant of the previous query with AND in place of OR will\nalways return an empty answer set.\nA correct statement of Query Q6 using\nAND is the following:\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves RI, Boats BI, Reserves R2, Boats B2\nS.sid = Rl.sid AND R1.bid = Bl.bid\nAND S.sid = R2.sid AND R2.bid = B2.bid\nAND B1.color='red' AND B2.color = 'green'\nWe can think of RI and BI as rows that prove that sailor S.sid has reserved a\nred boat. R2 and B2 similarly prove that the same sailor has reserved a green\nboat. S.sname is not included in the result unless five such rows S, RI, BI, R2,\nand B2 are found.\nThe previous query is difficult to understand (and also quite inefficient to ex-\necute, as it turns out). In particular, the similarity to the previous OR query\n(Query Q5) is completely lost. A better solution for these two queries is to use\nUNION and INTERSECT.\nThe OR query (Query Q5) can be rewritten as follows:\nSELECT\nFROM\nWHERE\nUNION\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sicl = R.sid AND R.bid = B.bid AND B.color = 'red'\nS2.sname\nSailors S2, Boats B2, Reserves H2\nS2.sid = H2.sid AND R2.bid = B2.bicl AND B2.color = 'green'\nThis query sa,)'s that we want the union of the set of sailors who have reserved\nred boats and the set of sailors who have reserved green boats.\nIn complete\nsymmetry, the AND query (Query Q6) can be rewritten a.s follovvs:\nSELECT S.snarne\n\nSqL: Q'lleries 7 Constraints, Triggers\nFROM\nSailors S, Reserves R, Boats B\nWHERE\nS.sid = R.sid AND R.bid = B.bid AND B.color = 'red'\nINTERSECT\nSELECT S2.sname\nFROM\nSailors S2, Boats B2, Reserves R2\nWHERE\nS2.sid = R2.sid AND R2.bid = B2.bid AND B2.color = 'green'\nThis query actually contains a subtle bug-if there are two sailors such as\nHoratio in our example instances B1, R2, and 83, one of whom has reserved a\nred boat and the other has reserved a green boat, the name Horatio is returned\neven though no one individual called Horatio has reserved both a red and a\ngreen boat. Thus, the query actually computes sailor names such that some\nsailor with this name has reserved a red boat and some sailor with the same\nname (perhaps a different sailor) has reserved a green boat.\nAs we observed in Chapter 4, the problem arises because we are using sname\nto identify sailors, and sname is not a key for Sailors! If we select sid instead of\nsname in the previous query, we would compute the set of sids of sailors who\nhave reserved both red and green boats. (To compute the names of such sailors\nrequires a nested query; we will return to this example in Section 5.4.4.)\nOur next query illustrates the set-difference operation in SQL.\n(Q19) Find the sids of all sailor's who have reserved red boats but not green\nboats.\nSELECT\nFROM\nWHERE\nEXCEPT\nSELECT\nFROM\nWHERE\nS.sid\nSailors S, Reserves R, Boats B\nS2.sid\nSailors S2, Reserves R2, Boats B2\nSailors 22, 64, and 31 have reserved red boats.\nSailors 22, 74, and 31 have\nreserved green boats. Hence, the answer contains just the sid 64.\nIndeed, since the Reserves relation contains sid information, there is no need\nto look at the Sailors relation, and we can use the following simpler query:\nSELECT\nFROM\nWHERE\nEXCEPT\nH..sid\nBoats B, Reserves R\nR.bicl = B.bid AND B.color = 'red'",
          "pages": [
            175,
            176,
            177,
            178
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "SELECT R2.sid\nFROM\nBoats B2, Reserves R2\nWHERE\nR2.bicl = B2.bid AND B2.color = :green'\nCHAPTER~5\nObserve that this query relies on referential integrity; that is, there are no\nreservations for nonexisting sailors. Note that UNION, INTERSECT, and EXCEPT\ncan be used on any two tables that are union-compatible, that is, have the same\nnumber of columns and the columns, taken in order, have the same types. For\nexample, we can write the following query:\n(Q20) Find all sids of sailors who have a rating of 10 or reserved boat 104.\nSELECT\nFROM\nWHERE\nUNION\nSELECT\nFROM\nWHERE\nS.sid\nSailors S\nS.rating = 10\nR.sid\nReserves R\nR.bid = 104\nThe first part of the union returns the sids 58 and 71. The second part returns\n22 and 31.\nThe answer is, therefore, the set of sids 22, 31, 58, and 71.\nA\nfinal point to note about UNION, INTERSECT, and EXCEPT follows. In contrast\nto the default that duplicates are not eliminated unless DISTINCT is specified\nin the basic query form, the default for UNION queries is that duplicates are\neliminated! To retain duplicates, UNION ALL must be used; if so, the number\nof copies of a row in the result is always m + n, where m and n are the num-\nbers of times that the row appears in the two parts of the union.\nSimilarly,\nINTERSECT ALL retains cluplicates--the number of copies of a row in the result\nis min(m, n )-~ancl EXCEPT ALL also retains duplicates~thenumber of copies\nof a row in the result is m - n, where 'm corresponds to the first relation.\n5.4\nNESTED QUERIES\nOne of the most powerful features of SQL is nested queries. A nested query\nis a query that has another query embedded within it; the embedded query\nis called a suhquery. The embedded query can of course be a nested query\nitself; thus queries that have very deeply nested structures are possible. When\nwriting a query, we sornetimes need to express a condition that refers to a table\nthat must itself be computed. The query used to compute this subsidiary table\nis a subquery and appears as part of the main query.\nA subquery typically\nappears within the WHERE clause of a query. Subqueries can sometimes appear\nin the FROM clause or the HAVING clause (which we present in Section 5.5).\n\nSQL:\nQueT~ie8] Constraints] Triggers\n$\nr-~'-'-'~\n-.-------\n-~---\n,.,.._,---..--\n, ,-, ,-_\n,\n. ---------\nI Relational Algebra and SQL: Nesting of queries is a feature that is not\nI\navailable in relational algebra, but nested queries can be translated into\ni\nalgebra, as we will see in Chapter 15. Nesting in SQL is inspired more by\n,\nrelational calculus than algebra. In conjunction with some of SQL's other\nfeatures, such as (multi)set operators and aggregation, nesting is a very\nexpressive construct.\nThis section discusses only subqueries that appear in the WHERE clause. The\ntreatment of subqueries appearing elsewhere is quite similar. Some examples of\nsubqueries that appear in the FROM clause are discussed later in Section 5.5.1.\n5.4.1\nIntroduction to Nested Queries\nAs an example, let us rewrite the following query, which we discussed earlier,\nusing a nested subquery:\n(Ql) Find the names of sailors who have reserved boat 103.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.sid IN ( SELECT\nFROM\nWHERE\nR.sid\nReserves R\nR.bid = 103 )\nThe nested subquery computes the (multi)set of sids for sailors who have re-\nserved boat 103 (the set contains 22,31, and 74 on instances R2 and 83), and\nthe top-level query retrieves the names of sailors whose sid is in this set. The\nIN operator allows us to test whether a value is in a given set of elements; an\nSQL query is used to generate the set to be tested. Note that it is very easy to\nmodify this query to find all sailors who have not reserved boat 103-we can\njust replace IN by NOT IN!\nThe best way to understand a nested query is to think of it in terms of a con-\nceptual evaluation strategy. In our example, the strategy consists of examining\nrows in Sailors and, for each such row, evaluating the subquery over Reserves.\nIn general, the conceptual evaluation strategy that we presented for defining\nthe semantics of a query can be extended to cover nested queries as follows:\nConstruct the cross-product of the tables in the FROM clause of the top-level\nquery as hefore. For each row in the cross-product, while testing the qllalifica-\n\ntion in the WHERE clause, (re)compute the subquery.5 Of course, the subquery\nmight itself contain another nested subquery, in which case we apply the same\nidea one more time, leading to an evaluation strategy \\vith several levels of\nnested loops.\nAs an example of a multiply nested query, let us rewrite the following query.\n(Q2) Find the names of sailors 'who ha'ue reserved a red boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.sid IN ( SELECT R.sid\nFROM\nReserves R\nWHERE\nR.bid IN (SELECT B.bid\nFROM\nBoats B\nWHERE\nB.color = 'red'\nThe innermost subquery finds the set of bids of red boats (102 and 104 on\ninstance E1). The subquery one level above finds the set of sids of sailors who\nhave reserved one of these boats. On instances E1, R2, and 83, this set of sids\ncontains 22, 31, and 64. The top-level query finds the names of sailors whose\nsid is in this set of sids; we get Dustin, Lubber, and Horatio.\nTo find the names of sailors who have not reserved a red boat, wc replace the\noutermost occurrence of IN by NOT IN, as illustrated in the next query.\n(Q21) Find the names of sailors who have not reserved a red boat.\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.sid NOT IN ( SELECT\nFROM\nWHERE\nR.sid\nReserves R\nR.bid IN ( SELECT B.bid\nFROM\nBoats B\nWHERE\nB.color = 'red' )\nThis qucry computes the names of sailors whose sid is not in the set 22, 31,\nand 64.\nIn contrast to Query Q21, we can modify the previous query (the nested version\nof Q2) by replacing the inner occurrence (rather than the outer occurence) of\n5Since the inner subquery in our example does not depend on the 'current' row from the outer\nquery ill any way, you rnight wonder why we have to recompute the subquery for each outer row. For\nan answer, see Section 5.4.2.\n\nSQL: Queries, Con.,trf1'inis, Triggers\nIN with NOT IN. This modified query would compute the naU1eS of sailors who\nhave reserved a boat that is not red, that is, if they have a reservation, it is not\nfor a red boat. Let us consider how. In the inner query, we check that R.bid\nis not either 102 or 104 (the bids of red boats). The outer query then finds the\nsids in Reserves tuples \\vhere the bid is not 102 or 104. On instances E1, R2,\nand 53, the outer query computes the set of sids 22, 31, 64, and 74. Finally,\nwe find the names of sailors whose sid is in this set.\n\\Ve can also modify the nested query Q2 by replacing both occurrences of IN\nwith NOT IN. This variant finds the names of sailors who have not reserved a\nboat that is not red, that is, who have reserved only red boats (if they've re-\nserved any boats at all). Proceeding as in the previous paragraph, on instances\nE1, R2, and 53, the outer query computes the set of sids (in Sailors) other\nthan 22, 31, 64, and 74. This is the set 29, 32, 58, 71, 85, and 95. We then find\nthe names of sailors whose sid is in this set.\n5.4.2\nCorrelated Nested Queries\nIn the nested queries seen thus far, the inner subquery has been completely\nindependent of the outer query. In general, the inner subquery could depend on\nthe row currently being examined in the outer query (in terms of our conceptual\nevaluation strategy). Let us rewrite the following query once more.\n(Q1) Pind the names of sailors who have reserved boat nv,mber 103.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nEXISTS ( SELECT *\nFROM\nReserves R\nWHERE\nR.bid = 103\nAND R.sid = S.sid )\nThe EXISTS operator is another set comparison operator, such as IN. It allows\nus to test whether a set is nonempty, an implicit comparison with the empty\nset.\nThus, for each Sailor row 5, we test whether the set of Reserves rows\nR such that R.bid = 103 AND S.sid = R.sid is nonempty. If so, sailor 5 has\nreserved boat t03, and we retrieve the name.\n'1'he subquery clearly depends\non the current row Sand IIlUSt be re-evaluated for each row in Sailors. The\noccurrence of S in the subquery (in the form of the literal S.sid) is called a\ncOTTelation, and such queries are called con-elated queries.\nThis query also illustrates the use of the special symbol * in situations where\nall we want to do is to check that a qualifying row exists, and do Hot really\n\nCHAPTER,.5\nwant to retrieve any columns from the row. This is one of the two uses of * in\nthe SELECT clause that is good programming style; the other is &':1 an argument\nof the COUNT aggregate operation, which we describe shortly.\nAs a further example, by using NOT EXISTS instead of EXISTS, we can compute\nthe names of sailors who have not reserved a red boat.\nClosely related to\nEXISTS is the UNIQUE predicate. \\Vhen we apply UNIQUE to a subquery, the\nresulting condition returns true if no row appears twice in the answer to the\nsubquery, that is, there are no duplicates; in particular, it returns true if the\nanswer is empty. (And there is also a NOT UNI QUE version.)\n5.4.3\nSet-Comparison Operators\nWe have already seen the set-comparison operators EXISTS, IN, and UNIQUE,\nalong with their negated versions. SQL also supports op ANY and op ALL, where\nop is one of the arithmetic comparison operators {<, <=, =, <>, >=, >}. (SOME\nis also available, but it is just a synonym for ANY.)\n(Q22) Find sailors whose rating is better than some sailor called Horatio.\nSELECT S.sid\nFROM\nSailors S\nWHERE\nS.rating > ANY ( SELECT\nFROM\nWHERE\nS2.rating\nSailors S2\nS2.sname = 'Horatio' )\nIf there are several sailors called Horatio, this query finds all sailors whose rating\nis better than that of some sailor called Horatio. On instance 83, this computes\nthe sids 31, 32, 58, 71, and 74. \\\\That if there were no sailor called Horatio? In\nthis case the comparison S.rating > ANY ... is defined to return false, and the\nquery returns an elnpty answer set. To understand comparisons involving ANY,\nit is useful to think of the comparison being carried out repeatedly.\nIn this\nexample, S. rating is successively compared with each rating value that is an\nanswer to the nested query. Intuitively, the subquery must return a row that\nmakes the comparison true, in order for S. rat'ing > ANY ... to return true.\n(Q23) Find sailors whose rating is better than every sailor' called Horat·to.\nvVe can obtain all such queries with a simple modification to Query Q22: Just\nreplace ANY with ALL in the WHERE clause of the outer query. On instance 8~~,\nwe would get the sid\", 58 and 71. If there were no sailor called Horatio, the\ncomparison S.rating > ALL ... is defined to return true! The query would then\nreturn the names of all sailors. Again, it is useful to think of the comparison\n\nSQL: C2uerie,s, ConstTain,ts, Triggers\nbeing carried out repeatedly. Intuitively, the comparison must be true for every\nreturned row for S.rating> ALL ... to return true.\nAs another illustration of ALL, consider the following query.\n(Q24J Find the 8ailor's with the highest rating.\nSELECT S.sid\nFROM\nSailors S\nWHERE\nS.rating >= ALL ( SELECT S2.rating\nFROM\nSailors S2 )\nThe subquery computes the set of all rating values in Sailors. The outer WHERE\ncondition is satisfied only when S.rating is greater than or equal to each of\nthese rating values, that is, when it is the largest rating value. In the instance\n53, the condition is satisfied only for rating 10, and the answer includes the\nsid.\" of sailors with this rating, Le., 58 and 71.\nNote that IN and NOT IN are equivalent to = ANY and <> ALL, respectively.\n5.4.4\nMore Examples of Nested Queries\nLet us revisit a query that we considered earlier using the INTERSECT operator.\n(Q6) Find the names of sailors who have reserved both a red and a green boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sid = R.sid AND R.bid = B.bid AND B.color = 'red'\nAND S.sid IN ( SELECT S2.sid\nFROM\nSailors S2, Boats B2, Reserves R2\nWHERE\nS2.sid = R2.sid AND R2.bid = B2.bid\nAND B2.color = 'green' )\nThis query can be understood as follows: \"Find all sailors who have reserved\na red boat and, further, have sids that are included in the set of sids of sailors\nwho have reserved a green boat.\"\nThis formulation of the query illustrates\nhow queries involving INTERSECT can be rewritten using IN, which is useful to\nknow if your system does not support INTERSECT. Queries using EXCEPT can\nbe similarly rewritten by using NOT IN. To find the side:, of sailors who have\nreserved red boats but not green boats, we can simply replace the keyword IN\nin the previous query by NOT IN.\n\nCHAPTER\"S\nAs it turns out, writing this query (Q6) using INTERSECT is more complicated\nbecause we have to use sids to identify sailors (while intersecting) and have to\nreturn sailor names:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.sid IN (( SELECT R.sid\nFROM\nBoats B, Reserves R\nWHERE\nR.bid = B.bid AND B.color = 'red' )\nINTERSECT\n(SELECT R2.sid\nFROM\nBoats B2, Reserves R2\nWHERE\nR2.bid = B2.bid AND B2.color = 'green' ))\nOur next example illustrates how the division operation in relational algebra\ncan be expressed in SQL.\n(Q9) Find the names of sailors who have TeseTved all boats.\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS (( SELECT B.bid\nFROM\nBoats B )\nEXCEPT\n(SELECT R.bid\nFROM\nReserves R\nWHERE\nR.sid = S.sid ))\nNote that this query is correlated--for each sailor S, we check to see that the\nset of boats reserved by S includes every boat. An alternative way to do this\nquery without using EXCEPT follows:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS ( SELECT\nFROM\nWHERE\nB.bid\nBoats B\nNOT EXISTS ( SELECT R.bid\nFROM\nReserves R\nWHERE\nR.bid = B.bid\nAND R.sid = S.sid ))\nIntuitively, for each sailor we check that there is no boat that has not been\nreserved by this sailor.\n\nSQL: Q'ueT'ics. Constraint8, Triggers\nlQJ\nSQL:1999 Aggregate Functions: The collection of aggregate functions\nis greatly expanded in the new standard, including several statistical\ntions such as standard deviation, covariance, and percentiles. However,\nnew aggregate functions are in the SQLjOLAP package and may not\nsupported by all vendors.\n5.5\nAGGREGATE OPERATORS\nIn addition to simply retrieving data, we often want to perform some compu-\ntation or summarization. As we noted earlier in this chapter, SQL allows the\nuse of arithmetic expressions. We now consider a powerful class of constructs\nfor computing aggregate values such as MIN and SUM. These features represent\na significant extension of relational algebra. SQL supports five aggregate oper-\nations, which can be applied on any column, say A, of a relation:\n1. COUNT ([DISTINCT] A): The number of (unique) values in the A column.\n2. SUM ([DISTINCT] A): The sum of all (unique) values in the A column.\n3. AVG ([DISTINCT] A): The average of all (unique) values in the A column.\n4. MAX (A): The maximum value in the A column.\n5. MIN (A): The minimum value in the A column.\nNote that it does not make sense to specify DISTINCT in conjunction with MIN\nor MAX (although SQL does not preclude this).\n(Q25) Find the average age of all sailors.\nSELECT AVG (S.age)\nFROM\nSailors S\nOn instance 53, the average age is 37.4. Of course, the WHERE clause can be\nused to restrict the sailors considered in computing the average age.\n(Q26) Find the average age of sailors with a rating of 10.\nSELECT AVG (S.age)\nFROM\nSailors S\nWHERE\nS.rating = 10\nThere are two such sailors, and their average age is 25.5. MIN (or MAX) can be\nused instead of AVG in the above queries to find the age of the youngest (oldest)",
          "pages": [
            179,
            180,
            181,
            182,
            183,
            184,
            185,
            186
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "1,..')\n0 ...\nsailor. However) finding both the name and the age of the oldest sailor is more\ntricky, as the next query illustrates.\n(Q,\"21) Find the name and age of the oldest sailor.\nConsider the following attempt to answer this query:\nSELECT S.sname, MAX (S.age)\nFROM\nSailors S\nThe intent is for this query to return not only the maximum age but also the\nname of the sailors having that age. However, this query is illegal in SQL-if\nthe SELECT clause uses an aggregate operation, then it must use only aggregate\noperations unless the query contains a GROUP BY clause! (The intuition behind\nthis restriction should become clear when we discuss the GROUP BY clause in\nSELECT clause. We have to use a nested query to compute the desired answer\nto Q27:\nSELECT\nFROM\nWHERE\nS.sname, S.age\nSailors S\nS.age = ( SELECT MAX (S2.age)\nFROM Sailors S2 )\nObserve that we have used the result of an aggregate operation in the subquery\nas an argument to a comparison operation. Strictly speaking, we are comparing\nan age value with the result of the subquery, which is a relation.\nHowever,\nbecause of the use of the aggregate operation, the subquery is guaranteed to\nreturn a single tuple with a single field, and SQL Gonverts such a relation to a\nfield value for the sake of the comparison. The following equivalent query for\nQ27 is legal in the SQL standard but, unfortunately, is not supported in many\nsystems:\nSELECT\nFROM\nWHERE\nS.sname, S.age\nSailors S\n( SELECT MAX (S2.age)\nFROM Sailors S2 ) = S.age\n\\Vc can count the number of sailors using COUNT. This exarnple illustrates the\nuse of * as an argument to COUNT, which is useful when \\ve want to count all\nrows.\n(Q28) Count the n:umbCT of sa:iloTs.\nSELECT COUNT (*)\n\nFROM\nSailors S\nvVe can think of * as shorthand for all the columns (in the cross-product of the\nfrom-list in the FROM clause). Contrast this query with the following query,\nwhich computes the number of distinct sailor names. (Remember that ,'mame\nis not a key!)\n(Q29) Count the nmnber of d'i.fferent sailor names.\nSELECT COUNT ( DISTINCT S.sname )\nFROM\nSailors S\nOn instance 83, the answer to Q28 is 10, whereas the answer to Q29 is 9\n(because two sailors have the same name, Horatio). If DISTINCT is omitted,\nthe answer to Q29 is 10, because the name Horatio is counted twice. If COUNT\ndoes not include DISTINCT, then COUNT (*) gives the same answer as COUNT (x) ,\nwhere x is any set of attributes.\nIn our example, without DISTINCT Q29 is\nequivalent to Q28.\nHowever, the use of COUNT (*) is better querying style,\nsince it is immediately clear that all records contribute to the total count.\nAggregate operations offer an alternative to the ANY and ALL constructs. For\nexample, consider the following query:\n(Q30) Find the names of sailors who are older than the oldest sailor with a\nrating of 10.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.age > ( SELECT MAX ( S2.age )\nFROM\nSailors S2\nWHERE\nS2.rating = 10 )\nOn instance 83, the oldest sailor with rating 10 is sailor 58, whose age is\n~j5.\nThe names of older sailors are Bob, Dustin, Horatio, and Lubber. Using ALL,\nthis query could alternatively be written as follows:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.age > ALL ( SELECT\nFROM\nWHERE\nS2.age\nSailors S2\nS2.rating = 10 )\nHowever, the ALL query is more error proncone could easily (and incorrectly!)\nuse ANY instead of ALL, and retrieve sailors who are older than some sailor with\n\nCHAPTEFt,~5\nRelationa~ Algebra and SQL: ~~~~:egation is a fUIl~~~:·mental operati(~:l-'-l\nthat canIlot be expressed in relational algebra. Similarly, SQL'8 grouping\nI\nconstruct cannot be expressed in algebra.\nI\nL..-\n._.\n.....__\n.\nI\na rating of 10. The use of ANY intuitively corresponds to the use of MIN, instead\nof MAX, in the previous query.\n5.5.1\nThe GROUP BY and HAVING Clauses\nThus far, we have applied aggregate operations to all (qualifying) rows in a\nrelation.\nOften we want to apply aggregate operations to each of a number\nof groups of rows in a relation, where the number of groups depends on the\nrelation instance (i.e., is not known in advance).\nFor example, consider the\nfollowing query.\n(Q31) Find the age of the youngest sailor for each rating level.\nIf we know that ratings are integers in the range 1 to la, we could write 10\nqueries of the form:\nSELECT MIN (S.age)\nFROM\nSailors S\nWHERE\nS.rating = i\nwhere i\n=\n1,2, ... ,10. vVriting 10 such queries is tedious. More important,\nwe may not know what rating levels exist in advance.\nTo write such queries, we need a major extension to the basic SQL query\nform, namely, the GROUP BY clause.\nIn fact, the extension also includes an\noptional HAVING clause that can be used to specify qualificatioIls over groups\n(for example, we may be interested only in rating levels> 6. The general form\nof an SQL query with these extensions is:\nSELECT\n[ DISTINCT] select-list\nFROM\nfrom-list\nWHERE\n'qualification\nGROUP BY grouping-list\nHAVING\ngroup-qualification\nUsing the GROUP BY clause, we can write Q:n a.s follows:\nSELECT\nS.rating, MIN (S.age)",
          "pages": [
            187,
            188,
            189
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The GROUP BY clause is used to group rows that have the same values in specified columns into aggregated data. It's essential for performing calculations on groups of data and summarizing information.",
        "explanation": "Imagine you're managing a library and want to know how many books are checked out by each member. The GROUP BY clause helps you organize the data so you can easily count the number of books per member. You group the rows based on the member's ID, then apply an aggregate function like COUNT() to find out how many books each member has borrowed.",
        "key_points": [
          "Key point 1: Groups rows with the same values in specified columns",
          "Key point 2: Used with aggregate functions (COUNT, SUM, AVG) for calculations",
          "Key point 3: Helps summarize data and perform calculations on groups"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "SELECT department, COUNT(employee_id) AS employee_count\nFROM employees\nGROUP BY department;",
            "explanation": "This example groups employees by their department and counts how many employees are in each department."
          },
          {
            "title": "Practical Example",
            "code": "SELECT customer_id, SUM(amount) AS total_spent\nFROM orders\nWHERE order_date BETWEEN '2023-01-01' AND '2023-12-31'\nGROUP BY customer_id;",
            "explanation": "This practical example calculates the total amount spent by each customer in a given year."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to include the aggregate function",
            "incorrect_code": "SELECT department, employee_id\nFROM employees\nGROUP BY department;",
            "correct_code": "SELECT department, COUNT(employee_id) AS employee_count\nFROM employees\nGROUP BY department;",
            "explanation": "The GROUP BY clause alone doesn't perform any calculations. You must use an aggregate function like COUNT() to get meaningful results."
          }
        ],
        "practice": {
          "question": "Create a query that groups orders by customer and calculates the total amount spent by each customer in the last quarter of 2023.",
          "solution": "SELECT customer_id, SUM(amount) AS total_spent\nFROM orders\nWHERE order_date BETWEEN '2023-10-01' AND '2023-12-31'\nGROUP BY customer_id;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTERf)\nnames (possibly prefixed by range variables) and constants, and colurnnswrne\nis a ne\"v name for this column in the output of the query. It can also contain\naggregates such as smn and count, ",
      "content_relevance": {
        "score": 0.8,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.0,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "having": {
      "id": "having",
      "title": "HAVING Clause",
      "definition": "Filtering groups based on aggregate conditions",
      "difficulty": "intermediate",
      "page_references": [
        183,
        184,
        185,
        186,
        187,
        188,
        189,
        190,
        191,
        192,
        193,
        194,
        195,
        196
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER,.5\nwant to retrieve any columns from the row. This is one of the two uses of * in\nthe SELECT clause that is good programming style; the other is &':1 an argument\nof the COUNT aggregate operation, which we describe shortly.\nAs a further example, by using NOT EXISTS instead of EXISTS, we can compute\nthe names of sailors who have not reserved a red boat.\nClosely related to\nEXISTS is the UNIQUE predicate. \\Vhen we apply UNIQUE to a subquery, the\nresulting condition returns true if no row appears twice in the answer to the\nsubquery, that is, there are no duplicates; in particular, it returns true if the\nanswer is empty. (And there is also a NOT UNI QUE version.)\n5.4.3\nSet-Comparison Operators\nWe have already seen the set-comparison operators EXISTS, IN, and UNIQUE,\nalong with their negated versions. SQL also supports op ANY and op ALL, where\nop is one of the arithmetic comparison operators {<, <=, =, <>, >=, >}. (SOME\nis also available, but it is just a synonym for ANY.)\n(Q22) Find sailors whose rating is better than some sailor called Horatio.\nSELECT S.sid\nFROM\nSailors S\nWHERE\nS.rating > ANY ( SELECT\nFROM\nWHERE\nS2.rating\nSailors S2\nS2.sname = 'Horatio' )\nIf there are several sailors called Horatio, this query finds all sailors whose rating\nis better than that of some sailor called Horatio. On instance 83, this computes\nthe sids 31, 32, 58, 71, and 74. \\\\That if there were no sailor called Horatio? In\nthis case the comparison S.rating > ANY ... is defined to return false, and the\nquery returns an elnpty answer set. To understand comparisons involving ANY,\nit is useful to think of the comparison being carried out repeatedly.\nIn this\nexample, S. rating is successively compared with each rating value that is an\nanswer to the nested query. Intuitively, the subquery must return a row that\nmakes the comparison true, in order for S. rat'ing > ANY ... to return true.\n(Q23) Find sailors whose rating is better than every sailor' called Horat·to.\nvVe can obtain all such queries with a simple modification to Query Q22: Just\nreplace ANY with ALL in the WHERE clause of the outer query. On instance 8~~,\nwe would get the sid\", 58 and 71. If there were no sailor called Horatio, the\ncomparison S.rating > ALL ... is defined to return true! The query would then\nreturn the names of all sailors. Again, it is useful to think of the comparison\n\nSQL: C2uerie,s, ConstTain,ts, Triggers\nbeing carried out repeatedly. Intuitively, the comparison must be true for every\nreturned row for S.rating> ALL ... to return true.\nAs another illustration of ALL, consider the following query.\n(Q24J Find the 8ailor's with the highest rating.\nSELECT S.sid\nFROM\nSailors S\nWHERE\nS.rating >= ALL ( SELECT S2.rating\nFROM\nSailors S2 )\nThe subquery computes the set of all rating values in Sailors. The outer WHERE\ncondition is satisfied only when S.rating is greater than or equal to each of\nthese rating values, that is, when it is the largest rating value. In the instance\n53, the condition is satisfied only for rating 10, and the answer includes the\nsid.\" of sailors with this rating, Le., 58 and 71.\nNote that IN and NOT IN are equivalent to = ANY and <> ALL, respectively.\n5.4.4\nMore Examples of Nested Queries\nLet us revisit a query that we considered earlier using the INTERSECT operator.\n(Q6) Find the names of sailors who have reserved both a red and a green boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sid = R.sid AND R.bid = B.bid AND B.color = 'red'\nAND S.sid IN ( SELECT S2.sid\nFROM\nSailors S2, Boats B2, Reserves R2\nWHERE\nS2.sid = R2.sid AND R2.bid = B2.bid\nAND B2.color = 'green' )\nThis query can be understood as follows: \"Find all sailors who have reserved\na red boat and, further, have sids that are included in the set of sids of sailors\nwho have reserved a green boat.\"\nThis formulation of the query illustrates\nhow queries involving INTERSECT can be rewritten using IN, which is useful to\nknow if your system does not support INTERSECT. Queries using EXCEPT can\nbe similarly rewritten by using NOT IN. To find the side:, of sailors who have\nreserved red boats but not green boats, we can simply replace the keyword IN\nin the previous query by NOT IN.\n\nCHAPTER\"S\nAs it turns out, writing this query (Q6) using INTERSECT is more complicated\nbecause we have to use sids to identify sailors (while intersecting) and have to\nreturn sailor names:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.sid IN (( SELECT R.sid\nFROM\nBoats B, Reserves R\nWHERE\nR.bid = B.bid AND B.color = 'red' )\nINTERSECT\n(SELECT R2.sid\nFROM\nBoats B2, Reserves R2\nWHERE\nR2.bid = B2.bid AND B2.color = 'green' ))\nOur next example illustrates how the division operation in relational algebra\ncan be expressed in SQL.\n(Q9) Find the names of sailors who have TeseTved all boats.\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS (( SELECT B.bid\nFROM\nBoats B )\nEXCEPT\n(SELECT R.bid\nFROM\nReserves R\nWHERE\nR.sid = S.sid ))\nNote that this query is correlated--for each sailor S, we check to see that the\nset of boats reserved by S includes every boat. An alternative way to do this\nquery without using EXCEPT follows:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS ( SELECT\nFROM\nWHERE\nB.bid\nBoats B\nNOT EXISTS ( SELECT R.bid\nFROM\nReserves R\nWHERE\nR.bid = B.bid\nAND R.sid = S.sid ))\nIntuitively, for each sailor we check that there is no boat that has not been\nreserved by this sailor.",
          "pages": [
            183,
            184,
            185
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "SQL: Q'ueT'ics. Constraint8, Triggers\nlQJ\nSQL:1999 Aggregate Functions: The collection of aggregate functions\nis greatly expanded in the new standard, including several statistical\ntions such as standard deviation, covariance, and percentiles. However,\nnew aggregate functions are in the SQLjOLAP package and may not\nsupported by all vendors.\n5.5\nAGGREGATE OPERATORS\nIn addition to simply retrieving data, we often want to perform some compu-\ntation or summarization. As we noted earlier in this chapter, SQL allows the\nuse of arithmetic expressions. We now consider a powerful class of constructs\nfor computing aggregate values such as MIN and SUM. These features represent\na significant extension of relational algebra. SQL supports five aggregate oper-\nations, which can be applied on any column, say A, of a relation:\n1. COUNT ([DISTINCT] A): The number of (unique) values in the A column.\n2. SUM ([DISTINCT] A): The sum of all (unique) values in the A column.\n3. AVG ([DISTINCT] A): The average of all (unique) values in the A column.\n4. MAX (A): The maximum value in the A column.\n5. MIN (A): The minimum value in the A column.\nNote that it does not make sense to specify DISTINCT in conjunction with MIN\nor MAX (although SQL does not preclude this).\n(Q25) Find the average age of all sailors.\nSELECT AVG (S.age)\nFROM\nSailors S\nOn instance 53, the average age is 37.4. Of course, the WHERE clause can be\nused to restrict the sailors considered in computing the average age.\n(Q26) Find the average age of sailors with a rating of 10.\nSELECT AVG (S.age)\nFROM\nSailors S\nWHERE\nS.rating = 10\nThere are two such sailors, and their average age is 25.5. MIN (or MAX) can be\nused instead of AVG in the above queries to find the age of the youngest (oldest)\n\n1,..')\n0 ...\nsailor. However) finding both the name and the age of the oldest sailor is more\ntricky, as the next query illustrates.\n(Q,\"21) Find the name and age of the oldest sailor.\nConsider the following attempt to answer this query:\nSELECT S.sname, MAX (S.age)\nFROM\nSailors S\nThe intent is for this query to return not only the maximum age but also the\nname of the sailors having that age. However, this query is illegal in SQL-if\nthe SELECT clause uses an aggregate operation, then it must use only aggregate\noperations unless the query contains a GROUP BY clause! (The intuition behind\nthis restriction should become clear when we discuss the GROUP BY clause in\nSELECT clause. We have to use a nested query to compute the desired answer\nto Q27:\nSELECT\nFROM\nWHERE\nS.sname, S.age\nSailors S\nS.age = ( SELECT MAX (S2.age)\nFROM Sailors S2 )\nObserve that we have used the result of an aggregate operation in the subquery\nas an argument to a comparison operation. Strictly speaking, we are comparing\nan age value with the result of the subquery, which is a relation.\nHowever,\nbecause of the use of the aggregate operation, the subquery is guaranteed to\nreturn a single tuple with a single field, and SQL Gonverts such a relation to a\nfield value for the sake of the comparison. The following equivalent query for\nQ27 is legal in the SQL standard but, unfortunately, is not supported in many\nsystems:\nSELECT\nFROM\nWHERE\nS.sname, S.age\nSailors S\n( SELECT MAX (S2.age)\nFROM Sailors S2 ) = S.age\n\\Vc can count the number of sailors using COUNT. This exarnple illustrates the\nuse of * as an argument to COUNT, which is useful when \\ve want to count all\nrows.\n(Q28) Count the n:umbCT of sa:iloTs.\nSELECT COUNT (*)\n\nFROM\nSailors S\nvVe can think of * as shorthand for all the columns (in the cross-product of the\nfrom-list in the FROM clause). Contrast this query with the following query,\nwhich computes the number of distinct sailor names. (Remember that ,'mame\nis not a key!)\n(Q29) Count the nmnber of d'i.fferent sailor names.\nSELECT COUNT ( DISTINCT S.sname )\nFROM\nSailors S\nOn instance 83, the answer to Q28 is 10, whereas the answer to Q29 is 9\n(because two sailors have the same name, Horatio). If DISTINCT is omitted,\nthe answer to Q29 is 10, because the name Horatio is counted twice. If COUNT\ndoes not include DISTINCT, then COUNT (*) gives the same answer as COUNT (x) ,\nwhere x is any set of attributes.\nIn our example, without DISTINCT Q29 is\nequivalent to Q28.\nHowever, the use of COUNT (*) is better querying style,\nsince it is immediately clear that all records contribute to the total count.\nAggregate operations offer an alternative to the ANY and ALL constructs. For\nexample, consider the following query:\n(Q30) Find the names of sailors who are older than the oldest sailor with a\nrating of 10.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.age > ( SELECT MAX ( S2.age )\nFROM\nSailors S2\nWHERE\nS2.rating = 10 )\nOn instance 83, the oldest sailor with rating 10 is sailor 58, whose age is\n~j5.\nThe names of older sailors are Bob, Dustin, Horatio, and Lubber. Using ALL,\nthis query could alternatively be written as follows:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.age > ALL ( SELECT\nFROM\nWHERE\nS2.age\nSailors S2\nS2.rating = 10 )\nHowever, the ALL query is more error proncone could easily (and incorrectly!)\nuse ANY instead of ALL, and retrieve sailors who are older than some sailor with\n\nCHAPTEFt,~5\nRelationa~ Algebra and SQL: ~~~~:egation is a fUIl~~~:·mental operati(~:l-'-l\nthat canIlot be expressed in relational algebra. Similarly, SQL'8 grouping\nI\nconstruct cannot be expressed in algebra.\nI\nL..-\n._.\n.....__\n.\nI\na rating of 10. The use of ANY intuitively corresponds to the use of MIN, instead\nof MAX, in the previous query.\n5.5.1\nThe GROUP BY and HAVING Clauses\nThus far, we have applied aggregate operations to all (qualifying) rows in a\nrelation.\nOften we want to apply aggregate operations to each of a number\nof groups of rows in a relation, where the number of groups depends on the\nrelation instance (i.e., is not known in advance).\nFor example, consider the\nfollowing query.\n(Q31) Find the age of the youngest sailor for each rating level.\nIf we know that ratings are integers in the range 1 to la, we could write 10\nqueries of the form:\nSELECT MIN (S.age)\nFROM\nSailors S\nWHERE\nS.rating = i\nwhere i\n=\n1,2, ... ,10. vVriting 10 such queries is tedious. More important,\nwe may not know what rating levels exist in advance.\nTo write such queries, we need a major extension to the basic SQL query\nform, namely, the GROUP BY clause.\nIn fact, the extension also includes an\noptional HAVING clause that can be used to specify qualificatioIls over groups\n(for example, we may be interested only in rating levels> 6. The general form\nof an SQL query with these extensions is:\nSELECT\n[ DISTINCT] select-list\nFROM\nfrom-list\nWHERE\n'qualification\nGROUP BY grouping-list\nHAVING\ngroup-qualification\nUsing the GROUP BY clause, we can write Q:n a.s follows:\nSELECT\nS.rating, MIN (S.age)\n\nS(JL: queries. Constraints. Triggers\nFROM\nSailors S\nGROUP BY S.rating\nLet us consider some important points concerning the new clauses:\nII\nThe select-list in the SELECT clause consists of (1) a list of column names\nand (2) a list of terms having the form aggop ( column-name) AS new-\nname. vVe already saw AS used to rename output columns. Columns that\nare the result of aggregate operators do not already have a column name,\nand therefore giving the column a name with AS is especially useful.\nEvery column that appears in (1) must also appear in grouping-list. The\nreason is that each row in the result of the query corresponds to one gmup,\nwhich is a collection of rows that agree on the values of columns in grouping-\nlist. In general, if a column appears in list (1), but not in grouping-list,\nthere can be multiple rows within a group that have different values in this\ncolumn, and it is not clear what value should be assigned to this column\nin an answer row.\nWe can sometimes use primary key information to verify that a column\nhas a unique value in all rows within each group.\nFor example, if the\ngrouping-list contains the primary key of a table in the from-list, every\ncolumn of that table has a unique value within each group. In SQL:1999,\nsuch columns are also allowed to appear in part (1) of the select-list.\nII\nThe expressions appearing in the group-qualification in the HAVING clause\nmust have a single value per group. The intuition is that the HAVING clause\ndetermines whether an answer row is to be generated for a given group.\nTo satisfy this requirement in SQL-92, a column appearing in the group-\nqualification must appear a'3 the argument to an aggregation operator, or\nit must also appear in grouping-list. In SQL:1999, two new set functions\nhave been introduced that allow us to check whether every or any row in a\ngroup satisfies a condition; this allows us to use conditions similar to those\nin a WHERE clause.\nIII\nIf GROUP BY is omitted, the entire table is regarded as a single group.\nvVe explain the semantics of such a query through an example.\n(QS2) Find the age of the youngest sa'ilor who is eligible to vote (i.e., is at least\n18 years old) for each rating level with at least h.uo such sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, MIN (S.age) AS minage\nSailors S\nS.age >= 18\nS.rating\nCOUNT (*) > 1\n\nCHAPTERl,5\nvVe will evaluate this query on instance 83 of Sailors, reproduced in Figure 5.10\nfor convenience. The instance of Sailors on which this query is to be evaluated is\nshown in Figure 5.10. Extending the conceptual evaluation strategy presented\nin Section 5.2, we proceed as follows. The first step is to construct the cross-\nproduct of tables in the from-list. Because the only relation in the from-list\nin Query Q32 is Sailors, the result is just the instance shown in Figure 5.10.\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nHoratio\n35.0\nArt\n25.5\nBob\n63.5\nFrodo\n25.5\nInstance 53 of Sailors\nThe second step is to apply the qualification in the WHERE clause, S. age >= 18.\nThis step eliminates the row (71, zorba, 10, 16). The third step is to eliminate\nunwanted columns. Only columns mentioned in the SELECT clause, the GROUP\nBY clause, or the HAVING clause are necessary, which means we can eliminate\nsid and sname in our example. The result is shown in Figure 5.11.\nObserve\nthat there are two identical rows with rating 3 and age 25.5-SQL does not\neliminate duplicates except when required to do so by use of the DISTINCT\nkeyword! The number of copies of a row in the intermediate table of Figure\n5.11 is determined by the number of rows in the original table that had these\nvalues in the projected columns.\nThe fourth step is to sort the table according to the GROUP BY clause to identify\nthe groups. The result of this step is shown in Figure 5.12.\nThe fifth step ,-is to apply the group-qualification in the HAVING clause, that\nis, the condition COUNT (*) > 1. This step eliminates the groups with rating\nequal to 1, 9, and 10. Observe that the order in which the WHERE and GROUP\nBY clauses are considered is significant: If the WHERE clause were not consid-\nered first, the group with rating=10 would have met the group-qualification\nin the HAVING clause. The sixth step is to generate one answer row for each\nremaining group. The answer row corresponding to a group consists of a subset",
          "pages": [
            186,
            187,
            188,
            189,
            190,
            191
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "SqL: queries, Constraints, Triggers\n25.5\n25.5\n63.5\n55.5\n25.5\n35.0\n35.0\nI 10\n~~tl?l\n(J;fJ6;\nI 33.0\n.f'(Lf'tTbf}\niigge·····\n..\n. ....\n45.0\n33.0\n55.5\n25.5\n35.0\n35.0\n35.0\n25.5\n63.5\n25.5\nAfter Evaluation Step 3\nAfter Evaluation Step 4\nof the grouping columns, plus one or more columns generated by applying an\naggregation operator. In our example, each answer row has a rating column\nand a minage column, which is computed by applying MIN to the values in the\nage column of the corresponding group.\nThe result of this step is shown in\nI rating I minage I\n25.5\n35.0\n25.5\nFinal Result in Sample Evaluation\nIf the query contains DISTINCT in the SELECT clause, duplicates are eliminated\nin an additional, and final, step.\nSQL:1999 ha.s introduced two new set functions, EVERY and ANY. To illustrate\nthese functions, we can replace the HAVING clause in our example by\nHAVING\nCOUNT (*) > 1 AND EVERY ( S.age <= 60 )\nThe fifth step of the conceptual evaluation is the one affected by the change\nin the HAVING clause. Consider the result of the fourth step, shown in Figure\n5.12. The EVERY keyword requires that every row in a group must satisfy the\nattached condition to meet the group-qualification. The group for rat'ing 3 does\nmeet this criterion and is dropped; the result is shown in Figure 5.14.\n\nCHAPTER. 5\nSQL:1999 Extensions: Two new set functions, EVERY and ANY, have\nbeen added. vVhen they are used in the HAVING clause, the basic intuition\nthat the clause specifies a condition to be satisfied by each group, taken as\na whole, remains unchanged. However, the condition can now involve tests\non individual tuples in the group, whereas it previously relied exclusively\non aggregate functions over the group of tuples.\nIt is worth contrasting the preceding query with the following query, in which\nthe condition on age is in the WHERE clause instead of the HAVING clause:\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, MIN (S.age) AS minage\nSailors S\nS.age >= 18 AND S.age <= 60\nS.rating\nCOUNT (*) > 1\nNow, the result after the third step of conceptual evaluation no longer contains\nthe row with age 63.5. Nonetheless, the group for rating 3 satisfies the condition\nCOUNT (*) > 1, since it still has two rows, and meets the group-qualification\napplied in the fifth step. The final result for this query is shown in Figure 5.15.\n25.5\n45.0\n55.5\nI rating I minage I\n55.5\nrating I minage\n~\n.\nFinal Result of EVERY Query\nResult of Alternative Query\n5.5.2\nMore Examples of Aggregate Queries\n(Q33) FOT each red boat; find the number of reservations for this boat.\nSELECT\nB.bid, COUNT (*) AS reservationcount\nFROM\nBoats B, Reserves R\nWHERE\nR.bid = B.bid AND B.color = 'red'\nGROUP BY B.bid\nOn instances B1 and R2, the answer to this query contains the two tuples (102,\n3) and (104, 2).\nObserve that this version of the preceding query is illegal:\n\nSCdL: (J'ueries, Constraints. IhggeT8\nSELECT\nB.bicl, COUNT (*) AS reservationcount\nFROM\nBoats B, Reserves R\nWHERE\nR.bid = B.bid\nGROUP BY B.bid\nHAVING\nB.color = 'red'\nlijj9\nEven though the gToup-qualification B.coloT = 'Ted'is single-valued per group,\nsince the grouping attribute bid is a key for Boats (and therefore determines\ncoloT) , SQL disallows this query.6 Only columns that appear in the GROUP BY\nclause can appear in the HAVING clause, unless they appear as arguments to\nan aggregate operator in the HAVING clause.\n(Q34) Find the avemge age of sailoTs fOT each mting level that has at least two\nsailoTs.\nSELECT\nFROM\nGROUP BY\nHAVING\nS.rating, AVG (S.age) AS avgage\nSailors S\nS.rating\nCOUNT (*) > 1\nAfter identifying groups based on mting, we retain only groups with at least\ntwo sailors. The answer to this query on instance 83 is shown in Figure 5.16.\nI mting I avgage I\n44.5\n40.0\n40.5\n25.5\nQ34 Answer\nI mting I avgage I\n45.5\n40.0\n40.5\n35.0\nQ35 Answer\nI··rating I av.qage]\n45.5\n40.0\n40.5\nQ:36 Answer\nThe following alternative formulation of Query Q34 illustrates that the HAVING\nclause can have a nested subquery, just like the WHERE clause. Note that we\ncan use S. mtiTLg inside the nested subquery in the HAVING clause because it\nhas a single value for the current group of sailors:\nSELECT\nFROM\nGROUP BY\nHAVING\nS.rating, AVG ( S.age ) AS avgage\nSailors S\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.Hl,ting )\n6This query can be ea..'iily rewritten to be legal in SQL: 1999 using EVERY in the HAVING clause.\n\nCHAPTER .;5\n(Q35) Find the average age of sailors 'Who aTe of voting age\n(i.e.~ at least 18\nyear8 old) for each 'rating level that has at least two sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, AVG ( S.age ) AS avgage\nSailors S\nS. age >= 18\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.rating )\nIn this variant of Query Q34, we first remove tuples with age <= 18 and group\nthe remaining tuples by rating.\nFor each group, the subquery in the HAVING\nclause computes the number of tuples in Sailors (without applying the selection\nage <= 18) with the same rating value as the current group. If a group has\nless than two sailors, it is discarded.\nFor each remaining group, we output\nthe average age. The answer to this query on instance 53 is shown in Figure\n5.17. Note that the answer is very similar to the answer for Q34, with the only\ndifference being that for the group with rating 10, we now ignore the sailor\nwith age 16 while computing the average.\n(Q36) Find the average age oj sailors who aTe of voting age (i.e., at least 18\nyeaTs old) JOT each rating level that has at least two such sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nSailors S\nS. age> 18\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.rating AND S2.age >= 18 )\nThis formulation of the query reflects its similarity to Q35. The answer to Q36\non instance 53 is shown in Figure 5.18. It differs from the answer to Q35 in\nthat there is no tuple for rating 10, since there is only one tuple with rating 10\nand age 2 18.\nQuery Q36 is actually very similar to Q32, as the following simpler formulation\nshows:\nSELECT\nFROM\nWHERE\nGROUP BY\nS.rating, AVG\nSailors S\nS. age> 18\nS.rating\n( S.age ) AS avgage",
          "pages": [
            192,
            193,
            194,
            195
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The HAVING clause is used to filter groups of rows based on aggregate functions, similar to how WHERE filters individual rows.",
        "explanation": "Imagine you have a group of students and you want to find out which classes have an average score above a certain threshold. The HAVING clause helps with this by allowing you to apply conditions after the data has been grouped. It works in conjunction with GROUP BY, which groups rows based on one or more columns. After grouping, HAVING allows you to specify conditions that must be met for the group to be included in the final result set.",
        "key_points": [
          "Key point 1: The HAVING clause is used after GROUP BY to filter groups based on aggregate functions.",
          "Key point 2: It's similar to WHERE, but operates on groups of rows rather than individual rows.",
          "Key point 3: Common mistakes include forgetting to use GROUP BY before HAVING, or using WHERE instead of HAVING when needed."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "SELECT department, AVG(salary) AS avg_salary FROM employees GROUP BY department HAVING AVG(salary) > 5000;",
            "explanation": "This query groups employees by their department and calculates the average salary for each department. It then filters out departments where the average salary is not greater than 5000."
          },
          {
            "title": "Practical Example",
            "code": "SELECT customer_id, COUNT(order_id) AS order_count FROM orders GROUP BY customer_id HAVING COUNT(order_id) > 10;",
            "explanation": "This practical example shows how to find customers who have placed more than 10 orders. It groups the orders by customer ID and filters out those with an order count greater than 10."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using WHERE instead of HAVING",
            "incorrect_code": "SELECT department, AVG(salary) AS avg_salary FROM employees WHERE AVG(salary) > 5000;",
            "correct_code": "SELECT department, AVG(salary) AS avg_salary FROM employees GROUP BY department HAVING AVG(salary) > 5000;",
            "explanation": "The mistake here is using WHERE instead of HAVING. WHERE filters rows before grouping, while HAVING filters groups after they are created."
          }
        ],
        "practice": {
          "question": "Find the names and average ratings of sailors whose average rating is greater than 8.",
          "solution": "SELECT sname, AVG(rating) AS avg_rating FROM Sailors GROUP BY sname HAVING AVG(rating) > 8;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER,.5\nwant to retrieve any columns from the row. This is one of the two uses of * in\nthe SELECT clause that is good programming style; the other is &':1 an argument\nof the COUNT aggregate operati",
      "content_relevance": {
        "score": 0.7,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "subqueries": {
      "id": "subqueries",
      "title": "Subqueries",
      "definition": "Nested queries - using SELECT statements within other SQL statements",
      "difficulty": "intermediate",
      "page_references": [
        195,
        196,
        197,
        198,
        199,
        200,
        201,
        202,
        203,
        204,
        205,
        206,
        207,
        208,
        209,
        210,
        211,
        212
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER .;5\n(Q35) Find the average age of sailors 'Who aTe of voting age\n(i.e.~ at least 18\nyear8 old) for each 'rating level that has at least two sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, AVG ( S.age ) AS avgage\nSailors S\nS. age >= 18\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.rating )\nIn this variant of Query Q34, we first remove tuples with age <= 18 and group\nthe remaining tuples by rating.\nFor each group, the subquery in the HAVING\nclause computes the number of tuples in Sailors (without applying the selection\nage <= 18) with the same rating value as the current group. If a group has\nless than two sailors, it is discarded.\nFor each remaining group, we output\nthe average age. The answer to this query on instance 53 is shown in Figure\n5.17. Note that the answer is very similar to the answer for Q34, with the only\ndifference being that for the group with rating 10, we now ignore the sailor\nwith age 16 while computing the average.\n(Q36) Find the average age oj sailors who aTe of voting age (i.e., at least 18\nyeaTs old) JOT each rating level that has at least two such sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nSailors S\nS. age> 18\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.rating AND S2.age >= 18 )\nThis formulation of the query reflects its similarity to Q35. The answer to Q36\non instance 53 is shown in Figure 5.18. It differs from the answer to Q35 in\nthat there is no tuple for rating 10, since there is only one tuple with rating 10\nand age 2 18.\nQuery Q36 is actually very similar to Q32, as the following simpler formulation\nshows:\nSELECT\nFROM\nWHERE\nGROUP BY\nS.rating, AVG\nSailors S\nS. age> 18\nS.rating\n( S.age ) AS avgage\n\nSQL: QueTies, Constraints, Triggers\nHAVING\nCOUNT (*) > 1\nThis formulation of Q36 takes advantage of the fact that the WHERE clause is\napplied before grouping is done; thus, only sailors with age> 18 are left when\ngrouping is done. It is instructive to consider yet another way of writing this\nquery:\nSELECT\nFROM\nWHERE\nTemp.rating, Temp.avgage\n( SELECT\nS.rating, AVG ( S.age ) AS\nCOUNT (*) AS ratingcount\nFROM\nSailors S\nWHERE\nS. age> 18\nGROUP BY\nS.rating) AS Temp\nTemp.ratingcount > 1\navgage,\nThis alternative brings out several interesting points.\nFirst, the FROM clause\ncan also contain a nested subquery according to the SQL standard. 7 Second,\nthe HAVING clause is not needed at all. Any query with a HAVING clause can\nbe rewritten without one, but many queries are simpler to express with the\nHAVING clause.\nFinally, when a subquery appears in the FROM clause, using\nthe AS keyword to give it a name is necessary (since otherwise we could not\nexpress, for instance, the condition Temp. ratingcount > 1).\n(Q37) Find those ratings fOT which the average age of sailoTS is the m'inirnum\nover all ratings.\nWe use this query to illustrate that aggregate operations cannot be nested. One\nmight consider writing it as follows:\nSELECT\nFROM\nWHERE\nS.rating\nSailors S\nAVG (S.age) = ( SELECT\nMIN (AVG (S2.age))\nFROM\nSailors S2\nGROUP BY S2.rating )\nA little thought shows that this query will not work even if the expression MIN\n(AVG (S2.age)), which is illegal, were allowed. In the nested query, Sailors is\npartitioned int,o groups by rating, and the average age is computed for each\nrating value. for each group, applying MIN to this average age value for the\ngroup will return the same value! A correct version of this query follows. It\nessentially computes a temporary table containing the average age for each\nrating value and then finds the rating(s) for which this average age is the\nminimum.\n7Not all commercial database systems currently support nested queries in the FROM clause.\n\nGHAPTER r5\nr-_ m\n.\nI\nThe Relational Model and SQL: Null values arc not part of the bask\nI\nrelational model.\nLike SQL's treatment of tables as multisets of tuples,\n~liS is a del~.~~~~r~...~~~..1~._t_h_e_l_)ru_s_,i_c_l_l1_o_d_e_1.\n----'\nSELECT\nFROM\nWHERE\nTemp.rating, Temp.avgage\n( SELECT\nS.rating, AVG (S.age) AS avgage,\nFROM\nSailors S\nGROUP BY S.rating) AS Temp\nTemp.avgage = ( SELECT MIN (Temp.avgage) FROM Temp)\nThe answer to this query on instance 53 is (10, 25.5).\nAs an exercise, consider whether the following query computes the same answer.\nSELECT\nFROM\nGROUP BY\nTemp.rating, MIN (Temp.avgage )\n( SELECT\nS.rating, AVG (S.age) AS\nFROM\nSailors S\nGROUP BY\nS.rating) AS Temp\nTemp.rating\navgage,\n5.6\nNULL VALUES\nThus far, we have assumed that column values in a row are always known. In\npractice column values can be unknown. For example, when a sailor, say Dan,\njoins a yacht club, he may not yet have a rating assigned. Since the definition\nfor the Sailors table has a rating column, what row should we insert for Dan?\n\\\\That is needed here is a special value that denotes unknown. Suppose the Sailor\ntable definition was modified to include a rnaiden-name column. However, only\nmarried women who take their husband's last name have a maiden name. For\nwomen who do not take their husband's name and for men, the nw'idcn-nmnc\ncolumn is inapphcable. Again, what value do we include in this column for the\nrow representing Dan?\nSQL provides H special column value called null to use in such situations. \"Ve\nuse null when the column value is either 'lJ,nknown or inapplicable. Using our\nSailor table definition, we might enter the row (98. Dan, null, 39) to represent\nDan. The presence of null values complicates rnany issues, and we consider the\nimpact of null values on SQL in this section.\n\nSQL: Q'lteT'leS, ConstT'aJnt.\"\nTrigger's\n5.6.1\nComparisons Using Null Values\nConsider a comparison such as rat'in,g = 8. If this is applied to the row for Dan,\nis this condition true or false'? Since Dan's rating is unknown, it is reasonable\nto say that this comparison should evaluate to the value unknown. In fact, this\nis the C::lse for the comparisons rating> 8 and rating < 8 &'3 well. Perhaps less\nobviously, if we compare two null values using <, >, =, and so on, the result is\nalways unknown. For example, if we have null in two distinct rows of the sailor\nrelation, any comparison returns unknown.\nSQL also provides a special comparison operator IS NULL to test whether a\ncolumn value is null; for example, we can say rating IS NULL, which would\nevaluate to true on the row representing Dan. We can also say rat'ing IS NOT\nNULL, which would evaluate to false on the row for Dan.\n5.6.2\nLogical Connectives AND, OR, and NOT\nNow, what about boolean expressions such as mting = 8 OR age < 40 and\nmting = 8 AND age < 40?\nConsidering the row for Dan again, because age\n< 40, the first expression evaluates to true regardless of the value of rating, but\nwhat about the second? We can only say unknown.\nBut this example raises an important\npoint~once we have null values, we\nmust define the logical operators AND, OR, and NOT using a three-val1LCd logic in\nwhich expressions evaluate to true, false, or unknown. We extend the usu1'11\ninterpretations of AND, OR, and NOT to cover the case when one of the arguments\nis unknown &., follows. The expression NOT unknown is defined to be unknown.\nOR of two arguments evaluates to true if either argument evaluates to true,\nand to unknown if one argument evaluates to false and the other evaluates to\nunknown. (If both arguments are false, of course, OR evaluates to false.) AND\nof two arguments evaluates to false if either argument evaluates to false, and\nto unknown if one argument evaluates to unknown and the other evaluates to\ntrue or unknown. (If both arguments are true, AND evaluates to true.)\n5.6.3\nImpact on SQL Constructs\nBoolean expressions arise in many contexts in SQI\", and the impact of nv,ll\nvalues must be recognized. H)r example, the qualification in the WHERE clause\neliminates rows (in the cross-product of tables named in the FROM clause) for\nwhich the qualification does not evaluate to true. Therefore, in the presence\nof null values, any row that evaluates to false or unknown is eliminated. Elim-\ninating rows that evaluate to unknown h&') a subtle but signifieant impaet on\nqueries, especially nested queries involving EXISTS or UNIQUE.",
          "pages": [
            195,
            196,
            197,
            198
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER~5\nAnother issue in the presence of 'null values is the definition of when two rows\nin a relation instance are regarded a.'3 duplicates. The SQL definition is that two\nrows are duplicates if corresponding columns are either equal, or both contain\nTrull. Contra..9t this definition with the fact that if we compare two null values\nusing =, the result is unknown! In the context of duplicates, this comparison is\nimplicitly treated as true, which is an anomaly.\nAs expected, the arithmetic operations +, -, *, and / all return Tmll if one of\ntheir arguments is null. However, nulls can cause some unexpected behavior\nwith aggregate operations. COUNT(*) handles 'null values just like other values;\nthat is, they get counted. All the other aggregate operations (COUNT, SUM, AVG,\nMIN, MAX, and variations using DISTINCT) simply discard null values--thus SUM\ncannot be understood as just the addition of all values in the (multi)set of\nvalues that it is applied to; a preliminary step of discarding all null values must\nalso be accounted for. As a special case, if one of these operators-other than\nCOUNT-is applied to only null values, the result is again null.\n5.6.4\nOuter Joins\nSome interesting variants of the join operation that rely on null values, called\nouter joins, are supported in SQL. Consider the join of two tables, say Sailors\nMe Reserves. Tuples of Sailors that do not match some row in Reserves accord-\ning to the join condition c do not appear in the result. In an outer join, on\nthe other hanel, Sailor rows without a matching Reserves row appear exactly\nonce in the result, with the result columns inherited from Reserves assigned\nnull values.\nIn fact, there are several variants of the outer join idea. In a left outer join,\nSailor rows without a matching Reserves row appear in the result, but not vice\nversa. In a right outer join, Reserves rows without a matching Sailors row\nappear in the result, but not vice versa.\nIn a full outer join, both Sailors\nand Reserves rows without a match appear in the result. (Of course, rows with\na match always appear in the result, for all these variants, just like the usual\njoins, sometimes called inner joins, presented in Chapter 4.)\nSQL allows the desired type of join to be specified in the FROM clause.\nFor\nexample, the following query lists (sid, b'id) pairs corresponding to sailors and\nboats they ha~e reserved:\nSELECT S.sid, R.bid\nFROM\nSailors S NATURAL LEFT OUTER JOIN Reserves R\nThe NATURAL keyword specifies that the join condition is equality on all common\nattributes (in this example, sid), and the WHERE clause is not required (unless\n\nHj5\nwe want to specify additional, non-join conditions). On the instances of Sailors\nand Reserves shown in Figure 5.6, this query computes the result shown in\nI sid I bid I\nnull\nLeft Outer Join of Sailo7\"1 and Rese1<Jesl\n5.6.5\nDisallowing Null Values\nWe can disallow null values by specifying NOT NULL as part of the field def-\ninition; for example, sname CHAR(20)\nNOT NULL. In addition, the fields in a\nprimary key are not allowed to take on null values. Thus, there is an implicit\nNOT NULL constraint for every field listed in a PRIMARY KEY constraint.\nOur coverage of null values is far from complete. The interested reader should\nconsult one of the many books devoted to SQL for a more detailed treatment\nof the topic.\n5.7\nCOMPLEX INTEGRITY CONSTRAINTS IN SQL\nIn this section we discuss the specification of complex integrity constraints that\nutilize the full power of SQL queries.\nThe features discussed in this section\ncomplement the integrity constraint features of SQL presented in Chapter 3.\n5.7.1\nConstraints over a Single Table\nWe can specify complex constraints over a single table using table constraints,\nwhich have the form CHECK conditional-expression. For example, to ensure that\nrating must be an integer in the range 1 to 10, we could use:\nCREATE TABLE Sailors ( sid\nINTEGER,\nsname CHAR(10),\nrating\nINTEGER,\nage\nREAL,\nPRIMARY KEY (sid),\nCHECK (rating >= 1 AND rating <= 10 ))\n\nCHAPTER*5\nTo enforce the constraint that Interlake boats cannot be reserved, we could use:\nCREATE TABLE Reserves (sid\nINTEGER,\nbid\nINTEGER,\nday\nDATE,\nFOREIGN KEY (sid) REFERENCES Sailors\nFOREIGN KEY (bid) REFERENCES Boats\nCONSTRAINT noInterlakeRes\nCHECK ( 'Interlake' <>\n( SELECT B.bname\nFROM\nBoats B\nWHERE\nB.bid = Reserves.bid )))\nWhen a row is inserted into Reserves or an existing row is modified, the condi-\ntional expression in the CHECK constraint is evaluated. If it evaluates to false,\nthe command is rejected.\n5.7.2\nDomain Constraints and Distinct Types\nA user can define a new domain using the CREATE DOMAIN statement, which\nuses CHECK constraints.\nCREATE DOMAIN ratingval INTEGER DEFAULT 1\nCHECK ( VALUE >= 1 AND VALUE <= 10 )\nINTEGER is the underlying, or source, type for the domain ratingval, and\nevery ratingval value must be of this type. Values in ratingval are further\nrestricted by using a CHECK constraint; in defining this constraint, we use the\nkeyword VALUE to refer to a value in the domain.\nBy using this facility, we\ncan constrain the values that belong to a domain using the full power of SQL\nqueries.\nOnce a domain is defined, the name of the domain can be used to\nrestrict column values in a table; we can use the following line in a schema\ndeclaration, for example:\nrating\nratingval\nThe optional DEFAULT keyword is used to associate a default value with a do-\nmain.\nIf the domain ratingval is used for a column in some relation and\nno value is entered for this column in an inserted tuple, the default value 1\nassociated with ratingval is used.\nSQL's support for the concept of a domain is limited in an important respect.\nFor example, we can define two domains called SailorId and BoatId, each\n\niii?\nSQL:1999 Distinct Types: :Many systems, e.g., Informix UDS and IBM\nDB2, already support this feature. With its introduction, we expect that\nthe support for domains will be deprecated, and eventually eliminated, in\nfuture versions of the SqL standard. It is really just one part of a broad\nset of object-oriented features in SQL:1999, which we discuss in Chapter\n23.\nusing INTEGER as the underlying type. The intent is to force a comparison of a\nSailorId value with a BoatId value to always fail (since they are drawn from\ndifferent domains); however, since they both have the same base type, INTEGER,\nthe comparison will succeed in SqL. This problem is addressed through the\nintroduction of distinct types in SqL:1999:\nCREATE TYPE ratingtype AS INTEGER\nThis statement defines a new distinct type called ratingtype, with INTEGER\nas its source type.\nValues of type ratingtype can be compared with each\nother, but they cannot be compared with values of other types. In particular,\nratingtype values are treated as being distinct from values of the source type,\nINTEGER--····we cannot compare them to integers or combine them with integers\n(e.g., add an integer to a ratingtype value). If we want to define operations\non the new type, for example, an average function, we must do so explicitly;\nnone of the existing operations on the source type carryover. We discuss how\nsuch functions can be defined in Section 23.4.1.\n5.7.3\nAssertions: ICs over Several Tables\nTable constraints are associated with a single table, although the conditional\nexpression in the CHECK clause can refer to other tables.\nTable constraints\nare required to hold only if the a,ssociated table is nonempty.\nThus, when\na constraint involves two or more tables, the table constraint mechanism is\nsometimes cumbersome and not quite what is desired. To cover such situations,\nSqL supports the creation of assertions, which are constraints not associated\nwith anyone table.\nAs an example, suppose that we wish to enforce the constraint that the number\nof boats plus the number of sailors should be less than 100.\n(This condition\nIllight be required, say, to qualify as a 'smaIl' sailing club.) We could try the\nfollowing table constraint:\nCREATE TABLE Sailors ( sid\nINTEGER,\nsname CHAR ( 10) ,\n\nCHAPTER$5\nrating\nINTEGER,\nage\nREAL,\nPRIMARY KEY (sid),\nCHECK ( rating >= 1 AND rating <= 10)\nCHECK ( ( SELECT COUNT (S.sid) FROM Sailors S )\n+ ( SELECT COUNT (B.bid) FROM Boats B )\n< 100 ))\nThis solution suffers from two drawbacks.\nIt is associated with Sailors, al-\nthough it involves Boats in a completely symmetric way.\nMore important,\nif the Sailors table is empty, this constraint is defined (as per the semantics\nof table constraints) to always hold, even if we have more than 100 rows in\nBoats!\nvVe could extend this constraint specification to check that Sailors is\nnonempty, but this approach becomes cumbersome.\nThe best solution is to\ncreate an assertion, as follows:\nCREATE ASSERTION smallClub\nCHECK (( SELECT COUNT (S.sid) FROM Sailors S )\n+ ( SELECT COUNT (B.bid) FROM Boats B)\n< 100 )\n5.8\nTRIGGERS AND ACTIVE DATABASES\nA trigger is a procedure that is automatically invoked by the DBMS in re-\nsponse to specified changes to the database, and is typically specified by the\nDBA. A database that has a set of associated triggers is called an active\ndatabase. A trigger description contains three parts:\n•\nEvent: A change to the database that activates the trigger.\n..\nCondition: A query or test that is run when the trigger is activated.\n..\nAction: A procedure that is executed when the trigger is activated and\nits condition is true.\nA trigger can be thought of as a 'daemon' that monitors a databa.se, and is exe-\ncuted when the database is modified in a way that matches the event specifica-\ntion. An insert, delete, or update statement could activate a trigger, regardless\nof which user or application invoked the activating statement; users may not\neven be aware that a trigger wa.'3 executed as a side effect of their program.\nA condition in a trigger can be a true/false statement (e.g., all employee salaries\nare less than $100,000) or a query. A query is interpreted as true if the answer\n\nSQL: Que'ries, Constrairds, TriggeTs\nIt>9\nset is nonempty and false if the query ha.') no answers. If the condition part\nevaluates to true, the action a.,sociated with the trigger is executed.\nA trigger action can examine the answers to\nth(~ query in the condition part\nof the trigger, refer to old and new values of tuples modified by the statement\nactivating the trigger, execute Hew queries, and make changes to the database.\nIn fact, an action can even execute a series of data-definition commands (e.g.,\ncreate new tables, change authorizations) and transaction-oriented commands\n(e.g., commit) or call host-language procedures.\nAn important issue is when the action part of a trigger executes in relation to\nthe statement that activated the trigger. For example, a statement that inserts\nrecords into the Students table may activate a trigger that is used to maintain\nstatistics on how many studen~s younger than 18 are inserted at a time by a\ntypical insert statement. Depending on exactly what the trigger does, we may\nwant its action to execute before changes are made to the Students table or\nafterwards: A trigger that initializes a variable used to count the nurnber of\nqualifying insertions should be executed before, and a trigger that executes once\nper qualifying inserted record and increments the variable should be executed\nafter each record is inserted (because we may want to examine the values in\nthe new record to determine the action).\n5.8.1\nExamples of Triggers in SQL\nThe examples shown in Figure 5.20, written using Oracle Server syntax for\ndefining triggers, illustrate the basic concepts behind triggers. (The SQL:1999\nsyntax for these triggers is similar; we will see an example using SQL:1999\nsyntax shortly.) The trigger called iniLcount initializes a counter variable be-\nfore every execution of an INSERT statement that adds tuples to the Students\nrelation. The trigger called incr_count increments the counter for each inserted\ntuple that satisfies the condition age < 18.\nOne of the example triggers in Figure 5.20 executes before the aetivating state-\nment, and the other example executes after it. A trigger can also be scheduled\nto execute instead of the activating statement; or in deferred fashion, at the\nend of the transaction containing the activating statement; or in asynchronous\nfashion, as part of a separate transaction.\nThe example in Figure 5.20 illustrates another point about trigger execution:\nA user must be able to specify whether a trigger is to be executed once per\nmodified record or once per activating statement. If the action depends on in-\ndividual changed records, for example, we have to examine the age field of the\ninserted Students record to decide whether to increment the count, the trigger-\n\nCHAPTER 5\nCREATE TRIGGER iniLeount BEFORE INSERT ON Students\n1* Event *1\nDECLARE\ncount INTEGER:\nBEGIN\n1* Action *I\ncount := 0:\nEND\nCREATE TRIGGER incLcount AFTER INSERT ON Students\n1* Event *1\nWHEN (new.age < 18)\n1* Condition; 'new' is just-inserted tuple *1\nFOR EACH ROW\nBEGIN\n1* Action; a procedure in Oracle's PL/SQL syntax *1\ncount := count + 1;\nEND\nExamples Illustrating Triggers\ning event should be defined to occur for each modified record; the FOR EACH\nROW clause is used to do this. Such a trigger is called a row-level trigger. On\nthe other hand, the iniLcount trigger is executed just once per INSERT state-\nment, regardless of the number of records inserted, because we have omitted\nthe FOR EACH ROW phrase. Such a trigger is called a statement-level trigger.\nIn Figure 5.20, the keyword new refers to the newly inserted tuple. If an existing\ntuple were modified, the keywords old and new could be used to refer to the\nvalues before and after the modification. SQL:1999 also allows the action part\nof a trigger to refer to the set of changed records, rather than just one changed\nrecord at a time. For example, it would be useful to be able to refer to the set\nof inserted Students records in a trigger that executes once after the INSERT\nstatement; we could count the number of inserted records with age < 18 through\nan SQL query over this set. Such a trigger is shown in Figure 5.21 and is an\naJternative to the triggers shown in Figure 5.20.\nThe definition in Figure 5.21 uses the syntax of SQL:1999, in order to illustrate\nthe similarities and differences with respect to the syntax used in a typical\ncurrent DBMS. The keyword clause NEW TABLE enables us to give a table name\n(InsertedTuples) to the set of newly inserted tuples. The FOR EACH STATEMENT\nclause specifies a statement-level trigger and can be omitted because it is the\ndefault. This definition does not have a WHEN clause; if such a clause is included,\nit follows the FOR EACH STATEMENT clause, just before the action specification.\nThe trigger is evaluated once for each SQL statement that inserts tuples into\nStudents, and inserts a single tuple into a table that contains statistics on mod-\n\nS(JL: (JneTie,s, Constraints, Triggers\nifications to database tables. The first two fields of the tuple contain constants\n(identifying the modified table, Students, and the kind of modifying statement,\nan INSERT), and the third field is the number of inserted Students tuples with\nage < 18. (The trigger in Figure 5.20 only computes the count; an additional\ntrigger is required to insert the appropriate tuple into the statistics table.)\nCREATE TRIGGER seLcount AFTER INSERT ON Students\nj* Event *j\nREFERENCING NEW TABLE AS InsertedTuples\nFOR EACH STATEMENT\nINSERT\nj* Action *j\nINTO StatisticsTable(ModifiedTable, ModificationType, Count)\nSELECT 'Students', 'Insert', COUNT *\nFROM InsertedTuples I\nWHERE 1.age < 18\nSet-Oriented Trigger\n5.9\nDESIGNING ACTIVE DATABASES\nTriggers offer a powerful mechanism for dealing with changes to a database,\nbut they must be used with caution. The effect of a collection of triggers can\nbe very complex, and maintaining an active database can become very difficult.\nOften, a judicious use of integrity constraints can replace the use of triggers.\n5.9.1\nWhy Triggers Can Be Hard to Understand\nIn an active database system, when the DBMS is about to execute a statement\nthat modifies the databa.se, it checks whether some trigger is activated by the\nstatement. If so, the DBMS processes the trigger by evaluating its condition\npart, and then (if the condition evaluates to true) executing its action part.\nIf a statement activates more than one trigger, the DBMS typically processes\nall of them, in senne arbitrary order. An important point is that the execution\nof the action part of a trigger could in turn activate another trigger. In par-\nticular, the execution of the action part of a trigger could a,gain activate the\nsarne trigger; such triggers \"u'e called recursive triggers. The potential for\nsuch chain activations and the unpredictable order in which a DBMS processes\nactivated triggers can make it difficult to understand the effect of a collection\nof triggers.\n\nCHAPTER'5\n5.9.2\nConstraints versus Triggers\nA common use of triggers is to maintain databa..'3e consistency, and in such\ncases, we should always consider whether using an integrity constraint (e.g., a\nforeign key constraint) achieves the same goals. The meaning of a constraint is\nnot defined operationally, unlike the effect of a trigger. This property makes a\nconstraint easier to understand, and also gives the DBMS more opportunities\nto optimize execution. A constraint also prevents the data from being made\ninconsistent by any kind of statement, whereas a trigger is activated by a specific\nkind of statement (INSERT, DELETE, or UPDATE). Again, this restriction makes\na constraint easier to understand.\nOn the other hand, triggers allow us to maintain database integrity in more\nflexible ways, as the following examples illustrate.\n•\nSuppose that we have a table called Orders with fields iternid, quantity,\ncustornerid, and unitprice.\nWhen a customer places an order, the first\nthree field values are filled in by the user (in this example, a sales clerk).\nThe fourth field's value can be obtained from a table called Items, but it\nis important to include it in the Orders table to have a complete record of\nthe order, in case the price of the item is subsequently changed. We can\ndefine a trigger to look up this value and include it in the fourth field of\na newly inserted record. In addition to reducing the number of fields that\nthe clerk h&'3 to type in, this trigger eliminates the possibility of an entry\nerror leading to an inconsistent price in the Orders table.\n•\nContinuing with this example, we may want to perform some additional\nactions when an order is received.\nFor example, if the purchase is being\ncharged to a credit line issued by the company, we may want to check\nwhether the total cost of the purch&'3e is within the current credit limit.\nWe can use a trigger to do the check; indeed, we can even use a CHECK\nconstraint. Using a trigger, however, allows us to implement more sophis-\nticated policies for dealing with purchases that exceed a credit limit. For\ninstance, we may allow purchases that exceed the limit by no more than\n10% if the customer has dealt with the company for at least a year, and\nadd the customer to a table of candidates for credit limit increases.\n5.9.3\nOther Uses of Triggers\n.l\\'Iany potential uses of triggers go beyond integrity maintenance. Triggers can\nalert users to unusual events (&'3 reflected in updates to the databa..<;e).\nFor\nexample, we may want to check whether a customer placing an order h&s made\nenough purchases in the past month to qualify for an additional discount; if\nso, the sales clerk must be informed so that he (or she) can tell the customer",
          "pages": [
            199,
            200,
            201,
            202,
            203,
            204,
            205,
            206,
            207
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "SQL: Q'UeT'Ze,S, CO'l/stmints, Tr'iggers\nand possibly generate additional sales! \\Ve can rela;y this information by using\na trigger that checks recent purcha.ses and prints a message if the customer\nqualifies for the discount.\nTriggers can generate a log of events to support auditing and security checks.\nFor example, each time a customer places an order, we can create a record with\nthe customer's ID and current credit limit and insert this record in a customer\nhistory table.\nSubsequent analysis of this table might suggest candidates for\nan increased credit limit (e.g., customers who have never failed to pay a bill on\ntime and who have come within 10% of their credit limit at least three times\nin the last month).\nAs the examples in Section 5.8 illustrate, we can use triggers to gather statistics\non table accesses and modifications. Some database systems even use triggers\ninternally as the basis for managing replicas of relations (Section 22.11.1). Our\nlist of potential uses of triggers is not exhaustive; for example, triggers have\nalso been considered for workflow management and enforcing business rules.\n5.10\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhat are the parts of a basic SQL query? Are the input and result tables\nof an SQL query sets or multisets? How can you obtain a set of tuples as\nthe result of a query? (Section 5.2)\n•\nWhat are range variables in SQL? How can you give names to output\ncolumns in a query that are defined by arithmetic or string expressions?\nWhat support does SQL offer for string pattern matching? (Section 5.2)\n•\nWhat operations does SQL provide over (multi)sets of tuples, and how\nwould you use these in writing queries? (Section 5.3)\n•\nvVhat are nested queries?\nWhat is correlation in nested queries?\nHow\nwould you use the operators IN, EXISTS, UNIQUE, ANY, and ALL in writing\nnested queries? Why are they useful? Illustrate your answer by showing\nhow to write the division operator in SQL. (Section 5.4)\n•\n\\Vhat aggregate operators does SQL support? (Section 5.5)\n•\n\\i\\That is gmvping? Is there a counterpart in relational algebra? Explain\nthis feature, and discllss the interaction of the HAVING and WHERE clauses.\nMention any restrictions that mllst be satisfied by the fields that appear in\nthe GROUP BY clause. (Section 5.5.1)\n\nCHAPTER?5\n•\n\\Vhat are null values?\nAre they supported in the relational model,\n&'3\ndescribed in Chapter 3'1 Hc)\\v do they affect the meaning of queries? Can\nprimary key fields of a table contain null values? (Section 5.6)\n•\nvVhat types of SQL constraints can be specified using the query language?\nCan you express primary key constraints using one of these new kinds\nof constraints? If so, why does SQL provide for a separate primary key\nconstraint syntax? (Section 5.7)\n•\nWhat is a trigger, and what are its three parts? vVhat are the differences\nbetween row-level and statement-level triggers? (Section 5.8)\n•\n\\Vhy can triggers be hard to understand? Explain the differences between\ntriggers and integrity constraints, and describe when you would use trig-\ngers over integrity constrains and vice versa. What are triggers used for?\n(Section 5.9)\nEXERCISES\nOnline material is available for all exercises in this chapter on the book's webpage at\nhttp://www.cs.wisc.edu/-dbbOok\nThis includes scripts to create tables for each exercise for use with Oracle, IBM DB2, Microsoft\nSQL Server, and MySQL.\nExercise 5.1 Consider the following relations:\nStudent(snum: integer, sname: string, major: string, level: string, age: integer)\nClass( name: string, meets_at: time, room: string, fid: integer)\nEnrolled(snum: integer, cname: string)\nFaculty (fid: integer, fnarne: string, deptid: integer)\nThe meaning of these relations is straightforward; for example, Enrolled has one record per\nstudent-class pair such that the student is enrolled in the class.\nWrite the following queries in SQL. No duplicates should be printed in any of the ans\\vers.\n1. Find the nari1es of all Juniors (level = JR) who are enrolled in a class taught by 1. Teach.\n2. Find the age of the oldest student who is either a History major or enrolled in a course\ntaught by I. Teach.\n:3. Find the names of all classes that either meet in room R128 or have five or more students\nenrolled.\n4. Find the Ilames of all students who are enrolled in two classes that meet at the same\ntime.\n\nSCJL: Queries, ConstrainLsl Triggers\n175t\n5. Find the names of faculty members \\vho teach in every room in which some class is\ntaught.\n6. Find the names of faculty members for \\vhorn the combined enrollment of the courses\nthat they teach is less than five.\n7. Print the level and the average age of students for that level, for each level.\n8. Print the level and the average age of students for that level, for all levels except JR.\n9. For each faculty member that has taught classes only in room R128, print the faculty\nmember's name and the total number of classes she or he has taught.\n10. Find the names of students enrolled in the maximum number of classes.\n11. Find the names of students not enrolled in any class.\n12. For each age value that appears in Students, find the level value that appears most often.\nFor example, if there are more FR level students aged 18 than SR, JR, or SO students\naged 18, you should print the pair (18, FR).\nExercise 5.2 Consider the following schema:\nSuppliers( sid: integer, sname: string, address: string)\nParts(pid: integer, pname: string, color: string)\nCatalog( sid: integer, pid: integer, cost: real)\nThe Catalog relation lists the prices charged for parts by Suppliers.\nWrite the following\nqueries in SQL:\n1. Find the pnames of parts for which there is some supplier.\n2. Find the snames of suppliers who supply every part.\n3. Find the snames of suppliers who supply every red part.\n4. Find the pnamcs of parts supplied by Acme Widget Suppliers and no one else.\n5. Find the sids of suppliers who charge more for some part than the average cost of that\npart (averaged over all the suppliers who supply that part).\n6. For each part, find the sname of the supplier who charges the most for that part.\n7. Find the sids of suppliers who supply only red parts.\n8. Find the sids of suppliers who supply a red part anel a green part.\n9. Find the sirl'i of suppliers who supply a red part or a green part.\n10. For every supplier that only supplies green parts, print the name of the supplier and the\ntotal number of parts that she supplies.\n11. For every supplier that supplies a green part and a reel part, print the name and price\nof the most expensive part that she supplies.\nExercise 5.3 The following relations keep track of airline flight information:\nFlights(.flno: integer, from: string, to: string, di8tance: integer,\nrlepa7'i:s: time,\na'T'l~ivcs: time,\nTn~ice: integer)\nAircraft( aid: integer, aname: string, cTllisingT'ange: integer)\nCertified( eid: integer, aid: integer)\nEmployees( eid: integer I ename: string, salary: integer)\n\nCHAPTE&5\nNote that the Employees relation describes pilots and other kinds of employees as well; every\npilot is certified for some aircraft, and only pilots are certified to fly.\nWrite each of the\nfollO\\ving queries in SQL. (Additional queries using the same schema are listed in the exereises\nfoT' Chapter 4·)\n1. Find the names of aircraft such that all pilots certified to operate them earn more than\n$80,000.\n2. For each pilot who is certified for more than three aircraft, find the eid and the maximum\ncruisingmnge of the aircraft for which she or he is certified.\n3. Find the names of pilots whose salary is less than the price of the cheapest route from\nLos Angeles to Honolulu.\n4. For all aircraft with cmisingmnge over 1000 miles, find the name of the aircraft and the\naverage salary of all pilots certified for this aircraft.\n5. Find the names of pilots certified for some Boeing aircraft.\n6. Find the aids of all aircraft that can be used on routes from Los Angeles to Chicago.\n7. Identify the routes that can be piloted by every pilot who makes more than $100,000.\n8. Print the enames of pilots who can operate planes with cruisingmnge greater than 3000\nmiles but are not certified on any Boeing aircraft.\n9. A customer wants to travel from Madison to New York with no more than two changes\nof flight. List the choice of departure times from Madison if the customer wants to arrive\nin New York by 6 p.m.\n10. Compute the difference between the average salary of a pilot and the average salary of\nall employees (including pilots).\n11. Print the name and salary of every nonpilot whose salary is more than the average salary\nfor pilots.\n12. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles.\n13. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles, but on at least two such aircrafts.\n14. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles and who are certified on some Boeing aircraft.\nExercise 5.4 Consider the following relational schema. An employee can work in more than\none department; the pcLtime field of the Works relation shows the percentage of time that a\ngiven employee works in a given department.\nEmp(eid: integer, ename: string, age: integer, salary: real)\nWorks(eid: integer, did: integer, pet_time: integer)\nDept(did.· integer, budget: real, managerid: integer)\nWrite the following queries in SQL:\n1. Print the names and ages of each employee who works in both the Hardware department\nand the Software department.\n2. For each department with more than 20 full-time-equivalent employees (i.e., where the\npart~time and full-time employees add up to at least that many full-time employees),\nprint the did together with the number of employees that work in that department.",
          "pages": [
            208,
            209,
            210,
            211
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Subqueries are queries nested within another query. They allow you to perform complex operations by breaking down a problem into smaller parts.",
        "explanation": "Subqueries are essential for performing more advanced data manipulation and analysis tasks. They can be used in the SELECT, FROM, WHERE, and HAVING clauses of SQL. Subqueries help simplify complex queries by breaking them down into manageable parts. For example, you might use a subquery to find the average age of sailors who are voting age (at least 18) for each rating level that has at least two such sailors.",
        "key_points": [
          "Key point 1: Subqueries can be used in various clauses like SELECT, FROM, WHERE, and HAVING.",
          "Key point 2: They help break down complex queries into simpler parts, making them easier to understand and maintain.",
          "Key point 3: Common mistakes include forgetting the correct placement of subqueries (e.g., inside a WHERE clause instead of a FROM clause), and not properly handling data types in comparisons.",
          "Key point 4: Best practice is to always test subqueries independently before using them in larger queries to ensure they return the expected results.",
          "Key point 5: Subqueries are closely related to aggregate functions like COUNT, AVG, MAX, MIN, and SUM."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Find the average age of sailors who are voting age for each rating level\nSELECT S.rating, AVG(S.age) AS avg_age FROM Sailors S WHERE S.age >= 18 GROUP BY S.rating;",
            "explanation": "This example demonstrates how to use a subquery in the WHERE clause to filter data before grouping."
          },
          {
            "title": "Practical Example",
            "code": "-- Find the average age of sailors who are voting age for each rating level that has at least two such sailors\nSELECT S.rating, AVG(S.age) AS avg_age FROM Sailors S WHERE S.age >= 18 GROUP BY S.rating HAVING COUNT(*) >= 2;",
            "explanation": "This practical example shows how to use a subquery in the HAVING clause to filter groups based on their size."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Incorrect placement of subqueries",
            "incorrect_code": "-- Incorrect placement SELECT S.rating, AVG(S.age) AS avg_age FROM Sailors S WHERE (SELECT COUNT(*) FROM Sailors S2 WHERE S2.rating = S.rating) >= 2;",
            "correct_code": "-- Correct placement\nSELECT S.rating, AVG(S.age) AS avg_age FROM Sailors S WHERE S.age >= 18 GROUP BY S.rating HAVING COUNT(*) >= 2;",
            "explanation": "This mistake occurs when a subquery is placed in the wrong clause. Subqueries should be used in SELECT, FROM, WHERE, or HAVING."
          },
          {
            "mistake": "Incorrect handling of data types",
            "incorrect_code": "-- Incorrect handling\nSELECT S.rating, AVG(S.age) AS avg_age FROM Sailors S WHERE S.age >= '18' GROUP BY S.rating;",
            "correct_code": "-- Correct handling\nSELECT S.rating, AVG(S.age) AS avg_age FROM Sailors S WHERE S.age >= 18 GROUP BY S.rating;",
            "explanation": "This mistake happens when data types are not handled correctly in comparisons. Always ensure that the comparison values match the column data type."
          }
        ],
        "practice": {
          "question": "Create a subquery to find the average age of sailors who are voting age for each rating level that has at least three such sailors.",
          "solution": "-- Solution\nSELECT S.rating, AVG(S.age) AS avg_age FROM Sailors S WHERE S.age >= 18 GROUP BY S.rating HAVING COUNT(*) >= 3;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER .;5\n(Q35) Find the average age of sailors 'Who aTe of voting age\n(i.e.~ at least 18\nyear8 old) for each 'rating level that has at least two sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating,",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "correlated-subquery": {
      "id": "correlated-subquery",
      "title": "Correlated Subqueries",
      "definition": "Subqueries that reference columns from the outer query",
      "difficulty": "advanced",
      "page_references": [
        205,
        206,
        207,
        208,
        209,
        210,
        211,
        212,
        213,
        214,
        215,
        216,
        217,
        218,
        219,
        220
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER 5\nCREATE TRIGGER iniLeount BEFORE INSERT ON Students\n1* Event *1\nDECLARE\ncount INTEGER:\nBEGIN\n1* Action *I\ncount := 0:\nEND\nCREATE TRIGGER incLcount AFTER INSERT ON Students\n1* Event *1\nWHEN (new.age < 18)\n1* Condition; 'new' is just-inserted tuple *1\nFOR EACH ROW\nBEGIN\n1* Action; a procedure in Oracle's PL/SQL syntax *1\ncount := count + 1;\nEND\nExamples Illustrating Triggers\ning event should be defined to occur for each modified record; the FOR EACH\nROW clause is used to do this. Such a trigger is called a row-level trigger. On\nthe other hand, the iniLcount trigger is executed just once per INSERT state-\nment, regardless of the number of records inserted, because we have omitted\nthe FOR EACH ROW phrase. Such a trigger is called a statement-level trigger.\nIn Figure 5.20, the keyword new refers to the newly inserted tuple. If an existing\ntuple were modified, the keywords old and new could be used to refer to the\nvalues before and after the modification. SQL:1999 also allows the action part\nof a trigger to refer to the set of changed records, rather than just one changed\nrecord at a time. For example, it would be useful to be able to refer to the set\nof inserted Students records in a trigger that executes once after the INSERT\nstatement; we could count the number of inserted records with age < 18 through\nan SQL query over this set. Such a trigger is shown in Figure 5.21 and is an\naJternative to the triggers shown in Figure 5.20.\nThe definition in Figure 5.21 uses the syntax of SQL:1999, in order to illustrate\nthe similarities and differences with respect to the syntax used in a typical\ncurrent DBMS. The keyword clause NEW TABLE enables us to give a table name\n(InsertedTuples) to the set of newly inserted tuples. The FOR EACH STATEMENT\nclause specifies a statement-level trigger and can be omitted because it is the\ndefault. This definition does not have a WHEN clause; if such a clause is included,\nit follows the FOR EACH STATEMENT clause, just before the action specification.\nThe trigger is evaluated once for each SQL statement that inserts tuples into\nStudents, and inserts a single tuple into a table that contains statistics on mod-\n\nS(JL: (JneTie,s, Constraints, Triggers\nifications to database tables. The first two fields of the tuple contain constants\n(identifying the modified table, Students, and the kind of modifying statement,\nan INSERT), and the third field is the number of inserted Students tuples with\nage < 18. (The trigger in Figure 5.20 only computes the count; an additional\ntrigger is required to insert the appropriate tuple into the statistics table.)\nCREATE TRIGGER seLcount AFTER INSERT ON Students\nj* Event *j\nREFERENCING NEW TABLE AS InsertedTuples\nFOR EACH STATEMENT\nINSERT\nj* Action *j\nINTO StatisticsTable(ModifiedTable, ModificationType, Count)\nSELECT 'Students', 'Insert', COUNT *\nFROM InsertedTuples I\nWHERE 1.age < 18\nSet-Oriented Trigger\n5.9\nDESIGNING ACTIVE DATABASES\nTriggers offer a powerful mechanism for dealing with changes to a database,\nbut they must be used with caution. The effect of a collection of triggers can\nbe very complex, and maintaining an active database can become very difficult.\nOften, a judicious use of integrity constraints can replace the use of triggers.\n5.9.1\nWhy Triggers Can Be Hard to Understand\nIn an active database system, when the DBMS is about to execute a statement\nthat modifies the databa.se, it checks whether some trigger is activated by the\nstatement. If so, the DBMS processes the trigger by evaluating its condition\npart, and then (if the condition evaluates to true) executing its action part.\nIf a statement activates more than one trigger, the DBMS typically processes\nall of them, in senne arbitrary order. An important point is that the execution\nof the action part of a trigger could in turn activate another trigger. In par-\nticular, the execution of the action part of a trigger could a,gain activate the\nsarne trigger; such triggers \"u'e called recursive triggers. The potential for\nsuch chain activations and the unpredictable order in which a DBMS processes\nactivated triggers can make it difficult to understand the effect of a collection\nof triggers.\n\nCHAPTER'5\n5.9.2\nConstraints versus Triggers\nA common use of triggers is to maintain databa..'3e consistency, and in such\ncases, we should always consider whether using an integrity constraint (e.g., a\nforeign key constraint) achieves the same goals. The meaning of a constraint is\nnot defined operationally, unlike the effect of a trigger. This property makes a\nconstraint easier to understand, and also gives the DBMS more opportunities\nto optimize execution. A constraint also prevents the data from being made\ninconsistent by any kind of statement, whereas a trigger is activated by a specific\nkind of statement (INSERT, DELETE, or UPDATE). Again, this restriction makes\na constraint easier to understand.\nOn the other hand, triggers allow us to maintain database integrity in more\nflexible ways, as the following examples illustrate.\n•\nSuppose that we have a table called Orders with fields iternid, quantity,\ncustornerid, and unitprice.\nWhen a customer places an order, the first\nthree field values are filled in by the user (in this example, a sales clerk).\nThe fourth field's value can be obtained from a table called Items, but it\nis important to include it in the Orders table to have a complete record of\nthe order, in case the price of the item is subsequently changed. We can\ndefine a trigger to look up this value and include it in the fourth field of\na newly inserted record. In addition to reducing the number of fields that\nthe clerk h&'3 to type in, this trigger eliminates the possibility of an entry\nerror leading to an inconsistent price in the Orders table.\n•\nContinuing with this example, we may want to perform some additional\nactions when an order is received.\nFor example, if the purchase is being\ncharged to a credit line issued by the company, we may want to check\nwhether the total cost of the purch&'3e is within the current credit limit.\nWe can use a trigger to do the check; indeed, we can even use a CHECK\nconstraint. Using a trigger, however, allows us to implement more sophis-\nticated policies for dealing with purchases that exceed a credit limit. For\ninstance, we may allow purchases that exceed the limit by no more than\n10% if the customer has dealt with the company for at least a year, and\nadd the customer to a table of candidates for credit limit increases.\n5.9.3\nOther Uses of Triggers\n.l\\'Iany potential uses of triggers go beyond integrity maintenance. Triggers can\nalert users to unusual events (&'3 reflected in updates to the databa..<;e).\nFor\nexample, we may want to check whether a customer placing an order h&s made\nenough purchases in the past month to qualify for an additional discount; if\nso, the sales clerk must be informed so that he (or she) can tell the customer\n\nSQL: Q'UeT'Ze,S, CO'l/stmints, Tr'iggers\nand possibly generate additional sales! \\Ve can rela;y this information by using\na trigger that checks recent purcha.ses and prints a message if the customer\nqualifies for the discount.\nTriggers can generate a log of events to support auditing and security checks.\nFor example, each time a customer places an order, we can create a record with\nthe customer's ID and current credit limit and insert this record in a customer\nhistory table.\nSubsequent analysis of this table might suggest candidates for\nan increased credit limit (e.g., customers who have never failed to pay a bill on\ntime and who have come within 10% of their credit limit at least three times\nin the last month).\nAs the examples in Section 5.8 illustrate, we can use triggers to gather statistics\non table accesses and modifications. Some database systems even use triggers\ninternally as the basis for managing replicas of relations (Section 22.11.1). Our\nlist of potential uses of triggers is not exhaustive; for example, triggers have\nalso been considered for workflow management and enforcing business rules.\n5.10\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhat are the parts of a basic SQL query? Are the input and result tables\nof an SQL query sets or multisets? How can you obtain a set of tuples as\nthe result of a query? (Section 5.2)\n•\nWhat are range variables in SQL? How can you give names to output\ncolumns in a query that are defined by arithmetic or string expressions?\nWhat support does SQL offer for string pattern matching? (Section 5.2)\n•\nWhat operations does SQL provide over (multi)sets of tuples, and how\nwould you use these in writing queries? (Section 5.3)\n•\nvVhat are nested queries?\nWhat is correlation in nested queries?\nHow\nwould you use the operators IN, EXISTS, UNIQUE, ANY, and ALL in writing\nnested queries? Why are they useful? Illustrate your answer by showing\nhow to write the division operator in SQL. (Section 5.4)\n•\n\\Vhat aggregate operators does SQL support? (Section 5.5)\n•\n\\i\\That is gmvping? Is there a counterpart in relational algebra? Explain\nthis feature, and discllss the interaction of the HAVING and WHERE clauses.\nMention any restrictions that mllst be satisfied by the fields that appear in\nthe GROUP BY clause. (Section 5.5.1)",
          "pages": [
            205,
            206,
            207,
            208
          ],
          "relevance": {
            "score": 0.56,
            "sql_score": 1.0,
            "concept_score": 0.71,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER?5\n•\n\\Vhat are null values?\nAre they supported in the relational model,\n&'3\ndescribed in Chapter 3'1 Hc)\\v do they affect the meaning of queries? Can\nprimary key fields of a table contain null values? (Section 5.6)\n•\nvVhat types of SQL constraints can be specified using the query language?\nCan you express primary key constraints using one of these new kinds\nof constraints? If so, why does SQL provide for a separate primary key\nconstraint syntax? (Section 5.7)\n•\nWhat is a trigger, and what are its three parts? vVhat are the differences\nbetween row-level and statement-level triggers? (Section 5.8)\n•\n\\Vhy can triggers be hard to understand? Explain the differences between\ntriggers and integrity constraints, and describe when you would use trig-\ngers over integrity constrains and vice versa. What are triggers used for?\n(Section 5.9)\nEXERCISES\nOnline material is available for all exercises in this chapter on the book's webpage at\nhttp://www.cs.wisc.edu/-dbbOok\nThis includes scripts to create tables for each exercise for use with Oracle, IBM DB2, Microsoft\nSQL Server, and MySQL.\nExercise 5.1 Consider the following relations:\nStudent(snum: integer, sname: string, major: string, level: string, age: integer)\nClass( name: string, meets_at: time, room: string, fid: integer)\nEnrolled(snum: integer, cname: string)\nFaculty (fid: integer, fnarne: string, deptid: integer)\nThe meaning of these relations is straightforward; for example, Enrolled has one record per\nstudent-class pair such that the student is enrolled in the class.\nWrite the following queries in SQL. No duplicates should be printed in any of the ans\\vers.\n1. Find the nari1es of all Juniors (level = JR) who are enrolled in a class taught by 1. Teach.\n2. Find the age of the oldest student who is either a History major or enrolled in a course\ntaught by I. Teach.\n:3. Find the names of all classes that either meet in room R128 or have five or more students\nenrolled.\n4. Find the Ilames of all students who are enrolled in two classes that meet at the same\ntime.\n\nSCJL: Queries, ConstrainLsl Triggers\n175t\n5. Find the names of faculty members \\vho teach in every room in which some class is\ntaught.\n6. Find the names of faculty members for \\vhorn the combined enrollment of the courses\nthat they teach is less than five.\n7. Print the level and the average age of students for that level, for each level.\n8. Print the level and the average age of students for that level, for all levels except JR.\n9. For each faculty member that has taught classes only in room R128, print the faculty\nmember's name and the total number of classes she or he has taught.\n10. Find the names of students enrolled in the maximum number of classes.\n11. Find the names of students not enrolled in any class.\n12. For each age value that appears in Students, find the level value that appears most often.\nFor example, if there are more FR level students aged 18 than SR, JR, or SO students\naged 18, you should print the pair (18, FR).\nExercise 5.2 Consider the following schema:\nSuppliers( sid: integer, sname: string, address: string)\nParts(pid: integer, pname: string, color: string)\nCatalog( sid: integer, pid: integer, cost: real)\nThe Catalog relation lists the prices charged for parts by Suppliers.\nWrite the following\nqueries in SQL:\n1. Find the pnames of parts for which there is some supplier.\n2. Find the snames of suppliers who supply every part.\n3. Find the snames of suppliers who supply every red part.\n4. Find the pnamcs of parts supplied by Acme Widget Suppliers and no one else.\n5. Find the sids of suppliers who charge more for some part than the average cost of that\npart (averaged over all the suppliers who supply that part).\n6. For each part, find the sname of the supplier who charges the most for that part.\n7. Find the sids of suppliers who supply only red parts.\n8. Find the sids of suppliers who supply a red part anel a green part.\n9. Find the sirl'i of suppliers who supply a red part or a green part.\n10. For every supplier that only supplies green parts, print the name of the supplier and the\ntotal number of parts that she supplies.\n11. For every supplier that supplies a green part and a reel part, print the name and price\nof the most expensive part that she supplies.\nExercise 5.3 The following relations keep track of airline flight information:\nFlights(.flno: integer, from: string, to: string, di8tance: integer,\nrlepa7'i:s: time,\na'T'l~ivcs: time,\nTn~ice: integer)\nAircraft( aid: integer, aname: string, cTllisingT'ange: integer)\nCertified( eid: integer, aid: integer)\nEmployees( eid: integer I ename: string, salary: integer)\n\nCHAPTE&5\nNote that the Employees relation describes pilots and other kinds of employees as well; every\npilot is certified for some aircraft, and only pilots are certified to fly.\nWrite each of the\nfollO\\ving queries in SQL. (Additional queries using the same schema are listed in the exereises\nfoT' Chapter 4·)\n1. Find the names of aircraft such that all pilots certified to operate them earn more than\n$80,000.\n2. For each pilot who is certified for more than three aircraft, find the eid and the maximum\ncruisingmnge of the aircraft for which she or he is certified.\n3. Find the names of pilots whose salary is less than the price of the cheapest route from\nLos Angeles to Honolulu.\n4. For all aircraft with cmisingmnge over 1000 miles, find the name of the aircraft and the\naverage salary of all pilots certified for this aircraft.\n5. Find the names of pilots certified for some Boeing aircraft.\n6. Find the aids of all aircraft that can be used on routes from Los Angeles to Chicago.\n7. Identify the routes that can be piloted by every pilot who makes more than $100,000.\n8. Print the enames of pilots who can operate planes with cruisingmnge greater than 3000\nmiles but are not certified on any Boeing aircraft.\n9. A customer wants to travel from Madison to New York with no more than two changes\nof flight. List the choice of departure times from Madison if the customer wants to arrive\nin New York by 6 p.m.\n10. Compute the difference between the average salary of a pilot and the average salary of\nall employees (including pilots).\n11. Print the name and salary of every nonpilot whose salary is more than the average salary\nfor pilots.\n12. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles.\n13. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles, but on at least two such aircrafts.\n14. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles and who are certified on some Boeing aircraft.\nExercise 5.4 Consider the following relational schema. An employee can work in more than\none department; the pcLtime field of the Works relation shows the percentage of time that a\ngiven employee works in a given department.\nEmp(eid: integer, ename: string, age: integer, salary: real)\nWorks(eid: integer, did: integer, pet_time: integer)\nDept(did.· integer, budget: real, managerid: integer)\nWrite the following queries in SQL:\n1. Print the names and ages of each employee who works in both the Hardware department\nand the Software department.\n2. For each department with more than 20 full-time-equivalent employees (i.e., where the\npart~time and full-time employees add up to at least that many full-time employees),\nprint the did together with the number of employees that work in that department.\n\nSQL: quehes, ConstTaint.s, Triggers\n,\n,\njones\n30.0\n~-\n- jonah\n56.0\nahab\n44.0\nmoby\n'mdl\n15.0\nI s'id I sname I mting , age I\nAn Instance of Sailors\n3. Print the name of each employee whose salary exceeds the budget of all of the depart-\nments that he or she works in.\n4. Find the managerids of managers who manage only departments with budgets greater\nthan $1 million.\n5. Find the enames of managers who manage the departments with the largest budgets.\n6. If a manager manages more than one department, he or she controls the sum of all the\nbudgets for those departments. Find the managerids of managers who control more than\n$5 million.\n7. Find the managerids of managers who control the largest amounts.\n8. Find the enames of managers who manage only departments with budgets larger than\n$1 million, but at least one department with budget less than $5 million.\nExercise 5.5 Consider the instance of the Sailors relation shown in Figure 5.22.\n1. Write SQL queries to compute the average rating, using AVGj the sum of the ratings,\nusing SUM; and the number of ratings, using COUNT.\n2. If you divide the sum just computed by the count, would the result be the same as the\naverage? How would your answer change if these steps were carried out with respect to\nthe age field instead of mting?\n~3. Consider the following query:\nFind the names of sailors with a higher rating than all\nsailors with age < 21.\nThe following two SQL queries attempt to obtain the answer\nto this question.\nDo they both compute the result? If not, explain why. Under what\nconditions would they compute the same result?\nS2.rating\nSailors S2\nS2.age < 21\nSELECT *\nFROM\nSailors S\nWHERE\nS.rating > ANY (SELECT\nFROM\n\\-/HERE\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS ( SELECT *\nFROM\nSailors S2\nWHERE\nS2.age < 21\nAND S.rating <= S2.rating )\n4. Consider the instance of Sailors shown in Figure 5.22. Let us define instance Sl of Sailors\nto consist of the first two tuples, instance S2 to be the last two tuples, and S to be the\ngiven instance.\n\nCHAPTER'5\nShow the left outer join of S with itself, with the join condition being 8'id=sid.\n(b) Show the right outer join of S ,vith itself, with the join condition being s'id=sid.\n(c) Show the full outer join of S with itself, with the join condition being S'id=sid.\n(d) Show the left outer join of Sl with S2, with the join condition being sid=sid.\n(e) Show the right outer join of Sl with S2, with the join condition being sid=sid.\n(f) Show the full outer join of 81 with S2, with the join condition being sid=sid.\nExercise 5.6 Answer the following questions:\n1. Explain the term 'impedance mismatch in the context of embedding SQL commands in a\nhost language such as C.\n2. How can the value of a host language variable be passed to an embedded SQL command?\n3. Explain the WHENEVER command's use in error and exception handling.\n4. Explain the need for cursors.\n5. Give an example of a situation that calls for the use of embedded SQL; that is, interactive\nuse of SQL commands is not enough, and some host lang;uage capabilities are needed.\n6. Write a C program with embedded SQL commands to address your example in the\nprevious answer.\n7. Write a C program with embedded SQL commands to find the standard deviation of\nsailors' ages.\n8. Extend the previous program to find all sailors whose age is within one standard deviation\nof the average age of all sailors.\n9. Explain how you would write a C program to compute the transitive closure of a graph,\nrepresented as an 8QL relation Edges(jrom, to), using embedded SQL commands. (You\nneed not write the program, just explain the main points to be dealt with.)\n10. Explain the following terms with respect to cursors: 'tlpdatability, sens,itivity, and scml-\nlability.\n11. Define a cursor on the Sailors relation that is updatable, scrollable, and returns answers\nsorted by age. Which fields of Sailors can such a cursor not update? Why?\n12. Give an example of a situation that calls for dynamic 8QL; that is, even embedded SQL\nis not sufficient.\nExercise 5.7 Consider the following relational schema and briefly answer the questions that\nfollow:\nEmp( eid: integer, cname: string, age: integer, salary: real)\n\\Vorks( eid: integer, did: integer, pet-time: integer)\nDept( did.' integer, budget:\nre~l, managerid: integer)\n1. Define a table constraint on Emp that will ensure that ever)' employee makes at leELst\n$10,000.\n2. Define a table constraint on Dept that will ensure that all managers have age> ;'W.\n:3. Define an assertion on Dept that will ensure that all managers have age> 30. Compare\nthis assertion with the equivalent table constraint. Explain which is better.\n\nSQL: (JwTies, Const7nint.s, Triggers\n4. vVrite SQL statements to delete all information about employees whose salaries exceed\nthat of the manager of one or more departments that they work in. Be sure to ensure\nthat all the relevant integrity constraints are satisfied after your updates.\nExercise 5.8 Consider the following relations:\nStudent (sn'llrn: integer, sname: string, rnajor: string,\nlevel: string, age: integer)\nClass(narne: string, rneets_at: time, roorn: string, fid: integer)\nEnrolled(snurn: integer, cnarne: string)\nFaculty(fid: integer, fnarne: string, deptid: integer)\nThe meaning of these relations is straightforward; for example, Enrolled has one record per\nstudent-class pair such that the student is enrolled in the class.\n1. Write the SQL statements required to create these relations, including appropriate ver-\nsions of all primary and foreign key integrity constraints.\n2. Express each of the following integrity constraints in SQL unless it is implied by the\nprimary and foreign key constraint; if so, explain how it is implied. If the constraint\ncannot be expressed in SQL, say so. For each constraint, state what operations (inserts,\ndeletes, and updates on specific relations) must be monitored to enforce the constraint.\n(a) Every class has a minimum enrollment of 5 students and a maximum enrollment\nof 30 students.\n(b) At least one dass meets in each room.\n(c) Every faculty member must teach at least two courses.\n(d) Only faculty in the department with deptid=33 teach more than three courses.\n(e) Every student must be enrolled in the course called lVlathlOl.\n(f) The room in which the earliest scheduled class (i.e., the class with the smallest\nnucets_at value) meets should not be the same as the room in which the latest\nscheduled class meets.\n(g) Two classes cannot meet in the same room at the same time.\n(h) The department with the most faculty members must have fewer than twice the\nnumber of faculty members in the department with the fewest faculty members.\n(i) No department can have more than 10 faculty members.\n(j) A student cannot add more than two courses at a time (i.e., in a single update).\n(k) The number of CS majors must be more than the number of Math majors.\n(I) The number of distinct courses in which CS majors are enrolled is greater than the\nnumber of distinct courses in which Math majors are enrolled.\n(rn) The total enrollment in courses taught by faculty in the department with deptid=SS\nis greater than the number of ivlath majors.\n(n) There lIlUst be at least one CS major if there are any students whatsoever.\n(0) Faculty members from different departments cannot teach in the same room.\nExercise 5.9 Discuss the strengths and weaknesses of the trigger mechanism.\nContrast\ntriggers with other integrity constraints supported by SQL.\n\nExercise 5.10 Consider the following relational schema.\nAn employee can work in more\nthan one department; the pel-time field of the \\Vorks relation shows the percentage of time\nthat a given employee works in a given department.\nEmp( eid: integer, ename: string, age: integer, salary: real)\n\\Vorks( eid: integer, did: integer, pcLtime: integer)\nDept(did: integer, budget: real, mana,gerid: integer)\n\\Vrite SQL-92 integrity constraints (domain, key, foreign key, or CHECK constraints; or asser··\nbons) or SQL:1999 triggers to ensure each of the following requirements, considered indepen-\ndently.\n1. Employees must make a minimum salary of $1000.\n2. Every manager must be also be an employee.\n3. The total percentage of aU appointments for an employee must be under 100%.\n4. A manager must always have a higher salary than any employee that he or she manages.\n5. Whenever an employee is given a raise, the manager's salary must be increased to be at\nleast as much.\n6. Whenever an employee is given a raise, the manager's salary must be increased to be\nat least as much.\nFurther, whenever an employee is given a raise, the department's\nbudget must be increased to be greater than the sum of salaries of aU employees in the\ndepartment.\nPROJECT-BASED EXERCISE\nExercise 5.11 Identify the subset of SQL queries that are supported in Minibase.\nBIBLIOGRAPHIC NOTES\nThe original version of SQL was developed as the query language for IBM's System R project,\nand its early development can be traced in [107, 151].\nSQL has since become the most\nwidely used relational query language, and its development is now subject to an international\nstandardization process.\nA very readable and comprehensive treatment of SQL-92 is presented by Melton and Simon\nin [524], and the central features of SQL:1999 are covered in [525]. We refer readers to these\ntwo books for an authoritative treatment of SQL. A short survey of the SQL:1999 standard\nis presented in [237].\nDate offers an insightful critique of SQL in [202].\nAlthough some of\nthe problems have been addressed in SQL-92 and later revisions, others remain.\nA formal\nsemantics for a large subset ofSQL queries is presented in [560]. SQL:1999 is the current Inter-\nnational Organization for Standardization (ISO) and American National Standards Institute\n(ANSI) standard. Melton is the editor of the ANSI and ISO SQL:1999 standard, document\nANSI/ISO/IEe 9075-:1999.\nThe corresponding ISO document is ISO/lEe 9075-:1999.\nA\nsuccessor, planned for 2003, builds on SQL:1999 SQL:200:3 is close to ratification (a.s of June\n20(2). Drafts of the SQL:200:3 deliberations are available at the following URL:\nftp://sqlstandards.org/SC32/",
          "pages": [
            209,
            210,
            211,
            212,
            213,
            214,
            215
          ],
          "relevance": {
            "score": 0.39,
            "sql_score": 1.0,
            "concept_score": 0.57,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "SqL: queries, COll./:!tTo/inLs, Triggers\nlSI\n[774] contains a collection of papers that cover the active database field.\n[794J includes a\ngood in-depth introduction to active rules, covering smnantics, applications and design issues.\n[251] discusses SQL extensions for specifying integrity constraint checks through triggers.\n[123] also discusses a procedural mechanism, called an alerter, for monitoring a database.\n[185] is a recent paper that suggests how triggers might be incorporated into SQL extensions.\nInfluential active database prototypes include Ariel [366], HiPAC [516J, ODE [18], Postgres\n[722], RDL [690], and Sentinel [36]. [147] compares various architectures for active database\nsystems.\n[32] considers conditions under which a collection of active rules has the same behavior,\nindependent of evaluation order. Semantics of active databases is also studied in [285] and\n[792]. Designing and managing complex rule systems is discussed in [60, 225]. [142] discusses\nrule management using Chimera, a data model and language for active database systems.\n\n\n\nPART II\nAPPLICATION DEVELOPMENT",
          "pages": [
            216,
            217,
            218,
            219
          ],
          "relevance": {
            "score": 0.26,
            "sql_score": 0.4,
            "concept_score": 0.29,
            "non_sql_penalty": 0.0,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Such a trigger is shown in \naJternative to the triggers shown in \nThe definition in \nthe similarities and differences with respect to the syntax used in a typical\ncurrent DBMS.",
        "explanation": "CHAPTER 5\nCREATE TRIGGER iniLeount BEFORE INSERT ON Students\n1* Event *1\nDECLARE\ncount INTEGER:\nBEGIN\n1* Action *I\ncount := 0:\nEND\nCREATE TRIGGER incLcount AFTER INSERT ON Students\n1* Event *1\nWHEN (new.age < 18)\n1* Condition; 'new' is just-inserted tuple *1\nFOR EACH ROW\nBEGIN\n1* Action; a procedure in Oracle's PL/SQL syntax *1\ncount := count + 1;\nEND\nExamples Illustrating Triggers\ning event should be defined to occur for each modified record; the FOR EACH\nROW clause is used to do this.\n\nSuch a trigger is called a row-level trigger. On\nthe other hand, the iniLcount trigger is executed just once per INSERT state-\nment, regardless of the number of records inserted, because we have omitted\nthe FOR EACH ROW phrase. Such a trigger is called a statement-level trigger. In \ntuple were modified, the keywords old and new could be used to refer to the\nvalues before and after the modification.\n\nSQL:1999 also allows the action part\nof a trigger to refer to the set of changed records, rather than just one changed\nrecord at a time. For example, it would be useful to be able to refer to the set\nof inserted Students records in a trigger that executes once after the INSERT\nstatement; we could count the number of inserted records with age < 18 through\nan SQL query over this set. Such a trigger is shown in \naJternative to the triggers shown in \nThe definition in \nthe similarities and differences with respect to the syntax used in a typical\ncurrent DBMS.",
        "key_points": [
          "Such a trigger is called a row-level trigger.",
          "On\nthe other hand, the iniLcount trigger is executed just once per INSERT state-\nment, regardless of the number of records inserted, because we have omitted\nthe FOR EACH ROW phrase.",
          "Such a trigger is called a statement-level trigger.",
          "In \ntuple were modified, the keywords old and new could be used to refer to the\nvalues before and after the modification.",
          "SQL:1999 also allows the action part\nof a trigger to refer to the set of changed records, rather than just one changed\nrecord at a time."
        ],
        "examples": [
          {
            "title": "SQL Example 1",
            "code": "CREATE TRIGGER iniLeount BEFORE INSERT ON Students 1* Event *1 DECLARE count INTEGER: BEGIN 1* Action *I count := 0: END CREATE TRIGGER incLcount AFTER INSERT ON Students 1* Event *1 WHEN (new.age < 18) 1* Condition;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 2",
            "code": "with age < 18 through an SQL query over this set. Such a trigger is shown in aJternative to the triggers shown in The definition in the similarities and differences with respect to the syntax used in a typical current DBMS. The keyword clause NEW TABLE enables us to give a table name (InsertedTuples) to the set of newly inserted tuples. The FOR EACH STATEMENT clause specifies a statement-level trigger and can be omitted because it is the default. This definition does not have a WHEN clause;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 3",
            "code": "with this example, we may want to perform some additional actions when an order is received. For example, if the purchase is being charged to a credit line issued by the company, we may want to check whether the total cost of the purch&'3e is within the current credit limit. We can use a trigger to do the check;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 4",
            "code": "with purchases that exceed a credit limit. For instance, we may allow purchases that exceed the limit by no more than 10% if the customer has dealt with the company for at least a year, and add the customer to a table of candidates for credit limit increases. 5.9.3 Other Uses of Triggers .l\\'Iany potential uses of triggers go beyond integrity maintenance. Triggers can alert users to unusual events (&'3 reflected in updates to the databa..<;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 5",
            "code": "create tables for each exercise for use with Oracle, IBM DB2, Microsoft SQL Server, and MySQL. Student(snum: integer, sname: string, major: string, level: string, age: integer) Class( name: string, meets_at: time, room: string, fid: integer) Enrolled(snum: integer, cname: string) Faculty (fid: integer, fnarne: string, deptid: integer) The meaning of these relations is straightforward;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 6",
            "code": "with cruising range longer than 1000 miles. 13. Print the names of employees who are certified only on aircrafts with cruising range longer than 1000 miles, but on at least two such aircrafts. 14. Print the names of employees who are certified only on aircrafts with cruising range longer than 1000 miles and who are certified on some Boeing aircraft. one department;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 7",
            "code": "with budgets larger than $1 million, but at least one department with budget less than $5 million. 1. Write SQL queries to compute the average rating, using AVGj the sum of the ratings, using SUM;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 8",
            "code": "with the join condition being sid=sid. (f) Show the full outer join of 81 with S2, with the join condition being sid=sid. 1. Explain the term 'impedance mismatch in the context of embedding SQL commands in a host language such as C. 2. How can the value of a host language variable be passed to an embedded SQL command? 3. Explain the WHENEVER command's use in error and exception handling. 4. Explain the need for cursors. 5. Give an example of a situation that calls for the use of embedded SQL;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 9",
            "code": "with respect to cursors: 'tlpdatability, sens,itivity, and scml- lability. 11. Define a cursor on the Sailors relation that is updatable, scrollable, and returns answers sorted by age. Which fields of Sailors can such a cursor not update? Why? 12. Give an example of a situation that calls for dynamic 8QL;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 10",
            "code": "create these relations, including appropriate ver- sions of all primary and foreign key integrity constraints. 2. Express each of the following integrity constraints in SQL unless it is implied by the primary and foreign key constraint;",
            "explanation": "Example SQL query"
          },
          {
            "title": "SQL Example 11",
            "code": "with deptid=SS is greater than the number of ivlath majors. (n) There lIlUst be at least one CS major if there are any students whatsoever. (0) Faculty members from different departments cannot teach in the same room. Contrast triggers with other integrity constraints supported by SQL. An employee can work in more than one department;",
            "explanation": "Example SQL query"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using Correlated Subqueries in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "CHAPTER 5\nCREATE TRIGGER iniLeount BEFORE INSERT ON Students\n1* Event *1\nDECLARE\ncount INTEGER:\nBEGIN\n1* Action *I\ncount := 0:\nEND\nCREATE TRIGGER incLcount AFTER INSERT ON Students\n1* Event *1\nWHEN (new.age < 18)\n1* Condition; 'new' is just-inserted tuple *1\nFOR EACH ROW\nBEGIN\n1* Action; a procedure in Oracle's PL/SQL syntax *1\ncount := count + 1;\nEND\nExamples Illustrating Triggers\ning event should be defined to occur for each modified record; the FOR EACH\nROW clause is used to do this. Such a t",
      "llm_error": "Invalid control character at: line 4 column 219 (char 445)",
      "content_relevance": {
        "score": 0.46,
        "sql_score": 1.0,
        "concept_score": 0.71,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "create-table": {
      "id": "create-table",
      "title": "CREATE TABLE",
      "definition": "Creating database tables with column definitions and constraints",
      "difficulty": "beginner",
      "page_references": [
        225,
        226,
        227,
        228,
        229,
        230,
        231,
        232,
        233,
        234,
        235,
        236,
        237,
        238,
        239,
        240
      ],
      "sections": {
        "definition": {
          "text": "CHAPTEl} 6\n..\n\\Ve usually need to open a cursor if the embedded statement is a SELECT\n(i.e.) a query).\nHowever, we can avoid opening a cursor if the answer\ncontains a single row, as we see shortly.\n..\nINSERT, DELETE, and UPDATE staternents typically require no cursor, al-\nthough some variants of DELETE and UPDATE use a cursor.\nAs an example, we can find the name and age of a sailor, specified by assigning\na value to the host variable\nc~sir1, declared earlier, as follows:\nEXEC SQL SELECT\nINTO\nFROM\nWHERE\nS.sname, S.age\n:c_sname, :c_age\nSailors S\nS.sid = :c_sid;\nThe INTO clause allows us to assign the columns of the single answer row to\nthe host variables csname and c_age. Therefore, we do not need a cursor to\nembed this query in a host language program. But what about the following\nquery, which computes the names and ages of all sailors with a rating greater\nthan the current value of the host variable cminmting?\nSELECT S.sname, S.age\nFROM\nSailors S\nWHERE\nS.rating > :c_minrating\nThis query returns a collection of rows, not just one row.\n'When executed\ninteractively, the answers are printed on the screen. If we embed this query in\na C program by prefixing the cOlnmand with EXEC SQL, how can the answers\nbe bound to host language variables? The INTO clause is inadequate because\nwe must deal with several rows. The solution is to use a cursor:\nDECLARE sinfo CURSOR FOR\nSELECT S.sname, S.age\nFROM\nSailors S\nWHERE\nS.rating > :c_minrating;\nThis code can be included in a C program, and once it is executed, the cursor\n8ir~lo is defined. Subsequently, we can open the cursor:\nOPEN sinfo:\nThe value of cminmting in the SQL query associated with the cursor is the\nvalue of this variable when we open the cursor.\n(The cursor declaration is\nprocessed at compile-time, and the OPEN command is executed at run-time.)\n\nDatabase Applicat'ion Development\n191,\nA cursor can be thought of as 'pointing' to a row in the collection of answers\nto the query associated with it. vVhen a cursor is opened, it is positioned just\nbefore the first row. \\Ve can use the FETCH command to read the first row of\ncursor sinfo into host language variables:\nFETCH sinfo INTO :csname, :cage;\nWhen the FETCH statement is executed, the cursor is positioned to point at\nthe next row (which is the first row in the table when FETCH is executed for\nthe first time after opening the cursor) and the column values in the row are\ncopied into the corresponding host variables.\nBy repeatedly executing this\nFETCH statement (say, in a while-loop in the C program), we can read all the\nrows computed by the query, one row at a time. Additional parameters to the\nFETCH command allow us to position a cursor in very flexible ways, but we do\nnot discuss them.\nHow do we know when we have looked at all the rows associated with the\ncursor? By looking at the special variables SQLCODE or SQLSTATE, of course.\nSQLSTATE, for example, is set to the value 02000, which denotes NO DATA, to\nindicate that there are no more rows if the FETCH statement positions the cursor\nafter the last row.\nWhen we are done with a cursor, we can close it:\nCLOSE sinfo;\nIt can be opened again if needed, and the value of : cminrating in the\nSQL query associated with the cursor would be the value of the host variable\ncminrating at that time.\nProperties of Cursors\nThe general form of a cursor declaration is:\nDECLARE cursomame [INSENSITIVE] [SCROLL] CURSOR\n[WITH HOLD]\nFOR some query\n[ ORDER BY order-item-list ]\n[ FOR READ ONLY I FOR UPDATE ]\nA cursor can be declared to be a read-only cursor (FOR READ ONLY) or, if\nit is a cursor on a base relation or an updatable view, to be an updatable\ncursor (FOR UPDATE). If it is IIpdatable, simple variants of the UPDATE and\n\nCHAPTER 6\nil'\nDELETE commands allow us to update or delete the row on which the cursor\nis positioned. For example, if sinfa is an updatable cursor and open, we can\nexecute the following statement:\nUPDATE Sailors S\nSET\nS.rating = S.rating ~ 1\nWHERE\nCURRENT of sinfo;\nThis Embedded SQL statement modifies the rating value of the row currently\npointed to by cursor sinfa; similarly, we can delete this row by executing the\nnext statement:\nDELETE Sailors S\nWHERE\nCURRENT of sinfo;\nA cursor is updatable by default unless it is a scrollable or insensitive cursor\n(see below), in which case it is read-only by default.\nIf the keyword SCROLL is specified, the cursor is scrollable, which means that\nvariants of the FETCH command can be used to position the cursor in very\nflexible ways; otherwise, only the basic FETCH command, which retrieves the\nnext row, is allowed.\nIf the keyword INSENSITIVE is specified, the cursor behaves as if it is ranging\nover a private copy of the collection of answer rows. Otherwise, and by default,\nother actions of some transaction could modify these rows, creating unpre-\ndictable behavior.\nFor example, while we are fetching rows using the sinfa\ncursor, we might modify rating values in Sailor rows by concurrently executing\nthe command:\nUPDATE Sailors S\nSET\nS.rating = S.rating -\nConsider a Sailor row such that (1) it has not yet been fetched, and (2) its\noriginal rating value would have met the condition in the WHERE clause of the\nquery associated with sinfa, but the new rating value does not.\nDo we fetch\nsuch a Sailor row'? If INSENSITIVE is specified, the behavior is as if all answers\nwere computed,and stored when sinfo was opened; thus, the update command\nhas no effect on the rows fetched by sinfa if it is executed after sinfo is opened.\nIf INSENSITIVE is not specified, the behavior is implementation dependent in\nthis situation.\nA holdable cursor is specified using the WITH HOLD clause, and is not closed\nwhen the transaction is conunitted. The motivation for this cornes from long\n\nDatabase Apphcation Development\ntransactions in which we access (and possibly change) a large number of rows of\na table. If the transaction is aborted for any reason, the system potentially has\nto redo a lot of work when the transaction is restarted. Even if the transaction\nis not aborted, its locks are held for a long time and reduce the concurrency\nof the system. The alternative is to break the transaction into several smaller\ntransactions, but remembering our position in the table between transactions\n(and other similar details) is complicated and error-prone.\nAllowing the ap-\nplication program to commit the transaction it initiated, while retaining its\nhandle on the active table (i.e., the cursor) solves this problem: The applica-\ntion can commit its transaction and start a new transaction and thereby save\nthe changes it has made thus far.\nFinally, in what order do FETCH commands retrieve rows? In general this order\nis unspecified, but the optional ORDER BY clause can be used to specify a sort\norder. Note that columns mentioned in the ORDER BY clause cannot be updated\nthrough the cursor!\nThe order-item-list is a list of order-items; an order-item is a column name,\noptionally followed by one of the keywords ASC or DESC. Every column men-\ntioned in the ORDER BY clause must also appear in the select-list of the query\nassociated with the cursor; otherwise it is not clear what columns we should\nsort on. The keywords ASC or DESC that follow a column control whether the\nresult should be sorted-with respect to that column-in ascending or descend-\ning order; the default is ASC. This clause is applied as the last step in evaluating\nthe query.\nConsider the query discussed in Section 5.5.1, and the answer shown in Figure\n5.13. Suppose that a cursor is opened on this query, with the clause:\nORDER BY minage ASC, rating DESC\nThe answer is sorted first in ascending order by minage, and if several rows\nhave the same minage value, these rows are sorted further in descending order\nby rating. The cursor would fetch the rows in the order shown in Figure 6.1.\nI rating I minage I\n25.5\n25.5\n35.0\nOrder in which 'fuples Are Fetched",
          "pages": [
            225,
            226,
            227,
            228
          ],
          "relevance": {
            "score": 0.3,
            "sql_score": 1.0,
            "concept_score": 0.2,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER c6\n6.1.3\nDynamic SQL\nConsider an application such as a spreadsheet or a graphical front-end that\nneeds to access data from a DBMS. Such an application must accept commands\nfrom a user and, based on what the user needs, generate appropriate SQL\nstatements to retrieve the necessary data. In such situations, we may not be\nable to predict in advance just what SQL statements need to be executed, even\nthough there is (presumably) some algorithm by which the application can\nconstruct the necessary SQL statements once a user's command is issued.\nSQL provides some facilities to deal with such situations; these are referred\nto as Dynamic SQL. We illustrate the two main commands, PREPARE and\nEXECUTE, through a simple example:\nchar c_sqlstring[] = {\"DELETE FROM Sailors WHERE rating>5\"};\nEXEC SQL PREPARE readytogo FROM :csqlstring;\nEXEC SQL EXECUTE readytogo;\nThe first statement declares the C variable c_sqlstring and initializes its value to\nthe string representation of an SQL command. The second statement results in\nthis string being parsed and compiled as an SQL command, with the resulting\nexecutable bound to the SQL variable readytogo.\n(Since readytogo is an SQL\nvariable, just like a cursor name, it is not prefixed by a colon.)\nThe third\nstatement executes the command.\nMany situations require the use of Dynamic SQL. However, note that the\npreparation of a Dynamic SQL command occurs at run-time and is run-time\noverhead.\nInteractive and Embedded SQL commands can be prepared once\nat compile-time and then re-executecl as often as desired.\nConsequently you\nshould limit the use of Dynamic SQL to situations in which it is essential.\nThere are many more things to know about Dynamic SQL~~~how we can pa'3S\nparameters from the host language program to the SQL statement being pre-\nparcel, for example--but we do not discuss it further.\n6.2\nAN INTRODUCTION TO JDBC\nEmbedded SQL enables the integration of SQL with a general-purpose pro-\ngramming language. As described in Section 6.1.1, a DBMS-specific preproces-\nsor transforms the Embedded SQL statements into function calls in the host\nlanguage.\nThe details of this translation vary across DBMSs, and therefore\neven though the source code can be cOlnpiled to work with different DBMSs,\nthe final executable works only with one specific DBMS.\n\nDatabase Application Develop'Tnent\n195,\nODBC and JDBC, short for Open DataBase Connectivity and Java DataBase\nConnectivity, also enable the integration of SQL with a general-purpose pro-\ngramming language. Both ODBC and JDBC expose database capabilities in\na standardized way to the application programmer through an application\nprogramming interface (API). In contrast to Embedded SQL, ODBC and\nJDBC allow a single executable to access different DBMSs 'Without recompi-\nlation. Thus, while Embedded SQL is DBMS-independent only at the source\ncode level, applications using ODBC or JDBC are DBMS-independent at the\nsource code level and at the level of the executable. In addition, using ODBC\nor JDBC, an application can access not just one DBMS but several different\nones simultaneously.\nODBC and JDBC achieve portability at the level of the executable by introduc-\ning an extra level of indirection. All direct interaction with a specific DBMS\nhappens through a DBMS-specific driver.\nA driver is a software program\nthat translates the ODBC or JDBC calls into DBMS-specific calls.\nDrivers\nare loaded dynamically on demand since the DBMSs the application is going\nto access are known only at run-time. Available drivers are registered with a\ndriver manager.\nOne interesting point to note is that a driver does not necessarily need to\ninteract with a DBMS that understands SQL. It is sufficient that the driver\ntranslates the SQL commands from the application into equivalent commands\nthat the DBMS understands. Therefore, in the remainder of this section, we\nrefer to a data storage subsystem with which a driver interacts as a data\nsource.\nAn application that interacts with a data source through ODBC or JDBC se-\nlects a data source, dynamically loads the corresponding driver, and establishes\na connection with the data source. There is no limit on the number of open\nconnections, and an application can have several open connections to different\ndata sources. Each connection has transaction semantics; that is, changes from\none connection are visible to other connections only after the connection has\ncommitted its changes. While a connection is opcn, transactions are executed\nby submitting SQL statements, retrieving results, processing errors, and finally\ncommitting or rolling back. The application disconnects from the data source\nto terminate the interaction.\nIn the remainder of this chapter, we concentrate on JDBC.\n\nCHAPTEij. 6\nr--\nI\nJDBC Drivers: The most up-to-date source of .IDBC drivers is the Sun\nJDBC Driver page at\nhttp://industry.java.sun.com/products/jdbc/drivers\nJDBC drivers are available for all major database sytems.\n6.2.1\nArchitecture\nThe architecture of JDBC has four main components:\nthe application, the\ndriver manager, several data source specific dr-iveTs, and the corresponding\ndata SOUTces.\nThe application initiates and terminates the connection with a data source.\nIt sets transaction boundaries, submits SQL statements, and retrieves the\nresults-----all through a well-defined interface as specified by the JDBC API. The\nprimary goal of the dr-iver manager is to load JDBC drivers and pass JDBC\nfunction calls from the application to the correct driver. The driver manager\nalso handles JDBC initialization and information calls from the applications\nand can log all function calls. In addition, the driver manager performs· some\nrudimentary error checking.\nThe dr-iver establishes the connection with the\ndata source. In addition to submitting requests and returning request results,\nthe driver translates data, error formats, and error codes from a form that is\nspecific to the data source into the JDBC standard. The data source processes\ncommands from the driver and returns the results.\nDepending on the relative location of the data source and the application,\nseveral architectural scenarios are possible. Drivers in JDBC are cla.ssified into\nfour types depending on the architectural relationship between the application\nand the data source:\nIII\nType I\nBridges:\nThis type of driver translates JDBC function calls\ninto function calls of another API that is not native to the DBMS. An\nexample is a JDBC-ODBC bridge; an application can use JDBC calls to\naccess an ODBC compliant data source. The application loads only one\ndriver, the bridge.\nBridges have the advantage that it is easy to piggy-\nback the applica.tion onto an existing installation, and no new drivers have\nto be installed. But using bridges hl:l.-'3 several drawbacks. The increased\nnumber of layers between data source and application affects performance.\nIn addition, the user is limited to the functionality that the ODBC driver\nsupports.\niii\nType II\nDirect Thanslation to the Native API via N on-Java\nDriver: This type of driver translates JDBC function calls directly into\nmethod invocations of the API of one specific data source. The driver is\n\nDatabase Application Develop'll1,ent\n}\nusually ,vritten using a combination of C++ and Java; it is dynamically\nlinked and specific to the data source. This architecture performs signif-\nicantly better than a JDBC-ODBC bridge. One disadvantage is that the\ndatabase driver that implements the API needs to be installed on each\ncomputer that runs the application.\nII\nType\nIII~~Network Bridges:\nThe driver talks over a network to a\nmiddleware server that translates the JDBC requests into DBMS-specific\nmethod invocations.\nIn this case, the driver on the client site (Le., the\nnetwork bridge) is not DBMS-specific. The JDBC driver loaded by the ap~\nplication can be quite small, as the only functionality it needs to implement\nis sending of SQL statements to the middleware server. The middleware\nserver can then use a Type II JDBC driver to connect to the data source.\nII\nType IV-Direct Translation to the Native API via Java Driver:\nInstead of calling the DBMS API directly, the driver communicates with\nthe DBMS through Java sockets. In this case, the driver on the client side is\nwritten in Java, but it is DBMS-specific. It translates JDBC calls into the\nnative API of the database system. This solution does not require an in-\ntermediate layer, and since the implementation is all Java, its performance\nis usually quite good.\n6.3\nJDBC CLASSES AND INTERFACES\nJDBC is a collection of Java classes and interfaces that enables database access\nfrom prograrl1s written in the Java language.\nIt contains methods for con-\nnecting to a remote data source, executing SQL statements, examining sets\nof results from SQL statements, transaction management, and exception han-\ndling. The cla.sses and interfaces are part of the java. sql package. Thus, all\ncode fragments in the remainder of this section should include the statement\nimport java. sql .* at the beginning of the code; we omit this statement in\nthe remainder of this section.\nJDBC 2.0 also includes the j avax. sql pack-\nage, the JDBC Optional Package.\nThe package j avax. sql adds, among\nother things, the capability of connection pooling and the Row-Set interface.\n\\\\Te discuss connection pooling in Section 6.3.2, and the ResultSet interface in\n\\\\Te now illustrate the individual steps that are required to submit a databa.se\nquery to a data source and to retrieve the results.\n6.3.1\nJDBC Driver Management\nIn .lDBe, data source drivers are managed by the Drivermanager class, which\nmaintains a list of all currently loaded drivers. The Drivermanager cla.ss has\n\nCHAPTEa 6\nmethods registerDriver, deregisterDriver, and getDrivers to enable dy-\nnamic addition and deletion of drivers.\nThe first step in connecting to a data source is to load the corresponding JDBC\ndriver.\nThis is accomplished by using the Java mechanism for dynamically\nloading classes. The static method forName in the Class class returns the Java\nclass as specified in the argument string and executes its static constructor.\nThe static constructor of the dynamically loaded class loads an instance of the\nDriver class, and this Driver object registers itself with the DriverManager\nclass.\nThe following Java example code explicitly loads a JDBC driver:\nClass.forName(\"oracle/jdbc.driver.OracleDriver\");\nThere are two other ways ofregistering a driver. We can include the driver with\n-Djdbc. drivers=oracle/jdbc. driver at the command line when we start the\nJava application. Alternatively, we can explicitly instantiate a driver, but this\nmethod is used only rarely, as the name of the driver has to be specified in the\napplication code, and thus the application becomes sensitive to changes at the\ndriver level.\nAfter registering the driver, we connect to the data source.\n6.3.2\nConnections\nA session with a data source is started through creation of a Connection object;\nA connection identifies a logical session with a data source; multiple connections\nwithin the same Java program can refer to different data sources or the same\ndata source. Connections are specified through a JDBC URL, a URL that\nuses the jdbc protocol. Such a URL has the form\njdbc:<subprotocol>:<otherParameters>\nThe code example shown in Figure 6.2 establishes a connection to an Oracle\ndatabase assuming that the strings userld and password are set to valid values.\nIn JDBC, connections can have different properties. For example, a connection\ncan specify the granularity of transactions.\nIf autocommit is set for a con-\nnection, then each SQL statement is considered to be its own transaction. If\nautocommit is off, then a series of statements that compose a transaction can\nbe committed using the commit 0 method of the Connection cla..<;s, or aborted\nusing the rollbackO method. The Connection cla.'ss has methods to set the\n\nDatabase Appl'ication Development\nString uri =\n..jdbc:oracle:www.bookstore.com:3083..\nConnection connection;\ntry {\nConnection connection =\nDriverManager.getConnection(urI,userId,password);\n}\ncatch(SQLException excpt) {\nSystem.out.println(excpt.getMessageO);\nreturn;\n}\nEstablishing a Connection with JDBC\n- ----~--._--_._---~,._-----~---_._-----,\nI\nJDBC Connections:\nRemember to close connections to data sources\nand return shared connections to the connection pool. Database systems\nhave a limited number of resources available for connections, and orphan\nconnections can often only be detected through time-outs-and while the\ndatabase system is waiting for the connection to time-out, the resources\nused by the orphan connection are wasted.\nautocommit mode (Connection. setAutoCommit) and to retrieve the current\nautocommit mode (getAutoCommit).\nThe following methods are part of the\nConnection interface and permit setting and getting other properties:\n•\npublic int getTransactionIsolation() throws SQLExceptionand\npublic void setTransactionlsolation(int 1) throws SQLException.\nThese two functions get and set the current level of isolation for transac-\ntions handled in the current connection.\nAll five SQL levels of isolation\n(see Section 16.6 for a full discussion) are possible, and argument 1can be\nset as follows:\nTRANSACTIONJNONE\nTRANSACTIONJREAD.UNCOMMITTED\nTRANSACTIONJREAD.COMMITTED\nTRANSACTIONJREPEATABLEJREAD\nTRANSACTION.BERIALIZABLE\n•\npublic boolean getReadOnlyO throws SQLException and\npublic void setReadOnly(boolean readOnly) throws SQLException.\nThese two functions allow the user to specify whether the transactions\nexecutecl through this connection are rcad only.\n\nCHAPTER ()\n..\npublic boolean isClosed() throws SQLException.\nChecks whether the current connection has already been closed.\n..\nsetAutoCommit and get AutoCommit.\nvVe already discussed these two functions.\nEstablishing a connection to a data source is a costly operation since it in-\nvolves several steps, such as establishing a network connection to the data\nsource, authentication, and allocation of resources such as memory. In case an\napplication establishes many different connections from different parties (such\nas a Web server), connections are often pooled to avoid this overhead. A con-\nnection pool is a set of established connections to a data source. Whenever a\nnew connection is needed, one of the connections from the pool is used, instead\nof creating a new connection to the data source.\nConnection pooling can be handled either by specialized code in the application,\nor the optional j avax. sql package, which provides functionality for connection\npooling and allows us to set different parameters, such as the capacity of the\npool, and shrinkage and growth rates.\nMost application servers (see Section\n7.7.2) implement the j avax .sql package or a proprietary variant.\n6.3.3\nExecuting SQL Statements\nWe now discuss how to create and execute SQL statements using JDBC. In the\nJDBC code examples in this section, we assume that we have a Connection\nobject named con. JDBC supports three different ways of executing statements:\nStatement, PreparedStatement, and CallableStatement.\nThe Statement\nclass is the base class for the other two statment classes. It allows us to query\nthe data source with any static or dynamically generated SQL query. We cover\nthe PreparedStatement class here and the CallableStatement class in Section\n6.5, when we discuss stored procedures.\nThe PreparedStatement cla,Cis dynamicaJly generates precompiled SQL state-\nments that can be used several times; these SQL statements can have param-\neters, but their structure is fixed when the PreparedStatement object (repre-\nsenting the SQL statement) is created.\nConsider the sample code using a PreparedStatment object shown in Figure\n6.3.\nThe SQL query specifies the query string, but uses ''1'\nfor the values\nof the parameters, which are set later using methods setString, setFloat,\nand setlnt. The ''1' placeholders can be used anywhere in SQL statements\nwhere they can be replaced with a value. Examples of places where they can\nappear include the WHERE clause (e.g., 'WHERE author=?'), or in SQL UPDATE\nand INSERT staternents, as in Figure 6.3. The method setString is one way",
          "pages": [
            229,
            230,
            231,
            232,
            233,
            234,
            235
          ],
          "relevance": {
            "score": 0,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.3,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Database Application Develop'ment\n/ / initial quantity is always zero\nString sql = \"INSERT INTO Books VALUES('?, 7, '?, ?, 0, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\n/ / now instantiate the parameters with values\n/ / a,ssume that isbn, title, etc. are Java variables that\n/ / contain the values to be inserted\npstmt.clearParameters();\npstmt.setString(l, isbn);\npstmt.setString(2, title);\npstmt.setString(3, author);\npstmt.setFloat(5, price);\npstmt.setInt(6, year);\nint numRows = pstmt.executeUpdate();\nSQL Update Using a PreparedStatement Object\nto set a parameter value; analogous methods are available for int, float,\nand date. It is good style to always use clearParameters 0 before setting\nparameter values in order to remove any old data.\nThere are different ways of submitting the query string to the data source. In\nthe example, we used the executeUpdate command, which is used if we know\nthat the SQL statement does not return any records (SQL UPDATE, INSERT,\nALTER, and DELETE statements). The executeUpdate method returns an inte-\nger indicating the number of rows the SQL statement modified; it returns 0 for\nsuccessful execution without modifying any rows.\nThe executeQuery method is used if the SQL statement returns data, such &\"l\nin a regular SELECT query. JDBC has its own cursor mechanism in the form\nof a ResultSet object, which we discuss next. The execute method is more\ngeneral than executeQuery and executeUpdate; the references at the end of\nthe chapter provide pointers with more details.\n6.3.4\nResul,tSets\nAs discussed in the previous section, the statement executeQuery returns a,\nResultSet object, which is similar to a cursor. ResultSet cursors in JDBC\n2.0 are very powerful; they allow forward and reverse scrolling and in-place\nediting and insertions.\n\nCHAPTER\nIn its most basic form, the ResultSet object allows us to read one row of the\noutput of the query at a time.\nInitially, the ResultSet is positioned before\nthe first row, and we have to retrieve the first row with an explicit call to the\nnext 0 method. The next method returns false if there are no more rows in\nthe query answer, and true other\\vise. The code fragment shown in Figure 6.4\nillustrates the basic usage of a ResultSet object.\nResultSet rs=stmt.executeQuery(sqlQuery);\n/ / rs is now a cursor\n/ / first call to rs.nextO moves to the first record\n/ / rs.nextO moves to the next row\nString sqlQuery;\nResultSet rs = stmt.executeQuery(sqlQuery)\nwhile (rs.next()) {\n/ / process the data\n}\nUsing a ResultSet Object\nWhile next () allows us to retrieve the logically next row in the query answer,\nwe can move about in the query answer in other ways too:\n•\nprevious 0 moves back one row.\n•\nabsolute (int num) moves to the row with the specified number.\n•\nrelative (int num) moves forward or backward (if num is negative) rela-\ntive to the current position. relative (-1) has the same effect as previous.\n•\nfirst 0 moves to the first row, and last 0\nmoves to the last row.\nMatching Java and SQL Data Types\nIn considering the interaction of an application with a data source, the issues\nwe encountered in the context of Embedded SQL (e.g., passing information\nbetween the application and the data source through shared variables) arise\nagain. To deal with such issues, JDBC provides special data types and speci-\nfies their relationship to corresponding SQL data types. Figure 6.5 shows the\naccessor methods in a ResultSet object for the most common SQL datatypes.\nWith these accessor methods, we can retrieve values from the current row of\nthe query result referenced by the ResultSet object. There are two forms for\neach accessor method: One method retrieves values by column index, starting\nat one, and the other retrieves values by column name. The following exam-\nple shows how to access fields of the current ResultSet row using accesssor\nmethods.\n\nDatabase Application Development\n2Q3\nI SQL Type I\nJava cla.c;;s\nI ResultSet get method I\nBIT\nBoolean\ngetBooleanO\nCHAR\nString\ngetStringO\nVARCHAR\nString\ngetStringO\nDOUBLE\nDouble\ngetDoubleO\nFLOAT\nDouble\ngetDoubleO\nINTEGER\nInteger\ngetIntO\nREAL\nDouble\ngetFloatO\nDATE\njava.sql.Date\ngetDateO\nTIME\njava.sql.Time\ngetTimeO\nTIMESTAMP\njava.sql.TimeStamp\ngetTimestamp ()\nReading SQL Datatypes from a ResultSet Object\nResultSet rs=stmt.executeQuery(sqIQuery);\nString sqlQuerYi\nResultSet rs = stmt.executeQuery(sqIQuery)\nwhile (rs.nextO) {\nisbn = rs.getString(l);\ntitle = rs.getString(\" TITLE\");\n/ / process isbn and title\n}\n6.3.5\nExceptions and Warnings\nSimilar to the SQLSTATE variable, most of the methods in java. sql can throw\nan exception of the type SQLException if an error occurs.\nThe information\nincludes SQLState, a string that describes the error (e.g., whether the statement\ncontained an SQL syntax error).\nIn addition to the standard getMessage 0\nmethod inherited from Throwable, SQLException has two additional methods\nthat provide further information, and a method to get (or chain) additional\nexceptions:\nIII\npublic String getSQLState 0 returns an SQLState identifier based on\nthe SQL:1999 specification, as discussed in Section 6.1.1.\n..\npublic i:p.t getErrorCode () retrieves a vendor-specific error code.\nIII\npublic SQLException getNextExceptionO gets the next exception in a\nchain of exceptions associated with the current SQLException object.\nAn SQL\\¥arning is a subclass of SQLException. Warnings are not H•.'3 severe as\nerrors and the program can usually proceed without special handling of warn-\nings. \\Varnings are not thrown like other exceptions, and they are not caught a.,",
          "pages": [
            236,
            237,
            238
          ],
          "relevance": {
            "score": 0.3,
            "sql_score": 1.0,
            "concept_score": 0.4,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "A cursor is like a pointer that allows you to iterate through rows returned by a query one at a time, rather than retrieving all rows at once.",
        "explanation": "Cursors are essential when dealing with queries that return multiple rows because they allow you to process each row individually. This is particularly useful in applications where you need to perform operations on each row or display them one by one. Here’s how it works step-by-step:\n1. **Declare a Cursor**: You define the cursor and specify the query that will be executed.\n2. **Open the Cursor**: The cursor is opened, which executes the associated query and positions it before the first row of results.\n3. **Fetch Rows**: Using the FETCH command, you can read each row into host language variables one by one.\n4. **Close the Cursor**: Once all rows are processed, you close the cursor to free up resources.",
        "key_points": [
          "Key point 1: Cursors allow processing of large result sets row-by-row, which is memory efficient.",
          "Key point 2: Use DECLARE to define the cursor and specify the query. Use OPEN to execute the query and position the cursor.",
          "Key point 3: Always check SQLCODE or SQLSTATE after a FETCH to determine if there are more rows or if you've reached the end of the result set.",
          "Key point 4: Close the cursor when done to release resources.",
          "Key point 5: Cursors can be scrollable, insensitive, and holdable based on your requirements."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Declare a cursor\nDECLARE sinfo CURSOR FOR SELECT S.sname, S.age FROM Sailors S WHERE S.rating > :c_minrating;\n-- Open the cursor\nOPEN sinfo;\n-- Fetch rows into host variables\nFETCH sinfo INTO :csname, :cage;",
            "explanation": "This example demonstrates how to declare a cursor for a query that returns multiple rows and fetch each row one by one."
          },
          {
            "title": "Practical Example",
            "code": "-- Real-world scenario: Fetching customer details FROM a database DECLARE custinfo CURSOR FOR SELECT C.cust_id, C.cust_name FROM Customers C WHERE C.balance > :c_minbalance; OPEN custinfo; FETCH custinfo INTO :cust_id, :cust_name;",
            "explanation": "This practical example shows how you might use a cursor in an application to fetch customer details based on a balance threshold.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to open the cursor before fetching rows.",
            "incorrect_code": "-- Incorrect FETCH sinfo INTO :csname, :cage;",
            "correct_code": "-- Correct OPEN sinfo; FETCH sinfo INTO :csname, :cage;",
            "explanation": "Always remember to open the cursor with OPEN before attempting to fetch rows. Failing to do so will result in an error."
          },
          {
            "mistake": "Not checking SQLCODE or SQLSTATE after a FETCH.",
            "incorrect_code": "-- Incorrect FETCH sinfo INTO :csname, :cage;",
            "correct_code": "-- Correct FETCH sinfo INTO :csname, :cage; IF SQLCODE = 0 THEN -- Continue processing rows END IF;",
            "explanation": "It's crucial to check if there are more rows after each fetch. Failing to do so can lead to infinite loops or accessing invalid data."
          }
        ],
        "practice": {
          "question": "Write a SQL script that declares a cursor, opens it, and fetches all rows from the Employees table where the department_id is greater than 50.",
          "solution": "-- Solution\nDECLARE deptinfo CURSOR FOR SELECT emp_id, emp_name FROM Employees WHERE department_id > :c_deptid;\nOPEN deptinfo;\nFETCH deptinfo INTO :emp_id, :emp_name;\nWHILE SQLCODE = 0 DO -- Process each row here\nFETCH deptinfo INTO :emp_id, :emp_name;\nEND WHILE;\nCLOSE deptinfo;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTEl} 6\n..\n\\Ve usually need to open a cursor if the embedded statement is a SELECT\n(i.e.) a query).\nHowever, we can avoid opening a cursor if the answer\ncontains a single row, as we see shortly.\n..",
      "content_relevance": {
        "score": 0.2,
        "sql_score": 1.0,
        "concept_score": 0.4,
        "non_sql_penalty": 0.3,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "data-types": {
      "id": "data-types",
      "title": "SQL Data Types",
      "definition": "Integer, decimal, character, date/time, and blob data types in SQL",
      "difficulty": "beginner",
      "page_references": [
        228,
        229,
        230,
        231,
        232,
        233,
        234,
        235,
        236,
        237,
        238,
        239,
        240,
        241,
        242,
        243
      ],
      "sections": {
        "definition": {
          "text": "Database Apphcation Development\ntransactions in which we access (and possibly change) a large number of rows of\na table. If the transaction is aborted for any reason, the system potentially has\nto redo a lot of work when the transaction is restarted. Even if the transaction\nis not aborted, its locks are held for a long time and reduce the concurrency\nof the system. The alternative is to break the transaction into several smaller\ntransactions, but remembering our position in the table between transactions\n(and other similar details) is complicated and error-prone.\nAllowing the ap-\nplication program to commit the transaction it initiated, while retaining its\nhandle on the active table (i.e., the cursor) solves this problem: The applica-\ntion can commit its transaction and start a new transaction and thereby save\nthe changes it has made thus far.\nFinally, in what order do FETCH commands retrieve rows? In general this order\nis unspecified, but the optional ORDER BY clause can be used to specify a sort\norder. Note that columns mentioned in the ORDER BY clause cannot be updated\nthrough the cursor!\nThe order-item-list is a list of order-items; an order-item is a column name,\noptionally followed by one of the keywords ASC or DESC. Every column men-\ntioned in the ORDER BY clause must also appear in the select-list of the query\nassociated with the cursor; otherwise it is not clear what columns we should\nsort on. The keywords ASC or DESC that follow a column control whether the\nresult should be sorted-with respect to that column-in ascending or descend-\ning order; the default is ASC. This clause is applied as the last step in evaluating\nthe query.\nConsider the query discussed in Section 5.5.1, and the answer shown in Figure\n5.13. Suppose that a cursor is opened on this query, with the clause:\nORDER BY minage ASC, rating DESC\nThe answer is sorted first in ascending order by minage, and if several rows\nhave the same minage value, these rows are sorted further in descending order\nby rating. The cursor would fetch the rows in the order shown in Figure 6.1.\nI rating I minage I\n25.5\n25.5\n35.0\nOrder in which 'fuples Are Fetched\n\nCHAPTER c6\n6.1.3\nDynamic SQL\nConsider an application such as a spreadsheet or a graphical front-end that\nneeds to access data from a DBMS. Such an application must accept commands\nfrom a user and, based on what the user needs, generate appropriate SQL\nstatements to retrieve the necessary data. In such situations, we may not be\nable to predict in advance just what SQL statements need to be executed, even\nthough there is (presumably) some algorithm by which the application can\nconstruct the necessary SQL statements once a user's command is issued.\nSQL provides some facilities to deal with such situations; these are referred\nto as Dynamic SQL. We illustrate the two main commands, PREPARE and\nEXECUTE, through a simple example:\nchar c_sqlstring[] = {\"DELETE FROM Sailors WHERE rating>5\"};\nEXEC SQL PREPARE readytogo FROM :csqlstring;\nEXEC SQL EXECUTE readytogo;\nThe first statement declares the C variable c_sqlstring and initializes its value to\nthe string representation of an SQL command. The second statement results in\nthis string being parsed and compiled as an SQL command, with the resulting\nexecutable bound to the SQL variable readytogo.\n(Since readytogo is an SQL\nvariable, just like a cursor name, it is not prefixed by a colon.)\nThe third\nstatement executes the command.\nMany situations require the use of Dynamic SQL. However, note that the\npreparation of a Dynamic SQL command occurs at run-time and is run-time\noverhead.\nInteractive and Embedded SQL commands can be prepared once\nat compile-time and then re-executecl as often as desired.\nConsequently you\nshould limit the use of Dynamic SQL to situations in which it is essential.\nThere are many more things to know about Dynamic SQL~~~how we can pa'3S\nparameters from the host language program to the SQL statement being pre-\nparcel, for example--but we do not discuss it further.\n6.2\nAN INTRODUCTION TO JDBC\nEmbedded SQL enables the integration of SQL with a general-purpose pro-\ngramming language. As described in Section 6.1.1, a DBMS-specific preproces-\nsor transforms the Embedded SQL statements into function calls in the host\nlanguage.\nThe details of this translation vary across DBMSs, and therefore\neven though the source code can be cOlnpiled to work with different DBMSs,\nthe final executable works only with one specific DBMS.\n\nDatabase Application Develop'Tnent\n195,\nODBC and JDBC, short for Open DataBase Connectivity and Java DataBase\nConnectivity, also enable the integration of SQL with a general-purpose pro-\ngramming language. Both ODBC and JDBC expose database capabilities in\na standardized way to the application programmer through an application\nprogramming interface (API). In contrast to Embedded SQL, ODBC and\nJDBC allow a single executable to access different DBMSs 'Without recompi-\nlation. Thus, while Embedded SQL is DBMS-independent only at the source\ncode level, applications using ODBC or JDBC are DBMS-independent at the\nsource code level and at the level of the executable. In addition, using ODBC\nor JDBC, an application can access not just one DBMS but several different\nones simultaneously.\nODBC and JDBC achieve portability at the level of the executable by introduc-\ning an extra level of indirection. All direct interaction with a specific DBMS\nhappens through a DBMS-specific driver.\nA driver is a software program\nthat translates the ODBC or JDBC calls into DBMS-specific calls.\nDrivers\nare loaded dynamically on demand since the DBMSs the application is going\nto access are known only at run-time. Available drivers are registered with a\ndriver manager.\nOne interesting point to note is that a driver does not necessarily need to\ninteract with a DBMS that understands SQL. It is sufficient that the driver\ntranslates the SQL commands from the application into equivalent commands\nthat the DBMS understands. Therefore, in the remainder of this section, we\nrefer to a data storage subsystem with which a driver interacts as a data\nsource.\nAn application that interacts with a data source through ODBC or JDBC se-\nlects a data source, dynamically loads the corresponding driver, and establishes\na connection with the data source. There is no limit on the number of open\nconnections, and an application can have several open connections to different\ndata sources. Each connection has transaction semantics; that is, changes from\none connection are visible to other connections only after the connection has\ncommitted its changes. While a connection is opcn, transactions are executed\nby submitting SQL statements, retrieving results, processing errors, and finally\ncommitting or rolling back. The application disconnects from the data source\nto terminate the interaction.\nIn the remainder of this chapter, we concentrate on JDBC.\n\nCHAPTEij. 6\nr--\nI\nJDBC Drivers: The most up-to-date source of .IDBC drivers is the Sun\nJDBC Driver page at\nhttp://industry.java.sun.com/products/jdbc/drivers\nJDBC drivers are available for all major database sytems.\n6.2.1\nArchitecture\nThe architecture of JDBC has four main components:\nthe application, the\ndriver manager, several data source specific dr-iveTs, and the corresponding\ndata SOUTces.\nThe application initiates and terminates the connection with a data source.\nIt sets transaction boundaries, submits SQL statements, and retrieves the\nresults-----all through a well-defined interface as specified by the JDBC API. The\nprimary goal of the dr-iver manager is to load JDBC drivers and pass JDBC\nfunction calls from the application to the correct driver. The driver manager\nalso handles JDBC initialization and information calls from the applications\nand can log all function calls. In addition, the driver manager performs· some\nrudimentary error checking.\nThe dr-iver establishes the connection with the\ndata source. In addition to submitting requests and returning request results,\nthe driver translates data, error formats, and error codes from a form that is\nspecific to the data source into the JDBC standard. The data source processes\ncommands from the driver and returns the results.\nDepending on the relative location of the data source and the application,\nseveral architectural scenarios are possible. Drivers in JDBC are cla.ssified into\nfour types depending on the architectural relationship between the application\nand the data source:\nIII\nType I\nBridges:\nThis type of driver translates JDBC function calls\ninto function calls of another API that is not native to the DBMS. An\nexample is a JDBC-ODBC bridge; an application can use JDBC calls to\naccess an ODBC compliant data source. The application loads only one\ndriver, the bridge.\nBridges have the advantage that it is easy to piggy-\nback the applica.tion onto an existing installation, and no new drivers have\nto be installed. But using bridges hl:l.-'3 several drawbacks. The increased\nnumber of layers between data source and application affects performance.\nIn addition, the user is limited to the functionality that the ODBC driver\nsupports.\niii\nType II\nDirect Thanslation to the Native API via N on-Java\nDriver: This type of driver translates JDBC function calls directly into\nmethod invocations of the API of one specific data source. The driver is",
          "pages": [
            228,
            229,
            230,
            231
          ],
          "relevance": {
            "score": 0.27,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Database Application Develop'll1,ent\n}\nusually ,vritten using a combination of C++ and Java; it is dynamically\nlinked and specific to the data source. This architecture performs signif-\nicantly better than a JDBC-ODBC bridge. One disadvantage is that the\ndatabase driver that implements the API needs to be installed on each\ncomputer that runs the application.\nII\nType\nIII~~Network Bridges:\nThe driver talks over a network to a\nmiddleware server that translates the JDBC requests into DBMS-specific\nmethod invocations.\nIn this case, the driver on the client site (Le., the\nnetwork bridge) is not DBMS-specific. The JDBC driver loaded by the ap~\nplication can be quite small, as the only functionality it needs to implement\nis sending of SQL statements to the middleware server. The middleware\nserver can then use a Type II JDBC driver to connect to the data source.\nII\nType IV-Direct Translation to the Native API via Java Driver:\nInstead of calling the DBMS API directly, the driver communicates with\nthe DBMS through Java sockets. In this case, the driver on the client side is\nwritten in Java, but it is DBMS-specific. It translates JDBC calls into the\nnative API of the database system. This solution does not require an in-\ntermediate layer, and since the implementation is all Java, its performance\nis usually quite good.\n6.3\nJDBC CLASSES AND INTERFACES\nJDBC is a collection of Java classes and interfaces that enables database access\nfrom prograrl1s written in the Java language.\nIt contains methods for con-\nnecting to a remote data source, executing SQL statements, examining sets\nof results from SQL statements, transaction management, and exception han-\ndling. The cla.sses and interfaces are part of the java. sql package. Thus, all\ncode fragments in the remainder of this section should include the statement\nimport java. sql .* at the beginning of the code; we omit this statement in\nthe remainder of this section.\nJDBC 2.0 also includes the j avax. sql pack-\nage, the JDBC Optional Package.\nThe package j avax. sql adds, among\nother things, the capability of connection pooling and the Row-Set interface.\n\\\\Te discuss connection pooling in Section 6.3.2, and the ResultSet interface in\n\\\\Te now illustrate the individual steps that are required to submit a databa.se\nquery to a data source and to retrieve the results.\n6.3.1\nJDBC Driver Management\nIn .lDBe, data source drivers are managed by the Drivermanager class, which\nmaintains a list of all currently loaded drivers. The Drivermanager cla.ss has\n\nCHAPTEa 6\nmethods registerDriver, deregisterDriver, and getDrivers to enable dy-\nnamic addition and deletion of drivers.\nThe first step in connecting to a data source is to load the corresponding JDBC\ndriver.\nThis is accomplished by using the Java mechanism for dynamically\nloading classes. The static method forName in the Class class returns the Java\nclass as specified in the argument string and executes its static constructor.\nThe static constructor of the dynamically loaded class loads an instance of the\nDriver class, and this Driver object registers itself with the DriverManager\nclass.\nThe following Java example code explicitly loads a JDBC driver:\nClass.forName(\"oracle/jdbc.driver.OracleDriver\");\nThere are two other ways ofregistering a driver. We can include the driver with\n-Djdbc. drivers=oracle/jdbc. driver at the command line when we start the\nJava application. Alternatively, we can explicitly instantiate a driver, but this\nmethod is used only rarely, as the name of the driver has to be specified in the\napplication code, and thus the application becomes sensitive to changes at the\ndriver level.\nAfter registering the driver, we connect to the data source.\n6.3.2\nConnections\nA session with a data source is started through creation of a Connection object;\nA connection identifies a logical session with a data source; multiple connections\nwithin the same Java program can refer to different data sources or the same\ndata source. Connections are specified through a JDBC URL, a URL that\nuses the jdbc protocol. Such a URL has the form\njdbc:<subprotocol>:<otherParameters>\nThe code example shown in Figure 6.2 establishes a connection to an Oracle\ndatabase assuming that the strings userld and password are set to valid values.\nIn JDBC, connections can have different properties. For example, a connection\ncan specify the granularity of transactions.\nIf autocommit is set for a con-\nnection, then each SQL statement is considered to be its own transaction. If\nautocommit is off, then a series of statements that compose a transaction can\nbe committed using the commit 0 method of the Connection cla..<;s, or aborted\nusing the rollbackO method. The Connection cla.'ss has methods to set the\n\nDatabase Appl'ication Development\nString uri =\n..jdbc:oracle:www.bookstore.com:3083..\nConnection connection;\ntry {\nConnection connection =\nDriverManager.getConnection(urI,userId,password);\n}\ncatch(SQLException excpt) {\nSystem.out.println(excpt.getMessageO);\nreturn;\n}\nEstablishing a Connection with JDBC\n- ----~--._--_._---~,._-----~---_._-----,\nI\nJDBC Connections:\nRemember to close connections to data sources\nand return shared connections to the connection pool. Database systems\nhave a limited number of resources available for connections, and orphan\nconnections can often only be detected through time-outs-and while the\ndatabase system is waiting for the connection to time-out, the resources\nused by the orphan connection are wasted.\nautocommit mode (Connection. setAutoCommit) and to retrieve the current\nautocommit mode (getAutoCommit).\nThe following methods are part of the\nConnection interface and permit setting and getting other properties:\n•\npublic int getTransactionIsolation() throws SQLExceptionand\npublic void setTransactionlsolation(int 1) throws SQLException.\nThese two functions get and set the current level of isolation for transac-\ntions handled in the current connection.\nAll five SQL levels of isolation\n(see Section 16.6 for a full discussion) are possible, and argument 1can be\nset as follows:\nTRANSACTIONJNONE\nTRANSACTIONJREAD.UNCOMMITTED\nTRANSACTIONJREAD.COMMITTED\nTRANSACTIONJREPEATABLEJREAD\nTRANSACTION.BERIALIZABLE\n•\npublic boolean getReadOnlyO throws SQLException and\npublic void setReadOnly(boolean readOnly) throws SQLException.\nThese two functions allow the user to specify whether the transactions\nexecutecl through this connection are rcad only.\n\nCHAPTER ()\n..\npublic boolean isClosed() throws SQLException.\nChecks whether the current connection has already been closed.\n..\nsetAutoCommit and get AutoCommit.\nvVe already discussed these two functions.\nEstablishing a connection to a data source is a costly operation since it in-\nvolves several steps, such as establishing a network connection to the data\nsource, authentication, and allocation of resources such as memory. In case an\napplication establishes many different connections from different parties (such\nas a Web server), connections are often pooled to avoid this overhead. A con-\nnection pool is a set of established connections to a data source. Whenever a\nnew connection is needed, one of the connections from the pool is used, instead\nof creating a new connection to the data source.\nConnection pooling can be handled either by specialized code in the application,\nor the optional j avax. sql package, which provides functionality for connection\npooling and allows us to set different parameters, such as the capacity of the\npool, and shrinkage and growth rates.\nMost application servers (see Section\n7.7.2) implement the j avax .sql package or a proprietary variant.\n6.3.3\nExecuting SQL Statements\nWe now discuss how to create and execute SQL statements using JDBC. In the\nJDBC code examples in this section, we assume that we have a Connection\nobject named con. JDBC supports three different ways of executing statements:\nStatement, PreparedStatement, and CallableStatement.\nThe Statement\nclass is the base class for the other two statment classes. It allows us to query\nthe data source with any static or dynamically generated SQL query. We cover\nthe PreparedStatement class here and the CallableStatement class in Section\n6.5, when we discuss stored procedures.\nThe PreparedStatement cla,Cis dynamicaJly generates precompiled SQL state-\nments that can be used several times; these SQL statements can have param-\neters, but their structure is fixed when the PreparedStatement object (repre-\nsenting the SQL statement) is created.\nConsider the sample code using a PreparedStatment object shown in Figure\n6.3.\nThe SQL query specifies the query string, but uses ''1'\nfor the values\nof the parameters, which are set later using methods setString, setFloat,\nand setlnt. The ''1' placeholders can be used anywhere in SQL statements\nwhere they can be replaced with a value. Examples of places where they can\nappear include the WHERE clause (e.g., 'WHERE author=?'), or in SQL UPDATE\nand INSERT staternents, as in Figure 6.3. The method setString is one way\n\nDatabase Application Develop'ment\n/ / initial quantity is always zero\nString sql = \"INSERT INTO Books VALUES('?, 7, '?, ?, 0, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\n/ / now instantiate the parameters with values\n/ / a,ssume that isbn, title, etc. are Java variables that\n/ / contain the values to be inserted\npstmt.clearParameters();\npstmt.setString(l, isbn);\npstmt.setString(2, title);\npstmt.setString(3, author);\npstmt.setFloat(5, price);\npstmt.setInt(6, year);\nint numRows = pstmt.executeUpdate();\nSQL Update Using a PreparedStatement Object\nto set a parameter value; analogous methods are available for int, float,\nand date. It is good style to always use clearParameters 0 before setting\nparameter values in order to remove any old data.\nThere are different ways of submitting the query string to the data source. In\nthe example, we used the executeUpdate command, which is used if we know\nthat the SQL statement does not return any records (SQL UPDATE, INSERT,\nALTER, and DELETE statements). The executeUpdate method returns an inte-\nger indicating the number of rows the SQL statement modified; it returns 0 for\nsuccessful execution without modifying any rows.\nThe executeQuery method is used if the SQL statement returns data, such &\"l\nin a regular SELECT query. JDBC has its own cursor mechanism in the form\nof a ResultSet object, which we discuss next. The execute method is more\ngeneral than executeQuery and executeUpdate; the references at the end of\nthe chapter provide pointers with more details.\n6.3.4\nResul,tSets\nAs discussed in the previous section, the statement executeQuery returns a,\nResultSet object, which is similar to a cursor. ResultSet cursors in JDBC\n2.0 are very powerful; they allow forward and reverse scrolling and in-place\nediting and insertions.\n\nCHAPTER\nIn its most basic form, the ResultSet object allows us to read one row of the\noutput of the query at a time.\nInitially, the ResultSet is positioned before\nthe first row, and we have to retrieve the first row with an explicit call to the\nnext 0 method. The next method returns false if there are no more rows in\nthe query answer, and true other\\vise. The code fragment shown in Figure 6.4\nillustrates the basic usage of a ResultSet object.\nResultSet rs=stmt.executeQuery(sqlQuery);\n/ / rs is now a cursor\n/ / first call to rs.nextO moves to the first record\n/ / rs.nextO moves to the next row\nString sqlQuery;\nResultSet rs = stmt.executeQuery(sqlQuery)\nwhile (rs.next()) {\n/ / process the data\n}\nUsing a ResultSet Object\nWhile next () allows us to retrieve the logically next row in the query answer,\nwe can move about in the query answer in other ways too:\n•\nprevious 0 moves back one row.\n•\nabsolute (int num) moves to the row with the specified number.\n•\nrelative (int num) moves forward or backward (if num is negative) rela-\ntive to the current position. relative (-1) has the same effect as previous.\n•\nfirst 0 moves to the first row, and last 0\nmoves to the last row.\nMatching Java and SQL Data Types\nIn considering the interaction of an application with a data source, the issues\nwe encountered in the context of Embedded SQL (e.g., passing information\nbetween the application and the data source through shared variables) arise\nagain. To deal with such issues, JDBC provides special data types and speci-\nfies their relationship to corresponding SQL data types. Figure 6.5 shows the\naccessor methods in a ResultSet object for the most common SQL datatypes.\nWith these accessor methods, we can retrieve values from the current row of\nthe query result referenced by the ResultSet object. There are two forms for\neach accessor method: One method retrieves values by column index, starting\nat one, and the other retrieves values by column name. The following exam-\nple shows how to access fields of the current ResultSet row using accesssor\nmethods.\n\nDatabase Application Development\n2Q3\nI SQL Type I\nJava cla.c;;s\nI ResultSet get method I\nBIT\nBoolean\ngetBooleanO\nCHAR\nString\ngetStringO\nVARCHAR\nString\ngetStringO\nDOUBLE\nDouble\ngetDoubleO\nFLOAT\nDouble\ngetDoubleO\nINTEGER\nInteger\ngetIntO\nREAL\nDouble\ngetFloatO\nDATE\njava.sql.Date\ngetDateO\nTIME\njava.sql.Time\ngetTimeO\nTIMESTAMP\njava.sql.TimeStamp\ngetTimestamp ()\nReading SQL Datatypes from a ResultSet Object\nResultSet rs=stmt.executeQuery(sqIQuery);\nString sqlQuerYi\nResultSet rs = stmt.executeQuery(sqIQuery)\nwhile (rs.nextO) {\nisbn = rs.getString(l);\ntitle = rs.getString(\" TITLE\");\n/ / process isbn and title\n}\n6.3.5\nExceptions and Warnings\nSimilar to the SQLSTATE variable, most of the methods in java. sql can throw\nan exception of the type SQLException if an error occurs.\nThe information\nincludes SQLState, a string that describes the error (e.g., whether the statement\ncontained an SQL syntax error).\nIn addition to the standard getMessage 0\nmethod inherited from Throwable, SQLException has two additional methods\nthat provide further information, and a method to get (or chain) additional\nexceptions:\nIII\npublic String getSQLState 0 returns an SQLState identifier based on\nthe SQL:1999 specification, as discussed in Section 6.1.1.\n..\npublic i:p.t getErrorCode () retrieves a vendor-specific error code.\nIII\npublic SQLException getNextExceptionO gets the next exception in a\nchain of exceptions associated with the current SQLException object.\nAn SQL\\¥arning is a subclass of SQLException. Warnings are not H•.'3 severe as\nerrors and the program can usually proceed without special handling of warn-\nings. \\Varnings are not thrown like other exceptions, and they are not caught a.,",
          "pages": [
            232,
            233,
            234,
            235,
            236,
            237,
            238
          ],
          "relevance": {
            "score": 0.42,
            "sql_score": 1.0,
            "concept_score": 0.83,
            "non_sql_penalty": 0.3,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER\npart of the try\"-catch block around a java. sql statement. VVe Heed to specif-\nically test whether warnings exist.\nConnection, Statement, and ResultSet\nobjects all have a getWarnings 0\nmethod with which we can retrieve SQL\nwarnings if they exist. Duplicate retrieval of warnings can be avoided through\nclearWarnings O. Statement objects clear warnings automatically on execu-\ntion of the next statement; ResultSet objects clear warnings every time a new\ntuple is accessed.\nTypical code for obtaining SQLWarnings looks similar to the code shown in\ntry {\nstmt = con.createStatement();\nwarning = con.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\nwarning = warning.getNextWarningO;\n/ /get next warning\n}\ncon.clear\\Varnings() ;\nstmt.executeUpdate( queryString );\nwarning = stmt.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\n/ /get next warning\n}\n} / / end try\ncatch ( SQLException SQLe) {\n/ / code to handle exception\n} / / end catch\nProcessing JDBC Warnings and Exceptions\n6.3.6\nExamining Database Metadata\n\\Ve can use tlw DatabaseMetaData object to obtain information about the\ndatabase system itself, as well as information frorn the database catalog. For\nexample, the following code fragment shows how to obtain the name and driver\nversion of the JDBC driver:\nDataba..seMetaData md = con.getMetaD<Lta():\nSystem.out.println(\"Driver Information:\");\n\nDatabase Appl'imtion Developrnent\nSystem.out.println(\"Name:\" + md.getDriverNameO\n+ \"; version:\" + mcl.getDriverVersion());\n~\nThe DatabaseMetaData object has many more methods (in JDBC 2.0, exactly\n134); we list some methods here:\n•\npublic ResultSet getCatalogs 0\nthrows SqLException. This function\nreturns a ResultSet that can be used to iterate over all the catalog relations.\nThe functions getIndexInfo 0 and getTables 0 work analogously.\n•\npUblic int getMaxConnections 0\nreturns the ma.ximum number of connections possible.\nWe will conclude our discussion of JDBC with an example code fragment that\nexamines all database metadata shown in Figure 6.7.\nDatabaseMetaData dmd = con.getMetaDataO;\nResultSet tablesRS = dmd.getTables(null,null,null,null);\nstring tableName;\nwhile(tablesRS.next()) {\ntableNarne = tablesRS.getString(\"TABLE_NAME\");\n/ / print out the attributes of this table\nSystem.out.println(\"The attributes of table\"\n+ tableName + \" are:\");\nResultSet columnsRS = dmd.getColums(null,null,tableName, null);\nwhile (columnsRS.next()) {\nSystem.out.print(colummsRS.getString(\" COLUMN_NAME\")\n+\" \");\n}\n/ / print out the primary keys of this table\nSystem.out.println(\"The keys of table\" + tableName + \" are:\");\nResultSet keysRS = dmd.getPrimaryKeys(null,null,tableName);\nwhile (keysRS. next()) {\n'System.out.print(keysRS.getStringC'COLUMN_NAME\") +\" \");\n}\n}\nObtaining Infon-nation about it Data Source\n\nCHAPTER.:6\n6.4\nSQLJ\nSQLJ (short for 'SQL-Java') was developed by the SQLJ Group, a group of\ndatabase vendors and Sun. SQLJ was developed to complement the dynamic\nway of creating queries in JDBC with a static model. It is therefore very close\nto Embedded SQL. Unlike JDBC, having semi-static SQL queries allows the\ncompiler to perform SQL syntax checks, strong type checks of the compatibil-\nity of the host variables with the respective SQL attributes, and consistency\nof the query with the database schema-tables, attributes, views, and stored\nprocedures--all at compilation time. For example, in both SQLJ and Embed-\nded SQL, variables in the host language always are bound statically to the\nsame arguments, whereas in JDBC, we need separate statements to bind each\nvariable to an argument and to retrieve the result. For example, the following\nSQLJ statement binds host language variables title, price, and author to the\nreturn values of the cursor books.\n#sql books = {\nSELECT title, price INTO :title, :price\nFROM Books WHERE author = :author\n};\nIn JDBC, we can dynamically decide which host language variables will hold\nthe query result. In the following example, we read the title of the book into\nvariable ftitle if the book was written by Feynman, and into variable otitle\notherwise:\n/ / assume we have a ResultSet cursor rs\nauthor = rs.getString(3);\nif (author==\"Feynman\") {\nftitle = rs.getString(2):\n}\nelse {\notitle = rs.getString(2);\n}\nvVhen writing SQLJ applications, we just write regular Java code and embed\nSQL statements according to a set of rules. SQLJ applications are pre-processed\nthrough an SQLJ translation program that replaces the embedded SQLJ code\nwith calls to an SQLJ Java library. The modified program code can then be\ncompiled by any Java compiler. Usually the SQLJ Java library makes calls to\na JDBC driver, which handles the connection to the datab&'3e system.",
          "pages": [
            239,
            240,
            241
          ],
          "relevance": {
            "score": 0.17,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.3,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "SQL Data Types are specific formats used to store data in a database table. Understanding these types is crucial for designing efficient and accurate databases.",
        "explanation": "SQL Data Types define how data is stored and managed within a database. Each type has specific characteristics that dictate its usage, storage requirements, and operations. For example, INT stores integers, VARCHAR stores variable-length strings, and DATE stores dates. Choosing the right data type ensures that your database operates efficiently and accurately.",
        "key_points": [
          "Key point 1: Common SQL Data Types include INT, VARCHAR, DATE, FLOAT, and BOOLEAN.",
          "Key point 2: Choosing the correct data type depends on the nature of the data being stored.",
          "Key point 3: Avoid using overly large or small data types to save space and improve performance.",
          "Key point 4: Always ensure that your data fits within the defined data type limits.",
          "Key point 5: Understanding data types is essential for database design and query optimization."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Define a TABLE with various data types CREATE TABLE example_table ( id INT, name VARCHAR(100), birth_date DATE, salary FLOAT, is_active BOOLEAN );",
            "explanation": "This example demonstrates how to define a table with different SQL Data Types. Each column is assigned a specific data type that suits the type of data it will store.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Inserting data into a TABLE with proper data types INSERT INTO example_table (id, name, birth_date, salary, is_active) VALUES (1, 'John Doe', '1985-06-23', 75000.00, TRUE);",
            "explanation": "This practical example shows how to insert data into a table using the correct SQL Data Types. Each value corresponds to its respective column's data type.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using an incorrect data type for a column",
            "incorrect_code": "-- Incorrect use of VARCHAR CREATE TABLE example_table ( id INT, name INT, -- Incorrect: name should be VARCHAR birth_date DATE, salary FLOAT, is_active BOOLEAN );",
            "correct_code": "-- Correct use of VARCHAR CREATE TABLE example_table ( id INT, name VARCHAR(100), birth_date DATE, salary FLOAT, is_active BOOLEAN );",
            "explanation": "This mistake occurs when a column's data type does not match the data being stored. Always ensure that the data type accurately reflects the data."
          }
        ],
        "practice": {
          "question": "Create a table named 'employees' with columns for id (INT), name (VARCHAR), hire_date (DATE), and salary (FLOAT). Insert a record into this table.",
          "solution": "-- Create the employees table\nCREATE TABLE employees (\n    id INT,\n    name VARCHAR(100),\n    hire_date DATE,\n    salary FLOAT\n);\n-- Insert data into the employees table\nINSERT INTO employees (id, name, hire_date, salary)\nVALUES (1, 'Jane Smith', '2015-07-14', 80000.00);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Database Apphcation Development\ntransactions in which we access (and possibly change) a large number of rows of\na table. If the transaction is aborted for any reason, the system potentially has\nto red",
      "content_relevance": {
        "score": 0.32,
        "sql_score": 1.0,
        "concept_score": 0.83,
        "non_sql_penalty": 0.4,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "constraints": {
      "id": "constraints",
      "title": "Integrity Constraints",
      "definition": "PRIMARY KEY, FOREIGN KEY, UNIQUE, CHECK, and NOT NULL constraints",
      "difficulty": "intermediate",
      "page_references": [
        235,
        236,
        237,
        238,
        239,
        240,
        241,
        242,
        243,
        244,
        245,
        246,
        247,
        248,
        249,
        250,
        251,
        252,
        253
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER ()\n..\npublic boolean isClosed() throws SQLException.\nChecks whether the current connection has already been closed.\n..\nsetAutoCommit and get AutoCommit.\nvVe already discussed these two functions.\nEstablishing a connection to a data source is a costly operation since it in-\nvolves several steps, such as establishing a network connection to the data\nsource, authentication, and allocation of resources such as memory. In case an\napplication establishes many different connections from different parties (such\nas a Web server), connections are often pooled to avoid this overhead. A con-\nnection pool is a set of established connections to a data source. Whenever a\nnew connection is needed, one of the connections from the pool is used, instead\nof creating a new connection to the data source.\nConnection pooling can be handled either by specialized code in the application,\nor the optional j avax. sql package, which provides functionality for connection\npooling and allows us to set different parameters, such as the capacity of the\npool, and shrinkage and growth rates.\nMost application servers (see Section\n7.7.2) implement the j avax .sql package or a proprietary variant.\n6.3.3\nExecuting SQL Statements\nWe now discuss how to create and execute SQL statements using JDBC. In the\nJDBC code examples in this section, we assume that we have a Connection\nobject named con. JDBC supports three different ways of executing statements:\nStatement, PreparedStatement, and CallableStatement.\nThe Statement\nclass is the base class for the other two statment classes. It allows us to query\nthe data source with any static or dynamically generated SQL query. We cover\nthe PreparedStatement class here and the CallableStatement class in Section\n6.5, when we discuss stored procedures.\nThe PreparedStatement cla,Cis dynamicaJly generates precompiled SQL state-\nments that can be used several times; these SQL statements can have param-\neters, but their structure is fixed when the PreparedStatement object (repre-\nsenting the SQL statement) is created.\nConsider the sample code using a PreparedStatment object shown in Figure\n6.3.\nThe SQL query specifies the query string, but uses ''1'\nfor the values\nof the parameters, which are set later using methods setString, setFloat,\nand setlnt. The ''1' placeholders can be used anywhere in SQL statements\nwhere they can be replaced with a value. Examples of places where they can\nappear include the WHERE clause (e.g., 'WHERE author=?'), or in SQL UPDATE\nand INSERT staternents, as in Figure 6.3. The method setString is one way\n\nDatabase Application Develop'ment\n/ / initial quantity is always zero\nString sql = \"INSERT INTO Books VALUES('?, 7, '?, ?, 0, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\n/ / now instantiate the parameters with values\n/ / a,ssume that isbn, title, etc. are Java variables that\n/ / contain the values to be inserted\npstmt.clearParameters();\npstmt.setString(l, isbn);\npstmt.setString(2, title);\npstmt.setString(3, author);\npstmt.setFloat(5, price);\npstmt.setInt(6, year);\nint numRows = pstmt.executeUpdate();\nSQL Update Using a PreparedStatement Object\nto set a parameter value; analogous methods are available for int, float,\nand date. It is good style to always use clearParameters 0 before setting\nparameter values in order to remove any old data.\nThere are different ways of submitting the query string to the data source. In\nthe example, we used the executeUpdate command, which is used if we know\nthat the SQL statement does not return any records (SQL UPDATE, INSERT,\nALTER, and DELETE statements). The executeUpdate method returns an inte-\nger indicating the number of rows the SQL statement modified; it returns 0 for\nsuccessful execution without modifying any rows.\nThe executeQuery method is used if the SQL statement returns data, such &\"l\nin a regular SELECT query. JDBC has its own cursor mechanism in the form\nof a ResultSet object, which we discuss next. The execute method is more\ngeneral than executeQuery and executeUpdate; the references at the end of\nthe chapter provide pointers with more details.\n6.3.4\nResul,tSets\nAs discussed in the previous section, the statement executeQuery returns a,\nResultSet object, which is similar to a cursor. ResultSet cursors in JDBC\n2.0 are very powerful; they allow forward and reverse scrolling and in-place\nediting and insertions.\n\nCHAPTER\nIn its most basic form, the ResultSet object allows us to read one row of the\noutput of the query at a time.\nInitially, the ResultSet is positioned before\nthe first row, and we have to retrieve the first row with an explicit call to the\nnext 0 method. The next method returns false if there are no more rows in\nthe query answer, and true other\\vise. The code fragment shown in Figure 6.4\nillustrates the basic usage of a ResultSet object.\nResultSet rs=stmt.executeQuery(sqlQuery);\n/ / rs is now a cursor\n/ / first call to rs.nextO moves to the first record\n/ / rs.nextO moves to the next row\nString sqlQuery;\nResultSet rs = stmt.executeQuery(sqlQuery)\nwhile (rs.next()) {\n/ / process the data\n}\nUsing a ResultSet Object\nWhile next () allows us to retrieve the logically next row in the query answer,\nwe can move about in the query answer in other ways too:\n•\nprevious 0 moves back one row.\n•\nabsolute (int num) moves to the row with the specified number.\n•\nrelative (int num) moves forward or backward (if num is negative) rela-\ntive to the current position. relative (-1) has the same effect as previous.\n•\nfirst 0 moves to the first row, and last 0\nmoves to the last row.\nMatching Java and SQL Data Types\nIn considering the interaction of an application with a data source, the issues\nwe encountered in the context of Embedded SQL (e.g., passing information\nbetween the application and the data source through shared variables) arise\nagain. To deal with such issues, JDBC provides special data types and speci-\nfies their relationship to corresponding SQL data types. Figure 6.5 shows the\naccessor methods in a ResultSet object for the most common SQL datatypes.\nWith these accessor methods, we can retrieve values from the current row of\nthe query result referenced by the ResultSet object. There are two forms for\neach accessor method: One method retrieves values by column index, starting\nat one, and the other retrieves values by column name. The following exam-\nple shows how to access fields of the current ResultSet row using accesssor\nmethods.\n\nDatabase Application Development\n2Q3\nI SQL Type I\nJava cla.c;;s\nI ResultSet get method I\nBIT\nBoolean\ngetBooleanO\nCHAR\nString\ngetStringO\nVARCHAR\nString\ngetStringO\nDOUBLE\nDouble\ngetDoubleO\nFLOAT\nDouble\ngetDoubleO\nINTEGER\nInteger\ngetIntO\nREAL\nDouble\ngetFloatO\nDATE\njava.sql.Date\ngetDateO\nTIME\njava.sql.Time\ngetTimeO\nTIMESTAMP\njava.sql.TimeStamp\ngetTimestamp ()\nReading SQL Datatypes from a ResultSet Object\nResultSet rs=stmt.executeQuery(sqIQuery);\nString sqlQuerYi\nResultSet rs = stmt.executeQuery(sqIQuery)\nwhile (rs.nextO) {\nisbn = rs.getString(l);\ntitle = rs.getString(\" TITLE\");\n/ / process isbn and title\n}\n6.3.5\nExceptions and Warnings\nSimilar to the SQLSTATE variable, most of the methods in java. sql can throw\nan exception of the type SQLException if an error occurs.\nThe information\nincludes SQLState, a string that describes the error (e.g., whether the statement\ncontained an SQL syntax error).\nIn addition to the standard getMessage 0\nmethod inherited from Throwable, SQLException has two additional methods\nthat provide further information, and a method to get (or chain) additional\nexceptions:\nIII\npublic String getSQLState 0 returns an SQLState identifier based on\nthe SQL:1999 specification, as discussed in Section 6.1.1.\n..\npublic i:p.t getErrorCode () retrieves a vendor-specific error code.\nIII\npublic SQLException getNextExceptionO gets the next exception in a\nchain of exceptions associated with the current SQLException object.\nAn SQL\\¥arning is a subclass of SQLException. Warnings are not H•.'3 severe as\nerrors and the program can usually proceed without special handling of warn-\nings. \\Varnings are not thrown like other exceptions, and they are not caught a.,\n\nCHAPTER\npart of the try\"-catch block around a java. sql statement. VVe Heed to specif-\nically test whether warnings exist.\nConnection, Statement, and ResultSet\nobjects all have a getWarnings 0\nmethod with which we can retrieve SQL\nwarnings if they exist. Duplicate retrieval of warnings can be avoided through\nclearWarnings O. Statement objects clear warnings automatically on execu-\ntion of the next statement; ResultSet objects clear warnings every time a new\ntuple is accessed.\nTypical code for obtaining SQLWarnings looks similar to the code shown in\ntry {\nstmt = con.createStatement();\nwarning = con.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\nwarning = warning.getNextWarningO;\n/ /get next warning\n}\ncon.clear\\Varnings() ;\nstmt.executeUpdate( queryString );\nwarning = stmt.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\n/ /get next warning\n}\n} / / end try\ncatch ( SQLException SQLe) {\n/ / code to handle exception\n} / / end catch\nProcessing JDBC Warnings and Exceptions\n6.3.6\nExamining Database Metadata\n\\Ve can use tlw DatabaseMetaData object to obtain information about the\ndatabase system itself, as well as information frorn the database catalog. For\nexample, the following code fragment shows how to obtain the name and driver\nversion of the JDBC driver:\nDataba..seMetaData md = con.getMetaD<Lta():\nSystem.out.println(\"Driver Information:\");",
          "pages": [
            235,
            236,
            237,
            238,
            239
          ],
          "relevance": {
            "score": 0.08,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.3,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Database Appl'imtion Developrnent\nSystem.out.println(\"Name:\" + md.getDriverNameO\n+ \"; version:\" + mcl.getDriverVersion());\n~\nThe DatabaseMetaData object has many more methods (in JDBC 2.0, exactly\n134); we list some methods here:\n•\npublic ResultSet getCatalogs 0\nthrows SqLException. This function\nreturns a ResultSet that can be used to iterate over all the catalog relations.\nThe functions getIndexInfo 0 and getTables 0 work analogously.\n•\npUblic int getMaxConnections 0\nreturns the ma.ximum number of connections possible.\nWe will conclude our discussion of JDBC with an example code fragment that\nexamines all database metadata shown in Figure 6.7.\nDatabaseMetaData dmd = con.getMetaDataO;\nResultSet tablesRS = dmd.getTables(null,null,null,null);\nstring tableName;\nwhile(tablesRS.next()) {\ntableNarne = tablesRS.getString(\"TABLE_NAME\");\n/ / print out the attributes of this table\nSystem.out.println(\"The attributes of table\"\n+ tableName + \" are:\");\nResultSet columnsRS = dmd.getColums(null,null,tableName, null);\nwhile (columnsRS.next()) {\nSystem.out.print(colummsRS.getString(\" COLUMN_NAME\")\n+\" \");\n}\n/ / print out the primary keys of this table\nSystem.out.println(\"The keys of table\" + tableName + \" are:\");\nResultSet keysRS = dmd.getPrimaryKeys(null,null,tableName);\nwhile (keysRS. next()) {\n'System.out.print(keysRS.getStringC'COLUMN_NAME\") +\" \");\n}\n}\nObtaining Infon-nation about it Data Source\n\nCHAPTER.:6\n6.4\nSQLJ\nSQLJ (short for 'SQL-Java') was developed by the SQLJ Group, a group of\ndatabase vendors and Sun. SQLJ was developed to complement the dynamic\nway of creating queries in JDBC with a static model. It is therefore very close\nto Embedded SQL. Unlike JDBC, having semi-static SQL queries allows the\ncompiler to perform SQL syntax checks, strong type checks of the compatibil-\nity of the host variables with the respective SQL attributes, and consistency\nof the query with the database schema-tables, attributes, views, and stored\nprocedures--all at compilation time. For example, in both SQLJ and Embed-\nded SQL, variables in the host language always are bound statically to the\nsame arguments, whereas in JDBC, we need separate statements to bind each\nvariable to an argument and to retrieve the result. For example, the following\nSQLJ statement binds host language variables title, price, and author to the\nreturn values of the cursor books.\n#sql books = {\nSELECT title, price INTO :title, :price\nFROM Books WHERE author = :author\n};\nIn JDBC, we can dynamically decide which host language variables will hold\nthe query result. In the following example, we read the title of the book into\nvariable ftitle if the book was written by Feynman, and into variable otitle\notherwise:\n/ / assume we have a ResultSet cursor rs\nauthor = rs.getString(3);\nif (author==\"Feynman\") {\nftitle = rs.getString(2):\n}\nelse {\notitle = rs.getString(2);\n}\nvVhen writing SQLJ applications, we just write regular Java code and embed\nSQL statements according to a set of rules. SQLJ applications are pre-processed\nthrough an SQLJ translation program that replaces the embedded SQLJ code\nwith calls to an SQLJ Java library. The modified program code can then be\ncompiled by any Java compiler. Usually the SQLJ Java library makes calls to\na JDBC driver, which handles the connection to the datab&'3e system.\n\nDatabase Application Development\n2Q7\nAn important philosophical difference exists between Embedded SQL and SQLJ\nand JDBC. Since vendors provide their own proprietary versions of SQL, it is\nadvisable to write SQL queries according to the SQL-92 or SQL:1999 standard.\nHowever, when using Embedded SQL, it is tempting to use vendor-specific SQL\nconstructs that offer functionality beyond the SQL-92 or SQL:1999 standards.\nSQLJ and JDBC force adherence to the standards, and the resulting code is\nmuch more portable across different database systems.\nIn the remainder of this section, we give a short introduction to SQLJ.\n6.4.1\nWriting SQLJ Code\nWe will introduce SQLJ by means of examples. Let us start with an SQLJ code\nfragment that selects records from the Books table that match a given author.\nString title; Float price; String atithor;\n#sql iterator Books (String title, Float price);\nBooks books;\n/ / the application sets the author\n/ / execute the query and open the cursor\n#sql books =\n{\nSELECT title, price INTO :titIe, :price\nFROM Books WHERE author = :author\n};\n/ / retrieve results\nwhile (books.next()) {\nSystem.out.println(books.titleO + \", \" + books.price());\n}\nbooks.close() ;\nThe corresponding JDBC code fragment looks as follows (assuming we also\ndeclared price, name, and author:\nPrcparcdStatcment stmt = connection.prepareStatement(\n\" SELECT title, price FROM Books WHERE author = ?\");\n/ / set the parameter in the query ancl execute it\nstmt.setString(1, author);\nResultSet 1'8 = stmt.executeQuery();\n/ / retrieve the results\nwhile (rs.next()) {\n\nCHAPTER\nSystem.out.println(rs.getString(l) + \", \" + rs.getFloat(2));\n}\nComparing the JDBC and SQLJ code, we see that the SQLJ code is much\neasier to read than the JDBC code. Thus, SQLJ reduces software development\nand maintenance costs.\nLet us consider the individual components of the SQLJ code in more detail.\nAll SQLJ statements have the special prefix #sql. In SQLJ, we retrieve the\nresults of SQL queries with iterator objects, which are basically cursors. An\niterator is an instance of an iterator class. Usage of an iterator in SQLJ goes\nthrough five steps:\n•\nDeclare the Iterator Class: In the preceding code, this happened through\nthe statement\n#sql iterator Books (String title, Float price);\nThis statement creates a new Java class that we can use to instantiate\nobjects.\n•\nInstantiate an Iterator Object from the New Iterator Class: We\ninstantiated our iterator in the statement Books books;.\n•\nInitialize the Iterator Using a SQL Statement: In our example, this\nhappens through the statement #sql books\n;;;;;; ....\n•\nIteratively, Read the Rows From the Iterator Object: This step is\nvery similar to reading rows through a ResultSet object in JDBC.\n•\nClose the Iterator Object.\nThere are two types of iterator classes: named iterators and positional iterators.\nFor named iterators, we specify both the variable type and the name of each\ncolumn of the iterator. This allows us to retrieve individual columns by name as\nin our previous example where we could retrieve the title colunm from the Books\ntable using the expression books. titIe (). For positional iterators, we need\nto specifY only the variable type for each column of the iterator.\nTo access\nthe individual columns of the iterator, we use a FETCH ...\nINTO eonstruct,\nsimilar to Embedded SQL. Both iterator types have the same performance;\nwhich iterator to use depends on the programmer's taste.\nLet us revisit our example.\n\\Ve can make the iterator a positional iterator\nthrough the following statement:\n#sql iterator Books (String, Float);\nvVe then retrieve the individual rows from the iterator 3,.'3 follows:\n\nDatabase Application Development\nwhile (true) {\n#sql { FETCH :books INTO :title, :price, };\nif (books.endFetch()) {\nbreak:\n}\n/ / process the book\n}\n6.5\nSTORED PROCEDURES\nIt is often important to execute some parts of the application logic directly in\nthe process space of the database system. Running application logic directly\nat the databa.se has the advantage that the amount of data that is transferred\nbetween the database server and the client issuing the SQL statement can be\nminimized, while at the same time utilizing the full power of the databa.se\nserver.\nWhen SQL statements are issued from a remote application, the records in the\nresult of the query need to be transferred from the database system back to\nthe application. If we use a cursor to remotely access the results of an SQL\nstatement, the DBMS has resources such as locks and memory tied up while the\napplication is processing the records retrieved through the cursor. In contrast,\na stored procedure is a program that is executed through a single SQL\nstatement that can be locally executed and completed within the process space\nof the database server.\nThe results can be packaged into one big result and\nreturned to the application, or the application logic can be performed directly\nat the server, without having to transmit the results to the client at alL\nStored procedures are also beneficial for software engineering rea,sons.\nOnce\na stored procedure is registered with the database server, different users can\nre-use the stored procedure, eliminating duplication of efforts in writing SQL\nqueries or application logic, and making code maintenance ea.\"lY. In addition,\napplication programmers do not need to know the the databa.se schema if we\nencapsulate all databa.'3e access into stored procedures.\nAlthough they,are called stored procedur'es, they do not have to be procedures\nin a programming language sense; they can be functions.\n6.5.1\nCreating a Simple Stored Procedure\nLet us look at the example stored procedure written in SQL shown in Figure\n(i.S.\nvVe see that stored procedures must have a name; this stored procedure\n\nCHAPTER' 6\nhas the name 'ShowNumberOfOrders.'\nOtherwise, it just contains an SQL\nstatement that is precompiled and stored at the server.\nCREATE PROCEDURE ShowNumberOfOrders\nSELECT C.cid, C.cname, COUNT(*)\nFROM\nCustomers C, Orders a\nWHERE\nC.cid = O.cid\nGROUP BY C.cid, C.cname\nA Stored Procedure in SQL\nStored procedures can also have parameters.\nThese parameters have to be\nvalid SQL types, and have one of three different modes: IN,\nOUT, or INOUT.\nIN parameters are arguments to' the stored procedure.\nOUT parameters are\nreturned from the stored procedure; it assigns values to all OUT parameters\nthat the user can process. INOUT parameters combine the properties of IN and\nOUT parameters: They contain values to be passed to the stored procedures, and\nthe stored procedure can set their values as return values. Stored procedures\nenforce strict type conformance: If a parameter is of type INTEGER, it cannot\nbe called with an argument of type VARCHAR.\nLet us look at an example of a stored procedure with arguments. The stored\nprocedure shown in Figure 6.9 has two arguments: book_isbn and addedQty.\nIt updates the available number of copies of a book with the quantity from a\nnew shipment.\nCREATE PROCEDURE Addlnventory (\nIN book_isbn CHAR(lO),\nIN addedQty INTEGER)\nUPDATE Books\nSET\nWHERE\nqty_in_stock = qtyjn_stock + addedQty\nbookjsbn = isbn\nA Stored Procedure with Arguments\nStored procedures do not have to be written in SQL; they can be written in any\nhost language. As an example, the stored procedure shown in Figure 0.10 is a\nJava function that is dynamically executed by the databa..<;e server whenever it\nis called by the dient:\n6.5.2\nCalling Stored Procedures\nStored procedures can be called in interactive SQL with the CALL statement:\n\nDatabase Application Development\nCREATE PROCEDURE RallkCustomers(IN number INTEGER)\nLANGUAGE Java\nEXTERNAL NAME 'file:// /c:/storedProcedures/rank.jar'\nA Stored Procedure in Java\nCALL storedProcedureName(argumentl, argument2, ... , argumentN);\nIn Embedded SQL, the arguments to a stored procedure are usually variables\nin the host language. For example, the stored procedure AddInventory would\nbe called as follows:\nEXEC SQL BEGIN DECLARE SECTION\nchar isbn[lO];\nlong qty;\nEXEC SQL END DECLARE SECTION\n/ / set isbn and qty to some values\nEXEC SQL CALL AddInventory(:isbn,:qty);\nCalling Stored Procedures from JDBC\nWe can call stored procedures from JDBC using the CallableStatment class.\nCallableStatement is a subclass of PreparedStatement and provides the same\nfunctionality. A stored procedure could contain multiple SQL staternents or a\nseries of SQL statements-thus, the result could be many different ResultSet\nobjects.\nWe illustrate the case when the stored procedure result is a single\nResultSet.\nCallableStatement cstmt=\nCOIl.prepareCall(\" {call ShowNumberOfOrders}\");\nResultSet rs = cstmt.executeQueryO\nwhile (rs.next())\nCalling Stored Procedures from SQLJ\nThe stored procedure 'ShowNumberOfOrders' is called as follows using SQLJ:\n/ / create the cursor class\n#sql !terator CustomerInfo(int cid, String cname, int count);\n/ / create the cursor\n\nCustomerInfo customerinfo;\n/ / call the stored procedure\n#sql customerinfo = {CALL ShowNumberOfOrders};\nwhile (customerinfo.nextO) {\nSystem.out.println(customerinfo.cid() + \",\" +\ncustomerinfo.count()) ;\n}\n6.5.3\nSQLIPSM\nCHAPTER (5\nAll major databa...<;e systems provide ways for users to write stored procedures in\na simple, general purpose language closely aligned with SQL. In this section, we\nbriefly discuss the SQL/PSM standard, which is representative of most vendor-\nspecific languages. In PSM, we define modules, which are collections of stored\nprocedures, temporary relations, and other declarations.\nIn SQL/PSM, we declare a stored procedure as follows:\nCREATE PROCEDURE name (parameter1,... , parameterN)\nlocal variable declarations\nprocedure code;\nWe can declare a function similarly as follows:\nCREATE FUNCTION name (parameterl,... , parameterN)\nRETURNS sqIDataType\nlocal variable declarations\nfunction code;\nEach parameter is a triple consisting of the mode (IN, OUT, or INOUT as\ndiscussed in the previous section), the parameter name, and the SQL datatype\nof the parameter. We can seen very simple SQL/PSM procedures in Section\n6.5.1. In this case, the local variable declarations were empty, and the procedure\ncode consisted of an SQL query.\nWe start out with an example of a SQL/PSM function that illustrates the\nmain SQL/PSM constructs. The function takes as input a customer identified\nby her cid and a year. The function returns the rating of the customer, which\nis defined a...'3 follows: Customers who have bought more than ten books during\nthe year are rated 'two'; customer who have purcha...<;ed between 5 and 10 books\nare rated 'one', otherwise the customer is rated 'zero'. The following SQL/PSM\ncode computes the rating for a given customer and year.\nCREATE PROCEDURE RateCustomer\n\nDatabase Appl'ication Development\n(IN custId INTEGER, IN year INTEGER)\nRETURNS INTEGER\nDECLARE rating INTEGER;\nDECLARE numOrders INTEGER;\nSET numOrders =\n(SELECT COUNT(*) FROM Orders 0 WHERE O.tid = custId);\nIF (numOrders>10) THEN rating=2;\nELSEIF (numOrders>5) THEN rating=1;\nELSE rating=O;\nEND IF;\nRETURN rating;\nLet us use this example to give a short overview of some SQL/PSM constructs:\n•\nWe can declare local variables using the DECLARE statement. In our exam-\nple, we declare two local variables: 'rating', and 'numOrders'.\n•\nPSM/SQL functions return values via the RETURN statement. In our ex-\nample, we return the value of the local variable 'rating'.\n•\nvVe can assign values to variables with the SET statement. In our example,\nwe assigned the return value of a query to the variable 'numOrders'.\n•\nSQL/PSM h&<; branches and loops. Branches have the following form:\nIF (condition) THEN statements;\nELSEIF statements;\nELSEIF statements;\nELSE statements; END IF\nLoops are of the form\nLOOP\nstaternents:\nEND LOOP\n•\nQueries can be used as part of expressions in branches; queries that return\na single ;ralue can be assigned to variables as in our example above.\n•\n'We can use the same cursor statements &s in Embedded SQL (OPEN, FETCH,\nCLOSE), but we do not need the EXEC SQL constructs, and variables do not\nhave to be prefixed by a colon ':'.\nWe only gave a very short overview of SQL/PSM; the references at the end of\nthe chapter provide more information.",
          "pages": [
            240,
            241,
            242,
            243,
            244,
            245,
            246,
            247,
            248
          ],
          "relevance": {
            "score": 0.17,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.3,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER €i\n6.6\nCASE STUDY: THE INTERNET BOOK SHOP\nDBDudes finished logical database design, as discussed in Section 3.8, and now\nconsider the queries that they have to support. They expect that the applica-\ntion logic will be implemented in Java, and so they consider JDBC and SQLJ as\npossible candidates for interfacing the database system with application code.\nRecall that DBDudes settled on the following schema:\nBooks(isbn: CHAR(10), title: CHAR(8), author: CHAR(80),\nqty_in_stock: INTEGER, price: REAL, year_published: INTEGER)\nCustomers( cid: INTEGER, cname: CHAR(80), address: CHAR(200))\nOrders(ordernum: INTEGER, isbn: CHAR(lO), cid: INTEGER,\ncardnum: CHAR(l6), qty: INTEGER, order_date: DATE, ship_date: DATE)\nNow, DBDudes considers the types of queries and updates that will arise. They\nfirst create a list of tasks that will be performed in the application.\nTasks\nperformed by customers include the following.\nII\nCustomers search books by author name, title, or ISBN.\n..\nCustomers register with the website.\nRegistered customers might want\nto change their contact information.\nDBDudes realize that they have to\naugment the Customers table with additional information to capture login\nand password information for each customer; we do not discuss this aspect\nany further.\nIII\nCustomers check out a final shopping basket to complete a sale.\nIII\nCustomers add and delete books from a 'shopping basket' at the website.\n..\nCustomers check the status of existing orders and look at old orders.\nAdministrative ta.'3ks performed by employees of B&N are listed next.\nII\nEmployees look up customer contact information.\nIII\nEmployees add new books to the inventory.\n..\nEmployees fulfill orders, and need to update the shipping date of individual\nbooks.\n..\nEmployees analyze the data to find profitable customers and customers\nlikely to respond to special marketing campaigns.\nNext, DBDudes consider the types of queries that will a,rise out of these tasks.\nTo support searching for books by name, author, title, or ISBN, DBDudes\ndecide to write a stored procedure as follows:\n\nDatabase Application Development\nCREATE PROCEDURE SearchByISBN (IN book.isbn CHAR (10) )\nSELECT B.title, B.author,\nB.qty_in~'3tock,B.price, B.yeaLpublished\nFROM\nBooks B\nWHERE\nB.isbn = book.isbn\nPlacing an order involves inserting one or more records into the Orders table.\nSince DBDudes has not yet chosen the Java-based technology to program the\napplication logic, they assume for now that the individual books in the order\nare stored at the application layer in a Java array. To finalize the order, they\nwrite the following JDBC code shown in Figure 6.11, which inserts the elements\nfrom the array into the Orders table.\nNote that this code fragment assumes\nseveral Java variables have been set beforehand.\nString sql = \"INSERT INTO Orders VALUES(7, 7, 7, 7, 7, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\ncon.setAutoCommit(false);\ntry {\n/ / orderList is a vector of Order objects\n/ / ordernum is the current order number\n/ / dd is the ID of the customer, cardnum is the credit card number\nfor (int i=O; iiorderList.lengthO; i++)\n/ / now instantiate the parameters with values\nOrder currentOrder = orderList[i];\npstmt.clearParameters() ;\npstmt.setInt(l, ordernum);\npstmt.setString(2, Order.getlsbnO);\npstmt.setInt(3, dd);\npstmt.setString(4, creditCardNum);\npstmt.setlnt(5, Order.getQtyO);\npstmt.setDate(6, null);\npstmt.executeUpdate();\n}\ncon.commit();\ncatch (SqLException e){\ncon.rollbackO;\nSystem.out.println(e.getMessage());\n}\nInserting a Completed Order into the Database\n\nCHAPTER (}\nDBDudes writes other JDBC code and stored procedures for all of the remain-\ning tasks. They use code similar to some of the fragments that we have seen in\nthis chapter.\nII\nEstablishing a connection to a database, as shown in Figure 6.2.\nII\nAdding new books to the inventory, a'3 shown in Figure 6.3.\nII\nProcessing results from SQL queries a'3 shown in Figure 6.4-\nII\nFor each customer, showing how many orders he or she has placed.\nWe\nshowed a sample stored procedure for this query in Figure 6.8.\nII\nIncrea'3ing the available number of copies of a book by adding inventory,\nas shown in Figure 6.9.\nII\nRanking customers according to their purchases, as shown in Figure 6.10.\nDBDudcs takes care to make the application robust by processing exceptions\nand warnings, as shown in Figure 6.6.\nDBDudes also decide to write a trigger, which is shown in Figure 6.12. When-\never a new order is entered into the Orders table, it is inserted with ship~date\nset to NULL. The trigger processes each row in the order and calls the stored\nprocedure 'UpdateShipDate'. This stored procedure (whose code is not shown\nhere) updates the (anticipated) ship_date of the new order to 'tomorrow', in\ncase qtyjlLstock of the corresponding book in the Books table is greater than\nzero. Otherwise, the stored procedme sets the ship_date to two weeks.\nCREATE TRIGGER update_ShipDate\nAFTER INSERT ON Orders\nFOR EACH ROW\nBEGIN CALL UpdatcShipDate(new); END\n1* Event *j\n1* Action *j\nTrigger to Update the Shipping Date of New Orders\n6.7\nREVIEW QUESTIONS\nAnswers to the i'eview questions can be found in the listed sections.\nlYl\nvVhy is it not straightforward to integrate SQL queries with a host pro-\ngramming language? (Section 6.1.1)\nIIii\nHow do we declare variables in Ernbcdded SQL? (Section 6.1.1)\n\nDatabase Applicat'ion Deuelop'Tnent\n'*\n•\nHow do we use SQL statements within a host langl.lage? How do we check\nfor errors in statement execution? (Section 6.1.1)\n•\nExplain the impedance mismatch between host languages and SQL, and\ndescribe how cursors address this. (Section 6.1.2)\n•\n'\\That properties can cursors have? (Section 6.1.2)\n•\nWhat is Dynamic SQL and how is it different from Embedded SQL? (Sec-\ntion 6.1.3)\n•\nWhat is JDBC and what are its advantages? (Section 6.2)\n•\nWhat are the components of the JDBC architecture? Describe four differ-\nent architectural alternatives for JDBC drivers. (Section 6.2.1)\n•\nHow do we load JDBC drivers in Java code? (Section 6.3.1)\n•\nHow do we manage connections to data sources?\nWhat properties can\nconnections have? (Section 6.3.2)\n•\nWhat alternatives does JDBC provide for executing SQL DML and DDL\nstatements? (Section 6.3.3)\n•\nHow do we handle exceptions and warnings in JDBC? (Section 6.3.5)\n•\n'What functionality provides the DatabaseMetaDataclass? (Section 6.3.6)\n•\nWhat is SQLJ and how is it different from JDBC? (Section 6.4)\n•\nvVhy are stored procedures important? How do we declare stored proce-\ndures and how are they called from application code? (Section 6.5)\nEXERCISES\nExercise 6.1 Briefly answer the following questions.\n1. Explain the following terms: Cursor, Embedded SQL, JDBC, SQLJ, stored procedure.\n2. What are the differences between JDBC and SQLJ? \\Nhy do they both exist?\n3. Explain the term stored procedure, and give examples why stored procedures are useful.\nExercise 6.2 Explain how the following steps are performed in JDBC:\n1. Connect to a data source.\n2. Start, commit, and abort transactions.\n3. Call a stored procedure.\nHow are these steps performed in SQLJ?",
          "pages": [
            249,
            250,
            251,
            252
          ],
          "relevance": {
            "score": 0,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.4,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Integrity Constraints are rules that ensure data remains accurate, complete, and consistent within a database.",
        "explanation": "Integrity constraints are crucial for maintaining the reliability and accuracy of your database. They prevent incorrect or harmful data from being entered into your tables. There are several types of integrity constraints, including primary keys, foreign keys, not null constraints, unique constraints, and check constraints. Each type serves a specific purpose in ensuring that your data is valid and consistent.",
        "key_points": [
          "Primary Keys: Ensure each row has a unique identifier.",
          "Foreign Keys: Link tables together to maintain relationships.",
          "Not Null Constraints: Require that a column cannot have null values.",
          "Unique Constraints: Ensure that the values in a column are unique across all rows.",
          "Check Constraints: Validate data based on specific conditions."
        ],
        "examples": [
          {
            "title": "Creating a Table with Integrity Constraints",
            "code": "CREATE TABLE Students ( StudentID INT PRIMARY KEY, FirstName VARCHAR(50) NOT NULL, LastName VARCHAR(50) NOT NULL, Email VARCHAR(100) UNIQUE );",
            "explanation": "This example creates a table named 'Students' with primary key, not null, and unique constraints. The StudentID column is the primary key, ensuring each student has a unique identifier. The FirstName and LastName columns cannot be null, and the Email column must contain unique values.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Inserting Data into a Table with Constraints",
            "code": "INSERT INTO Students (StudentID, FirstName, LastName, Email) VALUES (1, 'John', 'Doe', 'john.doe@example.com');",
            "explanation": "This example inserts data into the 'Students' table. It demonstrates how constraints are enforced during data insertion. If any constraint is violated, the insert operation will fail.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to define a primary key",
            "incorrect_code": "CREATE TABLE Students ( FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "correct_code": "CREATE TABLE Students ( StudentID INT PRIMARY KEY, FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "explanation": "Not defining a primary key can lead to duplicate rows and make it difficult to uniquely identify records. Always ensure each table has a primary key."
          },
          {
            "mistake": "Using an incorrect data type for a column",
            "incorrect_code": "CREATE TABLE Students ( StudentID INT, Age CHAR(2) );",
            "correct_code": "CREATE TABLE Students ( StudentID INT, Age INT );",
            "explanation": "Using the wrong data type can lead to errors and inconsistencies. For example, using a CHAR for an age column will not allow numeric operations."
          }
        ],
        "practice": {
          "question": "Create a table named 'Orders' with columns for OrderID (primary key), CustomerID (foreign key referencing Customers.CustomerID), ProductName, Quantity, and Price. Ensure that the Quantity cannot be null and must be greater than zero.",
          "solution": "CREATE TABLE Orders (\n  OrderID INT PRIMARY KEY,\n  CustomerID INT,\n  ProductName VARCHAR(100),\n  Quantity INT NOT NULL CHECK (Quantity > 0),\n  Price DECIMAL(10,2)\n);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER ()\n..\npublic boolean isClosed() throws SQLException.\nChecks whether the current connection has already been closed.\n..\nsetAutoCommit and get AutoCommit.\nvVe already discussed these two functio",
      "content_relevance": {
        "score": 0.07,
        "sql_score": 1.0,
        "concept_score": 0.33,
        "non_sql_penalty": 0.4,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "primary-key": {
      "id": "primary-key",
      "title": "Primary Key Constraint",
      "definition": "Uniquely identifying each row in a table",
      "difficulty": "beginner",
      "page_references": [
        235,
        236,
        237,
        238,
        239,
        240,
        241,
        242,
        243
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER ()\n..\npublic boolean isClosed() throws SQLException.\nChecks whether the current connection has already been closed.\n..\nsetAutoCommit and get AutoCommit.\nvVe already discussed these two functions.\nEstablishing a connection to a data source is a costly operation since it in-\nvolves several steps, such as establishing a network connection to the data\nsource, authentication, and allocation of resources such as memory. In case an\napplication establishes many different connections from different parties (such\nas a Web server), connections are often pooled to avoid this overhead. A con-\nnection pool is a set of established connections to a data source. Whenever a\nnew connection is needed, one of the connections from the pool is used, instead\nof creating a new connection to the data source.\nConnection pooling can be handled either by specialized code in the application,\nor the optional j avax. sql package, which provides functionality for connection\npooling and allows us to set different parameters, such as the capacity of the\npool, and shrinkage and growth rates.\nMost application servers (see Section\n7.7.2) implement the j avax .sql package or a proprietary variant.\n6.3.3\nExecuting SQL Statements\nWe now discuss how to create and execute SQL statements using JDBC. In the\nJDBC code examples in this section, we assume that we have a Connection\nobject named con. JDBC supports three different ways of executing statements:\nStatement, PreparedStatement, and CallableStatement.\nThe Statement\nclass is the base class for the other two statment classes. It allows us to query\nthe data source with any static or dynamically generated SQL query. We cover\nthe PreparedStatement class here and the CallableStatement class in Section\n6.5, when we discuss stored procedures.\nThe PreparedStatement cla,Cis dynamicaJly generates precompiled SQL state-\nments that can be used several times; these SQL statements can have param-\neters, but their structure is fixed when the PreparedStatement object (repre-\nsenting the SQL statement) is created.\nConsider the sample code using a PreparedStatment object shown in Figure\n6.3.\nThe SQL query specifies the query string, but uses ''1'\nfor the values\nof the parameters, which are set later using methods setString, setFloat,\nand setlnt. The ''1' placeholders can be used anywhere in SQL statements\nwhere they can be replaced with a value. Examples of places where they can\nappear include the WHERE clause (e.g., 'WHERE author=?'), or in SQL UPDATE\nand INSERT staternents, as in Figure 6.3. The method setString is one way\n\nDatabase Application Develop'ment\n/ / initial quantity is always zero\nString sql = \"INSERT INTO Books VALUES('?, 7, '?, ?, 0, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\n/ / now instantiate the parameters with values\n/ / a,ssume that isbn, title, etc. are Java variables that\n/ / contain the values to be inserted\npstmt.clearParameters();\npstmt.setString(l, isbn);\npstmt.setString(2, title);\npstmt.setString(3, author);\npstmt.setFloat(5, price);\npstmt.setInt(6, year);\nint numRows = pstmt.executeUpdate();\nSQL Update Using a PreparedStatement Object\nto set a parameter value; analogous methods are available for int, float,\nand date. It is good style to always use clearParameters 0 before setting\nparameter values in order to remove any old data.\nThere are different ways of submitting the query string to the data source. In\nthe example, we used the executeUpdate command, which is used if we know\nthat the SQL statement does not return any records (SQL UPDATE, INSERT,\nALTER, and DELETE statements). The executeUpdate method returns an inte-\nger indicating the number of rows the SQL statement modified; it returns 0 for\nsuccessful execution without modifying any rows.\nThe executeQuery method is used if the SQL statement returns data, such &\"l\nin a regular SELECT query. JDBC has its own cursor mechanism in the form\nof a ResultSet object, which we discuss next. The execute method is more\ngeneral than executeQuery and executeUpdate; the references at the end of\nthe chapter provide pointers with more details.\n6.3.4\nResul,tSets\nAs discussed in the previous section, the statement executeQuery returns a,\nResultSet object, which is similar to a cursor. ResultSet cursors in JDBC\n2.0 are very powerful; they allow forward and reverse scrolling and in-place\nediting and insertions.\n\nCHAPTER\nIn its most basic form, the ResultSet object allows us to read one row of the\noutput of the query at a time.\nInitially, the ResultSet is positioned before\nthe first row, and we have to retrieve the first row with an explicit call to the\nnext 0 method. The next method returns false if there are no more rows in\nthe query answer, and true other\\vise. The code fragment shown in Figure 6.4\nillustrates the basic usage of a ResultSet object.\nResultSet rs=stmt.executeQuery(sqlQuery);\n/ / rs is now a cursor\n/ / first call to rs.nextO moves to the first record\n/ / rs.nextO moves to the next row\nString sqlQuery;\nResultSet rs = stmt.executeQuery(sqlQuery)\nwhile (rs.next()) {\n/ / process the data\n}\nUsing a ResultSet Object\nWhile next () allows us to retrieve the logically next row in the query answer,\nwe can move about in the query answer in other ways too:\n•\nprevious 0 moves back one row.\n•\nabsolute (int num) moves to the row with the specified number.\n•\nrelative (int num) moves forward or backward (if num is negative) rela-\ntive to the current position. relative (-1) has the same effect as previous.\n•\nfirst 0 moves to the first row, and last 0\nmoves to the last row.\nMatching Java and SQL Data Types\nIn considering the interaction of an application with a data source, the issues\nwe encountered in the context of Embedded SQL (e.g., passing information\nbetween the application and the data source through shared variables) arise\nagain. To deal with such issues, JDBC provides special data types and speci-\nfies their relationship to corresponding SQL data types. Figure 6.5 shows the\naccessor methods in a ResultSet object for the most common SQL datatypes.\nWith these accessor methods, we can retrieve values from the current row of\nthe query result referenced by the ResultSet object. There are two forms for\neach accessor method: One method retrieves values by column index, starting\nat one, and the other retrieves values by column name. The following exam-\nple shows how to access fields of the current ResultSet row using accesssor\nmethods.",
          "pages": [
            235,
            236,
            237
          ],
          "relevance": {
            "score": 0.18,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Database Application Development\n2Q3\nI SQL Type I\nJava cla.c;;s\nI ResultSet get method I\nBIT\nBoolean\ngetBooleanO\nCHAR\nString\ngetStringO\nVARCHAR\nString\ngetStringO\nDOUBLE\nDouble\ngetDoubleO\nFLOAT\nDouble\ngetDoubleO\nINTEGER\nInteger\ngetIntO\nREAL\nDouble\ngetFloatO\nDATE\njava.sql.Date\ngetDateO\nTIME\njava.sql.Time\ngetTimeO\nTIMESTAMP\njava.sql.TimeStamp\ngetTimestamp ()\nReading SQL Datatypes from a ResultSet Object\nResultSet rs=stmt.executeQuery(sqIQuery);\nString sqlQuerYi\nResultSet rs = stmt.executeQuery(sqIQuery)\nwhile (rs.nextO) {\nisbn = rs.getString(l);\ntitle = rs.getString(\" TITLE\");\n/ / process isbn and title\n}\n6.3.5\nExceptions and Warnings\nSimilar to the SQLSTATE variable, most of the methods in java. sql can throw\nan exception of the type SQLException if an error occurs.\nThe information\nincludes SQLState, a string that describes the error (e.g., whether the statement\ncontained an SQL syntax error).\nIn addition to the standard getMessage 0\nmethod inherited from Throwable, SQLException has two additional methods\nthat provide further information, and a method to get (or chain) additional\nexceptions:\nIII\npublic String getSQLState 0 returns an SQLState identifier based on\nthe SQL:1999 specification, as discussed in Section 6.1.1.\n..\npublic i:p.t getErrorCode () retrieves a vendor-specific error code.\nIII\npublic SQLException getNextExceptionO gets the next exception in a\nchain of exceptions associated with the current SQLException object.\nAn SQL\\¥arning is a subclass of SQLException. Warnings are not H•.'3 severe as\nerrors and the program can usually proceed without special handling of warn-\nings. \\Varnings are not thrown like other exceptions, and they are not caught a.,\n\nCHAPTER\npart of the try\"-catch block around a java. sql statement. VVe Heed to specif-\nically test whether warnings exist.\nConnection, Statement, and ResultSet\nobjects all have a getWarnings 0\nmethod with which we can retrieve SQL\nwarnings if they exist. Duplicate retrieval of warnings can be avoided through\nclearWarnings O. Statement objects clear warnings automatically on execu-\ntion of the next statement; ResultSet objects clear warnings every time a new\ntuple is accessed.\nTypical code for obtaining SQLWarnings looks similar to the code shown in\ntry {\nstmt = con.createStatement();\nwarning = con.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\nwarning = warning.getNextWarningO;\n/ /get next warning\n}\ncon.clear\\Varnings() ;\nstmt.executeUpdate( queryString );\nwarning = stmt.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\n/ /get next warning\n}\n} / / end try\ncatch ( SQLException SQLe) {\n/ / code to handle exception\n} / / end catch\nProcessing JDBC Warnings and Exceptions\n6.3.6\nExamining Database Metadata\n\\Ve can use tlw DatabaseMetaData object to obtain information about the\ndatabase system itself, as well as information frorn the database catalog. For\nexample, the following code fragment shows how to obtain the name and driver\nversion of the JDBC driver:\nDataba..seMetaData md = con.getMetaD<Lta():\nSystem.out.println(\"Driver Information:\");\n\nDatabase Appl'imtion Developrnent\nSystem.out.println(\"Name:\" + md.getDriverNameO\n+ \"; version:\" + mcl.getDriverVersion());\n~\nThe DatabaseMetaData object has many more methods (in JDBC 2.0, exactly\n134); we list some methods here:\n•\npublic ResultSet getCatalogs 0\nthrows SqLException. This function\nreturns a ResultSet that can be used to iterate over all the catalog relations.\nThe functions getIndexInfo 0 and getTables 0 work analogously.\n•\npUblic int getMaxConnections 0\nreturns the ma.ximum number of connections possible.\nWe will conclude our discussion of JDBC with an example code fragment that\nexamines all database metadata shown in Figure 6.7.\nDatabaseMetaData dmd = con.getMetaDataO;\nResultSet tablesRS = dmd.getTables(null,null,null,null);\nstring tableName;\nwhile(tablesRS.next()) {\ntableNarne = tablesRS.getString(\"TABLE_NAME\");\n/ / print out the attributes of this table\nSystem.out.println(\"The attributes of table\"\n+ tableName + \" are:\");\nResultSet columnsRS = dmd.getColums(null,null,tableName, null);\nwhile (columnsRS.next()) {\nSystem.out.print(colummsRS.getString(\" COLUMN_NAME\")\n+\" \");\n}\n/ / print out the primary keys of this table\nSystem.out.println(\"The keys of table\" + tableName + \" are:\");\nResultSet keysRS = dmd.getPrimaryKeys(null,null,tableName);\nwhile (keysRS. next()) {\n'System.out.print(keysRS.getStringC'COLUMN_NAME\") +\" \");\n}\n}\nObtaining Infon-nation about it Data Source\n\nCHAPTER.:6\n6.4\nSQLJ\nSQLJ (short for 'SQL-Java') was developed by the SQLJ Group, a group of\ndatabase vendors and Sun. SQLJ was developed to complement the dynamic\nway of creating queries in JDBC with a static model. It is therefore very close\nto Embedded SQL. Unlike JDBC, having semi-static SQL queries allows the\ncompiler to perform SQL syntax checks, strong type checks of the compatibil-\nity of the host variables with the respective SQL attributes, and consistency\nof the query with the database schema-tables, attributes, views, and stored\nprocedures--all at compilation time. For example, in both SQLJ and Embed-\nded SQL, variables in the host language always are bound statically to the\nsame arguments, whereas in JDBC, we need separate statements to bind each\nvariable to an argument and to retrieve the result. For example, the following\nSQLJ statement binds host language variables title, price, and author to the\nreturn values of the cursor books.\n#sql books = {\nSELECT title, price INTO :title, :price\nFROM Books WHERE author = :author\n};\nIn JDBC, we can dynamically decide which host language variables will hold\nthe query result. In the following example, we read the title of the book into\nvariable ftitle if the book was written by Feynman, and into variable otitle\notherwise:\n/ / assume we have a ResultSet cursor rs\nauthor = rs.getString(3);\nif (author==\"Feynman\") {\nftitle = rs.getString(2):\n}\nelse {\notitle = rs.getString(2);\n}\nvVhen writing SQLJ applications, we just write regular Java code and embed\nSQL statements according to a set of rules. SQLJ applications are pre-processed\nthrough an SQLJ translation program that replaces the embedded SQLJ code\nwith calls to an SQLJ Java library. The modified program code can then be\ncompiled by any Java compiler. Usually the SQLJ Java library makes calls to\na JDBC driver, which handles the connection to the datab&'3e system.\n\nDatabase Application Development\n2Q7\nAn important philosophical difference exists between Embedded SQL and SQLJ\nand JDBC. Since vendors provide their own proprietary versions of SQL, it is\nadvisable to write SQL queries according to the SQL-92 or SQL:1999 standard.\nHowever, when using Embedded SQL, it is tempting to use vendor-specific SQL\nconstructs that offer functionality beyond the SQL-92 or SQL:1999 standards.\nSQLJ and JDBC force adherence to the standards, and the resulting code is\nmuch more portable across different database systems.\nIn the remainder of this section, we give a short introduction to SQLJ.\n6.4.1\nWriting SQLJ Code\nWe will introduce SQLJ by means of examples. Let us start with an SQLJ code\nfragment that selects records from the Books table that match a given author.\nString title; Float price; String atithor;\n#sql iterator Books (String title, Float price);\nBooks books;\n/ / the application sets the author\n/ / execute the query and open the cursor\n#sql books =\n{\nSELECT title, price INTO :titIe, :price\nFROM Books WHERE author = :author\n};\n/ / retrieve results\nwhile (books.next()) {\nSystem.out.println(books.titleO + \", \" + books.price());\n}\nbooks.close() ;\nThe corresponding JDBC code fragment looks as follows (assuming we also\ndeclared price, name, and author:\nPrcparcdStatcment stmt = connection.prepareStatement(\n\" SELECT title, price FROM Books WHERE author = ?\");\n/ / set the parameter in the query ancl execute it\nstmt.setString(1, author);\nResultSet 1'8 = stmt.executeQuery();\n/ / retrieve the results\nwhile (rs.next()) {",
          "pages": [
            238,
            239,
            240,
            241,
            242
          ],
          "relevance": {
            "score": 0.17,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.3,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER\nSystem.out.println(rs.getString(l) + \", \" + rs.getFloat(2));\n}\nComparing the JDBC and SQLJ code, we see that the SQLJ code is much\neasier to read than the JDBC code. Thus, SQLJ reduces software development\nand maintenance costs.\nLet us consider the individual components of the SQLJ code in more detail.\nAll SQLJ statements have the special prefix #sql. In SQLJ, we retrieve the\nresults of SQL queries with iterator objects, which are basically cursors. An\niterator is an instance of an iterator class. Usage of an iterator in SQLJ goes\nthrough five steps:\n•\nDeclare the Iterator Class: In the preceding code, this happened through\nthe statement\n#sql iterator Books (String title, Float price);\nThis statement creates a new Java class that we can use to instantiate\nobjects.\n•\nInstantiate an Iterator Object from the New Iterator Class: We\ninstantiated our iterator in the statement Books books;.\n•\nInitialize the Iterator Using a SQL Statement: In our example, this\nhappens through the statement #sql books\n;;;;;; ....\n•\nIteratively, Read the Rows From the Iterator Object: This step is\nvery similar to reading rows through a ResultSet object in JDBC.\n•\nClose the Iterator Object.\nThere are two types of iterator classes: named iterators and positional iterators.\nFor named iterators, we specify both the variable type and the name of each\ncolumn of the iterator. This allows us to retrieve individual columns by name as\nin our previous example where we could retrieve the title colunm from the Books\ntable using the expression books. titIe (). For positional iterators, we need\nto specifY only the variable type for each column of the iterator.\nTo access\nthe individual columns of the iterator, we use a FETCH ...\nINTO eonstruct,\nsimilar to Embedded SQL. Both iterator types have the same performance;\nwhich iterator to use depends on the programmer's taste.\nLet us revisit our example.\n\\Ve can make the iterator a positional iterator\nthrough the following statement:\n#sql iterator Books (String, Float);\nvVe then retrieve the individual rows from the iterator 3,.'3 follows:",
          "pages": [
            243
          ],
          "relevance": {
            "score": 0.05,
            "sql_score": 0.5,
            "concept_score": 0.0,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "A primary key constraint is a database rule that uniquely identifies each record in a table. It ensures data integrity and allows for efficient querying.",
        "explanation": "Primary keys are crucial because they help maintain the accuracy of your data by ensuring no duplicate records exist. They also speed up data retrieval operations, as databases can quickly locate a specific record using its primary key. When you define a column or set of columns as a primary key, you're telling the database that these values must be unique and not null.",
        "key_points": [
          "A primary key uniquely identifies each row in a table.",
          "It ensures data integrity by preventing duplicate records.",
          "Primary keys speed up data retrieval operations.",
          "You can define a single column or a combination of columns as the primary key."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "CREATE TABLE Students ( StudentID INT PRIMARY KEY, FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "explanation": "This example creates a table named 'Students' with a primary key column 'StudentID'. Each student must have a unique ID, and this ID cannot be null.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "CREATE TABLE Orders ( OrderID INT PRIMARY KEY, CustomerID INT, OrderDate DATE );",
            "explanation": "This practical example creates an 'Orders' table with a primary key column 'OrderID'. Each order must have a unique ID, and this ID cannot be null. This ensures that each order can be uniquely identified in the database.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using a non-unique value as a primary key",
            "incorrect_code": "CREATE TABLE Invalid ( ID INT PRIMARY KEY, Name VARCHAR(50) ); -- Inserting duplicate IDs is allowed;",
            "correct_code": "CREATE TABLE Valid ( ID INT PRIMARY KEY, Name VARCHAR(50) ); -- Inserting duplicate IDs will cause an error;",
            "explanation": "Primary keys must be unique. If you try to insert a duplicate value, the database will throw an error."
          },
          {
            "mistake": "Forgetting to set a primary key",
            "incorrect_code": "CREATE TABLE MissingPK ( Name VARCHAR(50), Age INT ); -- No primary key defined;",
            "correct_code": "CREATE TABLE CorrectPK ( ID INT PRIMARY KEY, Name VARCHAR(50), Age INT ); -- Primary key set to 'ID';",
            "explanation": "Primary keys are essential for data integrity. If you don't define a primary key, the database won't enforce uniqueness and may allow duplicate records."
          }
        ],
        "practice": {
          "question": "Create a table named 'Books' with columns 'ISBN', 'Title', 'Author', and 'Price'. Set 'ISBN' as the primary key.",
          "solution": "CREATE TABLE Books (\n  ISBN VARCHAR(13) PRIMARY KEY,\n  Title VARCHAR(255),\n  Author VARCHAR(100),\n  Price DECIMAL(10, 2)\n);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER ()\n..\npublic boolean isClosed() throws SQLException.\nChecks whether the current connection has already been closed.\n..\nsetAutoCommit and get AutoCommit.\nvVe already discussed these two functio",
      "content_relevance": {
        "score": 0.17,
        "sql_score": 1.0,
        "concept_score": 0.33,
        "non_sql_penalty": 0.3,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "foreign-key": {
      "id": "foreign-key",
      "title": "Foreign Key Constraint",
      "definition": "Maintaining referential integrity between related tables",
      "difficulty": "intermediate",
      "page_references": [
        240,
        241,
        242,
        243,
        244,
        245,
        246,
        247,
        248,
        249,
        250,
        251,
        252,
        253,
        254,
        255
      ],
      "sections": {
        "definition": {
          "text": "Database Appl'imtion Developrnent\nSystem.out.println(\"Name:\" + md.getDriverNameO\n+ \"; version:\" + mcl.getDriverVersion());\n~\nThe DatabaseMetaData object has many more methods (in JDBC 2.0, exactly\n134); we list some methods here:\n•\npublic ResultSet getCatalogs 0\nthrows SqLException. This function\nreturns a ResultSet that can be used to iterate over all the catalog relations.\nThe functions getIndexInfo 0 and getTables 0 work analogously.\n•\npUblic int getMaxConnections 0\nreturns the ma.ximum number of connections possible.\nWe will conclude our discussion of JDBC with an example code fragment that\nexamines all database metadata shown in Figure 6.7.\nDatabaseMetaData dmd = con.getMetaDataO;\nResultSet tablesRS = dmd.getTables(null,null,null,null);\nstring tableName;\nwhile(tablesRS.next()) {\ntableNarne = tablesRS.getString(\"TABLE_NAME\");\n/ / print out the attributes of this table\nSystem.out.println(\"The attributes of table\"\n+ tableName + \" are:\");\nResultSet columnsRS = dmd.getColums(null,null,tableName, null);\nwhile (columnsRS.next()) {\nSystem.out.print(colummsRS.getString(\" COLUMN_NAME\")\n+\" \");\n}\n/ / print out the primary keys of this table\nSystem.out.println(\"The keys of table\" + tableName + \" are:\");\nResultSet keysRS = dmd.getPrimaryKeys(null,null,tableName);\nwhile (keysRS. next()) {\n'System.out.print(keysRS.getStringC'COLUMN_NAME\") +\" \");\n}\n}\nObtaining Infon-nation about it Data Source\n\nCHAPTER.:6\n6.4\nSQLJ\nSQLJ (short for 'SQL-Java') was developed by the SQLJ Group, a group of\ndatabase vendors and Sun. SQLJ was developed to complement the dynamic\nway of creating queries in JDBC with a static model. It is therefore very close\nto Embedded SQL. Unlike JDBC, having semi-static SQL queries allows the\ncompiler to perform SQL syntax checks, strong type checks of the compatibil-\nity of the host variables with the respective SQL attributes, and consistency\nof the query with the database schema-tables, attributes, views, and stored\nprocedures--all at compilation time. For example, in both SQLJ and Embed-\nded SQL, variables in the host language always are bound statically to the\nsame arguments, whereas in JDBC, we need separate statements to bind each\nvariable to an argument and to retrieve the result. For example, the following\nSQLJ statement binds host language variables title, price, and author to the\nreturn values of the cursor books.\n#sql books = {\nSELECT title, price INTO :title, :price\nFROM Books WHERE author = :author\n};\nIn JDBC, we can dynamically decide which host language variables will hold\nthe query result. In the following example, we read the title of the book into\nvariable ftitle if the book was written by Feynman, and into variable otitle\notherwise:\n/ / assume we have a ResultSet cursor rs\nauthor = rs.getString(3);\nif (author==\"Feynman\") {\nftitle = rs.getString(2):\n}\nelse {\notitle = rs.getString(2);\n}\nvVhen writing SQLJ applications, we just write regular Java code and embed\nSQL statements according to a set of rules. SQLJ applications are pre-processed\nthrough an SQLJ translation program that replaces the embedded SQLJ code\nwith calls to an SQLJ Java library. The modified program code can then be\ncompiled by any Java compiler. Usually the SQLJ Java library makes calls to\na JDBC driver, which handles the connection to the datab&'3e system.\n\nDatabase Application Development\n2Q7\nAn important philosophical difference exists between Embedded SQL and SQLJ\nand JDBC. Since vendors provide their own proprietary versions of SQL, it is\nadvisable to write SQL queries according to the SQL-92 or SQL:1999 standard.\nHowever, when using Embedded SQL, it is tempting to use vendor-specific SQL\nconstructs that offer functionality beyond the SQL-92 or SQL:1999 standards.\nSQLJ and JDBC force adherence to the standards, and the resulting code is\nmuch more portable across different database systems.\nIn the remainder of this section, we give a short introduction to SQLJ.\n6.4.1\nWriting SQLJ Code\nWe will introduce SQLJ by means of examples. Let us start with an SQLJ code\nfragment that selects records from the Books table that match a given author.\nString title; Float price; String atithor;\n#sql iterator Books (String title, Float price);\nBooks books;\n/ / the application sets the author\n/ / execute the query and open the cursor\n#sql books =\n{\nSELECT title, price INTO :titIe, :price\nFROM Books WHERE author = :author\n};\n/ / retrieve results\nwhile (books.next()) {\nSystem.out.println(books.titleO + \", \" + books.price());\n}\nbooks.close() ;\nThe corresponding JDBC code fragment looks as follows (assuming we also\ndeclared price, name, and author:\nPrcparcdStatcment stmt = connection.prepareStatement(\n\" SELECT title, price FROM Books WHERE author = ?\");\n/ / set the parameter in the query ancl execute it\nstmt.setString(1, author);\nResultSet 1'8 = stmt.executeQuery();\n/ / retrieve the results\nwhile (rs.next()) {\n\nCHAPTER\nSystem.out.println(rs.getString(l) + \", \" + rs.getFloat(2));\n}\nComparing the JDBC and SQLJ code, we see that the SQLJ code is much\neasier to read than the JDBC code. Thus, SQLJ reduces software development\nand maintenance costs.\nLet us consider the individual components of the SQLJ code in more detail.\nAll SQLJ statements have the special prefix #sql. In SQLJ, we retrieve the\nresults of SQL queries with iterator objects, which are basically cursors. An\niterator is an instance of an iterator class. Usage of an iterator in SQLJ goes\nthrough five steps:\n•\nDeclare the Iterator Class: In the preceding code, this happened through\nthe statement\n#sql iterator Books (String title, Float price);\nThis statement creates a new Java class that we can use to instantiate\nobjects.\n•\nInstantiate an Iterator Object from the New Iterator Class: We\ninstantiated our iterator in the statement Books books;.\n•\nInitialize the Iterator Using a SQL Statement: In our example, this\nhappens through the statement #sql books\n;;;;;; ....\n•\nIteratively, Read the Rows From the Iterator Object: This step is\nvery similar to reading rows through a ResultSet object in JDBC.\n•\nClose the Iterator Object.\nThere are two types of iterator classes: named iterators and positional iterators.\nFor named iterators, we specify both the variable type and the name of each\ncolumn of the iterator. This allows us to retrieve individual columns by name as\nin our previous example where we could retrieve the title colunm from the Books\ntable using the expression books. titIe (). For positional iterators, we need\nto specifY only the variable type for each column of the iterator.\nTo access\nthe individual columns of the iterator, we use a FETCH ...\nINTO eonstruct,\nsimilar to Embedded SQL. Both iterator types have the same performance;\nwhich iterator to use depends on the programmer's taste.\nLet us revisit our example.\n\\Ve can make the iterator a positional iterator\nthrough the following statement:\n#sql iterator Books (String, Float);\nvVe then retrieve the individual rows from the iterator 3,.'3 follows:\n\nDatabase Application Development\nwhile (true) {\n#sql { FETCH :books INTO :title, :price, };\nif (books.endFetch()) {\nbreak:\n}\n/ / process the book\n}\n6.5\nSTORED PROCEDURES\nIt is often important to execute some parts of the application logic directly in\nthe process space of the database system. Running application logic directly\nat the databa.se has the advantage that the amount of data that is transferred\nbetween the database server and the client issuing the SQL statement can be\nminimized, while at the same time utilizing the full power of the databa.se\nserver.\nWhen SQL statements are issued from a remote application, the records in the\nresult of the query need to be transferred from the database system back to\nthe application. If we use a cursor to remotely access the results of an SQL\nstatement, the DBMS has resources such as locks and memory tied up while the\napplication is processing the records retrieved through the cursor. In contrast,\na stored procedure is a program that is executed through a single SQL\nstatement that can be locally executed and completed within the process space\nof the database server.\nThe results can be packaged into one big result and\nreturned to the application, or the application logic can be performed directly\nat the server, without having to transmit the results to the client at alL\nStored procedures are also beneficial for software engineering rea,sons.\nOnce\na stored procedure is registered with the database server, different users can\nre-use the stored procedure, eliminating duplication of efforts in writing SQL\nqueries or application logic, and making code maintenance ea.\"lY. In addition,\napplication programmers do not need to know the the databa.se schema if we\nencapsulate all databa.'3e access into stored procedures.\nAlthough they,are called stored procedur'es, they do not have to be procedures\nin a programming language sense; they can be functions.\n6.5.1\nCreating a Simple Stored Procedure\nLet us look at the example stored procedure written in SQL shown in Figure\n(i.S.\nvVe see that stored procedures must have a name; this stored procedure",
          "pages": [
            240,
            241,
            242,
            243,
            244
          ],
          "relevance": {
            "score": 0.17,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.3,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER' 6\nhas the name 'ShowNumberOfOrders.'\nOtherwise, it just contains an SQL\nstatement that is precompiled and stored at the server.\nCREATE PROCEDURE ShowNumberOfOrders\nSELECT C.cid, C.cname, COUNT(*)\nFROM\nCustomers C, Orders a\nWHERE\nC.cid = O.cid\nGROUP BY C.cid, C.cname\nA Stored Procedure in SQL\nStored procedures can also have parameters.\nThese parameters have to be\nvalid SQL types, and have one of three different modes: IN,\nOUT, or INOUT.\nIN parameters are arguments to' the stored procedure.\nOUT parameters are\nreturned from the stored procedure; it assigns values to all OUT parameters\nthat the user can process. INOUT parameters combine the properties of IN and\nOUT parameters: They contain values to be passed to the stored procedures, and\nthe stored procedure can set their values as return values. Stored procedures\nenforce strict type conformance: If a parameter is of type INTEGER, it cannot\nbe called with an argument of type VARCHAR.\nLet us look at an example of a stored procedure with arguments. The stored\nprocedure shown in Figure 6.9 has two arguments: book_isbn and addedQty.\nIt updates the available number of copies of a book with the quantity from a\nnew shipment.\nCREATE PROCEDURE Addlnventory (\nIN book_isbn CHAR(lO),\nIN addedQty INTEGER)\nUPDATE Books\nSET\nWHERE\nqty_in_stock = qtyjn_stock + addedQty\nbookjsbn = isbn\nA Stored Procedure with Arguments\nStored procedures do not have to be written in SQL; they can be written in any\nhost language. As an example, the stored procedure shown in Figure 0.10 is a\nJava function that is dynamically executed by the databa..<;e server whenever it\nis called by the dient:\n6.5.2\nCalling Stored Procedures\nStored procedures can be called in interactive SQL with the CALL statement:\n\nDatabase Application Development\nCREATE PROCEDURE RallkCustomers(IN number INTEGER)\nLANGUAGE Java\nEXTERNAL NAME 'file:// /c:/storedProcedures/rank.jar'\nA Stored Procedure in Java\nCALL storedProcedureName(argumentl, argument2, ... , argumentN);\nIn Embedded SQL, the arguments to a stored procedure are usually variables\nin the host language. For example, the stored procedure AddInventory would\nbe called as follows:\nEXEC SQL BEGIN DECLARE SECTION\nchar isbn[lO];\nlong qty;\nEXEC SQL END DECLARE SECTION\n/ / set isbn and qty to some values\nEXEC SQL CALL AddInventory(:isbn,:qty);\nCalling Stored Procedures from JDBC\nWe can call stored procedures from JDBC using the CallableStatment class.\nCallableStatement is a subclass of PreparedStatement and provides the same\nfunctionality. A stored procedure could contain multiple SQL staternents or a\nseries of SQL statements-thus, the result could be many different ResultSet\nobjects.\nWe illustrate the case when the stored procedure result is a single\nResultSet.\nCallableStatement cstmt=\nCOIl.prepareCall(\" {call ShowNumberOfOrders}\");\nResultSet rs = cstmt.executeQueryO\nwhile (rs.next())\nCalling Stored Procedures from SQLJ\nThe stored procedure 'ShowNumberOfOrders' is called as follows using SQLJ:\n/ / create the cursor class\n#sql !terator CustomerInfo(int cid, String cname, int count);\n/ / create the cursor\n\nCustomerInfo customerinfo;\n/ / call the stored procedure\n#sql customerinfo = {CALL ShowNumberOfOrders};\nwhile (customerinfo.nextO) {\nSystem.out.println(customerinfo.cid() + \",\" +\ncustomerinfo.count()) ;\n}\n6.5.3\nSQLIPSM\nCHAPTER (5\nAll major databa...<;e systems provide ways for users to write stored procedures in\na simple, general purpose language closely aligned with SQL. In this section, we\nbriefly discuss the SQL/PSM standard, which is representative of most vendor-\nspecific languages. In PSM, we define modules, which are collections of stored\nprocedures, temporary relations, and other declarations.\nIn SQL/PSM, we declare a stored procedure as follows:\nCREATE PROCEDURE name (parameter1,... , parameterN)\nlocal variable declarations\nprocedure code;\nWe can declare a function similarly as follows:\nCREATE FUNCTION name (parameterl,... , parameterN)\nRETURNS sqIDataType\nlocal variable declarations\nfunction code;\nEach parameter is a triple consisting of the mode (IN, OUT, or INOUT as\ndiscussed in the previous section), the parameter name, and the SQL datatype\nof the parameter. We can seen very simple SQL/PSM procedures in Section\n6.5.1. In this case, the local variable declarations were empty, and the procedure\ncode consisted of an SQL query.\nWe start out with an example of a SQL/PSM function that illustrates the\nmain SQL/PSM constructs. The function takes as input a customer identified\nby her cid and a year. The function returns the rating of the customer, which\nis defined a...'3 follows: Customers who have bought more than ten books during\nthe year are rated 'two'; customer who have purcha...<;ed between 5 and 10 books\nare rated 'one', otherwise the customer is rated 'zero'. The following SQL/PSM\ncode computes the rating for a given customer and year.\nCREATE PROCEDURE RateCustomer\n\nDatabase Appl'ication Development\n(IN custId INTEGER, IN year INTEGER)\nRETURNS INTEGER\nDECLARE rating INTEGER;\nDECLARE numOrders INTEGER;\nSET numOrders =\n(SELECT COUNT(*) FROM Orders 0 WHERE O.tid = custId);\nIF (numOrders>10) THEN rating=2;\nELSEIF (numOrders>5) THEN rating=1;\nELSE rating=O;\nEND IF;\nRETURN rating;\nLet us use this example to give a short overview of some SQL/PSM constructs:\n•\nWe can declare local variables using the DECLARE statement. In our exam-\nple, we declare two local variables: 'rating', and 'numOrders'.\n•\nPSM/SQL functions return values via the RETURN statement. In our ex-\nample, we return the value of the local variable 'rating'.\n•\nvVe can assign values to variables with the SET statement. In our example,\nwe assigned the return value of a query to the variable 'numOrders'.\n•\nSQL/PSM h&<; branches and loops. Branches have the following form:\nIF (condition) THEN statements;\nELSEIF statements;\nELSEIF statements;\nELSE statements; END IF\nLoops are of the form\nLOOP\nstaternents:\nEND LOOP\n•\nQueries can be used as part of expressions in branches; queries that return\na single ;ralue can be assigned to variables as in our example above.\n•\n'We can use the same cursor statements &s in Embedded SQL (OPEN, FETCH,\nCLOSE), but we do not need the EXEC SQL constructs, and variables do not\nhave to be prefixed by a colon ':'.\nWe only gave a very short overview of SQL/PSM; the references at the end of\nthe chapter provide more information.\n\nCHAPTER €i\n6.6\nCASE STUDY: THE INTERNET BOOK SHOP\nDBDudes finished logical database design, as discussed in Section 3.8, and now\nconsider the queries that they have to support. They expect that the applica-\ntion logic will be implemented in Java, and so they consider JDBC and SQLJ as\npossible candidates for interfacing the database system with application code.\nRecall that DBDudes settled on the following schema:\nBooks(isbn: CHAR(10), title: CHAR(8), author: CHAR(80),\nqty_in_stock: INTEGER, price: REAL, year_published: INTEGER)\nCustomers( cid: INTEGER, cname: CHAR(80), address: CHAR(200))\nOrders(ordernum: INTEGER, isbn: CHAR(lO), cid: INTEGER,\ncardnum: CHAR(l6), qty: INTEGER, order_date: DATE, ship_date: DATE)\nNow, DBDudes considers the types of queries and updates that will arise. They\nfirst create a list of tasks that will be performed in the application.\nTasks\nperformed by customers include the following.\nII\nCustomers search books by author name, title, or ISBN.\n..\nCustomers register with the website.\nRegistered customers might want\nto change their contact information.\nDBDudes realize that they have to\naugment the Customers table with additional information to capture login\nand password information for each customer; we do not discuss this aspect\nany further.\nIII\nCustomers check out a final shopping basket to complete a sale.\nIII\nCustomers add and delete books from a 'shopping basket' at the website.\n..\nCustomers check the status of existing orders and look at old orders.\nAdministrative ta.'3ks performed by employees of B&N are listed next.\nII\nEmployees look up customer contact information.\nIII\nEmployees add new books to the inventory.\n..\nEmployees fulfill orders, and need to update the shipping date of individual\nbooks.\n..\nEmployees analyze the data to find profitable customers and customers\nlikely to respond to special marketing campaigns.\nNext, DBDudes consider the types of queries that will a,rise out of these tasks.\nTo support searching for books by name, author, title, or ISBN, DBDudes\ndecide to write a stored procedure as follows:\n\nDatabase Application Development\nCREATE PROCEDURE SearchByISBN (IN book.isbn CHAR (10) )\nSELECT B.title, B.author,\nB.qty_in~'3tock,B.price, B.yeaLpublished\nFROM\nBooks B\nWHERE\nB.isbn = book.isbn\nPlacing an order involves inserting one or more records into the Orders table.\nSince DBDudes has not yet chosen the Java-based technology to program the\napplication logic, they assume for now that the individual books in the order\nare stored at the application layer in a Java array. To finalize the order, they\nwrite the following JDBC code shown in Figure 6.11, which inserts the elements\nfrom the array into the Orders table.\nNote that this code fragment assumes\nseveral Java variables have been set beforehand.\nString sql = \"INSERT INTO Orders VALUES(7, 7, 7, 7, 7, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\ncon.setAutoCommit(false);\ntry {\n/ / orderList is a vector of Order objects\n/ / ordernum is the current order number\n/ / dd is the ID of the customer, cardnum is the credit card number\nfor (int i=O; iiorderList.lengthO; i++)\n/ / now instantiate the parameters with values\nOrder currentOrder = orderList[i];\npstmt.clearParameters() ;\npstmt.setInt(l, ordernum);\npstmt.setString(2, Order.getlsbnO);\npstmt.setInt(3, dd);\npstmt.setString(4, creditCardNum);\npstmt.setlnt(5, Order.getQtyO);\npstmt.setDate(6, null);\npstmt.executeUpdate();\n}\ncon.commit();\ncatch (SqLException e){\ncon.rollbackO;\nSystem.out.println(e.getMessage());\n}\nInserting a Completed Order into the Database",
          "pages": [
            245,
            246,
            247,
            248,
            249,
            250
          ],
          "relevance": {
            "score": 0.18,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER (}\nDBDudes writes other JDBC code and stored procedures for all of the remain-\ning tasks. They use code similar to some of the fragments that we have seen in\nthis chapter.\nII\nEstablishing a connection to a database, as shown in Figure 6.2.\nII\nAdding new books to the inventory, a'3 shown in Figure 6.3.\nII\nProcessing results from SQL queries a'3 shown in Figure 6.4-\nII\nFor each customer, showing how many orders he or she has placed.\nWe\nshowed a sample stored procedure for this query in Figure 6.8.\nII\nIncrea'3ing the available number of copies of a book by adding inventory,\nas shown in Figure 6.9.\nII\nRanking customers according to their purchases, as shown in Figure 6.10.\nDBDudcs takes care to make the application robust by processing exceptions\nand warnings, as shown in Figure 6.6.\nDBDudes also decide to write a trigger, which is shown in Figure 6.12. When-\never a new order is entered into the Orders table, it is inserted with ship~date\nset to NULL. The trigger processes each row in the order and calls the stored\nprocedure 'UpdateShipDate'. This stored procedure (whose code is not shown\nhere) updates the (anticipated) ship_date of the new order to 'tomorrow', in\ncase qtyjlLstock of the corresponding book in the Books table is greater than\nzero. Otherwise, the stored procedme sets the ship_date to two weeks.\nCREATE TRIGGER update_ShipDate\nAFTER INSERT ON Orders\nFOR EACH ROW\nBEGIN CALL UpdatcShipDate(new); END\n1* Event *j\n1* Action *j\nTrigger to Update the Shipping Date of New Orders\n6.7\nREVIEW QUESTIONS\nAnswers to the i'eview questions can be found in the listed sections.\nlYl\nvVhy is it not straightforward to integrate SQL queries with a host pro-\ngramming language? (Section 6.1.1)\nIIii\nHow do we declare variables in Ernbcdded SQL? (Section 6.1.1)\n\nDatabase Applicat'ion Deuelop'Tnent\n'*\n•\nHow do we use SQL statements within a host langl.lage? How do we check\nfor errors in statement execution? (Section 6.1.1)\n•\nExplain the impedance mismatch between host languages and SQL, and\ndescribe how cursors address this. (Section 6.1.2)\n•\n'\\That properties can cursors have? (Section 6.1.2)\n•\nWhat is Dynamic SQL and how is it different from Embedded SQL? (Sec-\ntion 6.1.3)\n•\nWhat is JDBC and what are its advantages? (Section 6.2)\n•\nWhat are the components of the JDBC architecture? Describe four differ-\nent architectural alternatives for JDBC drivers. (Section 6.2.1)\n•\nHow do we load JDBC drivers in Java code? (Section 6.3.1)\n•\nHow do we manage connections to data sources?\nWhat properties can\nconnections have? (Section 6.3.2)\n•\nWhat alternatives does JDBC provide for executing SQL DML and DDL\nstatements? (Section 6.3.3)\n•\nHow do we handle exceptions and warnings in JDBC? (Section 6.3.5)\n•\n'What functionality provides the DatabaseMetaDataclass? (Section 6.3.6)\n•\nWhat is SQLJ and how is it different from JDBC? (Section 6.4)\n•\nvVhy are stored procedures important? How do we declare stored proce-\ndures and how are they called from application code? (Section 6.5)\nEXERCISES\nExercise 6.1 Briefly answer the following questions.\n1. Explain the following terms: Cursor, Embedded SQL, JDBC, SQLJ, stored procedure.\n2. What are the differences between JDBC and SQLJ? \\Nhy do they both exist?\n3. Explain the term stored procedure, and give examples why stored procedures are useful.\nExercise 6.2 Explain how the following steps are performed in JDBC:\n1. Connect to a data source.\n2. Start, commit, and abort transactions.\n3. Call a stored procedure.\nHow are these steps performed in SQLJ?\n\nCHAPTER (:)\nExercise 6.3 Compare exception handling and handling of warnings ill embedded SQL, dy-\nnamic SQL, .IDBC, and SQL.I.\nExercise 6.4 Answer the following questions.\n1. Why do we need a precompiler to translate embedded SQL and SQL.J? Why do we not\nneed a precompiler for .IDBC?\n2. SQL.J and embedded SQL use variables in the host language to pass parameters to SQL\nqueries, whereas .JDBC uses placeholders marked with a ''1'. Explain the difference, and\nwhy the different mechanisms are needed.\nExercise 6.5 A dynamic web site generates HTML pages from information stored in a\ndatabase.\nWhenever a page is requested, is it dynamically assembled from static data and\ndata in a database, resulting in a database access.\nConnecting to the database is usually\na\ntime~consuming process, since resources need to be allocated, and the user needs to be\nauthenticated.\nTherefore, connection pooling--setting up a pool of persistent database\nconnections and then reusing them for different requests can significantly improve the per-\nformance of database-backed websites.\nSince servlets can keep information beyond single\nrequests, we can create a connection pool, and allocate resources from it to new requests.\nWrite a connection pool class that provides the following methods:\nIII\nCreate the pool with a specified number of open connections to the database system.\n11II\nObtain an open connection from the pool.\nIII\nRelease a connection to the pool.\nIII\nDestroy the pool and close all connections.\nPROJECT-BASED EXERCISES\nIn the following exercises, you will create database-backed applications. In this chapter, you\nwill create the parts of the application that access the database.\nIn the next chapter, you\nwill extend this code to other &'3pects of the application. Detailed information about these\nexercises and material for more exercises can be found online at\nhttp://www.cs.wisc.edu/-dbbook\nExercise 6.6 Recall the Notown Records database that you worked with in Exercise 2.5 and\nExercise 3.15.\nYou have now been tasked with designing a website for Notown. It should\nprovide the following functionality:\nIII\nUsen; can sem'ch for records by name of the musician, title of the album, and Bame of\nthe song.\n11II\nUsers can register with the site, and registered users ca.n log on to the site. Once logged\non, users should not have to log on again unless they are inactive for a long time.\nIII\nUsers who have logged on to the site can add items to a shopping basket.\n11II\nUsers with items in their shopping basket can check out and ma.ke a purchase.\n\nDatabase Apphcation De'velopment\nNOtOWIl wants to use JDBC to access the datab&<;e,\n\\¥rite .JDBC code that performs the\nnecessary data access and manipulation. You will integrate this code with application logic\nand presentation in the next chapter.\nIf Notown had chosen SQLJ instead of JDBC, how would your code change?\nExercise 6.7 Recall the database schema for Prescriptions-R-X that you created in\nExer~\ncise 2.7.\nThe Prescriptions-R-X chain of pharmacies has now engaged you to design their\nnew website. The website has two different classes of users: doctors and patients. Doctors\nshould be able to enter new prescriptions for their patients and modify existing prescriptions.\nPatients should be able to declare themselves as patients of a doctor; they should be able\nto check the status of their prescriptions online; and they should be able to purchase the\nprescriptions online so that the drugs can be shipped to their home address.\nFollow the analogous steps from Exercise 6.6 to write JDBC code that performs the nec-\nessary data access and manipulation. You will integrate this code with application logic and\npresentation in the next chapter.\nExercise 6.8 Recall the university database schema that you worked with in Exercise 5.l.\nThe university has decided to move enrollment to an online system.\nThe website has two\ndifferent classes of users: faculty and students. Faculty should be able to create new courses\nand delete existing courses, and students should be able to enroll in existing courses.\nExercise 6.9 Recall the airline reservation schema that you worked on in Exercise 5.3. De-\nsign an online airline reservation system. The reservation system will have two types of users:\nairline employees, and airline passengers. Airline employees can schedule new flights and can-\ncel existing flights. Airline passengers can book existing flights from a given destination.\nBIBLIOGRAPHIC NOTES\nInformation on ODBC can be found on Microsoft's web page (www.microsoft.com/data/odbc),\nand information on JDBC can be found on tlw Java web page (j ava. sun. com/products/jdbc).\nThere exist rnany books on ODBC, for example, Sanders' ODBC Developer's Guicle [652] and\nthe lvIicrosoft ODBC SDK [5:3;3].\nBooks on JDBC include works by Hamilton et al. [359],\nReese [621], and White et a!. [773].",
          "pages": [
            251,
            252,
            253,
            254
          ],
          "relevance": {
            "score": 0,
            "sql_score": 1.0,
            "concept_score": 0.17,
            "non_sql_penalty": 0.5,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "A foreign key constraint is a rule that enforces referential integrity between two tables in a database. It ensures that all values in a column (or set of columns) of one table match the values in another table's primary key or unique column.",
        "explanation": "Foreign key constraints are crucial for maintaining data consistency and ensuring that relationships between tables are correctly maintained. Here’s how they work step-by-step:\n1. **Declaration**: You define a foreign key constraint on a column (or set of columns) in one table, specifying which column(s) in another table it should reference.\n2. **Enforcement**: The database system enforces this constraint to ensure that only valid values are inserted or updated in the foreign key column. If an attempt is made to insert a value that does not exist in the referenced table's primary key, the operation will fail.\n3. **Usage**: Foreign keys are used when you have a one-to-many relationship between two tables. For example, if you have a 'Students' table and a 'Grades' table, each grade record would reference the student ID from the Students table.",
        "key_points": [
          "A foreign key constraint ensures that data in one table is consistent with another table's primary key or unique column.",
          "It prevents invalid data entry by ensuring referential integrity between tables.",
          "Foreign keys are essential for maintaining relationships and consistency in relational databases."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- CREATE a foreign key constraint ALTER TABLE Grades ADD CONSTRAINT fk_student FOREIGN KEY (student_id) REFERENCES Students(student_id);",
            "explanation": "This example shows how to add a foreign key constraint to the 'Grades' table, ensuring that each grade record has a valid student ID that exists in the 'Students' table.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- INSERT data with a foreign key INSERT INTO Grades (student_id, subject, grade) VALUES (101, 'Math', 95);",
            "explanation": "This practical example demonstrates inserting a new grade record into the 'Grades' table. The student ID must exist in the 'Students' table for this operation to succeed.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using an invalid foreign key value",
            "incorrect_code": "-- Attempting to INSERT an invalid foreign key INSERT INTO Grades (student_id, subject, grade) VALUES (999, 'Math', 85); -- This student ID does NOT exist in the Students TABLE;",
            "correct_code": "-- Correct way to INSERT a valid foreign key INSERT INTO Grades (student_id, subject, grade) VALUES (101, 'Math', 85); -- Assuming 101 is a valid student ID;",
            "explanation": "A common mistake is trying to insert data with a foreign key value that does not exist in the referenced table. Always ensure the foreign key value exists before inserting or updating."
          }
        ],
        "practice": {
          "question": "Create a practical question that tests understanding of this concept",
          "solution": "Provide a clear solution with explanation"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Database Appl'imtion Developrnent\nSystem.out.println(\"Name:\" + md.getDriverNameO\n+ \"; version:\" + mcl.getDriverVersion());\n~\nThe DatabaseMetaData object has many more methods (in JDBC 2.0, exactly\n134",
      "content_relevance": {
        "score": 0,
        "sql_score": 1.0,
        "concept_score": 0.33,
        "non_sql_penalty": 0.5,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "insert": {
      "id": "insert",
      "title": "INSERT Statement",
      "definition": "Adding new rows to database tables",
      "difficulty": "beginner",
      "page_references": [
        260,
        261,
        262,
        263,
        264,
        265,
        266,
        267,
        268,
        269,
        270,
        271,
        272
      ],
      "sections": {
        "definition": {
          "text": "Internet Applications\n<H1>Barns and Nobble Internet Bookstore</H1>\nOur inventory:\n<H3>Science</H3>\n<B>The Character of Physical Law</B>\nThe HTTP response message has three parts:\na status line, several header\nlines, and the body of the message (which contains the actual object that the\nclient requested). The status line has three fields (analogous to the request\nline of the HTTP request message): the HTTP version (HTTP/1.1), a status\ncode (200), and an associated server message (OK). Common status codes and\nassociated messages are:\n•\n200 OK: The request succeeded and the object is contained in the body of\nthe response message\";\n•\n400 Bad Request: A generic error code indicating that the request could\nnot be fulfilled by the server.\n•\n404 Not Found: The requested object does not exist on the server.\n•\n505 HTTP Version Not Supported: The HTTP protocol version that the\nclient uses is not supported by the server. (Recall that the HTTP protocol\nversion sent in the client's request.)\nOur example has three header lines: The date header line indicates the time\nand date when the HTTP response was created (not that this is not the object\ncreation time). The Last-Modified header line indicates when the object was\ncreated. The Content-Length header line indicates the number of bytes in the\nobject being sent after the last header line.\nThe Content-Type header line\nindicates that the object in the entity body is HTML text.\nThe client (the Web browser) receives the response message, extracts the HTML\nfile, parses it, and displays it. In doing so, it might find additional URIs in the\nfile, and it then uses the HTTP protocol to retrieve each of these resources,\nestablishing a new connection each time.\nOne important issue is that the HTTP protocol is a stateless protocol. Every\nmessage----from, the client to the HTTP server and vice-versa-is self-contained,\nand the connection established with a request is maintained only until the\nresponse message is sent. The protocol provides no mechanism to automatically\n'remember' previous interactions between client and server.\nThe stateless nature of the HTTP protocol has a major impact on how Inter-\nnet applications are written. Consider a user who interacts with our exalIlple\n\nCHAPTER ,7\nbookstore application.\nAssume that the bookstore permits users to log into\nthe site and then carry out several actions, such as ordering books or changing\ntheir address, without logging in again (until the login expires or the user logs\nout). How do we keep track of whether a user is logged in or not? Since HTTP\nis stateless, we cannot switch to a different state (say the 'logged in' state) at\nthe protocol level. Instead, for every request that the user (more precisely, his\nor her Web browser) sends to the server, we must encode any state information\nrequired by the application, such as the user's login status. Alternatively, the\nserver-side application code must maintain this state information and look it\nup on a per-request basis. This issue is explored further in Section 7.7.5.\nNote that the statelessness of HTTP is a tradeoff between ease of implementa-\ntion of the HTTP protocol and ease of application development. The designers\nof HTTP chose to keep the protocol itself simple, and deferred any functionality\nbeyond the request of objects to application layers above the HTTP protocol.\n7.3\nHTML DOCUMENTS\nIn this section and the next, we focus on introducing HTML and XML. In\nforms that capture user input, communicate with an HTTP server, and convert\nthe results produced by the data management layer into one of these formats.\nHTML is a simple language used to describe a document. It is also called a\nmarkup language because HTML works by augmenting regular text with\n'marks' that hold special meaning for a Web browser. Commands in the lan-\nguage, called tags, consist (usually) of a start tag and an end tag of the\nform <TAG> and </TAG>, respectively. For example, consider the HTML frag-\nment shown in Figure 7.1. It describes a webpage that shows a list of books.\nThe document is enclosed by the tags <HTML> and </HTML>, marking it as an\nHTML document.\nThe remainder of the document-enclosed in <BODY> ...\n</BoDY>-contains information about three books. Data about each book is\nrepresented as an unordered list (UL) whose entries are marked with the LI\ntag. HTML defines the set of valid tags as well 8.'3 the meaning of the tags. :For\nexample, HTML specifies that the tag <TITLE> is a valid tag that denotes the\ntitle of the document.\nAs another example, the tag <UL> always denotes an\nunordered list.\nAudio, video, and even programs (written in Java, a highly portable language)\ncan be included in HTML documents. vVhen a user retrieves such a document\nusing a suitable browser, images in the document arc displayed, audio and video\nclips are played, and embedded programs are executed at the uset's machine;\nthe result is a rich multimedia presentation. The e8.\"ie with which HTML docu-\n\nInternet Applications\n<HTML>\n<HEAD>\n</HEAD>\n<BODY>\n<Hl>Barns and Nobble Internet Bookstore</Hl>\nOur inventory:\n<H3>Science</H3>\n<B>The Character of Physical Law</B>\n<UL>\n<LI>Author: Richard Feynman</LI>\n<LI>Published 1980</LI>\n<Ll>Hardcover</LI>\n</UL>\n<H3>Fiction</H3>\n<B>Waiting for the Mahatma</B>\n<UL>\n<LI>Author: R.K. Narayan</LI>\n<LI>Published 1981</Ll>\n</UL>\n<B>The English Teacher</B>\n<UL>\n<LI>Author: R.K. Narayan</LI>\n<LI>Published 1980</LI>\n<LI>Paperback</LI>\n</UL>\n</BODY>\n</HTML>\nBook Listing in HTML\n»\nments can be created--there are now visual editors that automatically generate\nHTML----and accessed using Internet browsers has fueled the explosive growth\nof the Web.\n7.4\nXML DOCUMENTS\nIn this section, we introduce XML a.'3 a document format, and consider how\napplications can utilize XML. Managing XML documents in a DBMS poses\nseveral new challenges; we discuss this a.'3pect of XML in Chapter 27.\n\nCHAPTER l\nvVhile HTl\\.fL can be used to mark up documents for display purposes, it is\nnot adequate to describe the structure of the content for more general applica-\ntions. For example, we can send the HTML document shown in Figure 7.1 to\nanother application that displays it, but the second application cannot distin-\nguish the first names of authors from their last names. (The application can\ntry to recover such information by looking at the text inside the tags, but this\ndefeats the purpose of using tags to describe document structure.) Therefore,\nHTML is unsuitable for the exchange of complex documents containing product\nspecifications or bids, for example.\nExtensible Markup Language (XML) is a markup language developed to\nremedy the shortcomings of HTML. In contrast to a fixed set of tags whose\nmeaning is specified by the language (as in HTML), XML allows users to de-\nfine new collections of tags that can be used to structure any type of data or\ndocument the user wishes to transmit. XML is an important bridge between\nthe document-oriented view of data implicit in HTML and the schema-oriented\nview of data that is central to a DBMS. It has the potential to make database\nsystems more tightly integrated into Web applications than ever before.\nXML emerged from the confluence of two technologies, SGML and HTML. The\nStandard Generalized Markup Language (SGML) is a metalanguage\nthat allows the definition of data and document interchange languages such as\nHTML. The SGML standard was published in 1988, and many organizations\nthat rnanage a large number of complex documents have adopted it. Due to its\ngenerality, SGML is complex and requires sophisticated programs to harness\nits full potential.\nXML was developed to have much of the power of SGML\nwhile remaining relatively simple.\nNonetheless, XML, like SGML, allows the\ndefinition of new document markup languages.\nAlthough XML does not prevent a user from designing tags that encode the\ndisplay of the data in a Web browser, there is a style language for XML called\nExtensible Style Language (XSL). XSL is a standard way of describing\nhow an XML docmnent that adheres to a certain vocabulary of tags should be\ndisplayed.\n7.4.1\nIntroduction to XML\nVve use the smaJI XML docmnent shown in Figure 7.2 a,s an example.\n11II\nElements: Elements, also called tags, a.rc the primary building blocks of\nan XML docmnent. The start of the content of an element ELM is marked\nwith <ELM>, which is called the start tag, and the end of the content end\nis marked with </ELM>, called the end tag.\nIn our example document.",
          "pages": [
            260,
            261,
            262,
            263
          ],
          "relevance": {
            "score": 0.3,
            "sql_score": 1.0,
            "concept_score": 0.6,
            "non_sql_penalty": 0.3,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Internet Applicai'ions\nThe Design Goals ofXML: XML wa..\"l developed'startingin 1996 by a\nworking group under guidance of the ';Yorld Wide Web Consortium (W3C)\nXML Special Interest Group.\nThe design goals for XML included the\nfollowing:\n1. XML should be compatible with SGML.\n2. It should be easy to write programs that process XML documents.\n3. The design of XML should be formal and concise.\nthe element BOOKLIST encloses all information in the sample document.\nThe element BOOK demarcates all data associated with a single book.\nXML elements are case sensitive:\nthe element BOOK is different from\nBook.\nElements must be properly nested. Start tags that appear inside\nthe content of other tags must have a corresponding end tag. For example,\nconsider the following XML fragment:\n<BOOK>\n<AUTHOR>\n<FIRSTNAME>Richard</FIRSTNAME>\n<LASTNAME>Feynluan</LASTNAME>\n</AUTHOR>\n</BOOK>\nThe element AUTHOR is completely nested inside the element BOOK, and\nboth the elements LASTNAME and FIRSTNAME are nested inside the element\nAUTHOR.\n..\nAttributes: An element can have descriptive attributes that provide ad-\nditional information about the element. The values of attributes are set\ninside the start tag of an element. For example, let ELM denote an element\nwith the attribute att. We can set the value of att to value through the\nfollowing expression:\n<ELM att=\" value II >.\nAll attribute values must be\nenclosed in quotes.\nIn Figure 7.2, the element BOOK has two attributes.\nThe attribute GENRE indicates the genre of the book (science or fiction)\nand the attribute FORMAT indicates whether the book is a hardcover or a\npaperback.\nIII\nEntity References: Entities are shortcuts for portions of common text or\nthe content of external files, and we call the usage of an entity in the XML\ndocument an entity reference. Wherever an entity reference appears in\nthe document, it is textually replaced by its content.\nEntity references\nstart with a '&' and end with a '; '. Five predefined entities in XML are\nplaceholders for chara.cters with special meaning in XML. For example, the\n\nCHAPTER~7\n<?xml version=11.0\" encoding=\"UTF-S Il standalone=llyes ll?>\n<BOOKLIST>\n<BOOK GENRE=\" Science\" FORMAT=\" Hardcover\" >\n<AUTHOR>\n<FIRSTNAME>Richard</FIRSTNAME>\n<LASTNAME>Feynman</LASTNAME>\n</AUTHOR>\n<TITLE>The Character of Physical Law</TITLE>\n<PUBLISHED>1980</PUBLISHED>\n</BOOK>\n<BOOK> GENRE=\" Fiction\" >\n<AUTHOR>\n<FIRSTNAME>R.K.</FIRSTNAME>\n<LASTNAME>Narayan</LASTNAME>\n</AUTHOR>\n<TITLE>Waiting for the Mahatma</TITLE>\n<PUBLISHED>1981</PUBLISHED>\n</BOOK>\n<BOOK GENRE=\" Fiction\" >\n<AUTHOR>\n<FIRSTNAME>R.K.</FIRSTNAME>\n<LASTNAME>Narayan</LASTNAME>\n</AUTHOR>\n<TITLE>The English Teacher</TITLE>\n<PUBLISHED>1980</PUBLISHED>\n</BOOK>\n</BOOKLIST>\nBook Information in XML\n< character that marks the beginning of an XML command is reserved and\nhas to be represented by the entity It. The other four reserved characters\nare &, >, \", and '; they are represented by the entities amp, gt, quot,\nand apos.\nFor example, the text '1 < 5' has to be encoded in an XML\ndocument &'3 follows:\n&apos; 1&1t ;5&apos;. We can also use entities to\ninsert arbitrary Unicode characters into the text. Unicode is a standard\nfor character representations, similar to ASCII. For example, we can display\nthe Japanese Hiragana character a using the entity reference &#x3042.\n•\nComments: We can insert comments anywhere in an XML document.\nComments start with <! - and end with ->. Comments can contain arbi-\ntrary text except the string --.\n\nInternet Applications\n•\nDocument Type Declarations (DTDs): In XML, we can define our\nown markup language. A DTD is a set of rules that allows us to specify\nour own set of elements, attributes, and entities. Thus, a DTD is basically\na grammar that indicates what tags are allowed, in what order they can\nappear, and how they can be nested. We discuss DTDs in detail in the\nnext section.\nWe call an XML document well-formed if it has no associated DTD but\nfollows these structural guidelines:\n•\nThe document starts with an XML declaration. An example of an XML\ndeclaration is the first line of the XML document shown in Figure 7.2.\n•\nA root element contains all the other elements. In our example, the root\nelement is the element BOOKLIST.\n•\nAll elements must be properly nested. This requirement states that start\nand end tags of an element must appear within the same enclosing element.\n7.4.2\nXML DTDs\nA DTD is a set of rules that allows us to specify our own set of elements,\nattributes, and entities. A DTD specifies which elements we can use and con-\nstraints on these elements, for example, how elements can be nested and where\nelements can appear in the document. We call a document valid if a DTD is\nassociated with it and the document is structured according to the rules set by\nthe DTD. In the remainder of this section, we use the example DTD shown in\n<!DOCTYPE BOOKLIST [\n<! ELEMENT BOOKLIST (BOOK)*>\n<! ELEMENT BOOK (AUTHOR,TITLE,PUBLISHED?»\n<!ELEMENT AUTHOR (FIRSTNAME,LASTNAME»\n<! ELEMENT FIRSTNAME (#PCDATA»\n<! ELEMENT LASTNAME (#PCDATA»\n<! ELEMENT TITLE (#PCDATA»\n<! ELEMENT PUBLISHED (#PCDATA»\n<! ATTLIST BOOK GENRE (ScienceIFiction) #REQUIRED>\n<!ATTLIST BOOK FORMAT (PaperbackIHardcover) \"Paperback\">\n]>\nBookstore XML DTD\n\nCHAPTER .;{\nA DTD is enclosed in <! DOCTYPE name [DTDdeclarationJ >, where name is\nthe name of the outermost enclosing tag, and DTDdeclaration is the text of\nthe rules of the DTD. The DTD starts with the outermost element---the root\nelenwnt--which is BOOKLIST in our example. Consider the next rule:\n<!ELEMENT BOOKLIST (BOOK)*>\nThis rule tells us that the element BOOKLIST consists of zero or more BOOK\nelements.\nThe * after BOOK indicates how many BOOK elements can appear\ninside the BOOKLIST element. A * denotes zero or more occurrences, a + denotes\none or more occurrences, and a? denotes zero or one occurrence. For example,\nif we want to ensure that a BOOKLIST has at least one book, we could change\nthe rule as follows:\n<!ELEMENT BOOKLIST (BOOK)+>\nLet us look at the next rule:\n<!ELEMENT BOOK (AUTHOR,TITLE,PUBLISHED?»\nThis rule states that a BOOK element contains a AUTHOR element, a TITLE ele-\nment, and an optional PUBLISHED clement. Note the use of the? to indicate\nthat the information is optional by having zero or one occurrence of the element.\nLet us move ahead to the following rule:\n< !ELEMENT LASTNAME (#PCDATA»\nUntil now we considered only elements that contained other elements.\nThis\nrule states that LASTNAME is an element that does not contain other elements,\nbut contains actual text. Elements that only contain other elements are said\nto have element content, whereas elements that also contain #PCDATA are\n::laid to have mixed content. In general, an element type declaration has the\nfollowing structure:\n< !ELEMENT (contentType»\nFive possible content types are:\nIII\nOther elements.\nII\nThe special syrnbol #PCDATA, which indicates (parsed) character data.\nII\nThe special symbol EMPTY, which indicates that the element has no content.\nElements that have no content are not required to have an end tag.\n11II\nThe special symbol ANY, which indicates that any content is permitted.\nThis content should be avoided whenever possible ::lince it disables all check-\ning of the document structure inside the element.\n\nInternet Apphcat'ions\n2~3\n•\nA regular expression constructed from the preceding four choices.\nA\nregular expression is one of the following:\n- expL exp2, exp3: A list of regular expressions.\n- exp*: An optional expression (zero or more occurrences).\n- exp?: An optional expression (zero or one occurrences).\n- exp+: A mandatory expression (one or more occurrences).\n- expl\nI exp2: expl or exp2.\nAttributes of elements are declared outside the element. For example, consider\nthe following attribute declaration from Figure 7.3:\n<! ATTLIST BOOK GENRE (ScienceIFiction) #REQUIRED»\nThis XML DTD fragment specifies the attribute GENRE, which is an attribute\nof the element BOOK. The attribute can take two values:\nScience or Fiction.\nEach BOOK element must be described in its start tag by a GENRE attribute\nsince the attribute is required as indicated by #REQUIRED. Let us look at the\ngeneral structure of a DTD attribute declaration:\n<! ATTLIST elementName (attName attType default)+>\nThe keyword ATTLIST indicates the beginning of an attribute declaration. The\nstring elementName is the name of the element with which the following at-\ntribute dcfinition is associated. What follows is the declaration of one or more\nattributes. Each attribute has a name, as indicated by attName, and a type,\nas indicated by attType. XML defines several possible types for an attribute.\nWe discuss only string types and enumerated types here. An attribute of\ntype string can take any string as a value. We can declare such an attribute by\nsetting its type field to CDATA. F'or example, we can declare a third attribute of\ntype string of the elernent BOOK a.s follows:\n<!ATTLIST BOOK edition CDATA \"1\">\nIf an attribute has an enumerated type, we list all its possible values in the\nattribute declaration. In our example, the itttribute GENRE is an enumerated\nattribute type; its possible attribute values are 'Science' and 'Fiction'.\nThe last part 'Of an attribute declaration is called its default specification.\nThe DTD in Figure 7.:3 shows two different default specifications: #REQUIRED\nitnd the string 'Pitperback'. The default specification #REQUIRED indicates that\nthe attribute is required and whenever its associated element itppears some-\nwhere in the XML document\n~t value for the attribute must be specified. The\ndebult specification indicated by the string 'Paperback' indicates that the at-\ntribute is not required; whenever its a.')sociated element itppears without setting\n\n<?xml version=11.0\" encoding=IUTF-8\" standalone=\"no\"?>\n<! DOCTYPE BOOKLIST SYSTEM\" books.dtd\" >\n<BOOKLIST>\n<BOOK GENRE=\" Science\" FORMAT=\" Hardcover\" >\n<AUTHOR>\nBook Information in XML\nXML Schema: The DTD mechanism has several limitations, in spite of\nits widespread use.\nFor example, elements and attributes cannot be as-\nsigned types in a flexible way, and elements are always ordered, even if the\napplication does not require this. XML Schema is a new W3C proposal\nthat provides a more powerful way to describe document structure than\nDTDs; it is a superset of DTDs, allowing legacy data to be handled eas-\nily.\nAn interesting aspect is that it supports uniqueness and foreign key\nconstraints.\na value for the attribute, the attribute automatically takes the value 'Paper-\nback'. For example, we can make the attribute value 'Science' the default value\nfor the GENRE attribute as follows:\n<! ATTLIST BOOK GENRE (ScienceIFiction) \"Science\" >\nIn our bookstore example, the XML document with a reference to the DTD is\nshown in Figure 7.4.\n7.4.3\nDomain-Specific DTDs\nRecently, DTDs have been developed for several specialized domains-including\na wide range of commercial, engineering, financial, industrial, and scientific\ndomains----and a lot of the excitement about XML h3...<; its origins in the belief\nthat more and more standardized DTDs will be developed. Standardized DTDs\nwould enable seamless data exchange among heterogeneous sources, a problem\nsolved today either by implementing specialized protocols such as Electronic\nData Interchange (EDI) or by implementing ad hoc solutions.\nEven in an environment where all XML data is valid, it is not possible to\nstraightforwardly integrate several XML documents by matching elements in\ntheir DTDs, because even when two elements have identical names in two\ndifferent DTDs, the meaning of the elements could be completely different.\nIf both documents use a single, standard DTD, we avoid this problem. The",
          "pages": [
            264,
            265,
            266,
            267,
            268,
            269
          ],
          "relevance": {
            "score": 0.34,
            "sql_score": 0.8,
            "concept_score": 0.6,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Internet Applications\ndevelopment of standardized DTDs is more a social process than a research\nproblem, since the major players in a given domain or industry segment have\nto collaborate.\nFor example, the mathematical markup language (MathML) has been\ndeveloped for encoding mathematical material on the Web.\nThere are two\ntypes of MathML elements. The 28 presentation elements describe the lay-\nout structure of a document; examples are the mrow element, which indicates a\nhorizontal row of characters, and the msup element, which indicates a base and a\nsubscript. The 75 content elements describe mathematical concepts. An ex-\nample is the plus element, which denotes the addition operator. (A third type\nof element, the math element, is used to pass parameters to the MathML pro-\ncessor.) MathML allows us to encode mathematical objects in both notations\nsince the requirements of the user of the objects might be different. Content\nelements encode the precise mathematical meaning of an object without ambi-\nguity, and the description can be used by applications such as computer algebra\nsystems. On the other hand, good notation can suggest the logical structure to\na human and emphasize key aspects of an object; presentation elements allow\nus to describe mathematical objects at this level.\nFor example, consider the following simple equation:\nx 2 - 4x - 32 = 0\nUsing presentation elements, the equation is represented as follows:\n<mrow>\n<mrow> <msup><mi>x</mi><mn>2</mn></msup>\n<mo>-</mo>\n<mrow><mn>4</mn>\n<mo>&invisibletimes;</mo>\n<mi>x</mi>\n</mrow>\n<mo>-</ mo><mn>32</ mn>\n</mrow><mo>=</mo><mn>O</nm>\n</mrow>\nUsing content elements, the equation is described as follows:\n<reln><eq/>\n<apply>\n<minus/>\n<apply> <power/> <ci>x</ci> <cn>2</cn> </apply>\n<apply> <times/> <cn>4</cn> <ci>x</ci> </apply>\n<cn>32</cn>\n\n</apply> <cn>O</cn>\n</reln>\nCHAPTER J7\nNote the additional power that we gain from using MathML instead of en-\ncoding the formula in HTML. The common way of displaying mathematical\nobjects inside an HTML object is to include images that display the objects,\nfor example, as in the following code fragment:\n<IMG SRC=lIimages/equation.gifll ALI=II x**2 - 4x - 32 = 10 II >\nThe equation is encoded inside an IMG tag with an alternative display format\nspecified in the ALI tag. Using this encoding of a mathematical object leads\nto the following presentation problems.\nFirst, the image is usually sized to\nmatch a certain font size, and on systems with other font sizes the image is\neither too small or too large. Second, on systems with a different background\ncolor, the picture does not blend into the background and the resolution of the\nimage is usually inferior when printing the document.\nApart from problems\nwith changing presentations, we cannot easily search for a formula or formula\nfragments on a page, since there is no specific markup tag.\n7.5\nTHE THREE-TIER APPLICATION ARCHITECTURE\nIn this section, we discuss the overall architecture of data-intensive Internet\napplications. Data-intensive Internet applications can be understood in terms\nof three different functional components: data management, application logic,\nand pTesentation. The component that handles data mallgement usually utilizes\na DBMS for data storage, but application logic and presentation involve much\nmore than just the DBMS itself.\nWe start with a short overview of the history of database-backed application\narchitectures, and introduce single-tier and client-server architectures in Section\n7.5.1. \\Ve explain the three-tier architecture in detail in Section 7.5.2, and show\nits advantages in Section 7.5.3.\n7.5.1\nSingle-Tier and Client-Server Architectures\nIn this section, we provide some perspective on the three-tier architecture by\ndiscussing single-tier and client-server architectures, the predecessors of the\nthree-tier architecture. Initially, data-intensive applications were combined into\na single tier, including the DBMS, application logic, and user interface, a\"\nillustrated in Figure 7.5. The application typically ran on a mainframe, and\nusers accessed it through dumb teT'minals that could perform only data input\nand display.\nThis approach ha..s the benefit of being easily maintained by a\ncentral administrator.\n\nInteTnet Applications\nClient\nApplication Logic\nDBMS\nj\nA Single-Tier Architecture\nA Two-Server Architecture: Thin Clients\nSingle-tier architectures have a,n important drawback: Users expect graphical\ninterfaces that require much more computational power than simple dumb ter-\nminals.\nCentralized computation of the graphical displays of such interfaces\nrequires much more computational power than a single server hclS available,\nand thus single-tier architectures do not scale to thousands of users. The com-\nmoditization of the PC and the availability of cheap client computers led to\nthe developlnent of the two-tier architecture.\nTwo-tier architectures, often also referred to a<; client-server architec-\ntures, consist of a client computer and a server computer, which interact\nthrough a well-defined protocol. What part of the functionality the client im-\nplements, and what part is left to the server, can vary. In the traditional client-\nserver architecture, the client implements just the graphical user interface,\nand the server. implements both the business logic and the data management;\nsuch clients are often called thin clients, and this architecture is illustra,ted in\nOther divisions are possible, such as more powerful clients that hnplement both\nuser interface and business logic, or clients that implement user interface and\npart of the business logic, with the remaining part being implemented at the",
          "pages": [
            270,
            271,
            272
          ],
          "relevance": {
            "score": 0.31,
            "sql_score": 0.7,
            "concept_score": 0.6,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The INSERT statement is used to add new rows of data into a table in a database. It's essential for populating tables with initial data and updating them as needed.",
        "explanation": "The INSERT statement solves the problem of adding new records to an existing table. Here’s how it works:\n1. **Specify the Table**: You start by naming the table where you want to insert the new row.\n2. **List Columns (Optional)**: If you don’t specify all columns, you must provide values for all non-nullable columns and default values for any that can be omitted.\n3. **Provide Values**: You then list the values corresponding to each column in the order they appear in the table or by explicitly naming the columns.\n\nYou use INSERT when:\n- Adding new products to an inventory system.\n- Recording user sign-ups on a website.\n- Updating employee records with their latest performance data.\n\nKey things to remember:\n- Always ensure that all required values are provided, either through column listing or default values.\n- Be mindful of data types and constraints when inserting values.\n- Use transactions for bulk inserts to maintain data integrity.",
        "key_points": [
          "Always provide values for all non-nullable columns unless using defaults.",
          "Specify columns explicitly if not adding values for all columns.",
          "Use transactions for bulk inserts to ensure data consistency."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "INSERT INTO books (title, author, published_year) VALUES ('The Character of Physical Law', 'Richard Feynman', 1980);",
            "explanation": "This example inserts a new book into the `books` table with title, author, and publication year.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "INSERT INTO users (username, email, registration_date) VALUES ('john_doe', 'john@example.com', CURRENT_DATE);",
            "explanation": "This practical example adds a new user to the `users` table with username, email, and the current date as the registration date.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to provide values for non-nullable columns",
            "incorrect_code": "INSERT INTO books (title) VALUES ('The Character of Physical Law');",
            "correct_code": "INSERT INTO books (title, author, published_year) VALUES ('The Character of Physical Law', 'Richard Feynman', 1980);",
            "explanation": "This mistake occurs when you try to insert a row without providing values for all non-nullable columns. Always ensure all required values are provided."
          }
        ],
        "practice": {
          "question": "Insert a new customer into the `customers` table with the following details: name = 'Jane Smith', email = 'jane@example.com'.",
          "solution": "INSERT INTO customers (name, email) VALUES ('Jane Smith', 'jane@example.com');\nThis solution correctly inserts a new customer into the `customers` table with the specified name and email."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Internet Applications\n<H1>Barns and Nobble Internet Bookstore</H1>\nOur inventory:\n<H3>Science</H3>\n<B>The Character of Physical Law</B>\nThe HTTP response message has three parts:\na status line, severa",
      "content_relevance": {
        "score": 0.4,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.4,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "update": {
      "id": "update",
      "title": "UPDATE Statement",
      "definition": "Modifying existing data in tables",
      "difficulty": "beginner",
      "page_references": [
        268,
        269,
        270,
        271,
        272,
        273,
        274,
        275,
        276,
        277,
        278,
        279,
        280,
        281
      ],
      "sections": {
        "definition": {
          "text": "Internet Apphcat'ions\n2~3\n•\nA regular expression constructed from the preceding four choices.\nA\nregular expression is one of the following:\n- expL exp2, exp3: A list of regular expressions.\n- exp*: An optional expression (zero or more occurrences).\n- exp?: An optional expression (zero or one occurrences).\n- exp+: A mandatory expression (one or more occurrences).\n- expl\nI exp2: expl or exp2.\nAttributes of elements are declared outside the element. For example, consider\nthe following attribute declaration from Figure 7.3:\n<! ATTLIST BOOK GENRE (ScienceIFiction) #REQUIRED»\nThis XML DTD fragment specifies the attribute GENRE, which is an attribute\nof the element BOOK. The attribute can take two values:\nScience or Fiction.\nEach BOOK element must be described in its start tag by a GENRE attribute\nsince the attribute is required as indicated by #REQUIRED. Let us look at the\ngeneral structure of a DTD attribute declaration:\n<! ATTLIST elementName (attName attType default)+>\nThe keyword ATTLIST indicates the beginning of an attribute declaration. The\nstring elementName is the name of the element with which the following at-\ntribute dcfinition is associated. What follows is the declaration of one or more\nattributes. Each attribute has a name, as indicated by attName, and a type,\nas indicated by attType. XML defines several possible types for an attribute.\nWe discuss only string types and enumerated types here. An attribute of\ntype string can take any string as a value. We can declare such an attribute by\nsetting its type field to CDATA. F'or example, we can declare a third attribute of\ntype string of the elernent BOOK a.s follows:\n<!ATTLIST BOOK edition CDATA \"1\">\nIf an attribute has an enumerated type, we list all its possible values in the\nattribute declaration. In our example, the itttribute GENRE is an enumerated\nattribute type; its possible attribute values are 'Science' and 'Fiction'.\nThe last part 'Of an attribute declaration is called its default specification.\nThe DTD in Figure 7.:3 shows two different default specifications: #REQUIRED\nitnd the string 'Pitperback'. The default specification #REQUIRED indicates that\nthe attribute is required and whenever its associated element itppears some-\nwhere in the XML document\n~t value for the attribute must be specified. The\ndebult specification indicated by the string 'Paperback' indicates that the at-\ntribute is not required; whenever its a.')sociated element itppears without setting\n\n<?xml version=11.0\" encoding=IUTF-8\" standalone=\"no\"?>\n<! DOCTYPE BOOKLIST SYSTEM\" books.dtd\" >\n<BOOKLIST>\n<BOOK GENRE=\" Science\" FORMAT=\" Hardcover\" >\n<AUTHOR>\nBook Information in XML\nXML Schema: The DTD mechanism has several limitations, in spite of\nits widespread use.\nFor example, elements and attributes cannot be as-\nsigned types in a flexible way, and elements are always ordered, even if the\napplication does not require this. XML Schema is a new W3C proposal\nthat provides a more powerful way to describe document structure than\nDTDs; it is a superset of DTDs, allowing legacy data to be handled eas-\nily.\nAn interesting aspect is that it supports uniqueness and foreign key\nconstraints.\na value for the attribute, the attribute automatically takes the value 'Paper-\nback'. For example, we can make the attribute value 'Science' the default value\nfor the GENRE attribute as follows:\n<! ATTLIST BOOK GENRE (ScienceIFiction) \"Science\" >\nIn our bookstore example, the XML document with a reference to the DTD is\nshown in Figure 7.4.\n7.4.3\nDomain-Specific DTDs\nRecently, DTDs have been developed for several specialized domains-including\na wide range of commercial, engineering, financial, industrial, and scientific\ndomains----and a lot of the excitement about XML h3...<; its origins in the belief\nthat more and more standardized DTDs will be developed. Standardized DTDs\nwould enable seamless data exchange among heterogeneous sources, a problem\nsolved today either by implementing specialized protocols such as Electronic\nData Interchange (EDI) or by implementing ad hoc solutions.\nEven in an environment where all XML data is valid, it is not possible to\nstraightforwardly integrate several XML documents by matching elements in\ntheir DTDs, because even when two elements have identical names in two\ndifferent DTDs, the meaning of the elements could be completely different.\nIf both documents use a single, standard DTD, we avoid this problem. The\n\nInternet Applications\ndevelopment of standardized DTDs is more a social process than a research\nproblem, since the major players in a given domain or industry segment have\nto collaborate.\nFor example, the mathematical markup language (MathML) has been\ndeveloped for encoding mathematical material on the Web.\nThere are two\ntypes of MathML elements. The 28 presentation elements describe the lay-\nout structure of a document; examples are the mrow element, which indicates a\nhorizontal row of characters, and the msup element, which indicates a base and a\nsubscript. The 75 content elements describe mathematical concepts. An ex-\nample is the plus element, which denotes the addition operator. (A third type\nof element, the math element, is used to pass parameters to the MathML pro-\ncessor.) MathML allows us to encode mathematical objects in both notations\nsince the requirements of the user of the objects might be different. Content\nelements encode the precise mathematical meaning of an object without ambi-\nguity, and the description can be used by applications such as computer algebra\nsystems. On the other hand, good notation can suggest the logical structure to\na human and emphasize key aspects of an object; presentation elements allow\nus to describe mathematical objects at this level.\nFor example, consider the following simple equation:\nx 2 - 4x - 32 = 0\nUsing presentation elements, the equation is represented as follows:\n<mrow>\n<mrow> <msup><mi>x</mi><mn>2</mn></msup>\n<mo>-</mo>\n<mrow><mn>4</mn>\n<mo>&invisibletimes;</mo>\n<mi>x</mi>\n</mrow>\n<mo>-</ mo><mn>32</ mn>\n</mrow><mo>=</mo><mn>O</nm>\n</mrow>\nUsing content elements, the equation is described as follows:\n<reln><eq/>\n<apply>\n<minus/>\n<apply> <power/> <ci>x</ci> <cn>2</cn> </apply>\n<apply> <times/> <cn>4</cn> <ci>x</ci> </apply>\n<cn>32</cn>",
          "pages": [
            268,
            269,
            270
          ],
          "relevance": {
            "score": 0.33,
            "sql_score": 0.6,
            "concept_score": 0.5,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "</apply> <cn>O</cn>\n</reln>\nCHAPTER J7\nNote the additional power that we gain from using MathML instead of en-\ncoding the formula in HTML. The common way of displaying mathematical\nobjects inside an HTML object is to include images that display the objects,\nfor example, as in the following code fragment:\n<IMG SRC=lIimages/equation.gifll ALI=II x**2 - 4x - 32 = 10 II >\nThe equation is encoded inside an IMG tag with an alternative display format\nspecified in the ALI tag. Using this encoding of a mathematical object leads\nto the following presentation problems.\nFirst, the image is usually sized to\nmatch a certain font size, and on systems with other font sizes the image is\neither too small or too large. Second, on systems with a different background\ncolor, the picture does not blend into the background and the resolution of the\nimage is usually inferior when printing the document.\nApart from problems\nwith changing presentations, we cannot easily search for a formula or formula\nfragments on a page, since there is no specific markup tag.\n7.5\nTHE THREE-TIER APPLICATION ARCHITECTURE\nIn this section, we discuss the overall architecture of data-intensive Internet\napplications. Data-intensive Internet applications can be understood in terms\nof three different functional components: data management, application logic,\nand pTesentation. The component that handles data mallgement usually utilizes\na DBMS for data storage, but application logic and presentation involve much\nmore than just the DBMS itself.\nWe start with a short overview of the history of database-backed application\narchitectures, and introduce single-tier and client-server architectures in Section\n7.5.1. \\Ve explain the three-tier architecture in detail in Section 7.5.2, and show\nits advantages in Section 7.5.3.\n7.5.1\nSingle-Tier and Client-Server Architectures\nIn this section, we provide some perspective on the three-tier architecture by\ndiscussing single-tier and client-server architectures, the predecessors of the\nthree-tier architecture. Initially, data-intensive applications were combined into\na single tier, including the DBMS, application logic, and user interface, a\"\nillustrated in Figure 7.5. The application typically ran on a mainframe, and\nusers accessed it through dumb teT'minals that could perform only data input\nand display.\nThis approach ha..s the benefit of being easily maintained by a\ncentral administrator.\n\nInteTnet Applications\nClient\nApplication Logic\nDBMS\nj\nA Single-Tier Architecture\nA Two-Server Architecture: Thin Clients\nSingle-tier architectures have a,n important drawback: Users expect graphical\ninterfaces that require much more computational power than simple dumb ter-\nminals.\nCentralized computation of the graphical displays of such interfaces\nrequires much more computational power than a single server hclS available,\nand thus single-tier architectures do not scale to thousands of users. The com-\nmoditization of the PC and the availability of cheap client computers led to\nthe developlnent of the two-tier architecture.\nTwo-tier architectures, often also referred to a<; client-server architec-\ntures, consist of a client computer and a server computer, which interact\nthrough a well-defined protocol. What part of the functionality the client im-\nplements, and what part is left to the server, can vary. In the traditional client-\nserver architecture, the client implements just the graphical user interface,\nand the server. implements both the business logic and the data management;\nsuch clients are often called thin clients, and this architecture is illustra,ted in\nOther divisions are possible, such as more powerful clients that hnplement both\nuser interface and business logic, or clients that implement user interface and\npart of the business logic, with the remaining part being implemented at the\n\n//~\\\nI--\"-\"-'---~-~I\n1\\/\nChent\n.. j\nI\n\\ I\nApplication Logic I\ni\ni\n/\n~.\n__-1\nClient\nApplication Logic\nA Two-Tier Architecture: Thick Clients\nCHAPTERt7\nserver level; such clients are often called thick clients, and this architecture is\nillustrated in Figure 7.7.\nCompared to the single-tier architecture, two-tier architectures physically sep-\narate the user interface from the data management layer. To implement two-\ntier architectures, we can no longer have dumb terminals on the client side;\nwe require computers that run sophisticated presentation code (and possibly,\napplication logic).\nOver the last ten years, a large number of client-server development tools such\nMicrosoft Visual Basic and Sybase Powerbuilder have been developed. These\ntools permit rapid development of client-server software, contributing to the\nsuccess of the client-server model, especially the thin-client version.\nThe thick-client model has several disadvantages when compared to the thin-\nclient model. First, there is no central place to update and maintain the busi-\nness logic, since the application code runs at many client sites. Second, a large\namount of trust is required between the server and the clients. As an exam--\npIe, the DBMS of a bank has to trust the (application executing at an) ATM\nmachine to leave the database in a consistent state. (One way to address this\nproblem is through stored procedures, trusted application code that is registered\nwith the DBMS and can be called from SQL statelnents. 'Ve discuss stored\nprocedures in detail in Section 6.5.)\nA third disadvantage of the thick-client architecture is that it does not scale\nwith the number of clients; it typically cannot handle more than a few hundred\nclients.\nThe application logic at the client issues SQL queries to the server\nand the server returns the query result to the client, where further processing\ntakes place. Large query results might be transferred between client and server.\n\nInter-net Applications\n2:19\nApplication\nLogic\nClient\n• • •\nClient\nA Standard Three-Tier Architecture\n(Stored procedures can mitigate this bottleneck.) Fourth, thick-client systems\ndo not scale as the application accesses more and more database systems. As-\nsume there are x different database systems that are accessed by y clients, then\nthere are x . y different connections open at any time, clearly not a scalable\nsolution.\nThese disadvantages of thick-client systems and the widespread adoption of\nstandard, very thin clients~notably,Web browsers~haveled to the widespread\nuse thin-client architectures.\n7.5.2\nThree~Tier Architectures\nThe thin-client two-tier architecture essentially separates presentation issues\nfrom the rest of the application.\nThe three-tier architecture goes one step\nfurther, and also separates application logic from data management:\nIII\nPresentation Tier: Users require a natural interface to make requests,\nprovide input, and to see results. The widespread use of the Internet has\nmade Web-based interfaces increasingly popular.\nIII\nMiddle Tier: The application logic executes here.\nAn enterprise-class\napplication reflects complex business processes, and is coded in a general\npurpose language such as C++ or Java.\nIII\nData Management Tier: Data-intensive Web applications involve DBMSs,\nwhich are the subject of this book.\nDifferent technologies have\nbeen developed to enable distribution of the three tiers of an application across\nmultiple hardware platforms and different physical sites. Figure 7.9 shows the\ntechnologies relevant to each tier.\n\n~i~-----\",\".~._,----~~---.~u·\"r-----··'\"\n,\nClient Program\nI\nJavaScript\n(Web Br_~=:~)__~_~_~~~~~__________\nJ\nHTIP\n---:;;\"\"io:~~I--\"\nservle~~~----l\nJSP\nI\n~PPlication Server) ~__.__.__ .....XSLT\n~\nJDBe. SQLJ\nData Storage---I----- XML -\n(Database system)__\n___ Stored p:cedure~~__\nTechnologies for the Three Tiers\nOverview of the Presentation Tier\nCHAPTERi' 7\nAt the presentation layer, we need to provide forms through which the user\ncan issue requests, and display responses that the middle tier generates. The\nhypertext markup language (HTML) discussed in Section 7.3 is the basic data\npresentation language.\nIt is important that this layer of code be easy to adapt to different display\ndevices and formats; for example, regular desktops versus handheld devices\nversus cell phones. This adaptivity can be achieved either at the middle tier\nthrough generation of different pages for different types of client, or directly at\nthe client through style sheets that specify how the data should be presented.\nIn the latter case, the middle tier is responsible for producing the appropriate\ndata in response to user requests, whereas the presentation layer decides how\nto display that information.\n\\Ve cover presentation tier technologies, including style sheets, in Section 7.6.\nOverview of the Middle Tier\nThe middle layer runs code that implements the business logic of the applica-\ntion: It controls what data needs to be input before an action can be executed,\ndetermines the control flow between multi-action steps, controls access to the\ndatabase layer, and often assembles dynamically generated HTML pages from\ndataba\"se query results.\n\nInternet Applications\n24;1\nThe middle tier code is responsible for supporting all the different roles involved\nin the application. For example, in an Internet shopping site implementation,\nwe would like customers to be able to browse the catalog and make purchases,\nadministrators to be able to inspect current inventory, and possibly data ana-\nlysts to ask summary queries about purchase histories. Each of these roles can\nrequire support for several complex actions.\nFor example, consider the a customer who wants to buy an item (after browsing\nor searching the site to find it). Before a sale can happen, the customer has\nto go through a series of steps: She has to add items to her shopping ba.sket,\nshe has to provide her shipping address and credit card number (unless she has\nan account at the site), and she has to finally confirm the sale with tax and\nshipping costs added. Controlling the flow among these steps and remembering\nalready executed steps is done at the middle tier of the application. The data\ncarried along during this series of steps might involve database accesses, but\nusually it is not yet permanent (for example, a shopping basket is not stored\nin the database until the sale is confirmed).\nWe cover the middle tier in detail in Section 7.7.\n7.5.3\nAdvantages of the Three-Tier Architecture\nThe three-tier architecture has the following advantages:\n1/\nHeterogeneous Systems: Applications can utilize the strengths of dif-\nferent platforms and different software components at the different tiers.\nIt is easy to modify or replace the code at any tier without affecting the\nother tiers.\nII\nThin Clients: Clients only need enough computation power for the pre-\nsentation layer. Typically, clients are Web browsers.\nII\nIntegrated Data Access: In many applications, the data must be ac-\ncessed from several sources.\nThis can be handled transparently at the\nmiddle tier, where we can centrally manage connections to all database\nsystems involved.\nII\nScalabilit,y to Many Clients: Each client is lightweight and all access to\nthe system is through the middle tier. The middle tier can share database\nconnections across clients, and if the middle tier becomes the bottle-neck,\nwe can deploy several servers executing the middle tier code; clients can\nconnect to anyone of these servers, if the logic is designed appropriately.\nThis is illustrated in Figure 7.10, which also shows how the middle tier\naccesses multiple data sources. Of course, we rely upon the DBMS for each\n\nCHAPTER\nMiddle~Tier Replication and Access to Multiple Data Sources\ndata source to be scalable (and this might involve additional parallelization\nor replication, as discussed in Chapter 22).\n•\nSoftware Development Benefits: By dividing the application cleanly\ninto parts that address presentation, data access, and business logic, we\ngain many advantages. The business logic is centralized, and is therefore\neasy to maintain, debug, and change.\nInteraction between tiers occurs\nthrough well-defined, standardized APls. Therefore, each application tier\ncan be built out of reusable components that can be individually developed,\ndebugged, and tested.\n7.6\nTHE PRESENTATION LAYER\nIn this section, we describe technologies for the client side of the three-tier ar-\nchitecture. vVe discuss HTML forms as a special means of pa.ssing arguments\nfrom the client to the middle tier (i.e., from the presentation tier to the middle\ntier) in Section 7.6.1. In Section 7.6.2, we introduce JavaScript, a Java-based\nscripting language that can be used for light-weight computation in the client\ntier (e.g., for simple animations). We conclude our discussion of client-side tech-\nnologies by presenting style sheets in Section 7.6.3. Style sheets are languages\nthat allow us to present the same webpage with different formatting for clients\nwith different presentation capabilities; for example, Web browsers versus cell\nphones, or even a Netscape browser versus Microsoft's Internet Explorer.\n7.6.1\nHTML Forms\nHTML forms are a common way of communicating data from the client tier to\nthe middle tier. The general format of a form is the following:\n<FORM ACTION=\"page.jsp\" METHOD=\"GET\" NAME=\"LoginForm\">",
          "pages": [
            271,
            272,
            273,
            274,
            275,
            276,
            277
          ],
          "relevance": {
            "score": 0.48,
            "sql_score": 1.0,
            "concept_score": 0.75,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Internet Applications\n</FORM>\nA single HTML document can contain more than one form. Inside an HTML\nform, we can have any HTML tags except another FORM element.\nThe FORM tag has three important attributes:\n•\nACTION: Specifies the URI of the page to which the form contents are\nsubmitted; if the ACTION attribute is absent, then the URI of the current\npage is used. In the sample above, the form input would be submited to\nthe page named page. j sp, which should provide logic for processing the\ninput from the form.\n(We will explain methods for reading form data at\nthe middle tier in Section 7.7.)\n•\nMETHOD: The HTTP/1.0 method used to submit the user input from the\nfilled-out form to the webserver. There are two choices, GET and POST; we\npostpone their discussion to the next section.\n•\nNAME: This attribute gives the form a name.\nAlthough not necessary,\nnaming forms is good style.\nIn Section 7.6.2, we discuss how to write\nclient-side programs in JavaScript that refer to forms by name and perform\nchecks on form fields.\nInside HTML forms, the INPUT, SELECT, and TEXTAREA tags are used to specify\nuser input elements; a form can have many elements of each type. The simplest\nuser input element is an INPUT field, a standalone tag with no terminating tag.\nAn example of an INPUT tag is the following:\n<INPUT TYPE=ltext\" NAME=\"title\">\nThe INPUT tag has several attributes. The three most important ones are TYPE,\nNAME, and VALUE. The TYPE attribute determines the type of the input field. If\nthe TYPE attribute h&'3 value text, then the field is a text input field. If the\nTYPE attribute has value password, then the input field is a text field where the\nentered characters are displayed as stars on the screen. If the TYPE attribute\nhas value reset, it is a simple button that resets all input fields within the\nform to their default values. If the TYPE attribute has value submit, then it is\na button that sends the values of the different input fields in the form to the\nserver. Note that reset and submit input fields affect the entire form.\nThe NAME attribute of the INPUT tag specifies the symbolic name for this field\nand is used to identify the value of this input fi.eld when it is sent to the server.\nNAME has to be set for INPUT tags of all types except submit and reset. In the\npreceding example, we specified title as the NAME of the input field.\n\nCHAPTER' 7\nThe VALUE attribute of an input tag can be used for text or password fields to\nspecify the default contents of the field.\nFor submit or reset buttons, VALUE\ndetermines the label of the button.\nThe form in Figure 7.11 shows two text fields, one regular text input field and\none password field. It also contains two buttons, a reset button labeled 'Reset\nValues' and a submit button labeled 'Log on.' Note that the two input fields\nare named, whereas the reset and submit button have no NAME attributes.\n<FORM ACTION=\"page.jsp\" METHoD=\"GET\" NAME=\"LoginForm\">\n<INPUT TYPE=\"text\" NAME=\"username\" VALUE=\" Joe\"><P>\n<INPUT TYPE=\"password\" NAME=\"p&ssword\"><P>\n<INPUT TYPE=\"reset\" VALUE=\"Reset Values\"><P>\n<INPUT TYPE=\"submit\" VALUE=\"Log on\">\n</FoRM>\nHTl'vlL Form with Two Text Fields and Two Buttons\nHTML forms have other ways of specifying user input, such as the aforemen-\ntioned TEXTAREA and SELECT tags; we do not discuss them.\nPassing Arguments to Server~Side Scripts\nAs mentioned at the beginning of Section 7.6.1, there are two different ways to\nsubmit HTML Form data to the webserver. If the method GET is used, then\nthe contents of the form are assembled into a query URI (as discussed next)\nand sent to the server. If the method POST is used, then the contents of the\nform are encoded as in the GET method, but the contents are sent in a separate\ndata block instead of appending them directly to the URI. Thus, in the GET\nmethod the form contents are directly visible to the user as the constructed\nURI, whereas in the POST method, the form contents are sent inside the HTTP\nrequest message body and are not visible to the user.\nUsing the GET method gives users the opportunity to bookmark the page with\nthe constructed URI and thus directly jump to it in subsequent sessions; this\nis not possible with the POST method. The choice of GET versus POST should\nbe determined' by the application and its requirements.\nLet us look at the encoding of the URI when the GET method is used.\nThe\nencoded URI has the following form:\naction?name1=vallle1&name2=value2&name;J=value3\n\nInternet Applicat'icJns\nThe action is the URI specified in the ACTION attribute to the FORM tag, or the\ncurrent document URI if no ACTION attribute was specified. The 'name=value'\npairs are the user inputs from the INPUT fields in the form.\nFor form INPUT\nfields where the user did not input anything, the name is stil present with an\nempty value (name=). As a concrete example, consider the PCl,.'3sword submission\nform at the end of the previous section.\nAssume that the user inputs 'John\nDoe' as username, and 'secret' as password. Then the request URI is:\npage.jsp?username=J01111+Doe&password=secret\nThe user input from forms can contain general ASCII characters, such as the\nspace character, but URIs have to be single, consecutive strings with no spaces.\nTherefore, special characters such as spaces, '=', and other unprintable charac-\nters are encoded in a special way. To create a URI that has form fields encoded,\nwe perform the following three steps:\n1. Convert all special characters in the names and values to '%xyz,' where\n'xyz' is the ASCII value of the character in hexadecimal. Special characters\ninclude =, &, %, +, and other unprintable characters. Note that we could\nencode all characters by their ASCII value.\n2. Convert all space characters to the '+' character.\n3. Glue corresponding names and values from an individual HTML INPUT tag\ntogether with '=' and then paste name-value pairs from different HTML\nINPUT tags together using'&' to create a request URI of the form:\naction?namel=value1&name2=value2&name3=value3\nNote that in order to process the input elements from the HTML form at\nthe middle tier, we need the ACTION attribute of the FORM tag to point to a\npage, script, or program that will process the values of the form fields the user\nentered. We discuss ways of receiving values from form fields in Sections 7.7.1\nand 7.7.3.\n7.6.2\nJavaScript\nJavaScript is a scripting language at the client tier with which we can add\nprograms to webpages that run directly at the client (Le., at the machine run-\nning the Web !)rowser).\nJavaScript is often used for the following types of\ncomputation at the client:\nIII\nBrowser Detection: J avaScript can be used to detect the browser type\nand load a browser-specific page.\nIII\nForm Validation: JavaScript is used to perform simple consistency checks\non form fields. For example, a JavaScript program might check whether a",
          "pages": [
            278,
            279,
            280
          ],
          "relevance": {
            "score": 0.16,
            "sql_score": 0.8,
            "concept_score": 0.25,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The UPDATE statement is used to modify existing records in a database table. It allows you to change data without having to delete and reinsert rows.",
        "explanation": "The UPDATE statement solves the problem of changing data in an existing table efficiently. Here’s how it works step-by-step:\n1. Identify the table that needs updating.\n2. Specify the new values for the columns.\n3. Define which records should be updated using a WHERE clause to target specific rows.\n\nYou use UPDATE when you need to change data in your database, such as correcting an error or adding new information.",
        "key_points": [
          "Key point 1: Always include a WHERE clause to avoid updating the entire table.",
          "Key point 2: Use SET to specify the new values for columns.",
          "Key point 3: Common mistakes include forgetting the WHERE clause, which updates all rows instead of just the intended ones."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- UPDATE a single column UPDATE employees SET salary = 50000 WHERE employee_id = 1;",
            "explanation": "This example updates the salary of an employee with ID 1 to $50,000.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- UPDATE multiple columns UPDATE orders SET status = 'Shipped', shipped_date = CURRENT_DATE WHERE order_id = 123;",
            "explanation": "This practical example updates the status and shipped date for an order with ID 123.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting the WHERE clause",
            "incorrect_code": "-- Incorrect: Updates all rows UPDATE employees SET salary = 50000;",
            "correct_code": "-- Correct: Updates specific row UPDATE employees SET salary = 50000 WHERE employee_id = 1;",
            "explanation": "Always include a WHERE clause to target the correct rows. Without it, all rows in the table will be updated."
          }
        ],
        "practice": {
          "question": "Update the email address of a customer with ID 456 in the 'customers' table.",
          "solution": "-- Solution: UPDATE customers SET email = 'newemail@example.com' WHERE customer_id = 456;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Internet Apphcat'ions\n2~3\n•\nA regular expression constructed from the preceding four choices.\nA\nregular expression is one of the following:\n- expL exp2, exp3: A list of regular expressions.\n- exp*: An",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "delete": {
      "id": "delete",
      "title": "DELETE Statement",
      "definition": "Removing rows from database tables",
      "difficulty": "beginner",
      "page_references": [
        275,
        276,
        277,
        278,
        279,
        280,
        281,
        282,
        283,
        284,
        285,
        286
      ],
      "sections": {
        "definition": {
          "text": "~i~-----\",\".~._,----~~---.~u·\"r-----··'\"\n,\nClient Program\nI\nJavaScript\n(Web Br_~=:~)__~_~_~~~~~__________\nJ\nHTIP\n---:;;\"\"io:~~I--\"\nservle~~~----l\nJSP\nI\n~PPlication Server) ~__.__.__ .....XSLT\n~\nJDBe. SQLJ\nData Storage---I----- XML -\n(Database system)__\n___ Stored p:cedure~~__\nTechnologies for the Three Tiers\nOverview of the Presentation Tier\nCHAPTERi' 7\nAt the presentation layer, we need to provide forms through which the user\ncan issue requests, and display responses that the middle tier generates. The\nhypertext markup language (HTML) discussed in Section 7.3 is the basic data\npresentation language.\nIt is important that this layer of code be easy to adapt to different display\ndevices and formats; for example, regular desktops versus handheld devices\nversus cell phones. This adaptivity can be achieved either at the middle tier\nthrough generation of different pages for different types of client, or directly at\nthe client through style sheets that specify how the data should be presented.\nIn the latter case, the middle tier is responsible for producing the appropriate\ndata in response to user requests, whereas the presentation layer decides how\nto display that information.\n\\Ve cover presentation tier technologies, including style sheets, in Section 7.6.\nOverview of the Middle Tier\nThe middle layer runs code that implements the business logic of the applica-\ntion: It controls what data needs to be input before an action can be executed,\ndetermines the control flow between multi-action steps, controls access to the\ndatabase layer, and often assembles dynamically generated HTML pages from\ndataba\"se query results.\n\nInternet Applications\n24;1\nThe middle tier code is responsible for supporting all the different roles involved\nin the application. For example, in an Internet shopping site implementation,\nwe would like customers to be able to browse the catalog and make purchases,\nadministrators to be able to inspect current inventory, and possibly data ana-\nlysts to ask summary queries about purchase histories. Each of these roles can\nrequire support for several complex actions.\nFor example, consider the a customer who wants to buy an item (after browsing\nor searching the site to find it). Before a sale can happen, the customer has\nto go through a series of steps: She has to add items to her shopping ba.sket,\nshe has to provide her shipping address and credit card number (unless she has\nan account at the site), and she has to finally confirm the sale with tax and\nshipping costs added. Controlling the flow among these steps and remembering\nalready executed steps is done at the middle tier of the application. The data\ncarried along during this series of steps might involve database accesses, but\nusually it is not yet permanent (for example, a shopping basket is not stored\nin the database until the sale is confirmed).\nWe cover the middle tier in detail in Section 7.7.\n7.5.3\nAdvantages of the Three-Tier Architecture\nThe three-tier architecture has the following advantages:\n1/\nHeterogeneous Systems: Applications can utilize the strengths of dif-\nferent platforms and different software components at the different tiers.\nIt is easy to modify or replace the code at any tier without affecting the\nother tiers.\nII\nThin Clients: Clients only need enough computation power for the pre-\nsentation layer. Typically, clients are Web browsers.\nII\nIntegrated Data Access: In many applications, the data must be ac-\ncessed from several sources.\nThis can be handled transparently at the\nmiddle tier, where we can centrally manage connections to all database\nsystems involved.\nII\nScalabilit,y to Many Clients: Each client is lightweight and all access to\nthe system is through the middle tier. The middle tier can share database\nconnections across clients, and if the middle tier becomes the bottle-neck,\nwe can deploy several servers executing the middle tier code; clients can\nconnect to anyone of these servers, if the logic is designed appropriately.\nThis is illustrated in Figure 7.10, which also shows how the middle tier\naccesses multiple data sources. Of course, we rely upon the DBMS for each\n\nCHAPTER\nMiddle~Tier Replication and Access to Multiple Data Sources\ndata source to be scalable (and this might involve additional parallelization\nor replication, as discussed in Chapter 22).\n•\nSoftware Development Benefits: By dividing the application cleanly\ninto parts that address presentation, data access, and business logic, we\ngain many advantages. The business logic is centralized, and is therefore\neasy to maintain, debug, and change.\nInteraction between tiers occurs\nthrough well-defined, standardized APls. Therefore, each application tier\ncan be built out of reusable components that can be individually developed,\ndebugged, and tested.\n7.6\nTHE PRESENTATION LAYER\nIn this section, we describe technologies for the client side of the three-tier ar-\nchitecture. vVe discuss HTML forms as a special means of pa.ssing arguments\nfrom the client to the middle tier (i.e., from the presentation tier to the middle\ntier) in Section 7.6.1. In Section 7.6.2, we introduce JavaScript, a Java-based\nscripting language that can be used for light-weight computation in the client\ntier (e.g., for simple animations). We conclude our discussion of client-side tech-\nnologies by presenting style sheets in Section 7.6.3. Style sheets are languages\nthat allow us to present the same webpage with different formatting for clients\nwith different presentation capabilities; for example, Web browsers versus cell\nphones, or even a Netscape browser versus Microsoft's Internet Explorer.\n7.6.1\nHTML Forms\nHTML forms are a common way of communicating data from the client tier to\nthe middle tier. The general format of a form is the following:\n<FORM ACTION=\"page.jsp\" METHOD=\"GET\" NAME=\"LoginForm\">",
          "pages": [
            275,
            276,
            277
          ],
          "relevance": {
            "score": 0.1,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Internet Applications\n</FORM>\nA single HTML document can contain more than one form. Inside an HTML\nform, we can have any HTML tags except another FORM element.\nThe FORM tag has three important attributes:\n•\nACTION: Specifies the URI of the page to which the form contents are\nsubmitted; if the ACTION attribute is absent, then the URI of the current\npage is used. In the sample above, the form input would be submited to\nthe page named page. j sp, which should provide logic for processing the\ninput from the form.\n(We will explain methods for reading form data at\nthe middle tier in Section 7.7.)\n•\nMETHOD: The HTTP/1.0 method used to submit the user input from the\nfilled-out form to the webserver. There are two choices, GET and POST; we\npostpone their discussion to the next section.\n•\nNAME: This attribute gives the form a name.\nAlthough not necessary,\nnaming forms is good style.\nIn Section 7.6.2, we discuss how to write\nclient-side programs in JavaScript that refer to forms by name and perform\nchecks on form fields.\nInside HTML forms, the INPUT, SELECT, and TEXTAREA tags are used to specify\nuser input elements; a form can have many elements of each type. The simplest\nuser input element is an INPUT field, a standalone tag with no terminating tag.\nAn example of an INPUT tag is the following:\n<INPUT TYPE=ltext\" NAME=\"title\">\nThe INPUT tag has several attributes. The three most important ones are TYPE,\nNAME, and VALUE. The TYPE attribute determines the type of the input field. If\nthe TYPE attribute h&'3 value text, then the field is a text input field. If the\nTYPE attribute has value password, then the input field is a text field where the\nentered characters are displayed as stars on the screen. If the TYPE attribute\nhas value reset, it is a simple button that resets all input fields within the\nform to their default values. If the TYPE attribute has value submit, then it is\na button that sends the values of the different input fields in the form to the\nserver. Note that reset and submit input fields affect the entire form.\nThe NAME attribute of the INPUT tag specifies the symbolic name for this field\nand is used to identify the value of this input fi.eld when it is sent to the server.\nNAME has to be set for INPUT tags of all types except submit and reset. In the\npreceding example, we specified title as the NAME of the input field.\n\nCHAPTER' 7\nThe VALUE attribute of an input tag can be used for text or password fields to\nspecify the default contents of the field.\nFor submit or reset buttons, VALUE\ndetermines the label of the button.\nThe form in Figure 7.11 shows two text fields, one regular text input field and\none password field. It also contains two buttons, a reset button labeled 'Reset\nValues' and a submit button labeled 'Log on.' Note that the two input fields\nare named, whereas the reset and submit button have no NAME attributes.\n<FORM ACTION=\"page.jsp\" METHoD=\"GET\" NAME=\"LoginForm\">\n<INPUT TYPE=\"text\" NAME=\"username\" VALUE=\" Joe\"><P>\n<INPUT TYPE=\"password\" NAME=\"p&ssword\"><P>\n<INPUT TYPE=\"reset\" VALUE=\"Reset Values\"><P>\n<INPUT TYPE=\"submit\" VALUE=\"Log on\">\n</FoRM>\nHTl'vlL Form with Two Text Fields and Two Buttons\nHTML forms have other ways of specifying user input, such as the aforemen-\ntioned TEXTAREA and SELECT tags; we do not discuss them.\nPassing Arguments to Server~Side Scripts\nAs mentioned at the beginning of Section 7.6.1, there are two different ways to\nsubmit HTML Form data to the webserver. If the method GET is used, then\nthe contents of the form are assembled into a query URI (as discussed next)\nand sent to the server. If the method POST is used, then the contents of the\nform are encoded as in the GET method, but the contents are sent in a separate\ndata block instead of appending them directly to the URI. Thus, in the GET\nmethod the form contents are directly visible to the user as the constructed\nURI, whereas in the POST method, the form contents are sent inside the HTTP\nrequest message body and are not visible to the user.\nUsing the GET method gives users the opportunity to bookmark the page with\nthe constructed URI and thus directly jump to it in subsequent sessions; this\nis not possible with the POST method. The choice of GET versus POST should\nbe determined' by the application and its requirements.\nLet us look at the encoding of the URI when the GET method is used.\nThe\nencoded URI has the following form:\naction?name1=vallle1&name2=value2&name;J=value3\n\nInternet Applicat'icJns\nThe action is the URI specified in the ACTION attribute to the FORM tag, or the\ncurrent document URI if no ACTION attribute was specified. The 'name=value'\npairs are the user inputs from the INPUT fields in the form.\nFor form INPUT\nfields where the user did not input anything, the name is stil present with an\nempty value (name=). As a concrete example, consider the PCl,.'3sword submission\nform at the end of the previous section.\nAssume that the user inputs 'John\nDoe' as username, and 'secret' as password. Then the request URI is:\npage.jsp?username=J01111+Doe&password=secret\nThe user input from forms can contain general ASCII characters, such as the\nspace character, but URIs have to be single, consecutive strings with no spaces.\nTherefore, special characters such as spaces, '=', and other unprintable charac-\nters are encoded in a special way. To create a URI that has form fields encoded,\nwe perform the following three steps:\n1. Convert all special characters in the names and values to '%xyz,' where\n'xyz' is the ASCII value of the character in hexadecimal. Special characters\ninclude =, &, %, +, and other unprintable characters. Note that we could\nencode all characters by their ASCII value.\n2. Convert all space characters to the '+' character.\n3. Glue corresponding names and values from an individual HTML INPUT tag\ntogether with '=' and then paste name-value pairs from different HTML\nINPUT tags together using'&' to create a request URI of the form:\naction?namel=value1&name2=value2&name3=value3\nNote that in order to process the input elements from the HTML form at\nthe middle tier, we need the ACTION attribute of the FORM tag to point to a\npage, script, or program that will process the values of the form fields the user\nentered. We discuss ways of receiving values from form fields in Sections 7.7.1\nand 7.7.3.\n7.6.2\nJavaScript\nJavaScript is a scripting language at the client tier with which we can add\nprograms to webpages that run directly at the client (Le., at the machine run-\nning the Web !)rowser).\nJavaScript is often used for the following types of\ncomputation at the client:\nIII\nBrowser Detection: J avaScript can be used to detect the browser type\nand load a browser-specific page.\nIII\nForm Validation: JavaScript is used to perform simple consistency checks\non form fields. For example, a JavaScript program might check whether a\n\nCHAPTER~ 7\nform input that asks for an email address contains the character '@,' or if\nall required fields have been input by the user.\n•\nBrowser Control: This includes opening pages in customized windows;\nexamples include the annoying pop-up advertisements that you see at many\nwebsites, which are programmed using JavaScript.\nJ avaScript is usually embedded into an HTML document with a special tag,\nthe SCRIPT tag. The SCRIPT tag has the attribute LANGUAGE, which indicates\nthe language in which the script is written.\nFor JavaScript, we set the lan-\nguage attribute to JavaScript.\nAnother attribute of the SCRIPT tag is the\nSRC attribute, which specifies an external file with JavaScript code that is au-\ntomatically embedded into the HTML document.\nUsually JavaScript source\ncode files use a '.js' extension. The following fragment shows a JavaScript file\nincluded in an HTML document:\n<SCRIPT LANGUAGE=\" JavaScript\" SRC=\"validateForm.js\"> </SCRIPT>\nThe SCRIPT tag can be placed inside HTML comments so that the JavaScript\ncode is not displayed verbatim in Web browsers that do not recognize the\nSCRIPT tag.\nHere is another JavaScipt code example that creates a pop-up\nbox with a welcoming message. We enclose the JavaScipt code inside HTML\ncomments for the reasons just mentioned.\n<SCRIPT LANGUAGE=\" JavaScript\" >\n<I--\nalert (\" Welcome to our bookstore\");\n//-->\n</SCRIPT>\nJavaScript provides two different commenting styles: single-line comments that\nstart with the '//' character, and multi-line comments starting with '/*' and\nending with ,*/' characters.l\nJavaScript has variables that can be numbers, boolean values (true or false),\nstrings, and some other data types that we do not discuss. Global variables have\nto be declared in advance of their usage with the keyword var, and they can\nbe used anywhere inside the HTML documents. Variables local to a JavaScript\nfunction (explained next) need not be declared. Variables do not have a fixed\ntype, but implicitly have the type of the data to which they have been assigned.\n1Actually, '<! --' also marks the start of a single-line comment, which is why we did not have\nto mark the HTML starting cormnent '<! --' in the preceding example using J avaScript comment\nnotation. In contrast, the HTML closing comment \"-->\" has to be commented out in JavaScript as\nit is interpreted otherwise.\n\nInternet Applications\n247,\nJavaScript has the usual assignment operators (=, + =, etc.), the usual arith-\nmetic operators (+, -, *, /, %), the usual comparison operators (==, ! =,\n>=, etc.), and the usual boolean operators (&& for logical AND,\n11 for logical\nOR, and! for negation).\nStrings can be concatenated using the '+' charac-\nter. The type of an object determines the behavior of operators; for example\n1+1 is 2, since we are adding numbers, whereas \"1\"+\"1\" is \"11,\" since we\nare concatenating strings. JavaScript contains the usual types of statements,\nsuch as assignments, conditional statements (if Ccondition) {statements;}\nelse {statements; }), and loops (for-loop, do-while, and while-loop).\nJavaScript allows us to create functions using the function keyword: function\nf Cargl, arg2) {statements;}. We can call functions from JavaScript code,\nand functions can return values using the keyword return.\nWe conclude this introduction to JavaScript with a larger example of a JavaScript\nfunction that tests whether the login and password fields of a HTML form are\nnot empty.\nThe JavaScript code is a function called testLoginEmptyO that tests whether\neither of the two input fields in the form named LoginForm is empty. In the\nfunction testLoginEmpty, we first use variable loginForm to refer to the form\nLoginForm using the implicitly defined variable document, which refers to the\ncurrent HTML page. (JavaScript has a library of objects that are implicitly de-\nfined.) We then check whether either of the strings loginForm. userif. value\nor loginForm. password. value is empty.\nThe function testLoginEmpty is checked within a form event handler.\nAn\nevent handler is a function that is called if an event happens on an object in\na webpage. The event handler we use is onSubmit, which is called if the submit\nbutton is pressed (or if the user presses return in a text field in the form). If\nthe event handler returns true, then the form contents are submitted to the\nserver, otherwise the form contents are not submitted to the server.\nJ avaScript has functionality that goes beyond the basics that we explained in\nthis section; the interested reader is referred to the bibliographic notes at the\nend of this chapter.\n7.6.3\nStyle Sheets\nDifferent clients have different displays, and we need correspondingly different\nways of displaying the same information.\nFor example, in the simplest ca.se,\nwe might need to use different font sizes or colors that provide high-contra.st\non a black-and-white screen. As a more sophisticated example, we might need\nto re-arrange objects on the page to accommodate small screens in personal\n\nCHAPTER 7\n<SCRIPT LANGUAGE==\" JavaScript\">\n<!--\nfunction testLoginEmpty()\n{\n10ginForm = document.LoginForm\nif ((loginForm.userid.value == \"\") II\n(loginFonn.password.value == I. II )) {\nalert(,Please enter values for userid and password.');\nreturn false;\n}\nelse\nreturn true;\n}\n//-->\n</SCRIPT>\n<Hi ALIGN = \"CENTER\" >Barns and Nobble Internet Bookstore</Hi>\n<H3 ALIGN = \"CENTER\">Plec1Se enter your userid and password:</H3>\n<FORM NAME = \"LoginForm ll METHOD=\"POST\"\nACTI ON= IITableOfContents.jsp\"\nonSubmit=\" return testLoginEmptyO\" >\nUserid: <INPUT TYPE=\"TEXT\" NAME=lI userid\"><P>\nPassword: <INPUT TYPE=\"PASSWORD\" NAME=\"password\"><P>\n<INPUT TYPE=\"SUBMIT\" VALUE=\"Login\" NAME=\"SUBMIT\">\n<INPUT TYPE=\"RESET\" VALUE=IIClear Input\" NAME=\"RESET\">\n</FORM>\nForm Validation with JavaScript\ndigital assistants (PDAs).\nAs another example, we might highlight different\ninfonnation to focus on some important part of the page. A style sheet is a\nmethod to adapt the same document contents to different presentation formats.\nA style sheet contains instructions that tell a 'Veb browser (or whatever the\nclient uses to display the webpage) how to translate the data of a document\ninto a presentation that is suitable for the client's display.\nStyle sheets separate the transformative aspect of the page from the ren-\ndering aspects of the page. During transformation, the objects in the XML\ndocument are rearranged to form a different structure, to omit parts of the\nXML document, or to merge two different XML documents into a single docu-\nment. During rendering, we take the existing hierarchical structure of the XML\ndocument and format the document according to the user's display device.",
          "pages": [
            278,
            279,
            280,
            281,
            282,
            283
          ],
          "relevance": {
            "score": 0.07,
            "sql_score": 0.9,
            "concept_score": 0.0,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Inte17u'.t Apphcations\nBODY {BACKGROUND-COLOR: yellow}\nHi {FONT-SIZE: 36pt}\nH3 {COLOR: blue}\nP {MARGIN-LEFT: 50px; COLOR: red}\nAn Example Style sheet\nThe use of style sheets has many advantages. First, we can reuse the same doc-\nument many times and display it differently depending on the context. Second,\nwe can tailor the display to the reader's preference such as font size, color style,\nand even level of detail. Third, we can deal with different output formats, such\nas different output devices (laptops versus cell phones), different display sizes\n(letter versus legal paper), and different display media (paper versus digital\ndisplay). Fourth, we can standardize the display format within a corporation\nand thus apply style sheet conventions to documents at any time.\nFurther,\nchanges and improvements to these display conventions can be managed at a\ncentral place.\nThere are two style sheet languages: XSL and ess. ess was created for HTML\nwith the goal of separating the display characteristics of different formatting\ntags from the tags themselves. XSL is an extension of ess to arbitrary XML\ndocurnents; besides allowing us to define ways of formatting objects, XSL con-\ntains a transformation language that enables us to rearrange objects.\nThe\ntarget files for ess are HTML files, whereas the target files for XSL are XML\nfiles.\nCascading Style Sheets\nA Cascading Style Sheet (CSS) defines how to display HTML elements.\n(In Section 7.13, we introduce a more general style sheet language designed for\nXML documents.)\nStyles are normally stored in style sheets, which are files\nthat contain style definitions.\nMany different HTML documents, such as all\ndocuments in a website, can refer to the same ess. Thus, we can change the\nformat of a website by changing a single file.\nThis is a very convenient way\nof changing the layout of many webpages at the seune time, and a first step\ntoward the separation of content from presentation.\nAn example style sheet is shown in Figure 7.13. It is included into an HTML\nfile with the following line:\n<LINK REL=\"style sheet\" TYPE=\"text/css\"\nHREF=\"books.css\" />\n\nCHAPTER t 7\nEach line in a CSS sheet consists of three parts; a selector, a property, and a\nvalue. They are syntactically arranged in the following way:\nselector {property: value}\nThe selector is the element or tag whose format we are defining. The property\nindicates the tag's attribute whose value we want to set in the style sheet, and\nthe property is the actual value of the attribute. As an example, consider the\nfirst line of the example style sheet shown in Figure 7.13:\nBODY {BACKGROUND-COLOR: yellow}\nThis line has the same effect as changing the HTML code to the following:\n<BODY BACKGROUND-COLOR=\"yellow\" >.\nThe value should always be quoted, as it could consist of several words. More\nthan one property for the same selector can be separated by semicolons as\nshown in the last line of the example in Figure 7.13:\nP {MARGIN-LEFT: 50px; COLOR: red}\nCascading style sheets have an extensive syntax; the bibliographic notes at the\nend of the chapter point to books and online resources on CSSs.\nXSL\nXSL is a language for expressing style sheets. An XSL style sheet is, like CSS,\na file that describes how to display an XML document of a given type. XSL\nshares the functionality of CSS and is compatible with it (although it uses a\ndifferent syntax).\nThe capabilities of XSL vastly exceed the functionality of CSS. XSL contains\nthe XSL Transformation language, or XSLT, a language that allows 11S to\ntransform the input XML document into a XML document with another struc-\nture. For example, with XSLT we can change the order of elements that we are\ndisplaying (e.g.; by sorting them), process elements more than once, suppress\nelements in one place and present them in another, and add generated text to\nthe presentation.\nXSL also contains the XML Path Language (XPath), a language that\nallows us to refer to parts of an XML document. We discuss XPath in Section",
          "pages": [
            284,
            285
          ],
          "relevance": {
            "score": 0.04,
            "sql_score": 0.8,
            "concept_score": 0.0,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "The DELETE statement is used to remove rows from a table in a database.",
        "explanation": "The DELETE statement is essential for managing data in a database by allowing you to remove unwanted records. It's crucial when you need to clean up old or incorrect data, or prepare the database for new entries. Here’s how it works:\n\n1. **Identify the Rows**: You specify which rows should be deleted using a WHERE clause that filters based on conditions.\n2. **Execute the Command**: The DELETE statement is executed, and the matching rows are removed from the table.\n\n**When to Use It**: Whenever you need to remove data from your database that is no longer needed or is incorrect. For example, deleting old sales records or removing duplicate entries.\n\n**Key Things to Remember**:\n- Always use a WHERE clause to avoid accidentally deleting all rows in the table.\n- Be cautious when using wildcards in the WHERE clause as they can match more than intended.\n- Test your DELETE statement on a small subset of data before running it on the entire table.",
        "key_points": [
          "Always include a WHERE clause to prevent accidental deletion of all rows",
          "Be careful with wildcard characters in the WHERE clause",
          "Test your DELETE statement on a sample dataset first"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "DELETE FROM employees WHERE employee_id = 101;",
            "explanation": "This example deletes a single row from the 'employees' table where the 'employee_id' is 101."
          },
          {
            "title": "Practical Example",
            "code": "DELETE FROM orders WHERE order_date < '2020-01-01';",
            "explanation": "This practical example deletes all orders from the 'orders' table that are older than January 1, 2020."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting the WHERE clause",
            "incorrect_code": "DELETE FROM employees;",
            "correct_code": "DELETE FROM employees WHERE employee_id = 101;",
            "explanation": "This mistake deletes all rows in the 'employees' table. Always include a WHERE clause to specify which rows should be deleted."
          },
          {
            "mistake": "Using wildcards without intention",
            "incorrect_code": "DELETE FROM employees WHERE department LIKE '%Sales%';",
            "correct_code": "DELETE FROM employees WHERE department = 'Sales';",
            "explanation": "This mistake deletes all employees in departments that contain the word 'Sales'. Use specific conditions to avoid unintended deletions."
          }
        ],
        "practice": {
          "question": "Write a DELETE statement to remove all customers from the 'customers' table who have not made any purchases in the last year.",
          "solution": "DELETE FROM customers WHERE last_purchase_date < DATE_SUB(CURDATE(), INTERVAL 1 YEAR);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "~i~-----\",\".~._,----~~---.~u·\"r-----··'\"\n,\nClient Program\nI\nJavaScript\n(Web Br_~=:~)__~_~_~~~~~__________\nJ\nHTIP\n---:;;\"\"io:~~I--\"\nservle~~~----l\nJSP\nI\n~PPlication Server) ~__.__.__ .....XSLT\n~\nJDBe. ",
      "content_relevance": {
        "score": 0.1,
        "sql_score": 1.0,
        "concept_score": 0.0,
        "non_sql_penalty": 0.2,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "views": {
      "id": "views",
      "title": "SQL Views",
      "definition": "Virtual tables based on the result of a query - stored SELECT statements",
      "difficulty": "intermediate",
      "page_references": [
        295,
        296,
        297,
        298,
        299,
        300,
        301,
        302,
        303,
        304,
        305,
        306,
        307,
        308,
        309,
        310,
        311,
        312
      ],
      "sections": {
        "definition": {
          "text": "CHAPTE~ 7\n/ / no 88L required\n/ / one month lifetime\nA cookie is a collection of (name,\nval'Ue)~~pairs that can be manipulated at\nthe presentation and middle tiers.\nCookies are ea..''!Y to use in Java servlets\nand Java8erver Pages and provide a simple way to make non-essential data\npersistent at the client. They survive several client sessions because they persist\nin the browser cache even after the browser is closed.\nOne disadvantage of cookies is that they are often perceived as as being invasive,\nand many users disable cookies in their Web browser; browsers allow users to\nprevent cookies from being saved on their machines. Another disadvantage is\nthat the data in a cookie is currently limited to 4KB, but for most applications\nthis is not a bad limit.\nWe can use cookies to store information such as the user's shopping basket, login\ninformation, and other non-permanent choices made in the current session.\nNext, we discuss how cookies can be manipulated from servlets at the middle\ntier.\nThe Servlet Cookie API\nA cookie is stored. in a small text file at the client and. contains (name, val'l1e/-\npairs, where both name and value are strings. We create a new cookie through\nthe Java Cookie class in the middle tier application code:\nCookie cookie = new Cookie( II username\" ,\"guest\" );\ncookie.setDomain(\"www.bookstore.com .. );\ncookie.set8ecure(false);\ncookie.setMaxAge(60*60*24*7*31);\nresponse.addCookie(cookie);\nLet us look at each part of this code. First, we create a new Cookie object with\nthe specified (name,\nval'l1e)~~·pair. Then we set attributes of the cookie; we list\nsome of the most common attributes below:\nIII\nsetDomain and getDomain:\nThe domain specifies the website that will\nreceive the cookie. The default value for this attribute is the domain that\ncreated the cookie.\nII\nsetSecure and getSecure: If this flag is true, then the cookie is sent only\nif we are llsing a secure version of the HTTP protocol, such <t,<; 88L.\nIII\nsetMaxAge and getMaxAge: The MaxAge attribute determines the lifetime\nof the cookie in seconds. If the value of MaxAge is less than or equal to\nzero, the cookie is deleted when the browser is closed.\n\nInte7~net Applications\n26~\n•\nsetName and getName: We did not use these functions in our code fragment;\nthey allow us to Ilame the cookie.\n•\nsetValue and getValue: These functions allow us to set and read the\nvalue of the cookie.\nThe cookie is added to the request object within the Java servlet to be sent\nto the client. Once a cookie is received from a site (www.bookstore.comin this\nexample), the client's Web browser appends it to all HTTP requests it sends\nto this site, until the cookie expires.\nWe can access the contents of a cookie in the middle-tier code through the\nrequest object getCookies 0 method, which returns an array of Cookie ob-\njects.\nThe following code fragment reads the array and looks for the cookie\nwith name 'username.'\nCookieD cookies = request.getCookiesO;\nString theUser;\nfor(int i=O; i < cookies.length; i++) {\nCookie cookie = cookies[i];\nif (cookie.getNameO.equals(\"username\"))\ntheUser = cookie.getValueO;\n}\nA simple test can be used to check whether the user has turned oft' cookies:\nSend a cookie to the user, and then check whether the request object that\nis returned still contains the cookie. Note that a cookie should never contain\nan unencrypted password or other private, unencrypted data, as the user can\neasily inspect, modify, and erase any cookie at any time, including in the middle\nof a session. The application logic needs to have sufficient consistency checks\nto ensure that the data in the cookie is valid.\n7.8\nCASE STUDY: THE INTERNET BOOK SHOP\nDBDudes now moves on to the implementation of the application layer and\nconsiders alternatives for connecting the DBMS to the World Wide Web.\nDBDudes begifls by considering session management. For example, users who\nlog in to the site, browse the catalog, and select books to buy do not want\nto re-enter their cllstomer identification numbers. Session management has to\nextend to the whole process of selecting books, adding them to a shopping cart,\npossibly removing books from the cart, and checking out and paying for the\nbooks.\n\nCHAPTERi 7\nDBDudes then considers whether webpages for books should be static or dy-\nnamic.\nIf there is a static webpage for each book, then we need an extra\ndatabase field in the Books relation that points to the location of the file.\nEven though this enables special page designs for different books, it is a very\nlabor-intensive solution.\nDBDudes convinces B&N to dynamically assemble\nthe webpage for a book from a standard template instantiated with informa-\ntion about the book in the Books relation. Thus, DBDudes do not use static\nHTML pages, such as the one shown in Figure 7.1, to display the inventory.\nDBDudes considers the use of XML a'S a data exchange format between the\ndatabase server and the middle tier, or the middle tier and the client tier.\nRepresentation of the data in XML at the middle tier as shown in Figures 7.2\nand 7.3 would allow easier integration of other data sources in the future, but\nB&N decides that they do not anticipate a need for such integration, and so\nDBDudes decide not to use XML data exchange at this time.\nDBDudes designs the application logic as follows. They think that there will\nbe four different webpages:\n•\nindex. j sp: The home page of Barns and Nobble. This is the main entry\npoint for the shop. This page has search text fields and buttons that allow\nthe user to search by author name, ISBN, or title of the book. There is\nalso a link to the page that shows the shopping cart, cart. j sp.\n•\nlogin. j sp:\nAllows registered users to log in.\nHere DBDudes use an\nHTML form similar to the one displayed in Figure 7.11.\nAt the middle\ntier, they use a code fragment similar to the piece shown in Figure 7.19\nand JavaServerPages as shown in Figure 7.20.\n•\nsearch. j sp: Lists all books in the database that match the search condi-\ntion specified by the user. The user can add listed items to the shopping\nbasket; each book ha'3 a button next to it that adds it.\n(If the item is\nalready in the shopping basket, it increments the quantity by one.) There\nis also a counter that shows the total number of items currently in the\nshopping basket. (DBDucles makes a note that that a quantity of five for a\nsingle item in the shopping basket should indicate a total purcha'3c quantity\nof five as well.) The search. j sp page also contains a button that directs\nthe user to cart. j sp.\nIII\ncart. j sp: Lists all the books currently in the shopping basket. The list-\ning should include all items in the shopping basket with the product name,\nprice, a text box for the quantity (which the user can use to change quanti-\nties of items), and a button to remove the item from the shopping basket.\nThis page has three other buttons: one button to continue shopping (which\nreturns the user to page index. j sp), a second button to update the shop-\n\nInter'net Applications\nping basket with the altered quantities from the text boxes, and a third\nbutton to place the order, which directs the user to the page confirm.jsp.\nII\nconi irm. j sp: Lists the complete order so far and allows the user to enter\nhis or her contact information or customer ID. There are two buttons on\nthis page: one button to cancel the order and a second button to submit\nthe final order. The cancel button ernpties the shopping ba.'3ket and returns\nthe user to the home page. The submit button updates the database with\nthe new order, empties the shopping basket, and returns the user to the\nhome page.\nDBDudes also considers the use of JavaScript at the presentation tier to check\nuser input before it is sent to the middle tier.\nFor example, in the page\nlogin. j sp, DBDudes is likely to write JavaScript code similar to that shown\nin Figure 7.12.\nThis leaves DBDudes with one final decision: how to connect applications to\nthe DBMS. They consider the two main alternatives presented in Section 7.7:\nCGI scripts versus using an application server infrastructure. If they use CGI\nscripts, they would have to encode session management logic-not an easy task.\nIf they use an application server, they can make use of all the functionality\nthat the application server provides.\nTherefore, they recommend that B&N\nimplement server-side processing using an application server.\nB&N accepts the decision to use an application server, but decides that no\ncode should be specific to any particular application server, since B&N does\nnot want to lock itself into one vendor. DBDudes agrees proceeds to build the\nfollowing pieces:\nIII\nDBDudes designs top level pages that allow customers to navigate the\nwebsite as well as various search forms and result presentations.\nII\nAssuming that DBDudes selects a Java-ba..sed application server, they have\nto write Java servlets to process form-generated requests. Potentially, they\ncould reuse existing (possibly commercially available) JavaBeans.\nThey\ncan use JDBC a.\" a databa.':ie interface; exarnples of JDBC code can be\nfound in Section 6.2. Instead of prograrnming servlets, they could resort\nto Java Server Pages and annotate pages with special .JSP markup tags.\nII\nDBDudes select an application server that uses proprietary markup tags,\nbut due to their arrangement with B&N, they are not allowed to use such\ntags in their code.\nFor completeness, we remark that if DBDudes and B&N had agreed to use CGr\nscripts, DBDucles would have had the following ta.sks:\n\nCHAPTER~ 7\nII\nCreate the top level HTML pages that allow users to navigate the site and\nvaTious forms that allow users to search the catalog by ISBN, author name,\nor title.\nAn example page containing a search form is shown in Figure\n7.1. In addition to the input forms, DBDudes must develop appropriate\npresentations for the results.\nII\nDevelop the logic to track a customer session. Relevant information must be\nstored either at the server side or in the customer's browser using cookies.\nII\nWrite the scripts that process user requests. For example, a customer can\nuse a form called 'Search books by title' to type in a title and search for\nbooks with that title. The CGI interface communicates with a script that\nprocesses the request. An example of such a script written in Perl using\nthe DBI library for data access is shown in Figure 7.16.\nOur discussion thus far covers only the customer interface, the part of the\nwebsite that is exposed to B&N's customers.\nDBDudes also needs to add\napplications that allow the employees and the shop owner to query and access\nthe database and to generate summary reports of business activities.\nComplete files for the case study can be found on the webpage for this book.\n7.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nII\nWhat are URIs and URLs? (Section 7.2.1)\nII\nHow does the HTTP protocol work? What is a stateless protocol? (Sec-\ntion 7.2.2)\nII\nExplain the main concepts of HTML. Why is it used only for data presen-\ntation and not data exchange? (Section 7.3)\nII\nWhat are some shortc.ornings of HTML, and how does XML address them?\n(Section 7.4)\nII\nWhat are the main components of an XML document? (Section 7.4.1)\nII\nWhy do we have XML DTDs? What is a well-formed XML document?\nWhat is a valid XML document? Give an example of an XML document\nthat is valid but not well-formed, and vice versa. (Section 7.4.2)\nII\n'What is the role of domain-specific DTDs? (Section 7.4.3)\nII\n\\Vhat is a three-tier architecture? 'What advantages does it offer over single-\ntier and two-tier architectures? Give a short overview of the functionality\nat each of the three tiers. (Section 7.5)",
          "pages": [
            295,
            296,
            297,
            298,
            299
          ],
          "relevance": {
            "score": 0,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.5,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Internet Apphcat-ions\n2&5\n•\nExplain hmv three-tier architectures address each of the following issues\nof databa.<;e-backed Internet applications: heterogeneity, thin clients, data\nintegration, scalability, software development. (Section 7.5.3)\n•\nWrite an HTML form.\nDescribe all the components of an HTML form.\n(Section 7.6.1)\n•\nWhat is the difference between the HTML GET and POST methods? How\ndoes URI encoding of an HT~IL form work? (Section 7.11)\n•\nWhat is JavaScript used for?\nWrite a JavaScipt function that checks\nwhether an HTML form element contains a syntactically valid email ad-\ndress. (Section 7.6.2)\n•\nWhat problem do style sheets address? What are the advantages of using\nstyle sheets? (Section 7.6.3)\n•\nWhat are Ca.5cading Style Sheets? Explain the components of Ca.<;cading\nStyle Sheets. What is XSL and how it is different from CSS? (Sections\n7.6.3 and 7.13)\n•\nWhat is CGl and what problem does it address? (Section 7.7.1)\n•\nWhat are application servers and how are they different from webservers?\n(Section 7.7.2)\n•\nWhat are servlets? How do servlets handle data from HTML forms? Ex-\nplain what happens during the lifetime of a servlet. (Section 7.7.3)\n•\nWhat is the difference between servlets and JSP? When should we use\nservlets and when should we use JSP? (Section 7.7.4)\n•\nWhy do we need to maintain state at the middle tier? What are cookies?\nHow does a browser handle cookies? How can we access the data in cookies\nfrom servlets? (Section 7.7.5)\nEXERCISES\nExercise 7.1 Briefly answer the following questions:\n1. Explain the following terms and describe what they are used for: HTML, URL, XML,\nJava, JSP, XSL, XSLT, servlet, cookie, HTTP, ess, DTD.\n2. What is eGl? Why was eGI introduced? What are the disadvantages of an architecture\nusing eel scripts?\n3. \\Vhat is the difference between a webserver and an application server? What fUl1cionality\ndo typical application servers provide?\n4. When is an XML document well-formed? When is an XML document valid?\nExercise 7.2 Briefly answer the following questions about the HTTP protocol:\n\nCHAPTER$ 7\n1. \\Nhat is a communication protocol?\n2. \"What is the structure of an HTTP request message? What is the structure of an HTTP\nresponse message? \\Vhy do HTTP messages carry a version field?\n3. vVhat is a stateless protocol? \"Why was HTTP designed to be stateless?\n4. Show the HTTP request message generated when you request the home page of this\nbook (http://TNWW . cs. wisc. edur dbbook). Show the HTTP response message that the\nserver generates for that page.\nExercise 7.3 In this exercise, you are asked to write the functionality of a generic shopping\nbasket; you will use this in several subsequent project exercises. Write a set of JSP pages that\ndisplays a shopping basket of items and allows users to add, remove, and change the quantity\nof items. To do this, use a cookie storage scheme that stores the following information:\n•\nThe UserId of the user who owns the shopping basket.\n•\nThe number of products stored in the shopping basket.\nI!\nA product id and a quantity for each product.\nWhen manipulating cookies, remember to set the Expires property such that the cookie can\npersist for a session or indefinitely. Experiment with cookies using JSP and make sure you\nknow how to retrieve, set values, and delete the cookie.\nYou need to create five JSP pages to make your prototype complete:\n..\nIndex Page (index. j sp): This is the main entry point. It has a link that directs the\nuser to the Products page so they can start shopping.\nI!\nProducts Page (products. j sp): Shows a listing of all products in the database with\ntheir descriptions and prices. This is the main page where the user fills out the shopping\nbasket. Each listed product should have a button next to it, which adds it to the shopping\nbasket.\n(If the item is already in the shopping basket, it increments the quantity by\none.) There should also be a counter to show the total number of items currently in the\nshopping basket. Note that if a user has a quantity of five of a single item in the shopping\nbasket, the counter should indicate a total quantity of five.\nThe page also contains a\nbutton that directs the user to the Cart page.\nI!\nCart Page (cart. jsp): Shows a listing of all items in the shopping basket cookie. The\nlisting for each item should include the product name, price, a text box for the quantity\n(the user can changc the quantity of items here), and a button to remove the item from\nthe shopping basket. This page has three other buttons: one button to continue shopping\n(which returns the user to the Products page), a second button to update the cookie\nwith the altered quantities from the text boxes, and a third button to place or confirm\nthe order, which directs the user to the Confirm page.\nI!\nConfirm Pl;tge (confirm. j sp) :\nList.s the final order.\nThere are two but.tons on this\npage.\nOne button cancels t.he order and the other submits the completed order. The\ncancel button just deletes the cookie and returns the lIser to the Index page. The submit\nbutton updates the database with the new order, delet.es the cookie, and returns the lIser\nto the Index page.\nExercise 7.4 In the previous exercise, replace the page products. jsp with the follmving\nsearch page search. j sp.\n'T'his page allows users to search products by name or descrip-\ntion.\nThere should be both a text box for the search text and radio buttons to allow the\n\nInternet Applications\n2@7\nuser to choose between search-by-name and search-by-description (as \\vell as a submit but-\nton to retrieve the results),\nThe page that handles search results should be modeled after\nproducts.jsp (as described in the previous exercise) and be called products.jsp. It should\nretrieve all records where the search text is a substring of the name or description (as chosen\nby the user).\nTo integrate this with the previous exercise, simply replace all the links to\nproducts. j sp with search. j sp.\nExercise 7.5 'Write a simple authentication mechanism (without using encrypted transfer of\npasswords, for simplicity). We say a user is authenticated if she has provided a valid username-\npassword combination to the system; otherwise, we say the user is not authenticated. Assume\nfor simplicity that you have a database schema that stores only a customer id and a password:\nPasswords(cid: integer, username: string, password: string)\n1. How and where are you going to track when a user is 'logged on' to the system?\n2. Design a page that allows a registered user to log on to the system.\n3. Design a page header that checks whether the user visiting this page is logged in.\nExercise 7.6 (Due to Jeff Derstadt) TechnoBooks.com is in the process of reorganizing its\nwebsite.\nA major issue is how to efficiently handle a large number of search results.\nIn a\nhuman interaction study, it found that modem users typically like to view 20 search results at\na time, and it would like to program this logic into the system. Queries that return batches of\nsorted results are called top N queries. (See Section 25.5 for a discussion of database support\nfor top N queries.)\nFor example, results 1-20 are returned, then results\n21~40, then 41-60,\nand so OIl. Different techniques are used for performing top N queries and TechnoBooks.com\nwould like you to implement two of them.\nInfrastructure:\nCreate a database with a table called Books and populate it with some\nbooks, using the format that follows. This gives you III books in your database with a title\nof AAA, BBB, CCC, DDD, or EEE, but the keys are not sequential for books with the same\ntitle.\nBooks( bookid: INTEGER, title: CHAR(80), author: CHAR(80), price: REAL)\nFor i = 1 to 111 {\nInsert the tuple (i, \"AAA\", \"AAA Author\", 5.99)\ni=i+l\nInsert the tuple (i, \"BBB\", \"BBB Author\", 5.99)\ni = i + 1\nInsert the tuple (i, \"CCC\", \"CCC Author\", 5.99)\ni=i+1\nInsert the tuple (i, \"DDD\", \"DDD Author\", 5.99)\n1=i+l\nInsert the tuple (i, \"EEE\", \"EEE Author\", 5.99)\nPlaceholder Technique:\nThe simplest approach to top N queries is to store a placeholder\nfor the first and last result tuples, and then perform the same query. When the new query\nresults are returned, you can iterate to the placeholders and return the previous or next 20\nresults.\n\nI Tuples Shown\nLower Placeholder\nPrevious Set\nUpper Placeholder\nNext Set I\n1-20\nNone\n\"-\n21-40\n21-40\n1-20\n41-60\n41-60\n21-40\n61-80\nWrite a webpage in JSP that displays the contents of the Books table, sorted by the Title and\nBookId, and showing the results 20 at a time. There should be a link (where appropriate) to\nget the previous 20 results or the next 20 results. To do this, you can encode the placeholders\nin the Previous or Next Links as follows. Assume that you are displaying records 21-40. Then\nthe previous link is display. j sp?lower=21 and the next link is display. j sp?upper=40.\nYou should not display a previous link when there are no previous results; nor should you\nshow a Next link if there are no more results. When your page is called again to get another\nbatch of results, you can perform the same query to get all the records, iterate through the\nresult set until you are at the proper starting point, then display 20 more results.\nWhat are the advantages and disadvantages of this technique?\nQuery Constraints Technique:\nA second technique for performing top N queries is to\npush boundary constraints into the query (in the WHERE clause) so that the query returns only\nresults that have not yet been displayed. Although this changes the query, fewer results are\nreturned and it saves the cost of iterating up to the boundary.\nFor example, consider the\nfollowing table, sorted by (title, primary key).\nI Batch I Result Number\nTitle\nI Primary Key\nAAA\nBBB\neee\nDDD\nDDD\nDDD\nEEE\nEEE\nFFF\nFFF\nFFF\n.\"~\n:~\nFFF\nGGG\nEHH\nHHH\nIn batch 1, rows 1 t.hrough 5 are displayed, in batch 2 rows 6 through 10 are displayed, and so\non. Using the placeholder technique, all 15 results would be returned for each batch. Using\nthe constraint technique, batch 1 displays results 1-5 but returns results 1-15, batch 2 will\ndisplay results 6-10 but returns only results 6-15, and batch\n:~ will display results 11-15 but\nreturn only results 11-15.\n\nInternet Applications\nThe constraint can be pushed into the query because of the sorting of this table. Consider\nthe following query for batch 2 (displaying results 6-10):\nEXEC SQL SELECT B.Title\nFROM\nBooks B\nWHERE\n(B.Title = 'DDD' AND B.BookId > 101) OR (B.Title > 'DDD')\nORDER BY B.Title, B.Bookld\nThis query first selects all books with the title 'DDD,' but with a primary key that is greater\nthan that of record 5 (record 5 has a primary key of 101). This returns record 6. Also, any\nbook that has a title after 'DDD' alphabetically is returned. You can then display the first\nfive results.\nThe following information needs to be retained to have Previous and Next buttons that return\nmore results:\n•\nPrevious: The title of the first record in the previous set, and the primary key of the\nfirst record in the previous set.\n•\nNext: The title of the first record in the next set; the primary key of the first record in\nthe next set.\nThese four pieces of information can be encoded into the Previous and Next buttons as in the\nprevious part. Using your database table from the first part, write a JavaServer Page that\ndisplays the book information 20 records at a time. The page should include Previous and\nNext buttons to show the previous or next record set if there is one. Use the constraint query\nto get the Previous and Next record sets.\nPROJECT~BASEDEXERCISES\nIn this chapter, you continue the exercises from the previous chapter and create the parts of\nthe application that reside at the middle tier and at the presentation tier. More information\nabout these exercises and material for more exercises can be found online at\nhttp://~.cs.wisc.edu/-dbbook\nExercise 7.7 Recall the Notown Records website that you worked on in Exercise 6.6. Next,\nyou are asked to develop the actual pages for the Notown Records website. Design the part\nof the website that involves the presentation tier and the middle tier, and integrate the code\nthat you wrote in Exercise 6.6 to access the database.\nI. Describe in detail the set of webpages that users can access. Keep the following issues\nin mind:\n•\nAll users start at a common page.\n•\nFor each action, what input does the user provide? How will the user provide it -by\nclicking on a link or through an HTML form?\n•\nWhat sequence of steps does a user go through to purchase a record? Describe the\nhigh-level application flow by showing how each lIser action is handled.\n\nCHAPTER .,7\n2. vVrite the webpages in HTML without dynamic content.\n3. vVrite a page that allows users to log on to the site. Use cookies to store the information\npermanently at the user's browser.\n4. Augment the log-on page with JavaScript code that checks that the username consists\nonly of the characters from a to z.\n5. Augment the pages that allow users to store items in a shopping basket with a condition\nthat checks whether the user has logged on to the site. If the user has not yet logged on,\nthere should be no way to add items to the shopping cart. Implement this functionality\nusing JSP by checking cookie information from the user.\n6. Create the remaining pages to finish the website.\nExercise 7.8 Recall the online pharmacy project that you worked on in Exercise 6.7 in\nChapter 6. Follow the analogous steps from Exercise 7.7 to design the application logic and\npresentation layer and finish the website.\nExercise 7.9 Recall the university database project that you worked on in Exercise 6.8 in\nExercise 7.10 Recall the airline reservation project that you worked on in Exercise 6.9 in\nBIBLIOGRAPHIC NOTES\nThe latest version of the standards mentioned in this chapter can be found at the website\nof the World Wide Web Consortium (www. w3. org).\nIt contains links to information about\nI-ITML, cascading style sheets, XIvIL, XSL, and much more.\nThe book by Hall is a gen-\neral introduction to Web progn1.111ming technologies [357]; a good starting point on the Web\nis www.Webdeve1oper.com.\nThere are many introductory books on CGI progranuning, for\nexample [210, 198].\nThe JavaSoft (java. sun. com) home page is a good starting point for\nServlets, .JSP, and all other Java-related technologies. The book by Hunter [394] is a good\nintroduction to Java Servlets. Microsoft supports Active Server Pages (ASP), a comparable\ntedmology to .lSI'. l'vIore information about ASP can be found on the Microsoft Developer's\nNetwork horne page (msdn. microsoft. com).\nThere are excellent websites devoted to the advancement of XML, for example 1.l1-iTW. xm1. com\nand www.ibm.com/xm1. that also contain a plethora of links with information about the other\nstandards. There are good introductory books on many diflerent aspects of XML, for exarnple\n[195, 158,597,474, :381, 320]. Information about UNICODE can be found on its home page\nhttp://www.unicode.org.\nInforrnation about .lavaServer Pages ane! servlets can be found on the JavaSoft home page at\njava. sun. com at java. sun. com/products/j sp and at java. sun. com/products/servlet.\n\nPART III\nSTORAGE AND INDEXING\n\n",
          "pages": [
            300,
            301,
            302,
            303,
            304,
            305,
            306,
            307
          ],
          "relevance": {
            "score": 0,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.5,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "OVERVIEW'OF STORAGE\nAND INDEXING\n..\nHow does a DBMS store and access persistent data?\n..\nWhy is I/O cost so important for database operations?\n..\nHow does a DBMS organize files of data records on disk to minimize\nI/O costs?\n...\nWhat is an index, and why is it used?\n..\nWhat is the relationship between a file of data records and any indexes\non this file of records?\n..\nWhat are important properties of indexes?\n..\nHow does a hash-based index work, and when is it most effective?\n..\nHow does a tree-based index work, and when is it most effective?\n...\nHow can we use indexes to optimize performance for a given workload?\n..\nKey concepts: external storage, buffer manager, page I/O; file orga-\nnization, heap files, sorted files; indexes, data entries, search keys, clus-\ntered index, clustered file, primary index; index organization, hash-\nbased and tree-based indexes; cost comparison, file organizations and\ncommon operations; performance tuning, workload, composite search\nkeys, use of clustering,\n____________________J\nIf you don't find it in the index, look very carefully through the entire catalog.\n--Sears, Roebuck, and Co., Consumers' Guide, 1897\nThe ba.'3ic abstraction of data in a DBMS is a collection of records, or a file,\nand each file consists of one or more pages.\nThe files and access methods\n\nCHAPTER 8\nsoftware layer organizes data carefully to support fast access to desired subsets\nof records.\nUnderstanding how records are organized is essential to using a\ndatabase system effectively, and it is the main topic of this chapter.\nA file organization is a method of arranging the records in a file when the\nfile is stored on disk. Each file organization makes certain operations efficient\nbut other operations expensive.\nConsider a file of employee records, each containing age, name, and sal fields,\nwhich we use as a running example in this chapter.\nIf we want to retrieve\nemployee records in order of increasing age, sorting the file by age is a good file\norganization, but the sort order is expensive to maintain if the file is frequently\nmodified. Further, we are often interested in supporting more than one oper-\nation on a given collection of records. In our example, we may also want to\nretrieve all employees who make more than $5000. We have to scan the entire\nfile to find such employee records.\nA technique called indexing can help when we have to access a collection of\nrecords in multiple ways, in addition to efficiently supporting various kinds of\nselection. Section 8.2 introduces indexing, an important aspect of file organi-\nzation in a DBMS. We present an overview of index data structures in Section\n8.3; a more detailed discussion is included in Chapters 10 and 11.\nWe illustrate the importance of choosing an appropriate file organization in\nThe cost model used in this analysis, presented in Section 8.4.1, is used in\nlater chapters as welL In Section 8.5, we highlight some important choices to\nbe made in creating indexes.\nChoosing a good collection of indexes to build\nis arguably the single most powerful tool a database administrator has for\nimproving performance.\n8.1\nDATA ON EXTERNAL STORAGE\nA DBMS stores vast quantities of data, and the data must persist across pro-\ngram executions. Therefore, data is stored on external storage devices such as\ndisks and tapes, and fetched into main memory as needed for processing. The\nunit of information read from or written to disk is a page. The size of a page\nis a DBMS parameter, and typical values are 4KB or 8KB.\nThe cost of page I/O (input from disk to main Inemory and output from mem-\nory to disk) dominates the cost of typical database operations, and databa,'>e\nsystems are carefully optimized to rninimize this cost. While the details of how\n\nStorage and Indexing\n:175\nfiles of records are physically stored on disk and how main memory is utilized\nare covered in Chapter 9, the following points are important to keep in mind:\n•\nDisks are the most important external storage devices. They allow us to\nretrieve any page at a (more or less) fixed cost per page. However, if we\nread several pages in the order that they are stored physically, the cost can\nbe much less than the cost of reading the same pages in a random order.\n•\nTapes are sequential access devices and force us to read data one page after\nthe other. They are mostly used to archive data that is not needed on a\nregular basis.\n•\nEach record in a file has a unique identifier called a record id, or rid for\nshort. An rid ha.'3 the property that we can identify the disk address of the\npage containing the record by using the rid.\nData is read into memory for processing, and written to disk for persistent\nstorage, by a layer of software called the buffer manager. When the files and\naccess methods layer (which we often refer to as just the file layer) needs to\nprocess a page, it asks the buffer manager to fetch the page, specifying the\npage's rid. The buffer manager fetches the page from disk if it is not already\nin memory.\nSpace on disk is managed by the disk space m,anager, according to the DBMS\nsoftware architecture described in Section 1.8. When the files and access meth-\nods layer needs additional space to hold new records in a file, it asks the disk\nspace manager to allocate an additional disk page for the file; it also informs\nthe disk space manager when it no longer needs one of its disk pages. The disk\nspace manager keeps track of the pages in use by the file layer; if a page is freed\nby the file layer, the space rnanager tracks this, and reuses the space if the file\nlayer requests a new page later on.\nIn the rest of this chapter, we focus on the files and access methods layer.\n8.2\nFILE ORGANIZATIONS AND INDEXING\nThe file of records is an important abstraction in a DBMS, and is imple-\nmented by the files and access methods layer of the code. A file can be created,\ndestroyed, and have records inserted into and deleted from it. It also supports\nscallS; a scan operation allows us to step through all the records in the file one\nat a time. A relatioll is typically stored a.':l a file of records.\nThe file layer stores the records in a file in a collection of disk pages. It keeps\ntrack of pages allocated to each file, and as records are inserted into and deleted\nfrom the file, it also tracks availa.ble space within pages allocated to the file.\n\nCHAPTER 8\nThe simplest file structure is an unordered file, or heap file.\nRecords in a\nheap file are stored in random order across the pages of the file.\nA heap file\norganization supports retrieval of all records, or retrieval of a particular record\nspecified by its rid; the file manager must keep track of the pages allocated for\nthe file. (\"Ve defer the details of how a heap file is implemented to Chapter 9.)\nAn index is a data structure that organizes data records on disk to optimize\ncertain kinds of retrieval operations. An index allows us to efficiently retrieve\nall records that satisfy search conditions on the search key fields of the index.\nWe can also create additional indexes on a given collection of data records,\neach with a different search key, to speed up search operations that are not\nefficiently supported by the file organization used to store the data records.\nConsider our example of employee records. We can store the records in a file\norganized as an index on employee age; this is an alternative to sorting the file\nby age. Additionally, we can create an auxiliary index file based on salary, to\nspeed up queries involving salary. The first file contains employee records, and\nthe second contains records that allow us to locate employee records satisfying\na query on salary.\n\"Ve use the term data entry to refer to the records stored in an index file. A\ndata entry with search key value k, denoted as k*, contains enough information\nto locate (one or more) data records with search key value k. We can efficiently\nsearch an index to find the desired data entries, and then use these to obtain\ndata records (if these are distinct from data entries).\nThere are three main alternatives for what to store as a data entry in an index:\n1. A data entry h\nis an actual data record (with search key value k).\n2. A data entry is a (k, rid) pair, where rid is the record id of a data record\nwith search key value k.\n3. A data entry is a (k. rid-list) pair, where rid-list is a list of record ids of\ndata records with search key value k.\nOf course, if the index is used to store actual data records, Alternative (1),\neach entry b\nis a data record with search key value k. We can think of such an\nindex &'3 a special file organization. Such an indexed file organization can\nbe used instead of, for exarnple, a sorted file or an unordered file of records.\nAlternatives (2) and (3), which contain data entries that point to data records,\nare independent of the file organization that is used for the indexed file (i.e.,",
          "pages": [
            308,
            309,
            310,
            311
          ],
          "relevance": {
            "score": 0.47,
            "sql_score": 1.0,
            "concept_score": 0.33,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "A SQL view is a virtual table that is based on the result-set of a SQL query. It allows you to simplify complex queries and provide a layer of abstraction between the application and the database.",
        "explanation": "SQL views are incredibly useful for several reasons:\n1. **Simplification**: Complex queries can be encapsulated in a view, making them easier to understand and use throughout your application.\n2. **Security**: Views can restrict access to certain data by only showing specific columns or rows, enhancing security.\n3. **Consistency**: If the underlying data changes, views automatically update without needing any changes to the application code using them.\n4. **Performance**: Some databases optimize queries on views for better performance.\nTo create a view, you use the `CREATE VIEW` statement followed by the view name and the `AS` keyword, then the SQL query that defines the view.",
        "key_points": [
          "A view is a virtual table based on a query result.",
          "Views simplify complex queries and provide abstraction.",
          "They enhance security by restricting access to data.",
          "Views automatically update when underlying data changes.",
          "Some databases optimize views for better performance."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "CREATE VIEW employee_details AS\nSELECT first_name, last_name, email FROM employees;",
            "explanation": "This example creates a view named `employee_details` that shows only the first name, last name, and email of all employees."
          },
          {
            "title": "Practical Example",
            "code": "CREATE VIEW sales_summary AS\nSELECT product_id, SUM(quantity) as total_quantity FROM sales GROUP BY product_id;",
            "explanation": "This practical example creates a view `sales_summary` that shows the total quantity sold for each product."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to use `AS` after `CREATE VIEW`",
            "incorrect_code": "CREATE VIEW employee_details SELECT first_name, last_name, email FROM employees;",
            "correct_code": "CREATE VIEW employee_details AS\nSELECT first_name, last_name, email FROM employees;",
            "explanation": "The `AS` keyword is crucial to define the view's content. Without it, SQL will throw an error."
          },
          {
            "mistake": "Using `SELECT *` in a view",
            "incorrect_code": "CREATE VIEW all_employees AS\nSELECT * FROM employees;",
            "correct_code": "CREATE VIEW employee_details AS\nSELECT first_name, last_name, email FROM employees;",
            "explanation": "While it's tempting to use `SELECT *`, specifying only the necessary columns makes the view more efficient and easier to understand."
          }
        ],
        "practice": {
          "question": "Create a view named `customer_orders` that shows the customer ID, order date, and total amount for each order.",
          "solution": "CREATE VIEW customer_orders AS\nSELECT c.customer_id, o.order_date, SUM(od.quantity * od.price) as total_amount FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN order_details od ON o.order_id = od.order_id GROUP BY c.customer_id, o.order_date;"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTE~ 7\n/ / no 88L required\n/ / one month lifetime\nA cookie is a collection of (name,\nval'Ue)~~pairs that can be manipulated at\nthe presentation and middle tiers.\nCookies are ea..''!Y to use in Java",
      "content_relevance": {
        "score": 0,
        "sql_score": 1.0,
        "concept_score": 0.33,
        "non_sql_penalty": 0.5,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "normalization": {
      "id": "normalization",
      "title": "Database Normalization",
      "definition": "Organizing data to reduce redundancy and improve integrity (1NF, 2NF, 3NF, BCNF)",
      "difficulty": "intermediate",
      "page_references": [
        325,
        326,
        327,
        328,
        329,
        330,
        331,
        332,
        333,
        334,
        335,
        336,
        337,
        338,
        339,
        340,
        341,
        342,
        343,
        344,
        345,
        346,
        347,
        348,
        349,
        350
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER 8\nScan: As for an unclustered tree index, all data entries can be retrieved in-\nexpensively, at a cost of O.125B(D + 8RC) I/Os. However, for each entry, we\nincur the additional cost of one I/O to fetch the corresponding data record; the\ncost of this step is BR(D + C). This is prohibitively expensive, and further,\nresults are unordered. So no one ever scans a hash index.\nSearch with Equality Selection: This operation is supported very efficiently\nfor matching selections, that is, equality conditions are specified for each field\nin the composite search key (age, sal). The cost of identifying the page that\ncontains qualifying data entries is H.\nAssuming that this bucket consists of\njust one page (i.e., no overflow pages), retrieving it costs D. If we assume that\nwe find the data entry after scanning half the records on the page, the cost of\nscanning the page is O.5(8R)C =\n4RC. Finally, we have to fetch the data\nrecord from the employee file, which is another D. The total cost is therefore\nH + 2D + 4RC, which is even lower than the cost for a tree index.\nIf several records qualify, they are not guaranteed to be adjacent to each other.\nThe cost of retrieving all such records is the cost of locating the first qualifying\ndata entry (H+D+4RC) plus one I/O per qualifying record. The cost of using\nan unclustered index therefore depends heavily on the number of qualifying\nrecords.\nSearch with Range Selection: The hash structure offers no help, and the\nentire heap file of employee records must be scanned at a cost of B(D + RC).\nInsert: We must first insert the record in the employee heap file, at a cost\nof 2D + C. In addition, the appropriate page in the index must be located,\nmodified to insert a new data entry, and then written back.\nThe additional\ncost is H + 2D + C.\nDelete: We need to locate the data record in the employee file and the data\nentry in the index; this search step costs H + 2D + 4RC.\nNow, we need to\nwrite out the modified pages in the index and the data file, at a cost of 2D.\n8.4.7\nComparison of I/O Costs\ncussed. A heap file has good storage efficiency and supports fast scanning and\ninsertion of records. However, it is slow for searches and deletions.\nA sorted file also offers good storage efficiency. but insertion and ddetion of\nrecords is slow. Searches are fa.ster than in heap files. It is worth noting that,\nin a real DBMS, a file is almost never kept fully sorted.\n\nStorage and Inde:r'lng\n29)\nSearch+\nD\nSorted\nBD\nDlog2B\nDlog2B +#\nSear-ch +\nSear-ch+\nmatching pages\nBD\nBD\nClustered\n1.5BD\nDlogF1.5B\nDlo9F1.5B+#\nSear-ch +\nSearch+\nmatching pages\nD\nD\nUnclustered\nBD(R +\nD(l\n+\nD(lo9FO.15B+#\nD(3\n+\nSear-ch+\ntree index\n0.15)\nlogFO.15B)\nmatching recor-ds)\nlogFO.15B)\n2D\nUnclustered\nBD(R +\n2D\nBD\n4D\nSearch+\nhash index\n0.125)\n2D\nA Comparison of I/O Costs\nA clustered file offers all the advantages of a sorted file and supports inserts\nand deletes efficiently. (There is a space overhead for these benefits, relative to\na sorted file, but the trade-off is well worth it.) Searches are even faster than in\nsorted files, although a sorted file can be faster when a large number of records\nare retrieved sequentially, because of blocked I/O efficiencies.\nUnclustered tree and hash indexes offer fast searches, insertion, and deletion,\nbut scans and range searches with many matches are slow. Hash indexes are a\nlittle faster on equality searches, but they do not support range searches.\nIn summary, Figure 8.4 demonstrates that no one file organization is uniformly\nsuperior in all situations.\n8.5\nINDEXES AND PERFORMANCE TUNING\nIn this section, we present an overview of choices that arise when using indexes\nto improve performance in a database system.\nThe choice of indexes has a\ntremendous impact on system performance, and must be made in the context\nof the expected workload, or typical mix of queries and update operations.\nA full discussion of indexes and performance requires an understanding of\ndatabase query evaluation and concurrency control.\nWe therefore return to\nthis topic in Chapter 20, where we build on the discussion in this section. In\nparticular, we discuss examples involving multiple tables in Chapter 20 because\nthey require an understanding of join algorithms and query evaluation plans.\n\nCHAPTER. 8\n8.5.1\nImpact of the Workload\nThe first thing to consider is the expected workload and the common opera-\ntions. Different file organizations and indexes, a:\"l we have seen, support different\noperations well.\nIn generaL an index supports efficient retrieval of data entries that satisfy a\ngiven selection condition. Recall from the previous section that there are two\nimportant kinds of selections:\nequality selection and range selection.\nHash-\nbased indexing techniques are optimized only for equality selections and fa.re\npoorly on range selections. where they are typically worse than scanning the\nentire file of records.\nTree-based indexing techniques support both kinds of\nselection conditions efficiently, explaining their widespread use.\nBoth tree and hash indexes can support inserts, deletes, and updates quite\nefficiently.\nTree-based indexes, in particular, offer a superior alternative to\nmaintaining fully sorted files of records. In contrast to simply maintaining the\ndata entries in a sorted file, our discussion of (B+ tree) tree-structured indexes\nin Section 8.3.2 highlights two important advantages over sorted files:\n1. vVo can handle inserts and deletes of data entries efficiently.\n2. Finding the correct leaf page when searching for a record by search key\nvalue is much faster than binary search of the pages in a sorted file.\nThe one relative disadvantage is that the pages in a sorted file can be allocated\nin physical order on disk, making it much faster to retrieve several pages in\nsequential order. Of course. inserts and deletes on a sorted file are extremely\nexpensive.\nA variant of B+ trees, called Indexed Sequential Access Method\n(ISAM), offers the benefit of sequential allocation of leaf pages, plus the benefit\nof fast searches. Inserts and deletes are not handled as well a'3 in B+ trees, but\nare rnuch better than in a sorted file. \\Ve will study tree-structured indexing\nin detail in Cha,pter 10.\n8.5.2\nClustered Index Organization\nAs we smv in Section 8.2.1. a clustered index is really a file organization for\nthe underlying data records. Data records can be la.rge, and we should avoid\nreplicating them; so there can be at most one clustered index on a given collec-\ntion of records. On the other hanel. we UU1 build several uncIustered indexes\non a data file. Suppose that employee records are sorted by age, or stored in a\nclustered file with search keyage. If. in addition. we have an index on the sal\nfield, the latter nlUst be an llnclllstered index. \\:Ve can also build an uncIustered\nindex on. say, depaThnent, if there is such a field.\n\nStomge and Inde:rin,g\n29,3\nClustered indexes, while less expensive to maintain than a fully sorted file, are\nnonetJleless expensive to maintain. When a new record h&'3 to be inserted into\na full leaf page, a new leaf page must be allocated and sorne existing records\nhave to be moved to the new page. If records are identified by a combination of\npage id and slot, &'5 is typically the case in current database systems, all places\nin the datab&\"ie that point to a moved record (typically, entries in other indexes\nfor the same collection of records) must also be updated to point to the new\nlocation.\nLocating all such places and making these additional updates can\ninvolve several disk I/Os.\nClustering must be used sparingly and only when\njustified by frequent queries that benefit from clustering. In particular, there\nis no good reason to build a clustered file using hashing, since range queries\ncannot be answered using h&c;h-indexcs.\nIn dealing with the limitation that at most one index can be clustered, it is\noften useful to consider whether the information in an index's search key is\nsufficient to answer the query. If so, modern database systems are intelligent\nenough to avoid fetching the actual data records.\nFor example, if we have\nan index on age, and we want to compute the average age of employees, the\nDBMS can do this by simply examining the data entries in the index. This is an\nexample of an index-only evaluation. In an index-only evaluation of a query\nwe need not access the data records in the files that contain the relations in the\nquery; we can evaluate the query completely through indexes on the files. An\nimportant benefit of index-only evaluation is that it works equally efficiently\nwith only unclustered indexes, as only the data entries of the index are used in\nthe queries. Thus, unclustered indexes can be used to speed up certain queries\nif we recognize that the DBMS will exploit index-only evaluation.\nDesign Examples Illustrating Clustered Indexes\nTo illustrate the use of a clustered index 011 a range query, consider the following\nexample:\nSELECT\nFROM\nWHERE\nE.dno\nEmployees E\nE.age > 40\nIf we have a H+ tree index on age, we can use it to retrieve only tuples that\nsatisfy the selection E. age> 40. \\iVhether such an index is worthwhile depends\nfirst of all on the selectivity of the condition. vVhat fraction of the employees are\nolder than 40'1 If virtually everyone is older than 40 1 we gain little by using an\nindex 011 age; a sequential scan of the relation would do almost as well. However,\nsuppose that only 10 percent of the employees are older than 40. Now, is an\nindex useful? The answer depends on whether the index is clustered. If the\n\nCHAPTER~ 8\nindex is unclustered, we could have one page I/O per qualifying employee, and\nthis could be more expensive than a sequential scan, even if only 10 percent\nof the employees qualify!\nOn the other hand, a clustered B+ tree index on\nage requires only 10 percent of the l/Os for a sequential scan (ignoring the few\nl/Os needed to traverse from the root to the first retrieved leaf page and the\nl/Os for the relevant index leaf pages).\nAs another example, consider the following refinement of the previous query:\nSELECT\nFROM\nWHERE\nGROUP BY\nKdno, COUNT(*)\nEmployees E\nE.age> 10\nE.dno\nIf a B+ tree index is available on age, we could retrieve tuples using it, sort\nthe retrieved tuples on dna, and so answer the query. However, this may not\nbe a good plan if virtually all employees are more than 10 years old. This plan\nis especially bad if the index is not clustered.\nLet us consider whether an index on dna might suit our purposes better. We\ncould use the index to retrieve all tuples, grouped by dna, and for each dna\ncount the number of tuples with age> 10.\n(This strategy can be used with\nboth hash and B+ tree indexes; we only require the tuples to be grouped, not\nnecessarily sorted, by dna.) Again, the efficiency depends crucially on whether\nthe index is clustered. If it is, this plan is likely to be the best if the condition\non age is not very selective. (Even if we have a clustered index on age, if the\ncondition on age is not selective, the cost of sorting qualifying tuples on dna is\nlikely to be high.) If the index is not clustered, we could perform one page I/O\nper tuple in Employees, and this plan would be terrible. Indeed, if the index\nis not clustered, the optimizer will choose the straightforward plan based on\nsorting on dna. Therefore, this query suggests that we build a clustered index\non dna if the condition on age is not very selective. If the condition is very\nselective, we should consider building an index (not necessarily clustered) on\nage instead.\nClustering is also important for an index on a search key that does not include\na candidate key, that is, an index in which several data entries can have the\nsame key value. To illustrate this point, we present the following query:\nSELECT E.dno\nFROM\nEmployees E\nWHERE\nE.hobby='Stamps'\n\nStomge and Indexing\nIf many people collect stamps, retrieving tuples through an unclustered index\non hobby can be very inefficient. It may be cheaper to simply scan the relation\nto retrieve all tuples and to apply the selection on-the-fly to the retrieved tuples.\nTherefore, if such a query is important, we should consider making the index\non hobby a clustered index. On the other hand, if we assume that eid is a key\nfor Employees, and replace the condition E.hobby= 'Stamps' by E. eid=552, we\nknow that at most one Employees tuple will satisfy this selection condition. In\nthis case, there is no advantage to making the index clustered.\nThe next query shows how aggregate operations can influence the choice of\nindexes:\nSELECT\nE.dno, COUNT(*)\nFROM\nEmployees E\nGROUP BY E.dno\nA straightforward plan for this query is to sort Employees on dno to compute\nthe count of employees for each dno. However, if an index-hash or B+ tree---\non dno is available, we can answer this query by scanning only the index. For\neach dno value, we simply count the number of data entries in the index with\nthis value for the search key. Note that it does not matter whether the index\nis clustered because we never retrieve tuples of Employees.\n8.5.3\nComposite Search Keys\nThe search key for an index can contain several fields; such keys are called\ncomposite search keys or concatenated keys. As an example, consider a\ncollection of employee records, with fields name, age, and sal, stored in sorted\norder by name. Figure 8.5 illustrates the difference between a composite index\nwith key (age, sa0, a composite index with key (sal, age), an index with key\nage, and an index with key sal. All indexes shown in the figure use Alternative\n(2) for data entries.\nIf the search key is composite, an equality query is one in which each field in\nthe search key is bound to a constant. For example, we can ask to retrieve all\ndata entries with age = 20 and sal = 10. The hashed file organization supports\nonly equality queries, since a ha\"ih function identifies the bucket containing\ndesired records only if a value is specified for each field in the search key.\nWith respect to a composite key index, in a range query not all fields in the\nsearch key are bound to constants. For example, we can ask to retrieve all data\nentries with age\n:=0:: 20; this query implies that any value is acceptable for the\nsal field. As another example of a range query, we can ask to retrieve all data\nentries with age < 30 and sal> 40.",
          "pages": [
            325,
            326,
            327,
            328,
            329,
            330
          ],
          "relevance": {
            "score": 0.2,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTEI48\n~I\nname\nage\nsal\n\"\n~-_.-\nbob\ncal\njoe\n----\nsue\nData\nIndex\n75,13\n80,11\n10,12\n<age, sal>\n,~..._---!\nI 11,80 ,,! Index\nR;:IO ~ __:~i\n1 12,20 ~_.1\nU~:~,\n<sal, age>\n../\nComposite Key Indexes\nNate that the index cannot help on the query sal > 40, because, intuitively,\nthe index organizes records by age first and then sal. If age is left unspeci-\nfied, qualifying records could be spread across the entire index. We say that\nan index matches a selection condition if the index can be used to retrieve\njust the tuples that satisf:y the condition. For selections of the form condition\n1\\ ... 1\\ condition, we can define when an index matches the selection as 1'01-\n10ws:4 For a hash index, a selection matches the index if it includes an equality\ncondition ('field = constant') on every field in the composite search key for the\nindex. For a tree index, a selection matches the index if it includes an equal-\nity or range condition on a prefi.T of the composite search key.\n(As examples,\n(age) and (age, sal, department) are prefixes of key (age, sal, depa7'tment) , but\n(age, department) and (sal, department) are not.)\nTrade-offs in Choosing Composite Keys\nA composite key index can support a broader range of queries because it\nmatches more selection conditions. Further, since data entries in a composite\nindex contain more information about the data record (i.e., more fields than\na single-attribute index), the opportunities for index-only evaluation strategies\nare increased.\n(Recall from Section 8.5.2 that an index-only evaluation does\nnot need to access data records, but finds all required field values in the data\nentries of indexes.)\nOn the negative side, a composite index must be updated in response to any\noperation (insert, delete, or update) that modifies any field in the search key.\nA composite index is Hlso likely to be larger than a singk'-attribute search key\n4 For a more general discussion, see Section 14.2.)\n\nStoTage and Inde.Ting\nindex because the size of entries is larger. For a composite B+ tree index, this\nalso means a potential increase in the number of levels, although key COlnpres-\nsion can be used to alleviate this problem (see Section 10.8.1).\nDesign Examples of Composite Keys\nConsider the following query, which returns all employees with 20 < age < 30\nand 3000 < sal < 5000:\nSELECT\nFROM\nWHERE\nE.eid\nEmployees E\nE.age BETWEEN 20 AND 30\nAND E.sal BETWEEN 3000 AND 5000\nA composite index on (age, sal) could help if the conditions in the WHERE clause\nare fairly selective. Obviously, a hash index will not help; a B+ tree (or ISAM)\nindex is required. It is also clear that a clustered index is likely to be superior\nto an unclustered index. For this query, in which the conditions on age and sal\nare equally selective, a composite, clustered B+ tree index on (age, sal) is as\neffective as a composite, clustered B+ tree index on (sal, age). However, the\norder of search key attributes can sometimes make a big difference, as the next\nquery illustrates:\nSELECT\nFROM\nWHERE\nE.eid\nEmployees E\nE.age = 25\nIn this query a composite, clustered B+ tree index on (age, sal) will give good\nperformance because records are sorted by age first and then (if two records\nhave the same age value) by sal. Thus, all records with age = 25 are clustered\ntogether. On the other hand, a composite, clustered B+ tree index on (sal, age)\nwill not perform as well. In this case, records are sorted by sal first, and there-\nfore two records with the same age value (in particular, with age = 25) may be\nquite far apart. In effect, this index allows us to use the range selection on sal,\nbut not the equality selection on age, to retrieve tuples.\n(Good performance\non both variants of the query can be achieved using a single spatial index. \\:Ye\ndiscuss spatial indexes in Chapter 28.)\nComposite indexes are also useful in dealing with many aggregate queries. Con-\nsider:\nSELECT AVG (E.sal)\n\nFROM\nWHERE\nEmployees E\nE.age = 25\nAND Ksal BETWEEN 3000 AND 5000\nCHAPTERt 8\nA composite B+ tree index on (age, sal) allows us to answer the query with\nan index-only scan.\nA composite B+ tree index on (sal, age) also allows us\nto answer the query with an index-only scan, although more index entries are\nretrieved in this case than with an index on (age, sal).\nHere is a variation of an earlier example:\nSELECT\nFROM\nWHERE\nGROUP BY\nKdno, COUNT(*)\nEmployees E\nE.sal=lO,OOO\nKdno\nAn index on dna alone does not allow us to evaluate this query with an index-\nonly scan, because we need to look at the sal field of each tuple to verify that\nsal = 10, 000. However, we can use an index-only plan if we have a composite\nB+ tree index on (sal, dna) or (dna, sal). In an index with key (sal, dno) , all\ndata entries with sal = 10,000 are arranged contiguously (whether or not the\nindex is clustered). Further, these entries are sorted by dna, making it easy to\nobtain a count for each dna group.\nNote that we need to retrieve only data\nentries with sal = 10, 000.\nIt is worth observing that this strategy does not work if the WHERE clause is\nmodified to use sal> 10, 000. Although it suffices to retrieve only index data\nentries-that is, an index-only strategy still applies-these entries must now\nbe sorted by dna to identify the groups (because, for example, two entries with\nthe same dna but different sal values may not be contiguous). An index with\nkey (dna, sal) is better for this query: Data entries with a given dna value are\nstored together, and each such group of entries is itself sorted by sal. For each\ndna group, we can eliminate the entries with sal not greater than 10,000 and\ncount the rest. (Using this index is less efficient than an index-only scan with\nkey (sal, dna) for the query with sal = 10, 000, because we must read all data\nentries. Thus, the choice between these indexes is influenced by which query is\nmore common.)\nAs another eXEunple, suppose we want to find the minimum sal for each dna:\nSELECT\nKdno, MIN(E.sal)\nFROM\nEmployees E\nGROUP BY E.dno\n\nStomge and Indexing\n2~9\nAn index on dna alone does not allow us to evaluate this query with an index-\nonly scan. However, we can use an index-only plan if we have a composite B+\ntree index on (dna, sal). Note that all data entries in the index with a given\ndna value are stored together (whether or not the index is clustered). :B\\lrther,\nthis group of entries is itself sorted by 8al. An index on (sal, dna) enables us\nto avoid retrieving data records, but the index data entries must be sorted on\ndno.\n8.5.4\nIndex Specification in SQL:1999\nA natural question to ask at this point is how we can create indexes using\nSQL. The SQL:1999 standard does not include any statement for creating or\ndropping index structures.\nIn fact, th.e standard does not even require SQL\nimplementations to support indexes! In practice, of course, every commercial\nrelational DBMS supports one or more kinds of indexes. The following com-\nmand to create a B+ tree index-we discuss B+ tree indexes in Chapter 10----·-is\nillustrative:\nCREATE INDEX IndAgeRating ON Students\nWITH\nSTRUCTURE = BTREE,\nKEY = (age, gpa)\nThis specifies that a B+ tree index is to be created on the Students table using\nthe concatenation of the age and gpa columns as the key. Thus, key values are\npairs of the form (age, gpa) , and there is a distinct entry for each such pair.\nOnce created, the index is automatically maintained by the DBMS adding or\nremoving data entries in response to inserts or deletes of records on the Students\nrelation.\n8.6\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\n'Where does a DBMS store persistent data? How does it bring data into\nmain memory for processing? What DBMS component reads and writes\ndata from main memory, and what is the unit of I/O? (Section 8.1)\n•\n'What is a file organization? vVhat is an index? What is the relationship\nbetween files and indexes?\nCan we have several indexes on a single file\nof records?\nCan an index itself store data records (i.e., act as a file)?\n(Section 8.2)\nIII\nWhat is the 8earch key for an index? What is a data entry in an index?\n(Section 8.2)\n\nCHAPTER S\n•\nvVhat is a clustered index? vVhat is a prinwry index? How many clustered\nindexes can you build on a file? How many unclustered indexes can you\nbuild? (Section 8.2.1)\n•\nHmv is data organized in a hash-ba'lcd index?\n\\Vhen would you use a\nhash-based index? (Section 8.3.1)\n•\nHow is data organized in a tree-based index? vVhen would you use a tree-\nbased index? (Section 8.3.2)\n•\nConsider the following operations:\nscans, equality and 'range selections,\ninserts, and deletes, and the following file organizations: heap files, sorted\nfiles, clustered files, heap files with an unclustered tree index on the search\nkey, and heap files with an unclusteTed hash index. Which file organization\nis best suited for each operation? (Section 8.4)\n•\nWhat are the main contributors to the cost of database operations? Discuss\na simple cost model that reflects this. (Section 8.4.1)\n•\nHow does the expected workload influence physical database design deci-\nsiems such as what indexes to build? vVhy is the choice of indexes a central\naspect of physical database design? (Section 8.5)\n•\nWhat issues are considered in using clustered indexes? What is an indcl;-\nonly evaluation method? \\\\That is its primary advantage? (Section 8.5.2)\n•\nWhat is a composite 8earch key? What are the pros and cons of composite\nsearch keys? (Section 8.5.3)\n•\nWhat SQL commands support index creation? (Section 8.5.4)\nEXERCISES\nExercise 8.1 Answer the following questions about data on external storage in a DBMS:\n1. \\Vhy does a DBMS store data on external storage?\n2. Why are I/O costs important in a DBMS?\n3. \\Vhat is a record id?\nGiven a record's id, how many I/Os are needed to fetch it into\nmain memory?\n4. \\Vhat is the role of the buffer manager in a DBMS? What is the role of the disk space\nmanager? How do these layers interact with the file and access methods layer?\nExercise 8.2 Answer the following questions about files and indexes:\n1. What operations arc supported by the file of records abstraction?\n2. \\Vhat is an index on a file of records? \\Nhat is a search key for an index? Why do we\nneed indexes?\n\nStorage and Inde:ring\nage I gpo, ]\nnarnc\n.-\n_.\n5:3831\nr\\ladayan\nrnadayan(Q:!music\nh\n1.8\nGulclu\nguldu@music\n2.0\nJones\njones(Q;cs\n3.4\nSmith\nsmith(@ee\n3.2\nSmith\nsrnithtg]math\n3.8\nAn Instance of t.he St.udents Relation. Sorted by age\n3. What alternatives are available for the data entries in an index?\n4. What is the difference between a primary index and a secondary index?\n\\Vhat is a\nduplicate data entry in an index? Can a primary index contain duplicates?\n5. What is the difference between a clustered index and an unclustered index? If an index\ncontains data records as 'data entries,' can it be unclustered?\n6. How many clustered indexes can you create on a file? Woule! you always create at least\none clustered index for a file?\n7. Consider Alternatives (1), (2) and (3) for 'data entries' in an index, as discussed in\nExercise 8.3 Consider a relation stored as a randomly ordered file for which the only index\nis an unclustered index on a field called sal. If you want to retrieve all records with sal> 20,\nis using the index always the best alternative? Explain.\nExercise 8.4 Consider the instance of the Students relation shown in Figure 8.6, sorted by\nage: For the purposes of this question, assume that these tuples are stored in a sorted file in\nthe order shown; the first tuple is on page 1 the second tuple is also on page 1; and so on.\nEach page can store up to three data records; so the fourth tuple is on page 2.\nExplain what the data entries in each of the following indexes contain. If the order of entries\nis significant, say so and explain why. If such all index cannot be constructeel, say so and\nexplain why.\n1. An unclustereel index on age using Alternative (1).\n2. An unclusterecl index on age using Alternative (2).\n3. An unclustered index on age using Alternative (:3).\n4. A clustered index on age using Alternative (1).\n5. A clustered index on age using Alt.ernative (2).\n6. A clustered index on age using Alternative (:3).\n7. An unc:lustered index on gpo using Alternative (1).\n8. An unclustered index on gpa using Alternative (2).\n9. An unclustered index on gpa using Alternative (3).\n10. A clustered index on gpa using Alternative (1).\n11. A clustered index on gpa using Alternative (2).\n12. A clustered index on gpa using Alternative (:3).\n\nCHAPTERf8\nSorted file\nClustered file\nUnclustered tree index\nUnclustered hash index\nI/O Cost Comparison\nExercise 8.5 Explain the difference between Hash indexes and B+-tree indexes. In partic-\nular, discuss how equality and range searches work, using an example.\nExercise 8.6 Fill in the I/O costs in Figure 8.7.\nExercise 8.7 If you were about to create an index on a relation, what considerations would\nguide your choice? Discuss:\n1. The choice of primary index.\n2. Clustered versus unclustered indexes.\n3. Hash versus tree indexes.\n4. The use of a sorted file rather than a tree-based index.\n5, Choice of search key for the index. What is a composite search key, and what consid-\nerations are made in choosing composite search keys? What are index-only plans, and\nwhat is the influence of potential index-only evaluation plans on the choice of search key\nfor an index?\nExercise 8.8 Consider a delete specified using an equality condition.\nFor each of the five\nfile organizations, what is the cost if no record qualifies? What is the cost if the condition is\nnot on a key?\nExercise 8.9 What main conclusions can you draw from the discussion of the five basic file\norganizations discussed in Section 8.4? Which of the five organizations would you choose for\na file where the most frequent operations are a<; follows?\n1. Search for records based on a range of field values.\n2. Perform inserts and scans, where the order of records docs not matter.\n3. Search for a record based on a particular field value.\nExercise 8.10 Consider the following relation:\nEmp( eid: integer, sal: integer l age: real, did: integer)\nThere is a clustered index on cid and an llnclustered index on age.\n1. How would you use the indexes to enforce the constraint that eid is a key?\n2. Give an example of an update that is definitely speeded\n1lJi because of the available\nindexes. (English description is sufficient.)\n\nStorage and Inde.7:ing\n~\n3. Give an example of an update that is definitely slowed down because of the indexes.\n(English description is sufficient.)\n4. Can you give an example of an update that is neither speeded up nor slowed down by\nthe indexes?\nExercise 8.11 Consider the following relations:\nEmp( eid: integer, ename: varchar, sal: integer, age: integer, did: integer)\nDept(did: integer, budget: integer, floor: integer, mgr_eid: integer)\nSalaries range from $10,000 to $100,000, ages vary from 20 to 80, each department has about\nfive employees on average, there are 10 floors, and budgets vary from $10,000 to $1 million.\nYou can assume uniform distributions of values.\nFor each of the following queries, which of the listed index choices would you choose to speed\nup the query? If your database system does not consider index-only plans (i.e., data records\nare always retrieved even if enough information is available in the index entry), how would\nyour answer change? Explain briefly.\n1. Query: Print ename, age, and sal for all employees.\n(a) Clustered hash index on (ename, age, sal) fields of Emp.\n(b) Unclustered hash index on (ename, age, sal) fields of Emp.\n(c) Clustered B+ tree index on (ename, age, sal) fields of Emp.\n(d) Unclustered hash index on (eid, did) fields of Emp.\n(e) No index.\n2. Query: Find the dids of departments that are on the 10th floor and have a budget of less\nthan $15,000.\n(a) Clustered hash index on the floor field of Dept.\n(b) Unclustered hash index on the floor' field of Dept.\n(c) Clustered B+ tree index on (floor, budget) fields of Dept.\n(d) Clustered B+ tree index on the budget: field of Dept.\n(e) No index.\nPROJECT-BASED EXERCISES\nExercise 8.12 Answer the following questions:\n1. What indexing techniques are supported in Minibase?\n2. \\;v'hat alternatives for data entries are supported'?\n:3. Are clustered indexes supported?\nBIBLIOGRAPHIC NOTES\nSeveral books discuss file organization in detail [29, :312, 442, 531, 648, 695, 775].\nBibliographic: notes for hash-indexes and B+-trees are included in Chapters 10 and 11.\n\nSTORING DATA:\nDISKS AND FILES\n..\nWhat are the different kinds of memory in a computer system?\n..\nWhat are the physical characteristics of disks and tapes, and how do\nthey affect the design of database systems?\n...\nWhat are RAID storage systems, and what are their advantages?\n..\nHow does a DBMS keep track of space on disks? How does a DBMS\naccess and modify data on disks? What is the significance of pages as\na unit of storage and transfer?\n,..\nHow does a DBMS create and maintain files of records?\nHow are\nrecords arranged on pages, and how are pages organized within a file?\n..\nKey concepts: memory hierarchy, persistent storage, random versus\nsequential devices; physical disk architecture, disk characteristics, seek\ntime, rotational delay, transfer time; RAID, striping, mirroring, RAID\nlevels; disk space manager; buffer manager, buffer pool, replacement\npolicy, prefetching, forcing; file implementation, page organization,\nrecord organization\nA memory is what is left when :iomething happens and does not cornpletely\nunhappen.\n. Edward DeBono\nThis chapter initiates a study of the internals of an RDBivIS. In terms of the\nDBj\\JS architecture presented in Section 1.8, it covers the disk space manager,\n\nBto'ring Data: Disks and Files\nthe buffer manager, and implementation-oriented aspects of the Jiles and access\nmethods layer.\ntems. Section 9.3 discusses how a DBMS manages disk space, and Section 9.4\nexplains how a DBMS fetches data from disk into main memory. Section 9.5\ndiscusses how a collection of pages is organized into a file and how auxiliary\ndata structures can be built to speed up retrieval of records from a file. Sec-\ntion 9.6 covers different ways to arrange a collection of records on a page, and\n9.1\nTHE MEMORY HIERARCHY\nMemory in a computer system is arranged in a hierarchy, a'S shown in Fig-\nure 9.1. At the top, we have primary storage, which consists of cache and\nmain memory and provides very fast access to data. Then comes secondary\nstorage, which consists of slower devices, such as magnetic disks. Tertiary\nstorage is the slowest class of storage devices; for example, optical disks and\ntapes. Currently, the cost of a given amount of main memory is about 100 times\nRequest for data\n----- .....\nData satisfying request\nCPU\n\",/\n.,\nCACHE\n....\n,/\nPrimary storage\nMAIN MEMORY\nf.: ....\n,/\nMAGNETIC DISK\nSecondary storage\n....\n,/\nTAPE\nTertiary storage\nThe Ivlemory Hierarchy\nthe cost of the same amount of disk space, and tapes are even less expensive\nthan disks. Slower storage devices such as tapes and disks play an important\nrole in database systems because the amount of data is typically very large.\nSince buying e110ugh main memory to store all data is prohibitively expensive,\nwe must store data on tapes and disks and build database systems that can\nretrieve data from lower levels of the memory hierarchy into main mernory as\nneeded for processing.",
          "pages": [
            331,
            332,
            333,
            334,
            335,
            336,
            337,
            338,
            339,
            340
          ],
          "relevance": {
            "score": 0.1,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "There are reasons other than cost for storing data on secondary and tertiaJ:y\nstorage. On systems with 32-bit addressing, only 232 bytes can be directly ref-\nerenced in main memory; the number of data objects may exceed this number!\nFurther, data must be maintained across program executions.\nThis requires\nstorage devices that retain information when the computer is restarted (after\na shutdown or a crash); we call such storage nonvolatile. Primary storage is\nusually volatile (although it is possible to make it nonvolatile by adding a bat-\ntery backup feature), whereas secondary and tertiary storage are nonvolatile.\nTapes are relatively inexpensive and can store very large amounts of data. They\nare a good choice for archival storage, that is, when we need to maintain data\nfor a long period but do not expect to access it very often. A Quantum DLT\n4000 drive is a typical tape device; it stores 20 GB of data and can store about\ntwice as much by compressing the data. It records data on 128 tape tracks,\nwhich can be thought of as a linear sequence of adjacent bytes, and supports\na sustained transfer rate of 1.5 MB/sec with uncompressed data (typically 3.0\nMB/sec with compressed data). A single DLT 4000 tape drive can be used to\naccess up to seven tapes in a stacked configuration, for a maximum compressed\ndata capacity of about 280 GB.\nThe main drawback of tapes is that they are sequential access devices. We must\nessentially step through all the data in order and cannot directly access a given\nlocation on tape. For example, to access the last byte on a tape, we would have\nto wind through the entire tape first. This makes tapes unsuitable for storing\noperational data, or data that is frequently accessed. Tapes are mostly used to\nback up operational data periodically.\n9.1.1\nMagnetic Disks\nMagnetic disks support direct access to a desired location and are widely used\nfor database applications. A DBMS provides seamless access to data on disk;\napplications need not worry about whether data is in main memory or disk.\nTo understand how disks work, eonsider Figure 9.2, which shows the structure\nof a disk in simplified form.\nData is stored on disk in units called disk blocks. A disk block is a contiguous\nsequence of bytes and is the unit in which data is written to a disk and read\nfrom a disk. Bloc:ks are arranged in concentric rings called tracks, on one or\nmore platters. Tracks can be recorded on one or both surfaces of a platter;\nwe refer to platters as single-sided or double-sided, accordingly. The set of all\ntracks with the SaIne diameter is called a cylinder, because the space occupied\nby these tracks is shaped like a cylinder; a cylinder contains one track per\nplatter surface. Each track is divided into arcs, called sectors, whose size is a\n\nStoring Data: Disks and Files\nDisk ann\nArm movement\nStructure of a Disk\n____\nBlock\nSectors\nCylinder\n- Tracks\n,.. Platter\n~07\ncharacteristic of the disk and cannot be changed. The size of a disk block can\nbe set when the disk is initialized as a multiple of the sector size.\nAn array of disk heads, one per recorded surface, is moved as a unit; when\none head is positioned over a block, the other heads are in identical positions\nwith respect to their platters. To read or write a block, a disk head must be\npositioned on top of the block.\nCurrent systems typically allow at most one disk head to read or write at any\none time. All the disk heads cannot read or write in\nparallel~-this technique\nwould increa.se data transfer rates by a factor equal to the number of disk\nheads and considerably speed up sequential scans. The rea.son they cannot is\nthat it is very difficult to ensure that all the heads are perfectly aligned on the\ncorresponding tracks. Current approaches are both expensive and more prone\nto faults than disks with a single active heacl. In practice, very few commercial\nproducts support this capability and then only in a limited way; for example,\ntwo disk heads may be able to operate in parallel.\nA disk controller interfaces a disk drive to the computer. It implements com-\nmands to read or write a sector by moving the arm assembly and transferring\ndata to and from the disk surfaces. A checksum is computed for when data\nis written to a sector and stored with the sector. The checksum is computed\nagain when the data on the sector is read back. If the sector is corrupted or the\n\nCHAPTER 9\nAn Example of a Current Disk: The IBM Deskstar 14G~~~Th~l\nIBM Deskstar 14GPX is a 3.5 inch§.J4.4 GB hfl,rd disk with an average\nseek time of 9.1 miUisecoudsTmsec) and an average rotational delay of\n4.17 msec. However, the time to seek from one track to the nexUs just 2.2\nmsec, the maximum seek time is 15.5 :rnsec. The disk has five double-sided\nplatters that spin at 7200 rotations per minute. Ea,ch platter holds 3.35 GB\nof data, with a density of 2.6 gigabit per square inch. The data transfer\nrate is about 13 MB per secmld.\nTo put these numbers in perspective,\nobserve that a disk access takes about 10 msecs, whereas accessing a main\nmemory location typically takes less than 60 nanoseconds!\nread is faulty for some reason, it is very unlikely that the checksum computed\nwhen the sector is read matches the checksum computed when the sector was\nwritten. The controller computes checksums, and if it detects an error, it tries\nto read the sector again. (Of course, it signals a failure if the sector is corrupted\nand read fails repeatedly.)\n~While direct access to any desired location in main memory takes approxi-\nmately the same time, determining the time to access a location on disk is\nmore complicated.\nThe time to access a disk block has several components.\nSeek time is the time taken to move the disk heads to the track on which\na desired block is located. As the size of a platter decreases, seek times also\ndecrease, since we have to move a disk head a shorter distance. Typical platter\ndiameters are 3.5 inches and 5.25 inches.\nRotational delay is the waiting\ntime for the desired block to rotate under the disk head; it is the time required\nfor half a rotation all average and is usually less than seek time.\nTransfer\ntime is the time to actually read or write the data in the block once the head\nis positioned, that is, the time for the disk to rotate over the block.\n9.1.2\nPerformance Implications of Disk Structure\n1. Data must be in mernory for the DBMS to operate on it.\n2. The unit for data transfer between disk and main memory is a block; if a\nsingle item on a block is needed, the entire block is transferred. Reading\nor writing a disk block is called an I/O (for input/output) operation.\n3. The time to read or write a block varies, depending on the location of the\ndata:\naccess time = seek time + rotational delay + tmn8feT time\nThese observations imply that the time taken for database operations is affected\nsignificantly by how data is stored OIl disks.\nThe time for moving blocks to\n\nStoring Data: D'isks and Files\nor from disk usually dOlninates the time taken for database operations.\nTo\nminimize this time, it is necessary to locate data records strategically on disk\nbecause of the geometry and mechanics of disks. In essence, if two records are\nfrequently used together, we should place them close together.\nThe 'closest'\nthat two records can be on a disk is to be on the same block. In decrea<;ing\norder of closeness, they could be on the same track, the same cylinder, or an\nadjacent cylinder.\nTwo records on the same block are obviously as close together as possible,\nbecause they are read or written as part of the same block.\nAs the platter\nspins, other blocks on the track being read or written rotate under the active\nhead. In current disk designs, all the data on a track can be read or written\nin one revolution. After a track is read or written, another disk head becomes\nactive, and another track in the same cylinder is read or written. This process\ncontinues until all tracks in the current cylinder are read or written, and then\nthe arm assembly moves (in or out) to an adjacent cylinder. Thus, we have a\nnatural notion of 'closeness' for blocks, which we can extend to a notion of next\nand previous blocks.\nExploiting this notion of next by arranging records so they are read or written\nsequentially is very important in reducing the time spent in disk l/Os. Sequen-\ntial access minimizes seek time and rotational delay and is much faster than\nrandom access. (This observation is reinforced and elaborated in Exercises 9.5\nand 9.6, and the reader is urged to work through them.)\n9.2\nREDUNDANT ARRAYS OF INDEPENDENT DISKS\nDisks are potential bottlenecks for system performance and storage system rfc'-\nliability. Even though disk performance ha,s been improving continuously, mi-\ncroprocessor performance ha.'s advanced much more rapidly. The performance\nof microprocessors has improved at about 50 percent or more per year, but\ndisk access times have improved at a rate of about 10 percent per year and\ndisk transfer rates at a rate of about 20 percent per year. In addition, since\ndisks contain mechanical elements, they have much higher failure rates than\nelectronic parts of a computer system. If a disk fails, all the data stored on it\nis lost.\nA disk array is an arrangement of several disks, organized to increase per-\nformance and improve reliability of the resulting storage system. Performance\nis increased through data striping. Data striping distributes data over several\ndisks to give the impression of having a single large, very fa'3t disk. Reliabil-\nity is improved through redundancy. Instead of having a single copy of the\ndata, redundant information is maintained. The redundant information is carc-\n\nCHAPTER. Q\nfully organized so that, in C&'3e of a disk failure, it can be used to reconstruct\nthe contents of the failed disk. Disk arrays that implement a combination of\ndata striping and redundancy are called redundant arrays of independent\ndisks, or in short, RAID.! Several RAID organizations, referred to as RAID\nlevels, have been proposed. Each RAID level represents a different trade-off\nbetween reliability and performance.\nIn the remainder of this section, we first discuss data striping and redundancy\nand then introduce the RAID levels that have become industry standards.\n9.2.1\nData Striping\nA disk array gives the user the abstraction of having a single, very large disk.\nIf the user issues an I/O request, we first identify the set of physical disk blocks\nthat store the data requested. These disk blocks may reside on a single disk in\nthe array or may be distributed over several disks in the array. Then the set\nof blocks is retrieved from the disk(s) involved. Thus, how we distribute the\ndata over the disks in the array influences how many disks are involved when\nan I/O request is processed.\nIn data striping, the data is segmented into equal-size partitions distributed\nover multiple disks. The size of the partition is called the striping unit. The\npartitions are usually distributed using a round-robin algorithm:\nIf the disk\narray consists of D disks, then partition i is written onto disk i\nmod D.\nAs an example, consider a striping unit of one bit. Since any D successive data\nbits are spread over all D data disks in the array, all I/O requests involve an\ndisks in the array. Since the smallest unit of transfer from a disk is a block,\neach I/O request involves transfer of at least D blocks. Since we can read the D\nblocks from the D disks in parallel, the transfer rate of each request is D times\nthe transfer rate of a single disk; each request uses the aggregated bandwidth\nof all disks in the array. But the disk access time of the array is ba.'3ically the\naccess time of a single disk, since all disk heads have to move for\" all requests.\nTherefore, for a disk array with a striping unit of a single bit, the number of\nrequests per time unit that the array can process and the average response time\nfor each individual request are similar to that of a single disk.\nAs another exarhple, consider a striping unit of a disk block. In this case, I/O\nrequests of the size of a disk block are processed by one disk in the array. If\nrnany I/O requests of the size of a disk block are made, and the requested\n1Historically, the J in RAID stood for inexpensive, as a large number of small disks was much more\neconornical than a single very large disk. Today, such very large disks are not even manufactured .. ··a\nsign of the impact of RAID.",
          "pages": [
            341,
            342,
            343,
            344,
            345
          ],
          "relevance": {
            "score": 0.2,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Database normalization is a process of organizing data to eliminate redundancy and improve data integrity. It involves decomposing tables into smaller, more manageable parts by removing duplicate columns and ensuring that each column contains atomic values.",
        "explanation": "Normalization solves the problem of data redundancy and inconsistency. When data is normalized, it becomes easier to manage and update. Here’s how it works step-by-step:\n1. **First Normal Form (1NF)**: Ensure each table has a primary key and all columns contain atomic values.\n2. **Second Normal Form (2NF)**: Eliminate partial dependencies by ensuring that non-key columns are fully dependent on the primary key.\n3. **Third Normal Form (3NF)**: Remove transitive dependencies to ensure that only relevant data is stored in each table.\nNormalization is crucial because it helps prevent common issues like data anomalies and ensures that data remains consistent across different parts of a database.",
        "key_points": [
          "1NF ensures tables are atomic and have unique primary keys.",
          "2NF removes partial dependencies, making sure non-key columns depend fully on the primary key.",
          "3NF eliminates transitive dependencies, ensuring only relevant data is stored in each table."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- CREATE a TABLE with redundant data CREATE TABLE Employee ( ID INT, Name VARCHAR(50), Department VARCHAR(50), ManagerID INT, DepartmentManagerID INT ); -- Normalize the TABLE by removing redundancy CREATE TABLE Employee ( ID INT PRIMARY KEY, Name VARCHAR(50) ); CREATE TABLE Department ( ID INT PRIMARY KEY, Name VARCHAR(50), ManagerID INT ); CREATE TABLE Manager ( ID INT PRIMARY KEY, Name VARCHAR(50) );",
            "explanation": "This example shows how a table with redundant data is normalized into three separate tables, each with its own primary key and relevant columns.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- INSERT data into the normalized tables INSERT INTO Employee (ID, Name) VALUES (1, 'John Doe'); INSERT INTO Department (ID, Name, ManagerID) VALUES (1, 'Engineering', 2); INSERT INTO Manager (ID, Name) VALUES (2, 'Jane Smith');",
            "explanation": "This practical example demonstrates inserting data into the normalized tables and how it helps in maintaining data integrity and reducing redundancy.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not identifying all dependencies correctly",
            "incorrect_code": "-- Incorrectly assuming no transitive dependency CREATE TABLE Employee ( ID INT, Name VARCHAR(50), DepartmentID INT, ManagerID INT, DepartmentManagerID INT );",
            "correct_code": "-- Correctly identifying AND removing transitive dependencies CREATE TABLE Employee ( ID INT PRIMARY KEY, Name VARCHAR(50) ); CREATE TABLE Department ( ID INT PRIMARY KEY, Name VARCHAR(50), ManagerID INT ); CREATE TABLE Manager ( ID INT PRIMARY KEY, Name VARCHAR(50) );",
            "explanation": "This mistake occurs when not all dependencies are identified, leading to data anomalies. Correcting it involves ensuring that only relevant data is stored in each table."
          }
        ],
        "practice": {
          "question": "Normalize the following table into 3NF:\nCREATE TABLE Employee (\n    ID INT,\n    Name VARCHAR(50),\n    DepartmentID INT,\n    ManagerID INT,\n    DepartmentManagerID INT\n);",
          "solution": "-- Normalize the table by removing redundancy\nCREATE TABLE Employee (\n    ID INT PRIMARY KEY,\n    Name VARCHAR(50)\n);\nCREATE TABLE Department (\n    ID INT PRIMARY KEY,\n    Name VARCHAR(50),\n    ManagerID INT\n);\nCREATE TABLE Manager (\n    ID INT PRIMARY KEY,\n    Name VARCHAR(50)\n);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER 8\nScan: As for an unclustered tree index, all data entries can be retrieved in-\nexpensively, at a cost of O.125B(D + 8RC) I/Os. However, for each entry, we\nincur the additional cost of one I/O",
      "content_relevance": {
        "score": 0.1,
        "sql_score": 1.0,
        "concept_score": 0.0,
        "non_sql_penalty": 0.2,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "1nf": {
      "id": "1nf",
      "title": "First Normal Form (1NF)",
      "definition": "Eliminating repeating groups and ensuring atomic values",
      "difficulty": "beginner",
      "page_references": [
        328,
        329,
        330,
        331,
        332,
        333,
        334,
        335,
        336,
        337
      ],
      "sections": {
        "definition": {
          "text": "Stomge and Inde:rin,g\n29,3\nClustered indexes, while less expensive to maintain than a fully sorted file, are\nnonetJleless expensive to maintain. When a new record h&'3 to be inserted into\na full leaf page, a new leaf page must be allocated and sorne existing records\nhave to be moved to the new page. If records are identified by a combination of\npage id and slot, &'5 is typically the case in current database systems, all places\nin the datab&\"ie that point to a moved record (typically, entries in other indexes\nfor the same collection of records) must also be updated to point to the new\nlocation.\nLocating all such places and making these additional updates can\ninvolve several disk I/Os.\nClustering must be used sparingly and only when\njustified by frequent queries that benefit from clustering. In particular, there\nis no good reason to build a clustered file using hashing, since range queries\ncannot be answered using h&c;h-indexcs.\nIn dealing with the limitation that at most one index can be clustered, it is\noften useful to consider whether the information in an index's search key is\nsufficient to answer the query. If so, modern database systems are intelligent\nenough to avoid fetching the actual data records.\nFor example, if we have\nan index on age, and we want to compute the average age of employees, the\nDBMS can do this by simply examining the data entries in the index. This is an\nexample of an index-only evaluation. In an index-only evaluation of a query\nwe need not access the data records in the files that contain the relations in the\nquery; we can evaluate the query completely through indexes on the files. An\nimportant benefit of index-only evaluation is that it works equally efficiently\nwith only unclustered indexes, as only the data entries of the index are used in\nthe queries. Thus, unclustered indexes can be used to speed up certain queries\nif we recognize that the DBMS will exploit index-only evaluation.\nDesign Examples Illustrating Clustered Indexes\nTo illustrate the use of a clustered index 011 a range query, consider the following\nexample:\nSELECT\nFROM\nWHERE\nE.dno\nEmployees E\nE.age > 40\nIf we have a H+ tree index on age, we can use it to retrieve only tuples that\nsatisfy the selection E. age> 40. \\iVhether such an index is worthwhile depends\nfirst of all on the selectivity of the condition. vVhat fraction of the employees are\nolder than 40'1 If virtually everyone is older than 40 1 we gain little by using an\nindex 011 age; a sequential scan of the relation would do almost as well. However,\nsuppose that only 10 percent of the employees are older than 40. Now, is an\nindex useful? The answer depends on whether the index is clustered. If the\n\nCHAPTER~ 8\nindex is unclustered, we could have one page I/O per qualifying employee, and\nthis could be more expensive than a sequential scan, even if only 10 percent\nof the employees qualify!\nOn the other hand, a clustered B+ tree index on\nage requires only 10 percent of the l/Os for a sequential scan (ignoring the few\nl/Os needed to traverse from the root to the first retrieved leaf page and the\nl/Os for the relevant index leaf pages).\nAs another example, consider the following refinement of the previous query:\nSELECT\nFROM\nWHERE\nGROUP BY\nKdno, COUNT(*)\nEmployees E\nE.age> 10\nE.dno\nIf a B+ tree index is available on age, we could retrieve tuples using it, sort\nthe retrieved tuples on dna, and so answer the query. However, this may not\nbe a good plan if virtually all employees are more than 10 years old. This plan\nis especially bad if the index is not clustered.\nLet us consider whether an index on dna might suit our purposes better. We\ncould use the index to retrieve all tuples, grouped by dna, and for each dna\ncount the number of tuples with age> 10.\n(This strategy can be used with\nboth hash and B+ tree indexes; we only require the tuples to be grouped, not\nnecessarily sorted, by dna.) Again, the efficiency depends crucially on whether\nthe index is clustered. If it is, this plan is likely to be the best if the condition\non age is not very selective. (Even if we have a clustered index on age, if the\ncondition on age is not selective, the cost of sorting qualifying tuples on dna is\nlikely to be high.) If the index is not clustered, we could perform one page I/O\nper tuple in Employees, and this plan would be terrible. Indeed, if the index\nis not clustered, the optimizer will choose the straightforward plan based on\nsorting on dna. Therefore, this query suggests that we build a clustered index\non dna if the condition on age is not very selective. If the condition is very\nselective, we should consider building an index (not necessarily clustered) on\nage instead.\nClustering is also important for an index on a search key that does not include\na candidate key, that is, an index in which several data entries can have the\nsame key value. To illustrate this point, we present the following query:\nSELECT E.dno\nFROM\nEmployees E\nWHERE\nE.hobby='Stamps'\n\nStomge and Indexing\nIf many people collect stamps, retrieving tuples through an unclustered index\non hobby can be very inefficient. It may be cheaper to simply scan the relation\nto retrieve all tuples and to apply the selection on-the-fly to the retrieved tuples.\nTherefore, if such a query is important, we should consider making the index\non hobby a clustered index. On the other hand, if we assume that eid is a key\nfor Employees, and replace the condition E.hobby= 'Stamps' by E. eid=552, we\nknow that at most one Employees tuple will satisfy this selection condition. In\nthis case, there is no advantage to making the index clustered.\nThe next query shows how aggregate operations can influence the choice of\nindexes:\nSELECT\nE.dno, COUNT(*)\nFROM\nEmployees E\nGROUP BY E.dno\nA straightforward plan for this query is to sort Employees on dno to compute\nthe count of employees for each dno. However, if an index-hash or B+ tree---\non dno is available, we can answer this query by scanning only the index. For\neach dno value, we simply count the number of data entries in the index with\nthis value for the search key. Note that it does not matter whether the index\nis clustered because we never retrieve tuples of Employees.\n8.5.3\nComposite Search Keys\nThe search key for an index can contain several fields; such keys are called\ncomposite search keys or concatenated keys. As an example, consider a\ncollection of employee records, with fields name, age, and sal, stored in sorted\norder by name. Figure 8.5 illustrates the difference between a composite index\nwith key (age, sa0, a composite index with key (sal, age), an index with key\nage, and an index with key sal. All indexes shown in the figure use Alternative\n(2) for data entries.\nIf the search key is composite, an equality query is one in which each field in\nthe search key is bound to a constant. For example, we can ask to retrieve all\ndata entries with age = 20 and sal = 10. The hashed file organization supports\nonly equality queries, since a ha\"ih function identifies the bucket containing\ndesired records only if a value is specified for each field in the search key.\nWith respect to a composite key index, in a range query not all fields in the\nsearch key are bound to constants. For example, we can ask to retrieve all data\nentries with age\n:=0:: 20; this query implies that any value is acceptable for the\nsal field. As another example of a range query, we can ask to retrieve all data\nentries with age < 30 and sal> 40.",
          "pages": [
            328,
            329,
            330
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTEI48\n~I\nname\nage\nsal\n\"\n~-_.-\nbob\ncal\njoe\n----\nsue\nData\nIndex\n75,13\n80,11\n10,12\n<age, sal>\n,~..._---!\nI 11,80 ,,! Index\nR;:IO ~ __:~i\n1 12,20 ~_.1\nU~:~,\n<sal, age>\n../\nComposite Key Indexes\nNate that the index cannot help on the query sal > 40, because, intuitively,\nthe index organizes records by age first and then sal. If age is left unspeci-\nfied, qualifying records could be spread across the entire index. We say that\nan index matches a selection condition if the index can be used to retrieve\njust the tuples that satisf:y the condition. For selections of the form condition\n1\\ ... 1\\ condition, we can define when an index matches the selection as 1'01-\n10ws:4 For a hash index, a selection matches the index if it includes an equality\ncondition ('field = constant') on every field in the composite search key for the\nindex. For a tree index, a selection matches the index if it includes an equal-\nity or range condition on a prefi.T of the composite search key.\n(As examples,\n(age) and (age, sal, department) are prefixes of key (age, sal, depa7'tment) , but\n(age, department) and (sal, department) are not.)\nTrade-offs in Choosing Composite Keys\nA composite key index can support a broader range of queries because it\nmatches more selection conditions. Further, since data entries in a composite\nindex contain more information about the data record (i.e., more fields than\na single-attribute index), the opportunities for index-only evaluation strategies\nare increased.\n(Recall from Section 8.5.2 that an index-only evaluation does\nnot need to access data records, but finds all required field values in the data\nentries of indexes.)\nOn the negative side, a composite index must be updated in response to any\noperation (insert, delete, or update) that modifies any field in the search key.\nA composite index is Hlso likely to be larger than a singk'-attribute search key\n4 For a more general discussion, see Section 14.2.)\n\nStoTage and Inde.Ting\nindex because the size of entries is larger. For a composite B+ tree index, this\nalso means a potential increase in the number of levels, although key COlnpres-\nsion can be used to alleviate this problem (see Section 10.8.1).\nDesign Examples of Composite Keys\nConsider the following query, which returns all employees with 20 < age < 30\nand 3000 < sal < 5000:\nSELECT\nFROM\nWHERE\nE.eid\nEmployees E\nE.age BETWEEN 20 AND 30\nAND E.sal BETWEEN 3000 AND 5000\nA composite index on (age, sal) could help if the conditions in the WHERE clause\nare fairly selective. Obviously, a hash index will not help; a B+ tree (or ISAM)\nindex is required. It is also clear that a clustered index is likely to be superior\nto an unclustered index. For this query, in which the conditions on age and sal\nare equally selective, a composite, clustered B+ tree index on (age, sal) is as\neffective as a composite, clustered B+ tree index on (sal, age). However, the\norder of search key attributes can sometimes make a big difference, as the next\nquery illustrates:\nSELECT\nFROM\nWHERE\nE.eid\nEmployees E\nE.age = 25\nIn this query a composite, clustered B+ tree index on (age, sal) will give good\nperformance because records are sorted by age first and then (if two records\nhave the same age value) by sal. Thus, all records with age = 25 are clustered\ntogether. On the other hand, a composite, clustered B+ tree index on (sal, age)\nwill not perform as well. In this case, records are sorted by sal first, and there-\nfore two records with the same age value (in particular, with age = 25) may be\nquite far apart. In effect, this index allows us to use the range selection on sal,\nbut not the equality selection on age, to retrieve tuples.\n(Good performance\non both variants of the query can be achieved using a single spatial index. \\:Ye\ndiscuss spatial indexes in Chapter 28.)\nComposite indexes are also useful in dealing with many aggregate queries. Con-\nsider:\nSELECT AVG (E.sal)\n\nFROM\nWHERE\nEmployees E\nE.age = 25\nAND Ksal BETWEEN 3000 AND 5000\nCHAPTERt 8\nA composite B+ tree index on (age, sal) allows us to answer the query with\nan index-only scan.\nA composite B+ tree index on (sal, age) also allows us\nto answer the query with an index-only scan, although more index entries are\nretrieved in this case than with an index on (age, sal).\nHere is a variation of an earlier example:\nSELECT\nFROM\nWHERE\nGROUP BY\nKdno, COUNT(*)\nEmployees E\nE.sal=lO,OOO\nKdno\nAn index on dna alone does not allow us to evaluate this query with an index-\nonly scan, because we need to look at the sal field of each tuple to verify that\nsal = 10, 000. However, we can use an index-only plan if we have a composite\nB+ tree index on (sal, dna) or (dna, sal). In an index with key (sal, dno) , all\ndata entries with sal = 10,000 are arranged contiguously (whether or not the\nindex is clustered). Further, these entries are sorted by dna, making it easy to\nobtain a count for each dna group.\nNote that we need to retrieve only data\nentries with sal = 10, 000.\nIt is worth observing that this strategy does not work if the WHERE clause is\nmodified to use sal> 10, 000. Although it suffices to retrieve only index data\nentries-that is, an index-only strategy still applies-these entries must now\nbe sorted by dna to identify the groups (because, for example, two entries with\nthe same dna but different sal values may not be contiguous). An index with\nkey (dna, sal) is better for this query: Data entries with a given dna value are\nstored together, and each such group of entries is itself sorted by sal. For each\ndna group, we can eliminate the entries with sal not greater than 10,000 and\ncount the rest. (Using this index is less efficient than an index-only scan with\nkey (sal, dna) for the query with sal = 10, 000, because we must read all data\nentries. Thus, the choice between these indexes is influenced by which query is\nmore common.)\nAs another eXEunple, suppose we want to find the minimum sal for each dna:\nSELECT\nKdno, MIN(E.sal)\nFROM\nEmployees E\nGROUP BY E.dno\n\nStomge and Indexing\n2~9\nAn index on dna alone does not allow us to evaluate this query with an index-\nonly scan. However, we can use an index-only plan if we have a composite B+\ntree index on (dna, sal). Note that all data entries in the index with a given\ndna value are stored together (whether or not the index is clustered). :B\\lrther,\nthis group of entries is itself sorted by 8al. An index on (sal, dna) enables us\nto avoid retrieving data records, but the index data entries must be sorted on\ndno.\n8.5.4\nIndex Specification in SQL:1999\nA natural question to ask at this point is how we can create indexes using\nSQL. The SQL:1999 standard does not include any statement for creating or\ndropping index structures.\nIn fact, th.e standard does not even require SQL\nimplementations to support indexes! In practice, of course, every commercial\nrelational DBMS supports one or more kinds of indexes. The following com-\nmand to create a B+ tree index-we discuss B+ tree indexes in Chapter 10----·-is\nillustrative:\nCREATE INDEX IndAgeRating ON Students\nWITH\nSTRUCTURE = BTREE,\nKEY = (age, gpa)\nThis specifies that a B+ tree index is to be created on the Students table using\nthe concatenation of the age and gpa columns as the key. Thus, key values are\npairs of the form (age, gpa) , and there is a distinct entry for each such pair.\nOnce created, the index is automatically maintained by the DBMS adding or\nremoving data entries in response to inserts or deletes of records on the Students\nrelation.\n8.6\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\n'Where does a DBMS store persistent data? How does it bring data into\nmain memory for processing? What DBMS component reads and writes\ndata from main memory, and what is the unit of I/O? (Section 8.1)\n•\n'What is a file organization? vVhat is an index? What is the relationship\nbetween files and indexes?\nCan we have several indexes on a single file\nof records?\nCan an index itself store data records (i.e., act as a file)?\n(Section 8.2)\nIII\nWhat is the 8earch key for an index? What is a data entry in an index?\n(Section 8.2)\n\nCHAPTER S\n•\nvVhat is a clustered index? vVhat is a prinwry index? How many clustered\nindexes can you build on a file? How many unclustered indexes can you\nbuild? (Section 8.2.1)\n•\nHmv is data organized in a hash-ba'lcd index?\n\\Vhen would you use a\nhash-based index? (Section 8.3.1)\n•\nHow is data organized in a tree-based index? vVhen would you use a tree-\nbased index? (Section 8.3.2)\n•\nConsider the following operations:\nscans, equality and 'range selections,\ninserts, and deletes, and the following file organizations: heap files, sorted\nfiles, clustered files, heap files with an unclustered tree index on the search\nkey, and heap files with an unclusteTed hash index. Which file organization\nis best suited for each operation? (Section 8.4)\n•\nWhat are the main contributors to the cost of database operations? Discuss\na simple cost model that reflects this. (Section 8.4.1)\n•\nHow does the expected workload influence physical database design deci-\nsiems such as what indexes to build? vVhy is the choice of indexes a central\naspect of physical database design? (Section 8.5)\n•\nWhat issues are considered in using clustered indexes? What is an indcl;-\nonly evaluation method? \\\\That is its primary advantage? (Section 8.5.2)\n•\nWhat is a composite 8earch key? What are the pros and cons of composite\nsearch keys? (Section 8.5.3)\n•\nWhat SQL commands support index creation? (Section 8.5.4)\nEXERCISES\nExercise 8.1 Answer the following questions about data on external storage in a DBMS:\n1. \\Vhy does a DBMS store data on external storage?\n2. Why are I/O costs important in a DBMS?\n3. \\Vhat is a record id?\nGiven a record's id, how many I/Os are needed to fetch it into\nmain memory?\n4. \\Vhat is the role of the buffer manager in a DBMS? What is the role of the disk space\nmanager? How do these layers interact with the file and access methods layer?\nExercise 8.2 Answer the following questions about files and indexes:\n1. What operations arc supported by the file of records abstraction?\n2. \\Vhat is an index on a file of records? \\Nhat is a search key for an index? Why do we\nneed indexes?",
          "pages": [
            331,
            332,
            333,
            334,
            335
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Storage and Inde:ring\nage I gpo, ]\nnarnc\n.-\n_.\n5:3831\nr\\ladayan\nrnadayan(Q:!music\nh\n1.8\nGulclu\nguldu@music\n2.0\nJones\njones(Q;cs\n3.4\nSmith\nsmith(@ee\n3.2\nSmith\nsrnithtg]math\n3.8\nAn Instance of t.he St.udents Relation. Sorted by age\n3. What alternatives are available for the data entries in an index?\n4. What is the difference between a primary index and a secondary index?\n\\Vhat is a\nduplicate data entry in an index? Can a primary index contain duplicates?\n5. What is the difference between a clustered index and an unclustered index? If an index\ncontains data records as 'data entries,' can it be unclustered?\n6. How many clustered indexes can you create on a file? Woule! you always create at least\none clustered index for a file?\n7. Consider Alternatives (1), (2) and (3) for 'data entries' in an index, as discussed in\nExercise 8.3 Consider a relation stored as a randomly ordered file for which the only index\nis an unclustered index on a field called sal. If you want to retrieve all records with sal> 20,\nis using the index always the best alternative? Explain.\nExercise 8.4 Consider the instance of the Students relation shown in Figure 8.6, sorted by\nage: For the purposes of this question, assume that these tuples are stored in a sorted file in\nthe order shown; the first tuple is on page 1 the second tuple is also on page 1; and so on.\nEach page can store up to three data records; so the fourth tuple is on page 2.\nExplain what the data entries in each of the following indexes contain. If the order of entries\nis significant, say so and explain why. If such all index cannot be constructeel, say so and\nexplain why.\n1. An unclustereel index on age using Alternative (1).\n2. An unclusterecl index on age using Alternative (2).\n3. An unclustered index on age using Alternative (:3).\n4. A clustered index on age using Alternative (1).\n5. A clustered index on age using Alt.ernative (2).\n6. A clustered index on age using Alternative (:3).\n7. An unc:lustered index on gpo using Alternative (1).\n8. An unclustered index on gpa using Alternative (2).\n9. An unclustered index on gpa using Alternative (3).\n10. A clustered index on gpa using Alternative (1).\n11. A clustered index on gpa using Alternative (2).\n12. A clustered index on gpa using Alternative (:3).\n\nCHAPTERf8\nSorted file\nClustered file\nUnclustered tree index\nUnclustered hash index\nI/O Cost Comparison\nExercise 8.5 Explain the difference between Hash indexes and B+-tree indexes. In partic-\nular, discuss how equality and range searches work, using an example.\nExercise 8.6 Fill in the I/O costs in Figure 8.7.\nExercise 8.7 If you were about to create an index on a relation, what considerations would\nguide your choice? Discuss:\n1. The choice of primary index.\n2. Clustered versus unclustered indexes.\n3. Hash versus tree indexes.\n4. The use of a sorted file rather than a tree-based index.\n5, Choice of search key for the index. What is a composite search key, and what consid-\nerations are made in choosing composite search keys? What are index-only plans, and\nwhat is the influence of potential index-only evaluation plans on the choice of search key\nfor an index?\nExercise 8.8 Consider a delete specified using an equality condition.\nFor each of the five\nfile organizations, what is the cost if no record qualifies? What is the cost if the condition is\nnot on a key?\nExercise 8.9 What main conclusions can you draw from the discussion of the five basic file\norganizations discussed in Section 8.4? Which of the five organizations would you choose for\na file where the most frequent operations are a<; follows?\n1. Search for records based on a range of field values.\n2. Perform inserts and scans, where the order of records docs not matter.\n3. Search for a record based on a particular field value.\nExercise 8.10 Consider the following relation:\nEmp( eid: integer, sal: integer l age: real, did: integer)\nThere is a clustered index on cid and an llnclustered index on age.\n1. How would you use the indexes to enforce the constraint that eid is a key?\n2. Give an example of an update that is definitely speeded\n1lJi because of the available\nindexes. (English description is sufficient.)",
          "pages": [
            336,
            337
          ],
          "relevance": {
            "score": 0.6,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "First Normal Form (1NF) is a database design principle that ensures each table has atomic columns and no repeating groups.",
        "explanation": "1NF is crucial for creating databases that are both efficient and easy to manage. It addresses two main issues: atomic columns and repeating groups.\n\n**Atomic Columns**: Each column in a table should contain only one value, not multiple values or parts of a value. This ensures that each piece of data is independent and can be processed individually.\n\n**No Repeating Groups**: A table should not have any repeating groups of rows. If you find yourself needing to repeat the same set of columns for multiple rows, it's likely time to normalize further.\n\n1NF helps prevent data redundancy and inconsistencies, making it easier to maintain and query the database.",
        "key_points": [
          "Each column should contain only one value (atomicity)",
          "No repeating groups in a table",
          "Prevents data redundancy and inconsistencies"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- CREATE a simple TABLE with atomic columns CREATE TABLE Employees ( EmployeeID INT, FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "explanation": "This example shows how to create a table where each column contains only one value, adhering to the atomicity rule.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- INSERT data into the Employees TABLE INSERT INTO Employees (EmployeeID, FirstName, LastName) VALUES (1, 'John', 'Doe'); INSERT INTO Employees (EmployeeID, FirstName, LastName) VALUES (2, 'Jane', 'Smith');",
            "explanation": "This practical example demonstrates inserting data into a properly structured table that follows 1NF.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Creating tables with repeating groups",
            "incorrect_code": "-- Incorrect: Repeating group in a single row CREATE TABLE Employees ( EmployeeID INT, FirstName VARCHAR(50), LastName VARCHAR(50), DepartmentIDs INT[] );",
            "correct_code": "-- Correct: Separate tables for normalization CREATE TABLE Employees ( EmployeeID INT, FirstName VARCHAR(50), LastName VARCHAR(50) ); CREATE TABLE Departments ( DepartmentID INT, DepartmentName VARCHAR(50) ); CREATE TABLE EmployeeDepartments ( EmployeeID INT, DepartmentID INT );",
            "explanation": "This mistake occurs when trying to store multiple values in a single column, which violates the atomicity rule. The correct approach is to normalize the data by creating separate tables and using relationships between them."
          }
        ],
        "practice": {
          "question": "Design a table for storing student information that adheres to 1NF. Include columns for student ID, name, age, and grade.",
          "solution": "CREATE TABLE Students (\n    StudentID INT,\n    FirstName VARCHAR(50),\n    LastName VARCHAR(50),\n    Age INT,\n    Grade VARCHAR(2)\n);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Stomge and Inde:rin,g\n29,3\nClustered indexes, while less expensive to maintain than a fully sorted file, are\nnonetJleless expensive to maintain. When a new record h&'3 to be inserted into\na full leaf ",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "2nf": {
      "id": "2nf",
      "title": "Second Normal Form (2NF)",
      "definition": "Eliminating partial dependencies on composite keys",
      "difficulty": "intermediate",
      "page_references": [
        335,
        336,
        337,
        338,
        339,
        340,
        341,
        342,
        343,
        344,
        345,
        346
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER S\n•\nvVhat is a clustered index? vVhat is a prinwry index? How many clustered\nindexes can you build on a file? How many unclustered indexes can you\nbuild? (Section 8.2.1)\n•\nHmv is data organized in a hash-ba'lcd index?\n\\Vhen would you use a\nhash-based index? (Section 8.3.1)\n•\nHow is data organized in a tree-based index? vVhen would you use a tree-\nbased index? (Section 8.3.2)\n•\nConsider the following operations:\nscans, equality and 'range selections,\ninserts, and deletes, and the following file organizations: heap files, sorted\nfiles, clustered files, heap files with an unclustered tree index on the search\nkey, and heap files with an unclusteTed hash index. Which file organization\nis best suited for each operation? (Section 8.4)\n•\nWhat are the main contributors to the cost of database operations? Discuss\na simple cost model that reflects this. (Section 8.4.1)\n•\nHow does the expected workload influence physical database design deci-\nsiems such as what indexes to build? vVhy is the choice of indexes a central\naspect of physical database design? (Section 8.5)\n•\nWhat issues are considered in using clustered indexes? What is an indcl;-\nonly evaluation method? \\\\That is its primary advantage? (Section 8.5.2)\n•\nWhat is a composite 8earch key? What are the pros and cons of composite\nsearch keys? (Section 8.5.3)\n•\nWhat SQL commands support index creation? (Section 8.5.4)\nEXERCISES\nExercise 8.1 Answer the following questions about data on external storage in a DBMS:\n1. \\Vhy does a DBMS store data on external storage?\n2. Why are I/O costs important in a DBMS?\n3. \\Vhat is a record id?\nGiven a record's id, how many I/Os are needed to fetch it into\nmain memory?\n4. \\Vhat is the role of the buffer manager in a DBMS? What is the role of the disk space\nmanager? How do these layers interact with the file and access methods layer?\nExercise 8.2 Answer the following questions about files and indexes:\n1. What operations arc supported by the file of records abstraction?\n2. \\Vhat is an index on a file of records? \\Nhat is a search key for an index? Why do we\nneed indexes?\n\nStorage and Inde:ring\nage I gpo, ]\nnarnc\n.-\n_.\n5:3831\nr\\ladayan\nrnadayan(Q:!music\nh\n1.8\nGulclu\nguldu@music\n2.0\nJones\njones(Q;cs\n3.4\nSmith\nsmith(@ee\n3.2\nSmith\nsrnithtg]math\n3.8\nAn Instance of t.he St.udents Relation. Sorted by age\n3. What alternatives are available for the data entries in an index?\n4. What is the difference between a primary index and a secondary index?\n\\Vhat is a\nduplicate data entry in an index? Can a primary index contain duplicates?\n5. What is the difference between a clustered index and an unclustered index? If an index\ncontains data records as 'data entries,' can it be unclustered?\n6. How many clustered indexes can you create on a file? Woule! you always create at least\none clustered index for a file?\n7. Consider Alternatives (1), (2) and (3) for 'data entries' in an index, as discussed in\nExercise 8.3 Consider a relation stored as a randomly ordered file for which the only index\nis an unclustered index on a field called sal. If you want to retrieve all records with sal> 20,\nis using the index always the best alternative? Explain.\nExercise 8.4 Consider the instance of the Students relation shown in Figure 8.6, sorted by\nage: For the purposes of this question, assume that these tuples are stored in a sorted file in\nthe order shown; the first tuple is on page 1 the second tuple is also on page 1; and so on.\nEach page can store up to three data records; so the fourth tuple is on page 2.\nExplain what the data entries in each of the following indexes contain. If the order of entries\nis significant, say so and explain why. If such all index cannot be constructeel, say so and\nexplain why.\n1. An unclustereel index on age using Alternative (1).\n2. An unclusterecl index on age using Alternative (2).\n3. An unclustered index on age using Alternative (:3).\n4. A clustered index on age using Alternative (1).\n5. A clustered index on age using Alt.ernative (2).\n6. A clustered index on age using Alternative (:3).\n7. An unc:lustered index on gpo using Alternative (1).\n8. An unclustered index on gpa using Alternative (2).\n9. An unclustered index on gpa using Alternative (3).\n10. A clustered index on gpa using Alternative (1).\n11. A clustered index on gpa using Alternative (2).\n12. A clustered index on gpa using Alternative (:3).\n\nCHAPTERf8\nSorted file\nClustered file\nUnclustered tree index\nUnclustered hash index\nI/O Cost Comparison\nExercise 8.5 Explain the difference between Hash indexes and B+-tree indexes. In partic-\nular, discuss how equality and range searches work, using an example.\nExercise 8.6 Fill in the I/O costs in Figure 8.7.\nExercise 8.7 If you were about to create an index on a relation, what considerations would\nguide your choice? Discuss:\n1. The choice of primary index.\n2. Clustered versus unclustered indexes.\n3. Hash versus tree indexes.\n4. The use of a sorted file rather than a tree-based index.\n5, Choice of search key for the index. What is a composite search key, and what consid-\nerations are made in choosing composite search keys? What are index-only plans, and\nwhat is the influence of potential index-only evaluation plans on the choice of search key\nfor an index?\nExercise 8.8 Consider a delete specified using an equality condition.\nFor each of the five\nfile organizations, what is the cost if no record qualifies? What is the cost if the condition is\nnot on a key?\nExercise 8.9 What main conclusions can you draw from the discussion of the five basic file\norganizations discussed in Section 8.4? Which of the five organizations would you choose for\na file where the most frequent operations are a<; follows?\n1. Search for records based on a range of field values.\n2. Perform inserts and scans, where the order of records docs not matter.\n3. Search for a record based on a particular field value.\nExercise 8.10 Consider the following relation:\nEmp( eid: integer, sal: integer l age: real, did: integer)\nThere is a clustered index on cid and an llnclustered index on age.\n1. How would you use the indexes to enforce the constraint that eid is a key?\n2. Give an example of an update that is definitely speeded\n1lJi because of the available\nindexes. (English description is sufficient.)\n\nStorage and Inde.7:ing\n~\n3. Give an example of an update that is definitely slowed down because of the indexes.\n(English description is sufficient.)\n4. Can you give an example of an update that is neither speeded up nor slowed down by\nthe indexes?\nExercise 8.11 Consider the following relations:\nEmp( eid: integer, ename: varchar, sal: integer, age: integer, did: integer)\nDept(did: integer, budget: integer, floor: integer, mgr_eid: integer)\nSalaries range from $10,000 to $100,000, ages vary from 20 to 80, each department has about\nfive employees on average, there are 10 floors, and budgets vary from $10,000 to $1 million.\nYou can assume uniform distributions of values.\nFor each of the following queries, which of the listed index choices would you choose to speed\nup the query? If your database system does not consider index-only plans (i.e., data records\nare always retrieved even if enough information is available in the index entry), how would\nyour answer change? Explain briefly.\n1. Query: Print ename, age, and sal for all employees.\n(a) Clustered hash index on (ename, age, sal) fields of Emp.\n(b) Unclustered hash index on (ename, age, sal) fields of Emp.\n(c) Clustered B+ tree index on (ename, age, sal) fields of Emp.\n(d) Unclustered hash index on (eid, did) fields of Emp.\n(e) No index.\n2. Query: Find the dids of departments that are on the 10th floor and have a budget of less\nthan $15,000.\n(a) Clustered hash index on the floor field of Dept.\n(b) Unclustered hash index on the floor' field of Dept.\n(c) Clustered B+ tree index on (floor, budget) fields of Dept.\n(d) Clustered B+ tree index on the budget: field of Dept.\n(e) No index.\nPROJECT-BASED EXERCISES\nExercise 8.12 Answer the following questions:\n1. What indexing techniques are supported in Minibase?\n2. \\;v'hat alternatives for data entries are supported'?\n:3. Are clustered indexes supported?\nBIBLIOGRAPHIC NOTES\nSeveral books discuss file organization in detail [29, :312, 442, 531, 648, 695, 775].\nBibliographic: notes for hash-indexes and B+-trees are included in Chapters 10 and 11.",
          "pages": [
            335,
            336,
            337,
            338
          ],
          "relevance": {
            "score": 0.6,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "STORING DATA:\nDISKS AND FILES\n..\nWhat are the different kinds of memory in a computer system?\n..\nWhat are the physical characteristics of disks and tapes, and how do\nthey affect the design of database systems?\n...\nWhat are RAID storage systems, and what are their advantages?\n..\nHow does a DBMS keep track of space on disks? How does a DBMS\naccess and modify data on disks? What is the significance of pages as\na unit of storage and transfer?\n,..\nHow does a DBMS create and maintain files of records?\nHow are\nrecords arranged on pages, and how are pages organized within a file?\n..\nKey concepts: memory hierarchy, persistent storage, random versus\nsequential devices; physical disk architecture, disk characteristics, seek\ntime, rotational delay, transfer time; RAID, striping, mirroring, RAID\nlevels; disk space manager; buffer manager, buffer pool, replacement\npolicy, prefetching, forcing; file implementation, page organization,\nrecord organization\nA memory is what is left when :iomething happens and does not cornpletely\nunhappen.\n. Edward DeBono\nThis chapter initiates a study of the internals of an RDBivIS. In terms of the\nDBj\\JS architecture presented in Section 1.8, it covers the disk space manager,\n\nBto'ring Data: Disks and Files\nthe buffer manager, and implementation-oriented aspects of the Jiles and access\nmethods layer.\ntems. Section 9.3 discusses how a DBMS manages disk space, and Section 9.4\nexplains how a DBMS fetches data from disk into main memory. Section 9.5\ndiscusses how a collection of pages is organized into a file and how auxiliary\ndata structures can be built to speed up retrieval of records from a file. Sec-\ntion 9.6 covers different ways to arrange a collection of records on a page, and\n9.1\nTHE MEMORY HIERARCHY\nMemory in a computer system is arranged in a hierarchy, a'S shown in Fig-\nure 9.1. At the top, we have primary storage, which consists of cache and\nmain memory and provides very fast access to data. Then comes secondary\nstorage, which consists of slower devices, such as magnetic disks. Tertiary\nstorage is the slowest class of storage devices; for example, optical disks and\ntapes. Currently, the cost of a given amount of main memory is about 100 times\nRequest for data\n----- .....\nData satisfying request\nCPU\n\",/\n.,\nCACHE\n....\n,/\nPrimary storage\nMAIN MEMORY\nf.: ....\n,/\nMAGNETIC DISK\nSecondary storage\n....\n,/\nTAPE\nTertiary storage\nThe Ivlemory Hierarchy\nthe cost of the same amount of disk space, and tapes are even less expensive\nthan disks. Slower storage devices such as tapes and disks play an important\nrole in database systems because the amount of data is typically very large.\nSince buying e110ugh main memory to store all data is prohibitively expensive,\nwe must store data on tapes and disks and build database systems that can\nretrieve data from lower levels of the memory hierarchy into main mernory as\nneeded for processing.\n\nThere are reasons other than cost for storing data on secondary and tertiaJ:y\nstorage. On systems with 32-bit addressing, only 232 bytes can be directly ref-\nerenced in main memory; the number of data objects may exceed this number!\nFurther, data must be maintained across program executions.\nThis requires\nstorage devices that retain information when the computer is restarted (after\na shutdown or a crash); we call such storage nonvolatile. Primary storage is\nusually volatile (although it is possible to make it nonvolatile by adding a bat-\ntery backup feature), whereas secondary and tertiary storage are nonvolatile.\nTapes are relatively inexpensive and can store very large amounts of data. They\nare a good choice for archival storage, that is, when we need to maintain data\nfor a long period but do not expect to access it very often. A Quantum DLT\n4000 drive is a typical tape device; it stores 20 GB of data and can store about\ntwice as much by compressing the data. It records data on 128 tape tracks,\nwhich can be thought of as a linear sequence of adjacent bytes, and supports\na sustained transfer rate of 1.5 MB/sec with uncompressed data (typically 3.0\nMB/sec with compressed data). A single DLT 4000 tape drive can be used to\naccess up to seven tapes in a stacked configuration, for a maximum compressed\ndata capacity of about 280 GB.\nThe main drawback of tapes is that they are sequential access devices. We must\nessentially step through all the data in order and cannot directly access a given\nlocation on tape. For example, to access the last byte on a tape, we would have\nto wind through the entire tape first. This makes tapes unsuitable for storing\noperational data, or data that is frequently accessed. Tapes are mostly used to\nback up operational data periodically.\n9.1.1\nMagnetic Disks\nMagnetic disks support direct access to a desired location and are widely used\nfor database applications. A DBMS provides seamless access to data on disk;\napplications need not worry about whether data is in main memory or disk.\nTo understand how disks work, eonsider Figure 9.2, which shows the structure\nof a disk in simplified form.\nData is stored on disk in units called disk blocks. A disk block is a contiguous\nsequence of bytes and is the unit in which data is written to a disk and read\nfrom a disk. Bloc:ks are arranged in concentric rings called tracks, on one or\nmore platters. Tracks can be recorded on one or both surfaces of a platter;\nwe refer to platters as single-sided or double-sided, accordingly. The set of all\ntracks with the SaIne diameter is called a cylinder, because the space occupied\nby these tracks is shaped like a cylinder; a cylinder contains one track per\nplatter surface. Each track is divided into arcs, called sectors, whose size is a\n\nStoring Data: Disks and Files\nDisk ann\nArm movement\nStructure of a Disk\n____\nBlock\nSectors\nCylinder\n- Tracks\n,.. Platter\n~07\ncharacteristic of the disk and cannot be changed. The size of a disk block can\nbe set when the disk is initialized as a multiple of the sector size.\nAn array of disk heads, one per recorded surface, is moved as a unit; when\none head is positioned over a block, the other heads are in identical positions\nwith respect to their platters. To read or write a block, a disk head must be\npositioned on top of the block.\nCurrent systems typically allow at most one disk head to read or write at any\none time. All the disk heads cannot read or write in\nparallel~-this technique\nwould increa.se data transfer rates by a factor equal to the number of disk\nheads and considerably speed up sequential scans. The rea.son they cannot is\nthat it is very difficult to ensure that all the heads are perfectly aligned on the\ncorresponding tracks. Current approaches are both expensive and more prone\nto faults than disks with a single active heacl. In practice, very few commercial\nproducts support this capability and then only in a limited way; for example,\ntwo disk heads may be able to operate in parallel.\nA disk controller interfaces a disk drive to the computer. It implements com-\nmands to read or write a sector by moving the arm assembly and transferring\ndata to and from the disk surfaces. A checksum is computed for when data\nis written to a sector and stored with the sector. The checksum is computed\nagain when the data on the sector is read back. If the sector is corrupted or the\n\nCHAPTER 9\nAn Example of a Current Disk: The IBM Deskstar 14G~~~Th~l\nIBM Deskstar 14GPX is a 3.5 inch§.J4.4 GB hfl,rd disk with an average\nseek time of 9.1 miUisecoudsTmsec) and an average rotational delay of\n4.17 msec. However, the time to seek from one track to the nexUs just 2.2\nmsec, the maximum seek time is 15.5 :rnsec. The disk has five double-sided\nplatters that spin at 7200 rotations per minute. Ea,ch platter holds 3.35 GB\nof data, with a density of 2.6 gigabit per square inch. The data transfer\nrate is about 13 MB per secmld.\nTo put these numbers in perspective,\nobserve that a disk access takes about 10 msecs, whereas accessing a main\nmemory location typically takes less than 60 nanoseconds!\nread is faulty for some reason, it is very unlikely that the checksum computed\nwhen the sector is read matches the checksum computed when the sector was\nwritten. The controller computes checksums, and if it detects an error, it tries\nto read the sector again. (Of course, it signals a failure if the sector is corrupted\nand read fails repeatedly.)\n~While direct access to any desired location in main memory takes approxi-\nmately the same time, determining the time to access a location on disk is\nmore complicated.\nThe time to access a disk block has several components.\nSeek time is the time taken to move the disk heads to the track on which\na desired block is located. As the size of a platter decreases, seek times also\ndecrease, since we have to move a disk head a shorter distance. Typical platter\ndiameters are 3.5 inches and 5.25 inches.\nRotational delay is the waiting\ntime for the desired block to rotate under the disk head; it is the time required\nfor half a rotation all average and is usually less than seek time.\nTransfer\ntime is the time to actually read or write the data in the block once the head\nis positioned, that is, the time for the disk to rotate over the block.\n9.1.2\nPerformance Implications of Disk Structure\n1. Data must be in mernory for the DBMS to operate on it.\n2. The unit for data transfer between disk and main memory is a block; if a\nsingle item on a block is needed, the entire block is transferred. Reading\nor writing a disk block is called an I/O (for input/output) operation.\n3. The time to read or write a block varies, depending on the location of the\ndata:\naccess time = seek time + rotational delay + tmn8feT time\nThese observations imply that the time taken for database operations is affected\nsignificantly by how data is stored OIl disks.\nThe time for moving blocks to\n\nStoring Data: D'isks and Files\nor from disk usually dOlninates the time taken for database operations.\nTo\nminimize this time, it is necessary to locate data records strategically on disk\nbecause of the geometry and mechanics of disks. In essence, if two records are\nfrequently used together, we should place them close together.\nThe 'closest'\nthat two records can be on a disk is to be on the same block. In decrea<;ing\norder of closeness, they could be on the same track, the same cylinder, or an\nadjacent cylinder.\nTwo records on the same block are obviously as close together as possible,\nbecause they are read or written as part of the same block.\nAs the platter\nspins, other blocks on the track being read or written rotate under the active\nhead. In current disk designs, all the data on a track can be read or written\nin one revolution. After a track is read or written, another disk head becomes\nactive, and another track in the same cylinder is read or written. This process\ncontinues until all tracks in the current cylinder are read or written, and then\nthe arm assembly moves (in or out) to an adjacent cylinder. Thus, we have a\nnatural notion of 'closeness' for blocks, which we can extend to a notion of next\nand previous blocks.\nExploiting this notion of next by arranging records so they are read or written\nsequentially is very important in reducing the time spent in disk l/Os. Sequen-\ntial access minimizes seek time and rotational delay and is much faster than\nrandom access. (This observation is reinforced and elaborated in Exercises 9.5\nand 9.6, and the reader is urged to work through them.)\n9.2\nREDUNDANT ARRAYS OF INDEPENDENT DISKS\nDisks are potential bottlenecks for system performance and storage system rfc'-\nliability. Even though disk performance ha,s been improving continuously, mi-\ncroprocessor performance ha.'s advanced much more rapidly. The performance\nof microprocessors has improved at about 50 percent or more per year, but\ndisk access times have improved at a rate of about 10 percent per year and\ndisk transfer rates at a rate of about 20 percent per year. In addition, since\ndisks contain mechanical elements, they have much higher failure rates than\nelectronic parts of a computer system. If a disk fails, all the data stored on it\nis lost.\nA disk array is an arrangement of several disks, organized to increase per-\nformance and improve reliability of the resulting storage system. Performance\nis increased through data striping. Data striping distributes data over several\ndisks to give the impression of having a single large, very fa'3t disk. Reliabil-\nity is improved through redundancy. Instead of having a single copy of the\ndata, redundant information is maintained. The redundant information is carc-",
          "pages": [
            339,
            340,
            341,
            342,
            343,
            344
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER. Q\nfully organized so that, in C&'3e of a disk failure, it can be used to reconstruct\nthe contents of the failed disk. Disk arrays that implement a combination of\ndata striping and redundancy are called redundant arrays of independent\ndisks, or in short, RAID.! Several RAID organizations, referred to as RAID\nlevels, have been proposed. Each RAID level represents a different trade-off\nbetween reliability and performance.\nIn the remainder of this section, we first discuss data striping and redundancy\nand then introduce the RAID levels that have become industry standards.\n9.2.1\nData Striping\nA disk array gives the user the abstraction of having a single, very large disk.\nIf the user issues an I/O request, we first identify the set of physical disk blocks\nthat store the data requested. These disk blocks may reside on a single disk in\nthe array or may be distributed over several disks in the array. Then the set\nof blocks is retrieved from the disk(s) involved. Thus, how we distribute the\ndata over the disks in the array influences how many disks are involved when\nan I/O request is processed.\nIn data striping, the data is segmented into equal-size partitions distributed\nover multiple disks. The size of the partition is called the striping unit. The\npartitions are usually distributed using a round-robin algorithm:\nIf the disk\narray consists of D disks, then partition i is written onto disk i\nmod D.\nAs an example, consider a striping unit of one bit. Since any D successive data\nbits are spread over all D data disks in the array, all I/O requests involve an\ndisks in the array. Since the smallest unit of transfer from a disk is a block,\neach I/O request involves transfer of at least D blocks. Since we can read the D\nblocks from the D disks in parallel, the transfer rate of each request is D times\nthe transfer rate of a single disk; each request uses the aggregated bandwidth\nof all disks in the array. But the disk access time of the array is ba.'3ically the\naccess time of a single disk, since all disk heads have to move for\" all requests.\nTherefore, for a disk array with a striping unit of a single bit, the number of\nrequests per time unit that the array can process and the average response time\nfor each individual request are similar to that of a single disk.\nAs another exarhple, consider a striping unit of a disk block. In this case, I/O\nrequests of the size of a disk block are processed by one disk in the array. If\nrnany I/O requests of the size of a disk block are made, and the requested\n1Historically, the J in RAID stood for inexpensive, as a large number of small disks was much more\neconornical than a single very large disk. Today, such very large disks are not even manufactured .. ··a\nsign of the impact of RAID.\n\nStoring Data: Disks and Piles\nRedundancy Schemes:\nAlternatives to the parity scheme include\nschemes based on Hamming codes and Reed-Solomon codes. In ad-\ndition to recovery from single disk failures, Hamming codes can identify-\nwhich disk failed. Reed-Solomon codes can recover from up to two simul-\ntaneous disk failures.\nA detailed discussion of these schemes is beyond\nthe scope of our discussion here; the bibliography provides pointers for the\ninterested reader.\nblocks reside on different disks, we can process all requests in parallel and thus\nreduce the average response time of an I/O request. Since we distributed the\nstriping partitions round-robin, large requests of the size of many contiguous\nblocks involve all disks. We can process the request by all disks in parallel and\nthus increase the transfer rate to the aggregated bandwidth of all D disks.\n9.2.2\nRedundancy\nWhile having more disks increases storage system performance, it also low-\ners overall storage system reliability. Assume that the mean-time-to-failure\n(MTTF), of a single disk is 50, 000 hours (about 5.7 years). Then, the MTTF\nof an array of 100 disks is only 50, 000/100 = 500 hours or about 21 days,\nassuming that failures occur independently and the failure probability of a disk\ndoes not change over time.\n(Actually, disks have a higher failure probability\nearly and late in their lifetimes.\nEarly failures are often due to undetected\nmanufacturing defects; late failures occur since the disk wears out. Failures do\nnot occur independently either: consider a fire in the building, an earthquake,\nor purchase of a set of disks that come from a 'bad' manufacturing batch.)\nReliability of a disk array can be increased by storing redundant information.\nIf a disk fails, the redundant information is used to reconstruct the data on the\nfailed disk.\nRedundancy can immensely increase the MTTF of a disk array.\nWhen incorporating redundancy into a disk array design, we have to make two\nchoices. First, we have to decide where to store the redundant information. We\ncan either store the redundant information on a small number of check disks\nor distribute the redundant information uniformly over all disks.\nThe second choice we have to make is how to compute the redundant infor-\nmation. Most disk arrays store parity information: In the parity scheme, an\nextra check disk contains information that can be used to recover from failure\nof anyone disk in the array. Assume that we have a disk array with D disks\nand consider the first bit on each data disk. Suppose that i of the D data bits\nare 1. The first bit on the check disk is set to 1 if i is odd; otherwise, it is set to",
          "pages": [
            345,
            346
          ],
          "relevance": {
            "score": 0.64,
            "sql_score": 0.8,
            "concept_score": 0.8,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Second Normal Form (2NF) is a database design principle that ensures data integrity by eliminating partial dependencies between columns and ensuring atomicity of each column.",
        "explanation": "In a relational database, Second Normal Form (2NF) helps prevent data redundancy and inconsistencies. It builds upon the First Normal Form (1NF), which eliminates repeating groups and atomic values. To achieve 2NF, a table must meet two conditions:\n\n1. **Atomicity**: Each column in the table should contain indivisible data. There should be no partial dependencies between columns.\n\n2. **Dependency on the whole key**: All non-key attributes (columns) must depend on the entire primary key of the table, not just a part of it.\n\nFor example, consider a table `Employees` with columns `EmployeeID`, `FirstName`, `LastName`, and `Department`. If we have a composite primary key `(EmployeeID, Department)`, then the column `Department` should depend on the entire primary key. If `Department` only depends on `EmployeeID` (partial dependency), this violates 2NF.\n\nUsing 2NF ensures that each piece of data is stored in a single place and reduces the risk of inconsistencies when updating or querying the database.",
        "key_points": [
          "Each column must be atomic ( indivisible)",
          "All non-key attributes must depend on the entire primary key",
          "Prevents data redundancy and inconsistencies"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- CREATE a TABLE in 2NF CREATE TABLE Employees ( EmployeeID INT PRIMARY KEY, FirstName VARCHAR(50), LastName VARCHAR(50), Department VARCHAR(50) );",
            "explanation": "This example creates an `Employees` table with columns that meet the 2NF criteria. Each column is atomic, and all non-key attributes depend on the entire primary key.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- INSERT data into a 2NF TABLE INSERT INTO Employees (EmployeeID, FirstName, LastName, Department) VALUES (1, 'John', 'Doe', 'Sales'); -- Query data FROM a 2NF TABLE SELECT EmployeeID, FirstName, LastName, Department FROM Employees WHERE EmployeeID = 1;",
            "explanation": "This example demonstrates inserting and querying data in a `Employees` table that adheres to the 2NF principles. It ensures that each piece of data is stored correctly and can be retrieved efficiently.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Creating a composite primary key with partial dependency",
            "incorrect_code": "-- Incorrect example CREATE TABLE Employees ( EmployeeID INT, FirstName VARCHAR(50), LastName VARCHAR(50), Department VARCHAR(50), PRIMARY KEY (EmployeeID) );",
            "correct_code": "-- Correct example CREATE TABLE Employees ( EmployeeID INT, FirstName VARCHAR(50), LastName VARCHAR(50), Department VARCHAR(50), PRIMARY KEY (EmployeeID, Department) );",
            "explanation": "This mistake occurs when a composite primary key is created with partial dependency. The correct approach is to ensure that all non-key attributes depend on the entire primary key."
          }
        ],
        "practice": {
          "question": "Design a table for storing `Orders` in 2NF. Each order has an `OrderID`, `CustomerID`, `ProductID`, and `Quantity`. Ensure atomicity and dependency on the whole key.",
          "solution": "Create a table with columns `OrderID`, `CustomerID`, `ProductID`, and `Quantity`. The primary key should be `(OrderID, CustomerID)` to ensure that each order is uniquely identified and that dependencies are met."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER S\n•\nvVhat is a clustered index? vVhat is a prinwry index? How many clustered\nindexes can you build on a file? How many unclustered indexes can you\nbuild? (Section 8.2.1)\n•\nHmv is data organize",
      "content_relevance": {
        "score": 0.6,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "3nf": {
      "id": "3nf",
      "title": "Third Normal Form (3NF)",
      "definition": "Eliminating transitive dependencies",
      "difficulty": "intermediate",
      "page_references": [
        342,
        343,
        344,
        345,
        346,
        347,
        348,
        349,
        350,
        351,
        352,
        353
      ],
      "sections": {
        "definition": {
          "text": "Storing Data: Disks and Files\nDisk ann\nArm movement\nStructure of a Disk\n____\nBlock\nSectors\nCylinder\n- Tracks\n,.. Platter\n~07\ncharacteristic of the disk and cannot be changed. The size of a disk block can\nbe set when the disk is initialized as a multiple of the sector size.\nAn array of disk heads, one per recorded surface, is moved as a unit; when\none head is positioned over a block, the other heads are in identical positions\nwith respect to their platters. To read or write a block, a disk head must be\npositioned on top of the block.\nCurrent systems typically allow at most one disk head to read or write at any\none time. All the disk heads cannot read or write in\nparallel~-this technique\nwould increa.se data transfer rates by a factor equal to the number of disk\nheads and considerably speed up sequential scans. The rea.son they cannot is\nthat it is very difficult to ensure that all the heads are perfectly aligned on the\ncorresponding tracks. Current approaches are both expensive and more prone\nto faults than disks with a single active heacl. In practice, very few commercial\nproducts support this capability and then only in a limited way; for example,\ntwo disk heads may be able to operate in parallel.\nA disk controller interfaces a disk drive to the computer. It implements com-\nmands to read or write a sector by moving the arm assembly and transferring\ndata to and from the disk surfaces. A checksum is computed for when data\nis written to a sector and stored with the sector. The checksum is computed\nagain when the data on the sector is read back. If the sector is corrupted or the\n\nCHAPTER 9\nAn Example of a Current Disk: The IBM Deskstar 14G~~~Th~l\nIBM Deskstar 14GPX is a 3.5 inch§.J4.4 GB hfl,rd disk with an average\nseek time of 9.1 miUisecoudsTmsec) and an average rotational delay of\n4.17 msec. However, the time to seek from one track to the nexUs just 2.2\nmsec, the maximum seek time is 15.5 :rnsec. The disk has five double-sided\nplatters that spin at 7200 rotations per minute. Ea,ch platter holds 3.35 GB\nof data, with a density of 2.6 gigabit per square inch. The data transfer\nrate is about 13 MB per secmld.\nTo put these numbers in perspective,\nobserve that a disk access takes about 10 msecs, whereas accessing a main\nmemory location typically takes less than 60 nanoseconds!\nread is faulty for some reason, it is very unlikely that the checksum computed\nwhen the sector is read matches the checksum computed when the sector was\nwritten. The controller computes checksums, and if it detects an error, it tries\nto read the sector again. (Of course, it signals a failure if the sector is corrupted\nand read fails repeatedly.)\n~While direct access to any desired location in main memory takes approxi-\nmately the same time, determining the time to access a location on disk is\nmore complicated.\nThe time to access a disk block has several components.\nSeek time is the time taken to move the disk heads to the track on which\na desired block is located. As the size of a platter decreases, seek times also\ndecrease, since we have to move a disk head a shorter distance. Typical platter\ndiameters are 3.5 inches and 5.25 inches.\nRotational delay is the waiting\ntime for the desired block to rotate under the disk head; it is the time required\nfor half a rotation all average and is usually less than seek time.\nTransfer\ntime is the time to actually read or write the data in the block once the head\nis positioned, that is, the time for the disk to rotate over the block.\n9.1.2\nPerformance Implications of Disk Structure\n1. Data must be in mernory for the DBMS to operate on it.\n2. The unit for data transfer between disk and main memory is a block; if a\nsingle item on a block is needed, the entire block is transferred. Reading\nor writing a disk block is called an I/O (for input/output) operation.\n3. The time to read or write a block varies, depending on the location of the\ndata:\naccess time = seek time + rotational delay + tmn8feT time\nThese observations imply that the time taken for database operations is affected\nsignificantly by how data is stored OIl disks.\nThe time for moving blocks to\n\nStoring Data: D'isks and Files\nor from disk usually dOlninates the time taken for database operations.\nTo\nminimize this time, it is necessary to locate data records strategically on disk\nbecause of the geometry and mechanics of disks. In essence, if two records are\nfrequently used together, we should place them close together.\nThe 'closest'\nthat two records can be on a disk is to be on the same block. In decrea<;ing\norder of closeness, they could be on the same track, the same cylinder, or an\nadjacent cylinder.\nTwo records on the same block are obviously as close together as possible,\nbecause they are read or written as part of the same block.\nAs the platter\nspins, other blocks on the track being read or written rotate under the active\nhead. In current disk designs, all the data on a track can be read or written\nin one revolution. After a track is read or written, another disk head becomes\nactive, and another track in the same cylinder is read or written. This process\ncontinues until all tracks in the current cylinder are read or written, and then\nthe arm assembly moves (in or out) to an adjacent cylinder. Thus, we have a\nnatural notion of 'closeness' for blocks, which we can extend to a notion of next\nand previous blocks.\nExploiting this notion of next by arranging records so they are read or written\nsequentially is very important in reducing the time spent in disk l/Os. Sequen-\ntial access minimizes seek time and rotational delay and is much faster than\nrandom access. (This observation is reinforced and elaborated in Exercises 9.5\nand 9.6, and the reader is urged to work through them.)\n9.2\nREDUNDANT ARRAYS OF INDEPENDENT DISKS\nDisks are potential bottlenecks for system performance and storage system rfc'-\nliability. Even though disk performance ha,s been improving continuously, mi-\ncroprocessor performance ha.'s advanced much more rapidly. The performance\nof microprocessors has improved at about 50 percent or more per year, but\ndisk access times have improved at a rate of about 10 percent per year and\ndisk transfer rates at a rate of about 20 percent per year. In addition, since\ndisks contain mechanical elements, they have much higher failure rates than\nelectronic parts of a computer system. If a disk fails, all the data stored on it\nis lost.\nA disk array is an arrangement of several disks, organized to increase per-\nformance and improve reliability of the resulting storage system. Performance\nis increased through data striping. Data striping distributes data over several\ndisks to give the impression of having a single large, very fa'3t disk. Reliabil-\nity is improved through redundancy. Instead of having a single copy of the\ndata, redundant information is maintained. The redundant information is carc-\n\nCHAPTER. Q\nfully organized so that, in C&'3e of a disk failure, it can be used to reconstruct\nthe contents of the failed disk. Disk arrays that implement a combination of\ndata striping and redundancy are called redundant arrays of independent\ndisks, or in short, RAID.! Several RAID organizations, referred to as RAID\nlevels, have been proposed. Each RAID level represents a different trade-off\nbetween reliability and performance.\nIn the remainder of this section, we first discuss data striping and redundancy\nand then introduce the RAID levels that have become industry standards.\n9.2.1\nData Striping\nA disk array gives the user the abstraction of having a single, very large disk.\nIf the user issues an I/O request, we first identify the set of physical disk blocks\nthat store the data requested. These disk blocks may reside on a single disk in\nthe array or may be distributed over several disks in the array. Then the set\nof blocks is retrieved from the disk(s) involved. Thus, how we distribute the\ndata over the disks in the array influences how many disks are involved when\nan I/O request is processed.\nIn data striping, the data is segmented into equal-size partitions distributed\nover multiple disks. The size of the partition is called the striping unit. The\npartitions are usually distributed using a round-robin algorithm:\nIf the disk\narray consists of D disks, then partition i is written onto disk i\nmod D.\nAs an example, consider a striping unit of one bit. Since any D successive data\nbits are spread over all D data disks in the array, all I/O requests involve an\ndisks in the array. Since the smallest unit of transfer from a disk is a block,\neach I/O request involves transfer of at least D blocks. Since we can read the D\nblocks from the D disks in parallel, the transfer rate of each request is D times\nthe transfer rate of a single disk; each request uses the aggregated bandwidth\nof all disks in the array. But the disk access time of the array is ba.'3ically the\naccess time of a single disk, since all disk heads have to move for\" all requests.\nTherefore, for a disk array with a striping unit of a single bit, the number of\nrequests per time unit that the array can process and the average response time\nfor each individual request are similar to that of a single disk.\nAs another exarhple, consider a striping unit of a disk block. In this case, I/O\nrequests of the size of a disk block are processed by one disk in the array. If\nrnany I/O requests of the size of a disk block are made, and the requested\n1Historically, the J in RAID stood for inexpensive, as a large number of small disks was much more\neconornical than a single very large disk. Today, such very large disks are not even manufactured .. ··a\nsign of the impact of RAID.",
          "pages": [
            342,
            343,
            344,
            345
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Storing Data: Disks and Piles\nRedundancy Schemes:\nAlternatives to the parity scheme include\nschemes based on Hamming codes and Reed-Solomon codes. In ad-\ndition to recovery from single disk failures, Hamming codes can identify-\nwhich disk failed. Reed-Solomon codes can recover from up to two simul-\ntaneous disk failures.\nA detailed discussion of these schemes is beyond\nthe scope of our discussion here; the bibliography provides pointers for the\ninterested reader.\nblocks reside on different disks, we can process all requests in parallel and thus\nreduce the average response time of an I/O request. Since we distributed the\nstriping partitions round-robin, large requests of the size of many contiguous\nblocks involve all disks. We can process the request by all disks in parallel and\nthus increase the transfer rate to the aggregated bandwidth of all D disks.\n9.2.2\nRedundancy\nWhile having more disks increases storage system performance, it also low-\ners overall storage system reliability. Assume that the mean-time-to-failure\n(MTTF), of a single disk is 50, 000 hours (about 5.7 years). Then, the MTTF\nof an array of 100 disks is only 50, 000/100 = 500 hours or about 21 days,\nassuming that failures occur independently and the failure probability of a disk\ndoes not change over time.\n(Actually, disks have a higher failure probability\nearly and late in their lifetimes.\nEarly failures are often due to undetected\nmanufacturing defects; late failures occur since the disk wears out. Failures do\nnot occur independently either: consider a fire in the building, an earthquake,\nor purchase of a set of disks that come from a 'bad' manufacturing batch.)\nReliability of a disk array can be increased by storing redundant information.\nIf a disk fails, the redundant information is used to reconstruct the data on the\nfailed disk.\nRedundancy can immensely increase the MTTF of a disk array.\nWhen incorporating redundancy into a disk array design, we have to make two\nchoices. First, we have to decide where to store the redundant information. We\ncan either store the redundant information on a small number of check disks\nor distribute the redundant information uniformly over all disks.\nThe second choice we have to make is how to compute the redundant infor-\nmation. Most disk arrays store parity information: In the parity scheme, an\nextra check disk contains information that can be used to recover from failure\nof anyone disk in the array. Assume that we have a disk array with D disks\nand consider the first bit on each data disk. Suppose that i of the D data bits\nare 1. The first bit on the check disk is set to 1 if i is odd; otherwise, it is set to\n\nCHAPTER 9\nO. This bit on the check disk is called the parity of the data bits. The check\ndisk contains parity information for each set of corresponding D data bits.\nTo recover the value of the first bit of a failed disk we first count the number\nof bits that are 1 on the D - 1 nonfailed disks; let this number be j. If j is odd\nand the parity bit is 1, or if j is even and the parity bit is 0, then the value\nof the bit on the failed disk must have been O. Otherwise, the value of the bit\non the failed disk must have been 1. Thus, with parity we can recover from\nfailure of anyone disk. Reconstruction of the lost information involves reading\nall data disks and the check disk.\nFor example, with an additional 10 disks with redundant information, the\nMTTF of our example storage system with 100 data disks can be increased\nto more than 250 years!\n\"What is more important, a large MTTF implies a\nsmall failure probability during the actual usage time of the storage system,\nwhich is usually much smaller than the reported lifetime or the MTTF. (Who\nactually uses lO-year-old disks?)\nIn a RAID system, the disk array is partitioned into reliability groups, where\na reliability group consists of a set of data disks and a set of check disks.\nA\ncommon 7'cdundancy scheme (see box) is applied to each group. The number\nof check disks depends on the RAID level chosen.\nIn the remainder of this\nsection, we assume for ease of explanation that there is only one reliability\ngroup.\nThe reader should keep in mind that actual RAID implementations\nconsist of several reliability groups, and the number of groups plays a role in\nthe overall reliability of the resulting storage system.\n9.2.3\nLevels of Redundancy\nThroughout the discussion of the different RAID levels, we consider sample\ndata that would just fit on four disks. That is, with no RAID technology our\nstorage system would consist of exactly four data disks.\nDepending on the\nRAID level chosen, the number of additional disb varies from zero to four.\nLevel 0: Nonredundant\nA RAID Level 0 system uses data striping to incre,clse the maximum bandwidth\navailable. No redundant information is maintained. \\\\ThUe being the solution\nwith the lowest cost, reliability is a problem, since the MTTF decreases linearly\nwith the number of disk drives in the array. RAID Level 0 has the best write\nperformance of all RAID levels, because absence of redundant information im-\nplies that no redundant information needs to be updated! Interestingly, RAID\nLevel 0 docs not have the best read perfonnancc of all RAID levels, since sys-\n\nStoTing Data: Disks (1'lul Piles\n,\ntems with redundancy have a choice of scheduling disk accesses, as explained\nin the next section.\nIn our example, the RAID Level a solution consists of only four data disks.\nIndependent of the number of data disks, the effective space utilization for a\nRAID Level a system is always 100 percent.\nLevell: Mirrored\nA RAID Level 1 system is the most expensive solution.\nInstead of having\none copy of the data, two identical copies of the data on two different disks are\nlnaintained. This type of redundancy is often called mirroring. Every write of\na disk block involves a write on both disks. These writes may not be performed\nsimultaneously, since a global system failure (e.g., due to a power outage) could\noccur while writing the blocks and then leave both copies in an inconsistent\nstate. Therefore, we always write a block on one disk first and then write the\nother copy on the mirror disk. Since two copies of each block exist on different\ndisks, we can distribute reads between the two disks and allow parallel reads\nof different disk blocks that conceptually reside on the same disk. A read of a\nblock can be scheduled to the disk that h&'3 the smaller expected access time.\nRAID Level 1 does not stripe the data over different disks, so the transfer rate\nfor a single request is comparable to the transfer rate of a single disk.\nIn our example, we need four data and four check disks with mirrored data for\na RAID Levell implementation. The effective space utilization is 50 percent,\nindependent of the number of data disks.\nLevel 0+1: Striping and Mirroring\nRAID Level 0+l---sometimes also referred to\nH..S RA ID Level 10- -combines\nstriping and mirroring. As in RAID Level L read requests of the size of a disk\nblock can be scheduled both to a disk and its mirror image. In addition, read\nrequests of the size of several contiguous blocks benefit froIl1 the aggregated\nbandwidth of all disks. Thc cost for writes is analogous to RAID LevelL\nAs in RAID Levell, our exa.Inple with four data disks requires four check disks\nand the effective space utilization is always 50 percent.\nLevel 2: Error-Correcting Codes\nIn RAID Level 2, the striping unit is a single bit. The redundancy scheme used\nis Hamming code. In our example with four data disks, only three check disks\n\nCHAPTER 9\nare needed. In general, the number of check disks grows logarithmically with\nthe number of data disks.\nStriping at the bit level has the implication that in a disk array with D data\ndisks, the smallest unit of transfer for a read is a set of D blocks. Therefore,\nLevel 2 is good for workloads with many large requests, since for each request,\nthe aggregated bandwidth of all data disks is used. But RAID Level 2 is bad\nfor small requests of the size of an individual block for the same reason. (See\nthe example in Section 9.2.1.)\nA write of a block involves reading D blocks\ninto main memory, modifying D + C blocks, and writing D + C blocks to\ndisk, where C is the number of check disks. This sequence of steps is called a\nread-modify-write cycle.\nFor a RAID Level 2 implementation with four data disks, three check disks\nare needed. In our example, the effective space utilization is about 57 percent.\nThe effective space utilization increases with the number of data disks.\nFor\nexample, in a setup with 10 data disks, four check disks are needed and the\neffective space utilization is 71 percent.\nIn a setup with 25 data disks, five\ncheck disks are required and the effective space utilization grows to 83 percent.\nLevel 3: Bit~Interieaved Parity\nWhile the redundancy schema used in RAID Level 2 improves in terms of cost\nover RAID Level 1, it keeps more redundant information than is necessary.\nHamming code, as used in RAID Level 2, has the advantage of being able to\nidentify which disk has failed.\nBut disk controllers can easily detect which\ndisk has failed. Therefore, the check disks do not need to contain information\nto identify the failed disk.\nInformation to recover the lost data is sufficient.\nInstead of using several disks to store Hamming code, RAID Level 3 has a\nsingle check disk with parity information.\nThus, the reliability overhead for\nRAID Level 3 is a single disk, the lowest overhead possible.\nThe performance characteristics of RAID Levels 2 and 3 are very similar. RAID\nLevel 3 can also process only one I/O at a time, the minimum transfer unit is\nD blocks, and a write requires a read-modify-write cycle.\nLevel 4: Block~Interleaved Parity\nRAID Level 4 hEk\"i a striping unit of a disk block, instead of a single bit as in\nRAID Level 3.\nBlock-level striping has the advantage that read requests of\nthe size of a disk block can be sen;ed entirely by the disk where the requested\nblock resides.\nLarge read requests of several disk blocks can still utilize the\naggregated bandwidth of the D disks.\n\nSto'ring Data: D'isks and Files\nThe \\vrite of a single block still requires a read-modify-write cycle, but only\none data disk and the check disk are involved. The parity on the check disk\ncan be updated without reading all D disk blocks, because the new parity can\nbe obtained by noticing the differences between the old data block and the new\ndata block and then applying the difference to the parity block on the check\ndisk:\nNewParity = (OldData XOR NewData) XOR OldParity\nThe read-modify-write cycle involves reading of the old data block and the old\nparity block, modifying the two blocks, and writing them back to disk, resulting\nin four disk accesses per write. Since the check disk is involved in each write,\nit can easily become the bottleneck.\nRAID Level 3 and 4 configurations with four data disks require just a single\ncheck disk. In our example, the effective space utilization is 80 percent. The\neffective space utilization increases with the number of data disks, since always\nonly one check disk is necessary.\nLevel 5: Block-Interleaved Distributed Parity\nRAID Level 5 improves on Level 4 by distributing the parity blocks uniformly\nover all disks, instead of storing them on a single check disk. This distribution\nhas two advantages. First, several write requests could be processed in parallel,\nsince the bottleneck of a unique check disk has been eliminated. Second, read\nrequests have a higher level of parallelism. Since the data is distributed over\nall disks, read requests involve all disks, whereas in systems with a dedicated\ncheck disk the check disk never participates in reads.\nA RAID Level 5 system has the best performance of all RAID levels with\nredundancy for small and large read ancllarge write requests. Small writes still\nrequire a read-modify-write cycle and are thus less efficient than in RAID Level\n1.\nIn our example, the corresponding RAID Level 5 system has five disks overall\nand thus the effective spa,ce utilization is the same as in RAID Levels 3 and 4.\nLevel 6: P+Q Redundancy\nThe motivation for RAID Level 6 is the observation that recovery from failure\nof a single disk is not sufficient in very large disk arrays. First, in large disk\narrays, a second disk lllight fail before replacement of an already failed disk\n\nCHAPTER~ 9\ncould take place. In addition, the probability of a disk failure during recovery\nof a failed disk is not negligible.\nA RAID Level 6 system uses Reed-Solomon codes to be able to recover from\nup to two simultaneous disk failures.\nRAID Level 6 requires (conceptually)\ntwo check disks, but it also uniformly distributes redundant information at the\nblock level as in RAID Level 5. Thus. the performance characteristics for small\nand large read requests and for large write requests are analogous to RAID\nLevel 5. For small writes, the read-modify-write procedure involves six instead\nof four disks as compared to RAID Level 5, since two blocks with redundant\ninformation need to be updated.\nFor a RAID Level 6 system with storage capacity equal to four data disks, six\ndisks are required. In our example, the effective space utilization is 66 percent.\n9.2.4\nChoice of RAID Levels\nIf data loss is not an issue, RAID Level 0 improves overall system performance\nat the lowest cost. RAID Level 0+1 is superior to RAID Level 1. The main\napplication areas for RAID Level 0+1 systems are small storage subsystems\nwhere the cost of mirroring is moderate. Sometimes, RAID Level 0+1 is used\nfor applications that have a high percentage of writes in their workload, since\nRAID Level 0+1 provides the best write performance.\nRAID Levels 2 and\n4 are always inferior to RAID Levels 3 and 5, respectively.\nRAID Level 3 is\nappropriate for workloads consisting mainly of large transfer requests of several\ncontiguous blocks.\nThe performance of a RAID Level 3 system is bad for\nworkloads with many small requests of a single disk block. RAID Level 5 is a\ngood general-purpose solution. It provides high performance for large as well\nas small requests. RAID Level 6 is appropriate if a higher level of reliability is\nrequired.\n9.3\nDISK SPACE MANAGEMENT\nI\nThe lowest level of software in the DB.lVIS architecture discussed in Section 1.8,\ncalled the disk space manager, manages space on disk. Abstractly, the disk\nspace manager supports the concept of a page as a unit of data and provides\ncOlmnands to allocate or deallocate a page and read or write a page. The size\nof a page is chosen to be the size of a disk block and pages are stored as disk\nblocks so that reading or writing a page can be done in one disk I/O.\nIt is often useful to allocate a sequence of pages (lS a contiguous sequence of\nblocks to hold data frequently accessed in sequential order.\nThis capability\nis essential for exploiting the advantages of sequentially accessing disk blocks,",
          "pages": [
            346,
            347,
            348,
            349,
            350,
            351
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "StoTing Data: Disks and Files\nwhich we discussed earlier in this chapter. Such a capability, if desired, must\nbe provided by the disk space manager to highcr-levellayers of the DBMS.\nThe disk space manager hides details of the underlying hardware (and possibly\nthe operating system) and allows higher levels of the software to think of the\ndata cLS a collection of pages.\n9.3.1\nKeeping Track of Free Blocks\nA database grows and shrinks\n<1.<; records are inserted and deleted over time.\nThe disk space manager keeps track of which disk blocks are in usc, in addition\nto keeping track of which pages are on which disk blocks. Although it is likely\nthat blocks are initially allocated sequentially on disk, subsequent allocations\nand deallocations could in general create 'holes.'\nOne way to keep track of block usage is to maintain a. list of free blocks. As\nblocks are deallocated (by the higher-level software that requests and uses these\nblocks), we can add them to the free list for future use. A pointer to the first\nblock on the free block list is stored in a known location on disk.\nA second way is to maintain a bitmap with one bit for each disk block, which\nindicates whether a block is in use or not.\nA bitmap also allows very fast\nidentification and allocation of contiguous areas on disk.\nThis is difficult to\naccomplish with a linked list approach.\n9.3.2\nUsing OS File Systems to Manage Disk Space\nOperating systems also manage space on disk. Typically, an operating system\nsupports the abstraction of a file as a sequence of bytes.\nThe as manages\nspace on the disk and translates requests, such as \"Read byte i of file f,\" into\ncorresponding low-level instructions:\n\"Read block m of track t of cylinder c\nof disk d.\" A database disk space manager could he built using OS files.\nJ:;'or\nexample, the entire database could reside in one or more as files for which\na number of blocks are allocated (by the aS) and initialized. The disk space\nmanager is then responsible for managing the space in these OS files.\nMany database systems do not rely on the as file system and instead do their\nown disk management, either from scratch or by extending as facilities. The\nreasons are practical <1.<; well eLe; technical One practical reason is that a DB~1S\nvendor who \\vishes to support several as platfonns cannot assume features\nspecific to any OS, for porta.bilit,'rr, and would therefore try to make the DBMS\ncode as self-contained as possible. A technical reason is that on a :32-bit systern,\nthe la.rgest file size is 4 GD, whereas a DBMS may want to access a single file\n\nCHAPTER 9\nlarger than that. A related problem is that typical as files cannot span disk\ndevices, which is often desirable or even necessary in a DBMS. Additional\ntechnical reasons why a DBMS does not rely on the as file system are outlined\nin Section 9.4.2.\n9.4\nBUFFER MANAGER\nTo understand the role of the buffer manager, consider a simple example. Sup-\npose that the database contains 1 million pages, but only 1000 pages of main\nmemory are available for holding data. Consider a query that requires a scan\nof the entire file. Because all the data cannot be brought into main memory at\none time, the DBMS must bring pages into main memory as they are needed\nand, in the process, decide what existing page in main memory to replace to\nmake space for the new page. The policy used to decide which page to replace\nis called the replacement policy.\nIn terms of the DBMS architecture presented in Section 1.8, the buffer man-\nager is the software layer responsible for bringing pages from disk to main\nmemory as needed. The buffer manager manages the available main memory\nby partitioning it into a collection of pages, which we collectively refer to as the\nbuffer pool. The main memory pages in the buffer pool are called frames;\nit is convenient to think of them as slots that can hold a page (which usually\nresides on disk or other secondary storage media).\nHigher levels of the DBMS code can be written without worrying about whether\ndata pages are in memory or not; they ask the buffer manager for the page,\nand it is brought into a frame in the buffer pool if it is not already there.\nOf course, the higher-level code that requests a page must also release the\npage when it is no longer needed, by informing the buffer manager, so that\nthe frame containing the page can be reused. The higher-level code must also\ninform the buffer manager if it modifies the requested page; the buffer manager\nthen makes sure that the change is propagated to the copy of the page on disk.\nBuffer management is illustrated in Figure 9.3.\nIn addition to the buffer pool itself, the buffer manager maintains some book-\nkeeping information and two variables for each frame in the pool:\npirLcount\nand dirty. The number of times that the page currently in a given frame has\nbeen requested but not released-the number of current users of the page--is\nrecorded in the pin_count variable for that frame. The Boolean variable dirty\nindicates whether the page ha.<; been modified since it was brought into the\nbuffer pool from disk.",
          "pages": [
            352,
            353
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Third Normal Form (3NF) is a database design principle that ensures data is stored in a way that minimizes redundancy and dependency issues, making it easier to manage and query.",
        "explanation": "In databases, data can be organized into tables, and each table can have relationships with other tables. When designing these tables, we want to ensure that the data is as simple and straightforward as possible. This is where Third Normal Form comes in. A database is said to be in 3NF if it meets three conditions:\n1. It is in First Normal Form (1NF), meaning each column contains atomic values and there are no repeating groups.\n2. It is in Second Normal Form (2NF), meaning all non-key columns are fully dependent on the primary key.\n3. It avoids partial dependencies, which means that if a table has a composite primary key, every non-key column must be dependent on the entire primary key, not just part of it.\nBy following these rules, we ensure that our database is organized in a way that makes it easier to understand and manage.",
        "key_points": [
          "Avoiding partial dependencies ensures data integrity and reduces redundancy.",
          "3NF helps in creating more efficient queries and reducing the need for complex joins.",
          "It simplifies the design of database schemas, making them easier to maintain and update."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- CREATE a TABLE in 3NF CREATE TABLE Students ( StudentID INT PRIMARY KEY, FirstName VARCHAR(50), LastName VARCHAR(50) );",
            "explanation": "This example demonstrates creating a simple table for students with a primary key and two non-key columns. This structure avoids partial dependencies.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- INSERT data into the Students TABLE INSERT INTO Students (StudentID, FirstName, LastName) VALUES (1, 'John', 'Doe'); -- Query data FROM the Students TABLE SELECT * FROM Students;",
            "explanation": "This practical example shows inserting a record into the Students table and then querying it. This demonstrates how 3NF helps in managing data efficiently.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Creating tables with composite primary keys without ensuring full dependency",
            "incorrect_code": "-- Incorrect example of a composite primary key CREATE TABLE Students ( StudentID INT, CourseID INT, PRIMARY KEY (StudentID) );",
            "correct_code": "-- Correct example of a composite primary key CREATE TABLE Students ( StudentID INT, CourseID INT, PRIMARY KEY (StudentID, CourseID) );",
            "explanation": "This mistake occurs when a table has a composite primary key but only one part of the key is used in non-key columns. To avoid this, ensure that all non-key columns depend on the entire primary key."
          }
        ],
        "practice": {
          "question": "Design a table for storing information about books in a library. Ensure it follows the rules of Third Normal Form.",
          "solution": "CREATE TABLE Books (\n    BookID INT PRIMARY KEY,\n    Title VARCHAR(100),\n    Author VARCHAR(50)\n);\nCREATE TABLE Library (\n    LibraryID INT PRIMARY KEY,\n    Address VARCHAR(200)\n);\nCREATE TABLE BookCopies (\n    CopyID INT PRIMARY KEY,\n    BookID INT,\n    LibraryID INT,\n    FOREIGN KEY (BookID) REFERENCES Books(BookID),\n    FOREIGN KEY (LibraryID) REFERENCES Library(LibraryID)\n);"
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "Storing Data: Disks and Files\nDisk ann\nArm movement\nStructure of a Disk\n____\nBlock\nSectors\nCylinder\n- Tracks\n,.. Platter\n~07\ncharacteristic of the disk and cannot be changed. The size of a disk block ",
      "content_relevance": {
        "score": 0.7,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "transactions": {
      "id": "transactions",
      "title": "Database Transactions",
      "definition": "Atomic units of work that maintain database consistency (ACID properties)",
      "difficulty": "intermediate",
      "page_references": [
        355,
        356,
        357,
        358,
        359,
        360,
        361,
        362,
        363,
        364,
        365,
        366,
        367,
        368,
        369,
        370,
        371,
        372,
        373,
        374,
        375,
        376
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER ,,9\nThe buffer manager will not read another page into a frame until its pi'll-count\nbecomes 0, that is, until all requestors of the page have unpilln~d it.\nIf a requested page is not in the buffer pool and a free frame is not available\nin the buffer pool, a frame with pirl-count 0 is chosen for replacement. If there\nare many such frames, a frame is chosen according to the buffer manager's\nreplacement policy. vVe discuss various replacement policies in Section 9.4.1.\n\\-\\Then a page is eventually chosen for replacement, if the dir'ty bit is not set,\nit means that the page h1-:1..<; not been modified since being brought into main\nmemory.\nHence, there is no need to write the page back to disk; the copy\non disk is identical to the copy in the frame, and the frame can simply be\noverwritten by the newly requested page. Otherwise, the modifications to the\npage must be propagated to the copy on disk.\n(The crash recovery protocol\nmay impose further restrictions, as we saw in Section 1.7. For example, in the\nWrite-Ahead Log (WAL) protocol, special log records are used to describe the\nchanges made to a page. The log records pertaining to the page to be replaced\nmay well be in the buffer; if so, the protocol requires that they be written to\ndisk before the page is written to disk.)\nIf no page in the buffer pool has pin_count 0 and a page that is not in the pool\nis requested, the buffer manager must wait until some page is released before\nresponding to the page request. In practice, the transaction requesting the page\nmay simply be aborted in this situation! So pages should be released-by the\ncode that calls the buffer manager to request the page- as soon as possible.\nA good question to ask at this point is, \"What if a page is requested by several\ndifferent transactions?\"\nThat is, what if the page is requested by programs\nexecuting independently on behalf of different users?\nSuch programs could\nmake conflicting changes to the page. The locking protocol (enforced by higher-\nlevel DBMS code, in particular the transaction manager) ensures that each\ntransaction obtains a shared or exclusive lock before requesting a page to read\nor rnodify.\nTwo different transactions cannot hold an exclusive lock on the\nsame page at the same time; this is how conflicting changes are prevented. The\nbuffer rnanager simply\n~1..'3surnes tha.t the appropriate lock has been obtained\nbefore a page is requested.\n9.4.1\nBuffer Replacement Policies\nThe policy used to choose an unpinned page for replacement can affect the time\ntaken for database operations considerably. Of the man,Y alternative policies,\neach is suitable in different situations.\n\nSto7~ing Data: Disks and Files\nThe best-known replacement policy is least recently used (LRU). This can\nbe implemented in the buffer manager using a queue of pointers to frames with\npin_count O.\nA frame is added to the end of the queue when it becomes a\ncandidate for replacement (that is, when the p'irLco'unt goes to 0). The page\nchosen for replacement is the one in the frame at the head of the queue.\nA variant of LRU, called clock replacement, has similar behavior but less\noverhead. The idea is to choose a page for replacement using a current variable\nthat takes on values 1 through N, where N is the number of buffer frames, in\ncircular order. Vie can think of the frames being arranged in a circle, like a\nclock's face, and current as a clock hand moving across the face. To approximate\nLRU behavior, each frame also has an associated referenced bit, which is turned\non when the page p'in~count goes to O.\nThe current frame is considered for replacement. If the frame is not chosen for\nreplacement, current is incremented and the next frame is considered; this pro-\ncess is repeated until some frame is chosen. If the current frame has pin_count\ngreater than 0, then it is not a candidate for replacement and current is in-\ncremented. If the current frame has the referenced bit turned on, the clock\nalgorithm turns the referenced bit off and increments cm'rent-this way, a re-\ncently referenced page is less likely to be replaced. If the current frame has\np'irLcount 0 and its referenced bit is off, then the page in it is chosen for re-\nplacement. If all frames are pinned in some sweep of the clock hand (that is,\nthe value of current is incremented until it repeats), this means that no page\nin the buffer pool is a replacement candidate.\nThe LRU and clock policies are not always the best replacement strategies for a\ndatabase system, particularly if many user requests require sequential scans of\nthe data. Consider the following illustrative situation. Suppose the buffer pool\nh<4'3 10 frames, and the file to be scanned has 10 or fewer pages.\nAssuming,\nfor simplicity, that there are no competing requests for pages, only the first\nscan of the file does any I/O. Page requests in subsequent scans always find the\ndesired page in the buffer pool. On the other hand, suppose that the file to be\nscanned has 11 pages (which is one more than the number of available pages\nin the buffer pool).\nUsing LRU, every scan of the file will result in reading\nevery page of the file! In this situation, called sequential flooding, LRU is\nthe worst possible replacement strategy.\nOther replacement policies include first in first out (FIFO) and most re-\ncently used (MRU), which also entail overhead similar to LRU, and random,\narnong others. The details of these policies should be evident from their names\nand the preceding discussion of LRU and clock.\n\n,----------------\n_ _.._._-_..----~~------------ -··-~~l\nBuffer Management in Practice: IBM DB2 and Sybase ASE allow\n!\nbuffers to be partitioned into named pools. Each database, table, or in-\nI\ndex can be bound to one of these pools. Each pool can be configured to\nI\nuse either LRU or clock replacement in ASE; DB2 uses a variant of clock\n!\nreplacement, with the initial clock value based on the nature of the page\n!\n(e.g., index non-leaves get a higher starting clock value, which delays their\nreplacement). Interestingly, a buffer pool client in DB2 can explicitly indi-\ncate that it hates a page, making the page the next choice for replacement.\nAs a special case, DB2 applies MRU for the pages fetched in some utility\noperations (e.g., RUNSTATS), and DB2 V6 also supports FIFO. Informix\nand Oracle 7 both maintain a single global buffer pool using LRU; Mi-\ncrosoft SQL Server has a single pool using clock replacement. In Oracle\n8, tables can be bound to one of two pools; one has high priority, and the\nsystem attempts to keep pages in this pool in memory.\nBeyond setting a maximum number of pins for a given transaction, there\nare typically no features for controlling buffer pool usage on a per-\ntransaction basis. Microsoft SQL Server, however, supports a reservation of\nbuffer pages by queries that require large amounts of memory (e.g., queries\ninvolving sorting or hashing).\n9.4.2\nBuffer Management in DBMS versus OS\nObvious similarities exist between virtual memory in operating systems and\nbuffer management in database management systems. In both cases, the goal\nis to provide access to more data than will fit in main memory, and the basic\nidea is to bring in pages from disk to main memory a.<.; needed, replacing pages\nno longer needed in main memory.\nWhy can't we build a DBMS using the\nvirtual memory capability of an OS? A DBMS can often predict the order\nin which pages will be accessed, or page reference patterns, much more\naccurately than is typical in an as environment, and it is desirable to utilize\nthis property. Further, a DBMS needs more control over when a page is written\nto disk than an as typically provides.\nA DBMS can often predict reference patterns because most page references\nare generated by higher-level operations (such as sequential scans or particular\nimplementations of various relational algebra opera.tors) with a. known pattern\nof page accesses. This ability to predict reference patterns allows for a better\nchoice of pages to replace and makes the idea of specialized buffer replacmnent\npolicies more attractive in the DBMS environment.\nEven more important, being able to predict reference patterns enables the usc\nof a simple and very effective strategy called prefetching of pages.\nThe\n\nStoTing Data: Disks and Files\n~\"\n~--~--\"------------------------~\nPrefetching:\nIBM DB2 supports both sequential alld list prefeteh\n(prefetching a list of pages). In general, the prefeteh size is 32 4KB· pages,\nbut this can be set by the user. £tor some sequential type datahaseutilities\n(e.g., COPY, RUNSTATS), DB2 prefetches up to 64 4KB pages,·!'cJr a\nsmaller buffer pool (i.e., less than 1000 buffers), the prefetch quantity is\nadjusted downward to 16 or 8 pages. The prefetch size can be configured by\nthe user; for certain environments, it may be best to prefetch 1000 pages at\na time! Sybase ASE supports asynchronous prefetching of up to 256 pages,\nand uses this capability to reduce latency during indexed access to a table\nin a range scan. Oracle 8 uses prefetching for sequential scan, retrieving\nlarge objects, and certain index scans.\nMicrosoft SQL Server supports\nprefetching for sequential scan and for scarlS along the leaf level ofa B+\ntree index, and the prefetch size can be adjusted a<; a scan progresses. SQL\nServer also uses asynchronous prefetching extensively. Informix supports\nprefetching with a user-defined prefetch size.\nbuffer manager can anticipate the next several page requests and fetch the\ncorresponding pages into memory before the pages are requested. This strategy\nhas two benefits. First, the pages are available in the buffer pool when they\nare requested. Second, reading in a contiguous block of pages is much faster\nthan reading the same pages at different times in response to distinct requests.\n(Review the discussion of disk geometry to appreciate why this is so.) If the\npages to be prcfetched are not contiguous, recognizing that several pages need\nto be fetched can nonetheless lead to faster I/O because an order of retrieval\ncan be chosen for these pages that minimizes seek times and rotational delays.\nIncidentally, note that the I/O can typically be done concurrently with CPU\ncomputation.\nOnce the prefetch request is issued to the disk, the disk is re-\nsponsible for reading the requested pages into memory pages and the CPU can\ncontinue to do other work.\nA DBMS also requires the ability to explicitly force a page to disk, that is, to\nensure that the copy of the page on disk is updated with the copy in memory.\nAs a related point, a DBMS must be able to ensure that certain pages in the\nbuffer pool are written to disk before certain other pages to implement the ';VAL\nprotocol for cra,<;h recovery, as we saw in Section 1.7. Virtual memory imple-\nmentations in operating systems cannot be relied on to provide such control\nover when pages are written to disk; the OS command to write a page to disk\nmay be implemented by essentially recording the write request and deferring\nthe actual modification of the disk copy. If the systern crashes in the interim,\nthe effects can be catastrophic for a DBMS. (Crash recovery is discllssed further\nin Chapter 18.)\n\nCHAPTER~9\nIndexes as Files: In Chapter 8, we presented indexes as a way of 6rga11i~~--··w·l\ning data records for efficient search. From an implementation standpoint,\nI\nI\ni\nindexes are just another kind of file, containing records that dil'ect traffic\non requests for data records. For example, a tree index is a collection of\nrecords organized into one page per node in the tree. It is convenient to\nactually think of a tree index as two files, because it contains two kinds\nof records: (1) a file of index entries, which are records with fields for the\nindex's search key, and fields pointing to a child node, and (2) a file of data\nentries, whose structure depends on the choice of data entry alternative.\n9.5\nFILES OF RECORDS\nWe now turn our attention from the way pages are stored on disk and brought\ninto main memory to the way pages are used to store records and organized\ninto logical collections or files. Higher levels of the DBMS code treat a page as\neffectively being a collection of records, ignoring the representation and storage\ndetails.\nIn fact, the concept of a collection of records is not limited to the\ncontents of a single page; a file can span several pages.\nIn this section, we\nconsider how a collection of pages can be organized as a file. We discuss how\nthe space on a page can be organized to store a collection of records in Sections\n9.6 and 9.7.\n9.5.1\nImplementing Heap Files\nThe data in the pages of a heap file is not ordered in any way, and the only\nguarantee is that one can retrieve all records in the file by repeated requests\nfor the next record. Every record in the file has a unique rid, and every page\nin a file is of the same size.\nSupported operations on a heap file include CTeatc and destmy files, 'insert a\nrecord, delete a record with a given rid, get a record with a given rid, and scan\nall records in the file. To get or delete a record with a given rid, note that we\nmust be able to find the id of the page containing the record, given the id of\nthe record.\nvVe must keep track of the pages in each heap file to support scans, and we must\nkeep track of pages that contain free space to implement insertion efficiently.\n\\Ve discuss two alternative ways to rnaintain this information. In each of these\nalternatives, pages must hold two pointers (which are page ids) for file-level\nbookkeeping in addition to the data.\n\nStoring Data: Disks aTul File,s\n32~\nLinked List of Pages\nOne possibility is to maintain a heap file as a doubly linked list of pages. The\nDBMS can remember where the first page is located by maintaining a table\ncontaining pairs of (heap_file_name, page_Laddr) in a known location on disk.\nWe call the first page of the file the header page.\nAn important task is to maintain information about empty slots created by\ndeleting a record from the heap file. This task has two distinct parts: how to\nkeep track of free space within a page and how to keep track of pages that have\nsome free space. We consider the first part in Section 9.6. The second part can\nbe addressed by maintaining a doubly linked list of pages with free space and\na doubly linked list of full pages; together, these lists contain all pages in the\nheap file. This organization is illustrated in Figure 9.4; note that each pointer\nis really a page id.\nData\npage\nData\npage\nLinked list of\nfull pages\nLinked list of pages\nwith free space\npage\nData\nData\npage\nHeap File Organization with a Linked List\nIf a new page is required, it is obtained by making a request to the disk space\nmanager and then added to the list of pages in the file (probably as a page\nwith free space, because it is unlikely that the new record will take up all the\nspace on the page). If a page is to be deleted from the heap file, it is removed\nfrom the list and the disk space Inanager is told to deallocate it. (Note that the\nscheme can easily be generalized to allocate or deallocate a sequence of several\npages and maintain a doubly linked list of these page sequences.)\nOne disadvantage of this scheIue is that virtually all pages in a file will be on\nthe free list if records are of variable length, because it is likely that every page\nha\",,, at least a few free bytes. To insert a typical record, we must retrieve and\nexaInine several pages on the free list before we find one with enough free space.\nThe directory-based heap file organization that we discuss next addresses this\nproblem.",
          "pages": [
            355,
            356,
            357,
            358,
            359,
            360
          ],
          "relevance": {
            "score": 0.3,
            "sql_score": 1.0,
            "concept_score": 0.2,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Directory of Pages\nCHAPTER,g\nAn alternative to a linked list of pages is to maintain a directory of pages.\nThe DBMS must remember where the first directory page of each heap file is\nlocated. The directory is itself a collection of pages and is shown as a linked\nlist in Figure 9.5. (Other organizations are possible for the directory itself, of\ncourse.)\nHeader page\nData\npage 2\nData\npage N\nDIRECTORY\nHeap File Organization with a Directory\nEach directory entry identifies a page (or a sequence of pages) in the heap file.\nAs the heap file grows or shrinks, the number of entries in the directory-and\npossibly the number of pages in the directory itself--grows or shrinks corre-\nspondingly. Note that since each directory entry is quite small in comparison to\na typical page, the size of the directory is likely to be very small in comparison\nto the size of the heap file.\nFree space can be managed by maintaining a bit per entry, indicating whether\nthe corresponding page has any free space, or a count per entry, indicating the\namount of free space on the page. If the file contains variable-length records,\nwe can examine the free space count for an entry to determine if the record\nfits on the page pointed to by the entry. Since several entries fit on a directory\npage, we can efficiently search for a data page with enough space to hold a\nrecord to be inserted.\n9.6\nPAGE FORMATS\nThe page abstraction is appropriate when dealing with I/O issue-s, but higher\nlevels of the DBMS see data a..<; a collection of records.\nIn this section, we\n\nStoTing Data: D'i.5ks and Files\n327.\nRids in COInmercial Systems:\nIBM DB2 l Informix, Microsoft SQL\nServer l Oracle 8, and Sybase ASE all implement record ids as a page id\nand slot number. Syba..c;e ASE uses the following page organization, which\nis typical: Pages contain a header followed by the rows and a slot array.\nThe header contains the page identity, its allocation state, page free space\nstate, and a timestamp. The slot array is simply a mapping of slot number\nto page offset.\nOracle 8 and SQL Server use logical record ids rather than page id and slot\nnumber in one special case: If a table has a clustered index, then records in\nthe table are identified using the key value for the clustered index. This has\nthe advantage that secondary indexes need not be reorganized if records\nare moved ac~oss pages.\nconsider how a collection of records can be arranged on a page. We can think\nof a page as a collection of slots, each of which contains a record. A record is\nidentified by using the pair (page id, slot number); this is the record id (rid).\n(We remark that an alternative way to identify records is to assign each record\na unique integer as its rid and maintain a table that lists the page and slot of\nthe corresponding record for each rid. Due to the overhead of maintaining this\ntable, the approach of using (page id, slot number) as an rid is more common.)\nWe now consider some alternative approaches to managing slots on a page.\nThe main considerations are how these approaches support operations such as\nsearching, inserting, or deleting records on a page.\n9.6.1\nFixed-Length Records\nIf all records on the page are guaranteed to be of the same length, record slots\narc uniform and can be arranged consecutively within a page. At any instant,\nsome slots are occupied by records and others are unoccupied. When a record\nis inserted into the page, we must locate an empty slot and place the record\nthere. The main issues are how we keep track of empty slots and how we locate\nall records on a page. The alternatives hinge on how we handle the deletion of\na record.\nThe first alternative is to store records in the first N slots (where N is the\nnumber of records on the page); whenever a record is deleted, we move the last\nrecord on the page into the vacated slot. This format allows us to locate the\nith record on a page by a simple offset calculation, and all empty slots appear\ntogether at the end of the page. However, this approach docs not work if there\n\nCHAPTER.9\nare external references to the record that is moved (because the rid contains\nthe slot number, which is now changed).\nThe second alternative is to handle deletions by using an array of bits, one per\nslot, to keep track of free slot information. Locating records on the page requires\nscanning the bit array to find slots whose bit is on; when a record is deleted,\nits bit is turned off. The two alternatives for storing fixed-length records are\nillustrated in Figure 9.6. Note that in addition to the information about records\non the page, a page usually contains additional file-level information (e.g., the\nid of the next page in the file).\nThe figure does not show this additional\ninformation.\nPacked\nUnpacked, Bitmap\nM\n1J\nNumber of slots\nSlot 1\nSlot 2\nSlot 3\no\n~\nSlot M\nI I 1011\\ M\nFree ~\nSpace\nJ\n'-S\"Page ~\nJ\nHeader\nNumber of records\no\n...\nIN\nSlot N\nSlot 1\nSlot 2\nAlternative Page Organizations for Fixed-Length Recorcls\nThe slotted page organization described for variable-length records in Section\n9.6.2 can also be used for fixed-length records. It becomes attractive if we need\nto move records around on a page for reasons other than keeping track of space\nfreed by deletions. A typical example is that we want to keep the records on a\npage sorted (according to the value in some field).\n9.6.2\nVariable-Length Records\nIf records are of variable length, then we cannot divide the page into a fixed\ncollection of slots. The problem is that, when a new record is to be inserted,\nwe have to find an empty slot of just the right length----if we use a slot that\nis too big, we waste space, ancl obviously we cannot use a slot that is smaller\nthan the record length. Therefore, when a record is inserted, we must allocate\njust the right amount of space for it, and when a record is deleted, we must\nmove records to fill the hole created by the deletion, to ensure that all the free\nspace on the page is contiguous. Therefore, the ability to move records on a\npage becomes very important.\n\nSt011ng Data: Disks (l'nd Files\n3#29\nThe most flexible organization for variable-length records is to maintain a di-\nrectory of slots for each page, with a (record offset, recOT'd length) pair per\nslot. The first component (record offset) is a 'pointer' to the record, as shown\nin Figure 9.7; it is the ofl'set in bytes from the start of the data area on the\npage to the start of the record, Deletion is readily accomplished by setting the\nrecord ofl'set to -1. Records can be moved around on the page because the rid,\nwhich is the page number and slot number (that is, position in the directory),\ndoes not change when the record is moved; only the record ofl'set stored in the\nslot changes.\nDATA AREA\nrid = (i,N)\nPAGE i\n/\noffset of record from\n/\nstart of data area\nI\n\\\n\\\nRecord with rid = (i,l)\n\\~__II\nlength =24\nPage Organization for Variable-Length R.ecords\nThe space available for new records must be managed carefully because the page\nis not preformatted into slots. One way to manage free space is to maintain a\npointer (that is, ofl'set from the start of the data area on the page) that indicates\nthe start of the free space area. vVhen a new record is too large to fit into the\nremaining free space, we have to move records on the page to reclairn the space\nfreed by records deleted earlier. The idea is to ensure that, after reorganization,\nall records appear in contiguous order, followed by the available free space.\nA subtle point to be noted is that the slot for a deleted record cannot always\nbe removed from the slot directory, because slot numbers are used to identify\nrecords-by deleting a slot, we change (decrement) the slot number of subse-\nquent slots in the slot directory, and thereby change the rid of records pointed\nto by subsequent slots. The only way to remove slots from the slot directory is\nto remove the last slot if the record that it points to is deleted. However, when\n\nCHAPTER .9\na record is inserted, the slot directory should be scanned for an element that\ncurrently does not point to any record, and this slot should be used for the new\nrecord. A new slot is added to the slot directory only if all existing slots point\nto records. If inserts are much more common than deletes (as is typically the\ncase), the number of entries in the slot directory is likely to be very close to\nthe actual number of records on the page.\nThis organization is also useful for fixed-length records if we need to move\nthem around frequently; for example, when we want to maintain them in some\nsorted order. Indeed, when all records are the same length, instead of storing\nthis common length information in the slot for each record, we can store it once\nin the system catalog.\nIn some special situations (e.g., the internal pages of a B+ tree, which we\ndiscuss in Chapter 10), we lIlay not care about changing the rid of a record. In\nthis case, the slot directory can be compacted after every record deletion; this\nstrategy guarantees that the number of entries in the slot directory is the same\nas the number of records on the page. If we do not care about modifying rids,\nwe can also sort records on a page in an efficient manner by simply moving slot\nentries rather than actual records, which are likely to be much larger than slot\nentries.\nA simple variation on the slotted organization is to maintain only record offsets\nin the slots.\nfor variable-length records, the length is then stored with the\nrecord (say, in the first bytes). This variation makes the slot directory structure\nfor pages with fixed-length records the salIle a..s for pages with variab1e~length\nrecords.\n9.7\nRECORD FORMATS\nIn this section, we discuss how to organize fields within a record. While choosing\na way to organize the fields of a record, we must take into account whether the\nfields of the record are of fixed or variable length and consider the cost of various\noperations on the record, including retrieval and modification of fields.\nBefore discussing record fonnats, we note that in addition to storing individual\nrecords,\ninforlI1(~tion conllnon to all records of a given record type (such a.'3 the\nnumber of fields and field types) is stored in the system catalog, which can\nbe thought of as a description of the contents of a database, maintained by the\nDBMS (Section 12.1).\nThis avoids repeated storage of the same information\nwith each record of a given type.\n\nStoTing Data: Disks and Files\n:331,\nRecord Formats in Commercial Aystems: In IBM DB2, fixed-length\nfields are at fixed offsets from the beginning of the record. Variable-length\nfields have ofIset and length in the fixed offset part of the record, and\nthe fields themselves follow the fixed-length part of the record. Informix,\nMicrosoft SQL Server, and Sybase ASE use the same organization with\nminor variations.\nIn Oracle 8, records are structured as if all fields are\npotentially of variable length; a record is a sequence of length-data pairs,\nwith a special length value used to denote a null value.\n9.7.1\nFixed-Length Records\nIn a fixed-length record, each field h&<; a fixed length (that is, the value in this\nfield is of the same length in all records), and the number of fields is also fixed.\nThe fields of such a record can be stored consecutively, and, given the address of\nthe record, the address of a particular field can be calculated using information\nabout the lengths of preceding fields, which is available in the system catalog.\nThis record organization is illustrated in Figure 9.8.\nBase address (B)\nAddress =B+L1+L2\nFi = Field i\nLi = Length of\nfield i\nOrgani'lation of Records with Fixed-Length Fields\n9.7.2\nVariable-Length Records\nIn the relational model, every record in a relation contains the same number\nof fields. If the number of fields is fixed, a record is of variable length only\nbecause some of its fields are of variable length.\nOne possible orga,nizatioll is to store fields consecutively, separated by delim-\niters (which are special characters that do not appear in the data itself). This\norganization requires a scan of the record to locate a desired field.\nAn alternative is to reserve some space at the beginning of a record for use 1:LS\nan array of integer offsets-the ith integer in this array is the starting address\nof the ith field value relative to the start of the record. Note that we also store\nan offset to the end of the record; this offset is needed to recognize where the\nlast held ends. Both alternatives are illustrated in Figure 9.9.\n\nCHAPTERr9\nFields delimited by special symbol $\nArray of field offsets\nAlternative Record Organizations for Variable-Length Fields\nThe second approach is typically superior. For the overhead of the offset array,\nwe get direct access to any field.\nWe also get a clean way to deal with null\nvalues. A mdl value is a special value used to denote that the value for a field\nis unavailable or inapplicable. If a field contains a null value, the pointer to the\nend of the field is set to be the same as the pointer to the beginning of the field.\nThat is, no space is used for representing the null value, and a comparison of\nthe pointers to the beginning and the end of the field is used to determine that\nthe value in the field is null.\nVariable~length record formats can obviously be used to store fixed-length\nrecords as well; sometimes, the extra overhead is justified by the added flexibil-\nity, because issues such as supporting n'ull values and adding fields to a recorcl\ntype arise with fixed-length records as well.\nI-laving variable-length fields in a record can raise some subtle issues, especially\nwhen a record is modified.\nIII\nModifying a field may cause it to grow, whieh requires us to shift all subse-\nquent fields to make space for the modification in all three record formats\njust presentcel.\nIII\nA modified record may no longer fit into the space remaining on its page.\nIf so, it may have to be moved to another page. If riels, which are used\nto 'point' to a record, include the page number (see Section 9.6), moving\na record to 'another page causes a problem. We may have to leave a 'for-\nwarding address' on this page identifying the ne\"v location of the record.\nAnd to ensure that space is ahvays available for this forwarding address,\nwe would have to allocate some minimum space for each record, regardless\nof its length.\n\nStoring Data: Disks and Files\nLarge Records in Real Systems: In Sybc1se ASE, a record can be at\nmost 1962 bytes. This limit is set by the 2KB log page size, since records\nare not allowed to be larger than a page. The exceptions to this rule. an~\nBLOBs and CLOBs, which consist of 1:1 set of bidirectionally linked pages.\nIBlvl DB2 and Microsoft SqL Server also do not allow records to span\npages, although large objects are allowed to span pages and are handled\nseparately from other data types. In DB2, record size is limited only by\nthe page size; in SQL Server, a record can be at most 8KB, excluding\nLOBs. Informix and Oracle 8 allow records to span pages. Informix allows\nrecords to be at most 32KB, while Oracle has no maximum record size;\nlarge records are organized as a singly directed list.\nIII\nA record may grow so large that it no longer fits on anyone page. We have\nto deal with this condition by breaking a record into smaller records. The\nsmaller records could be chained together-part of each smaller record is\na pointer to the next record in the chain---to enable retrieval of the entire\noriginal record.\n9.8\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\nExplain the term memory hierarchy.\nWhat are the differences between\nprimary, secondary, and tertiary storage? Give examples of each. Which\nof these is volatile, and which are pCf'sistenf?\nWhy is persistent storage\nmore important for a DBMS than, say, a program that generates prime\nnumbers? (Section 9.1)\nIII\nWhy are disks used so widely in a DBMS? What are their advantages\nover main memory and tapes?\n':Vhat are their relative disadvantages?\n(Section 9.1.1)\nIII\nWhat is a disk block or page?\nHow are blocks arranged in a disk?\nHow\ndoes this affect the time to access a block? Discuss seek tiTne, rotational\ndday, and transfer time. (Section 9.1.1)\nIII\nExplain how careful placement of pages on the disk to exploit the geometry\nof a disk can minimize the seek time and rotational delay when pages are\nread sequentially. (Section 9.1.2)\nIII\nExplain what a RAID systenl is and how it improves performance and\nreliability. Discuss str-iping and its impact on performance and nxlundancy\nand its irnpact on reliability.\nvVhat are the trade-offs between reliability\n\nCHAPTER.£)\nand performance in the different RAID organizations called RAID levels'?\n(Section 9.2)\n..\nWnat is the role of the DBMS d'isk space manager'?\nvVhy do database\nsystems not rely on the operating system instead? (Section 9.3)\n..\nWhy does every page request in a DBMS go through the buffer manager?\nWhat is the buffer poor? '\\That is the difference between a frame in a buffer\npool, a page in a file, and a block on a disk? (Section 9.4)\n..\nWhat information does the buffer manager maintain for each page in the\nbuffer pool?\n·What information is maintained for each frame?\nWhat is\nthe significance of p'in_count and the d'irty flag for a page?\nUnder what\nconditions can a page in the pool be replaced?\nUnder what conditions\nmust a replaced page be written back to disk? (Section 9.4)\n..\nWhy does the buffer manager have to replace pages in the buffer pool?\nHow is a page chosen for replacement? vVhat is sequent'ial flood'ing, and\nwhat replacement policy causes it? (Section 9.4.1)\n..\nA DBMS buffer manager can often predict the access pattern for disk pages.\nHow does it utilize this ability to minimize I/O costs? Discuss prefetch-\n'ing.\n\\iVhat is forc'ing, and why is it required to support the write-ahead\nlog protocol in a DBMS? In light of these points, explain why database\nsystems reimplement many services provided by operating systems. (Sec-\ntion 9.4.2)\n..\nWhy is the abstraction of a file of records important? How is the software\nin a DBMS layered to take advantage of this? (Section 9.5)\n..\nWhat is a heap file? How are pages organized in a heap file? Discuss list\nversus directory organizations. (Section 9.5.1)\nIII\nDescribe how records are arranged on a page.\n\\i\\That is a slot, and how\nare slots used to identify records? How do slots ena.ble us to move records\non a page withont altering the record's identifier?\n·What arc the differ-\nences in page organizations for fixed-length and variable-length records?\n(Section 9.6)\niii\n·What are the differences in how fields are arranged within fixed-length and\nvariable-length records? For variable-length records, explain how the array\nof offsets organization provides direct access to a specific field and supports\nnull values. (Section 9.7)",
          "pages": [
            361,
            362,
            363,
            364,
            365,
            366,
            367,
            368,
            369
          ],
          "relevance": {
            "score": 0.2,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Storing Data: Disks and Files\nEXERCISES\nExercise 9.1 \\-Vhat is the most important difference behveen a disk and a tape?\nExercise 9.2 Explain the terms seek time, mtat'ional delay, and transfer t'ime.\n33:)\nExercise 9.3 Both disks and main memory support direct access to any desired location\n(page). On average, main memory accesses are faster, of course. \\\\That is the other important\ndifference (from the perspective of the time required to access a desired page)?\nExercise 9.4 If you have a large file that is frequently scanned sequentially, explain how you\nwould store the pages in the file on a disk.\nExercise 9.5 Consider a disk with a sector size of 512 bytes, 2000 tracks per surface, 50\nsectors per track, five double-sided platters, and average seek time of 10 msec.\n1. What is the capacity of a track in bytes? What is the capacity of each surface? What is\nthe capacity of the disk?\n2. How many cylinders does the disk have?\n:~. Give examples of valid block sizes. Is 256 bytes a valid block size? 2048? 51,200?\n4. If the disk platters rotate at 5400 rpm (revolutions per minute), what is the maximum\nrotational delay?\n5. If one track of data can be transferred per revolution, what is the transfer rate?\nExercise 9.6 Consider again the disk specifications from Exercise 9.5 and suppose that a\nblock size of 1024 bytes is chosen. Suppose that a file containing 100,000 records of 100 bytes\neach is to be stored on such a disk and that no record is allowed to span two blocks.\nL How many records fit onto a block?\n2. How many blocks are required to store the entire file? If the file is arranged sequentially\non disk, how lllallY surfaces are needed?\n:3. How many records of 100 bytes each can be stored using this disk?\n4. If pages are stored sequentially on disk, with page 1 on block 1 of track 1, what page is\nstored on block 1 of track 1 on the next disk surface? How would your answer change if\nthe disk were capable of reading and writing from all heads in parallel?\n5. VVhat titne is required to read a file containing 100,000 records of 100 bytes each sequen-\ntially? Again, how \\vould your answer change if the disk were capable of reading/writing\nfrom all heads in parallel (and the data was arranged optimally)?\n6. \\\\That is the time required to read a file containing 100,000 records of 100 bytes each in a\nrandom order? To read a record, the block containing the recOl'd has to be fetched from\ndisk. Assume that each block request incurs the average seek time and rotational delay.\nExercise 9.7 Explain what. the buffer manager JIms! do to process a read request for a page.\n\\Vhat happens if the requested page is in the pool but not pinned?\nExercise 9.8 When does a buffer manager write a page to disk?\nExercise 9.9 What. does it mean to say that a page is p'inned in the buffer pool? Who is\nresponsible for pinning pages? \\Vho is responsible for unpinning pages?\n\nCHAPTERJ9\nExercise 9.10 'When a page in the bulTer pool is modified, how does the DBMS ensure that\nthis change is propagated to disk?\n(Explain the role of the buffer manager as well as the\nmodifier of the page.)\nExercise 9.11 \\Vhat happens if a page is requested when all pages in the buffer pool are\ndirty?\nExercise 9.12 \\Vhat is sequential flooding of the buffer pool?\nExercise 9.13 Name an important capability of a DBIVIS buffer manager that is not sup-\nported by a typical operating system's buffer manager.\nExercise 9.14 Explain the term prefetching. \\Vhy is it important?\nExercise 9.15 Modern disks often have their own main memory caches, typically about\n1 MB, and use this to prefetch pages.\nThe rationale for this technique is the empirical\nobservation that, if a disk page is requested by some (not necessarily database!) application,\n80% of the time the next page is requested as well. So the disk gambles by reading ahead.\n1. Give a nontechnical reason that a DBMS may not want to rely on prefetching controlled\nby the disk.\n2. Explain the impact on the disk's cache of several queries running concurrently, each\nscanning a different file.\n3. Is this problem addressed by the DBMS buffer manager prefetching pages? Explain.\n4. Modern disks support segmented caches, with about four to six segments, each of which\nis used to cache pages from a different file.\nDoes this technique help, with respect to\nthe preceding problem? Given this technique, does it matter whether the DBMS buffer\nmanager also does prefetching?\nExercise 9.16 Describe two possible record formats. What are the trade-offs between them?\nExercise 9.17 Describe two possible page formats. What are the trade-offs between them?\nExercise 9.18 Consider the page format for variable-length records that uses a slot directory.\n1. One approach to managing the slot directory is to use a maximum size (i.e., a maximum\nnumber of slots) and allocate the directory array when the page is created. Discuss the\npros and cons of this approach with respect to the approach discussed in the text.\n2. Suggest a modification to this page format that would allow us to sort records (according\nto the value in some field) without moving records and without changing the record ids.\nExercise 9.19 Consider the two internal organizations for heap files (using lists of pages and\na directory of pages) discussed in the text.\n1. Describe them briefly and explain the trade-offs. \\Vhich organization would you choose\nif records are variable in length?\n2. Can you suggest a single page format to implement both internal file organizations'?\nExercise 9.20 Consider a list-based organizat.ion of the pages in a heap file in which two\nlists are maintained: a list of all pages in the file and a list of all pages with free space. In\ncontrast, the list-based organizatioll discussed in the text maintains a list of full pages and a\nlist of pages with free space.\n\nStoring Data: Disks and Files\n3&7\n1. VVhat are the trade-offs, if any'? Is one of them clearly superior?\n2. For each of these organizations, describe a suitable page format.\nExercise 9.21 Modern disk drives store more sectors on the outer tracks than the inner\ntracks. Since the rotation speed is constant, the sequential data transfer rate is also higher on\nthe outer tracks. The seek time and rotational delay are unchanged. Given this information,\nexplain good strategies for placing files with the following kinds of access patterns:\n1. rrequent, random accesses to a small file (e.g., catalog relations).\n2. Sequential scans of a large file (e.g., selection from a relation with no index).\n3. Random accesses to a large file via an index (e.g., selection from a relation via the index).\n4. Sequential scans of a small file.\nExercise 9.22 Why do frames in the buffer pool have a pin count instead of a pin flag?\nPROJECT-BASED EXERCISES\nExercise 9.23 Study the public interfaces for the disk space manager, the buffer manager,\nand the heap file layer in Minibase.\n1. Are heap files with variable-length records supported?\n2. What page format is used in Minibase heap files?\n3. What happens if you insert a record whose length is greater than the page size?\n4. How is free space handled in Minibase?\nBIBLIOGRAPHIC NOTES\nSalzberg [648] and Wiederhold [776] discuss secondary storage devices and file organizations\nin detail.\nRAID wa.s originally proposed by Patterson, Gibson, and Katz [587]. The article by Chen\net al. provides an excellent survey of RAID [171] .\nBooks about RAID include Gibson's\ndissertation [.317] and the publications from the RAID Advisory Board [605].\nThe design and implementation of storage managers is discussed in [65, 1:33, 219, 477, 718].\nWith the exception of [219], these systems emphasize el:tensibili.ty, anel the papers contain\nmuch of interest from that stanelpoint as well. Other papers that cover storage management\nissues in the context of significant implemented prototype systems are [480] and [588]. The\nDali storage Inanager, which is optimized for main memory databases, is described in [406].\nThree techniques for ilnplementing long fields are compared in [96]. The impact of processor\ncache misses 011 DBMS performallce ha.'i received attention lately, as complex queries have\nbecome increasingly CPU-intensive. [:33] studies this issue, and shows that performance can be\nsignificantly improved by using a new arrangement of records within a page, in which records\non a page are stored in a column~oriented format (all field values for the first attribute followed\nby values for the second attribute, etc.).\nStonebraker discusses operating systems issues in the context of databases in [715]. Several\nbuffer management policies for databa.se systems are compared in [181]. Buffer management\nis also studied in [119, 169, 2G1, 2:35].\n\nTREE-STRUCTURED\nINDEXING\n...\nWhat is the intuition behind tree-structured indexes? Why are they\ngood for range selections?\n...\nHow does an ISAM index handle search, insert, and delete?\ni\"-\nHow does a B+ tree index handle search, insert, and delete?\n...\nWhat is the impact of duplicate key values on index implementation'?\n...\nWhat is key compression, and why is it important?\n...\nWhat is bulk-loading, and why is it important?\n...\nWhat happens to record identifiers when dynamic indexes are up-\ndated? How does this affect clustered indexes?\nItt\nKey concepts: ISAM, static indexes, overflow pages, locking issues;\nB+ trees, dynamic indexes, balance, sequence sets, node format; B+\ntree insert operation, node splits, delete operation, merge versus redis-\ntribution, minimum occupancy; duplicates, overflow pages, including\nrids in search keys; key compression; bulk-loading; effects of splits on\nrids in clustered indexes.\nOne that would have the fruit must climb the tree.\nI'homas Fuller\nVVe now consider two index data structures, called ISAM and B+ trees, b<:h':led\non tree organizations.\nThese structures provide efficient support for range\nsearches, including sorted file scans as a special c<h'3e. Unlike sorted files, these\n\nTree-StTuctuTed Indel:ing\nindex structures support efficient insertion and deletion.\nThey also provide\nsupport for equality selections, although they are not &'3 efficient in this case as\nhash-b::l.'3ed indexes, which are discussed in Chapter 11.\nAn ISAJVI1 tree is a static index structure that is effective when the file is\nnot frequently updated, but it is unsuitable for files that grow and shrink a\nlot.\n\\Ve discuss ISAM in Section 10.2.\nThe B+ tree is a dynamic structure\nthat adjusts to changes in the file gracefully. It is the most widely used index\nstructure because it adjusts well to changes and supports both equality and\nrange queries. We introduce B+ trees in Section 10.3. We cover B+ trees in\ndetail in the remaining sections. Section 10.3.1 describes the format of a tree\nnode.\nSection lOA considers how to search for records by using a B+ tree\nindex. Section 10.5 presents the algorithm for inserting records into a B+ tree,\nand Section 10.6 presents the deletion algorithm.\nduplicates are handled. \\Ve conclude with a discussion of some practical issues\nconcerning B+ trees in Section 10.8.\nNotation: In the ISAM and B+ tree structures, leaf pages contain data entries,\naccording to the terminology introduced in Chapter 8.\nFor convenience, we\ndenote a data entry with search key value k as k*.\nNon-leaf pages conta.in\ninde:c entries of the form (search key 'Value., page id) and are used to direct the\nsea.rch for a desired data entry (which is stored in some leaf). We often simply\nuse entr'Y where the context makes the nature of the entry (index or data) clear.\n10.1\nINTUITION FOR TREE INDEXES\nConsider a file of Students recorcls sorted by gpa. To answer a range selection\nsuch as \"Find all students with a gpa higher than 3.0,\" we must identify the\nfirst such student by doing a binary search of the file and then scan the file\nfrom that point on. If the file is large, the initial binary search can be quite\nexpensive, since cost is proportional to the number of pages fetched; can we\nimprove upon this method'?\nOIle idea is to create a second file with OIle record per page in the original\n(data) file, of the form (first key on page, pointer to page), again sortecl by the\nkey attribute (which is gpa in our example). The format of a page in the second\ninde:c file is illustrated in Figure 10.1.\nWe refer to pairs of the form (key, pointer)\n~l.S indc:J: entries or just entries \\'\\'hen\nthe context is dear. Note that each index page contains OIle pointer more than\nI ISAM stands for Indexed Sequential Access Method.",
          "pages": [
            370,
            371,
            372,
            373,
            374
          ],
          "relevance": {
            "score": 0.1,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "A database transaction is a sequence of operations that are treated as a single unit of work. It ensures data consistency and integrity by either fully completing all operations or rolling back any changes if an error occurs.",
        "explanation": "Database transactions solve the problem of ensuring data consistency when multiple operations need to be performed together. Here’s how they work:\n1. **Start**: A transaction begins with a BEGIN statement.\n2. **Execute**: Multiple SQL statements are executed within this block.\n3. **Commit**: If all operations succeed, the COMMIT statement is issued to save changes permanently.\n4. **Rollback**: If any operation fails, the ROLLBACK statement is used to undo all changes made during the transaction.\n\nTransactions are crucial in preventing data corruption and ensuring that the database remains consistent even in the face of errors or system failures.",
        "key_points": [
          "Key point 1: Transactions ensure atomicity (all operations either complete or fail together).",
          "Key point 2: They provide isolation, meaning changes made by one transaction are not visible to others until committed.",
          "Key point 3: Transactions guarantee consistency, maintaining the integrity of the database even under concurrent access.",
          "Key point 4: Proper use of transactions helps in managing data integrity and preventing data loss.",
          "Key point 5: Understanding ACID properties (Atomicity, Consistency, Isolation, Durability) is essential for effective transaction management."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Start a transaction BEGIN; -- INSERT data into the TABLE INSERT INTO employees (name, position) VALUES ('John Doe', 'Manager'); -- Commit the transaction to save changes COMMIT;",
            "explanation": "This example demonstrates starting a transaction, inserting data, and committing the changes. If any error occurs during these operations, the ROLLBACK statement can be used instead of COMMIT.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Transfer money from one account to another\nBEGIN;\n-- Debit the sender's account\nUPDATE accounts SET balance = balance - 100 WHERE id = 1;\n-- Credit the receiver's account\nUPDATE accounts SET balance = balance + 100 WHERE id = 2;\n-- Commit the transaction if both operations succeed\nCOMMIT;",
            "explanation": "This practical example shows how transactions are used in real-world scenarios, such as transferring money between bank accounts. It ensures that both debit and credit operations are completed successfully before any changes are saved."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Forgetting to commit or rollback",
            "incorrect_code": "-- UPDATE data without committing UPDATE employees SET position = 'Senior Manager' WHERE id = 1;",
            "correct_code": "-- Correct way with transaction management BEGIN; UPDATE employees SET position = 'Senior Manager' WHERE id = 1; COMMIT;",
            "explanation": "Forgetting to commit or rollback can lead to partial changes being saved, which is undesirable. Always ensure that all operations are either committed or rolled back."
          }
        ],
        "practice": {
          "question": "Write a SQL transaction to update the salary of an employee and then insert a record into a log table. Ensure that if any error occurs during these operations, both changes should be rolled back.",
          "solution": "-- Start a transaction\nBEGIN;\n-- Update employee's salary\nUPDATE employees SET salary = salary + 5000 WHERE id = 101;\n-- Insert record into log table\nINSERT INTO salary_log (employee_id, new_salary) VALUES (101, (SELECT salary FROM employees WHERE id = 101));\n-- Commit the transaction if both operations succeed\nCOMMIT;",
          "explanation": "This practice question tests understanding of transaction management by requiring the student to perform multiple operations within a single transaction and ensure that all changes are either committed or rolled back."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER ,,9\nThe buffer manager will not read another page into a frame until its pi'll-count\nbecomes 0, that is, until all requestors of the page have unpilln~d it.\nIf a requested page is not in the b",
      "content_relevance": {
        "score": 0.2,
        "sql_score": 1.0,
        "concept_score": 0.2,
        "non_sql_penalty": 0.2,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "acid": {
      "id": "acid",
      "title": "ACID Properties",
      "definition": "Atomicity, Consistency, Isolation, Durability - guarantees for transaction processing",
      "difficulty": "intermediate",
      "page_references": [
        355,
        356,
        357,
        358,
        359,
        360,
        361,
        362,
        363,
        364,
        365,
        366
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER ,,9\nThe buffer manager will not read another page into a frame until its pi'll-count\nbecomes 0, that is, until all requestors of the page have unpilln~d it.\nIf a requested page is not in the buffer pool and a free frame is not available\nin the buffer pool, a frame with pirl-count 0 is chosen for replacement. If there\nare many such frames, a frame is chosen according to the buffer manager's\nreplacement policy. vVe discuss various replacement policies in Section 9.4.1.\n\\-\\Then a page is eventually chosen for replacement, if the dir'ty bit is not set,\nit means that the page h1-:1..<; not been modified since being brought into main\nmemory.\nHence, there is no need to write the page back to disk; the copy\non disk is identical to the copy in the frame, and the frame can simply be\noverwritten by the newly requested page. Otherwise, the modifications to the\npage must be propagated to the copy on disk.\n(The crash recovery protocol\nmay impose further restrictions, as we saw in Section 1.7. For example, in the\nWrite-Ahead Log (WAL) protocol, special log records are used to describe the\nchanges made to a page. The log records pertaining to the page to be replaced\nmay well be in the buffer; if so, the protocol requires that they be written to\ndisk before the page is written to disk.)\nIf no page in the buffer pool has pin_count 0 and a page that is not in the pool\nis requested, the buffer manager must wait until some page is released before\nresponding to the page request. In practice, the transaction requesting the page\nmay simply be aborted in this situation! So pages should be released-by the\ncode that calls the buffer manager to request the page- as soon as possible.\nA good question to ask at this point is, \"What if a page is requested by several\ndifferent transactions?\"\nThat is, what if the page is requested by programs\nexecuting independently on behalf of different users?\nSuch programs could\nmake conflicting changes to the page. The locking protocol (enforced by higher-\nlevel DBMS code, in particular the transaction manager) ensures that each\ntransaction obtains a shared or exclusive lock before requesting a page to read\nor rnodify.\nTwo different transactions cannot hold an exclusive lock on the\nsame page at the same time; this is how conflicting changes are prevented. The\nbuffer rnanager simply\n~1..'3surnes tha.t the appropriate lock has been obtained\nbefore a page is requested.\n9.4.1\nBuffer Replacement Policies\nThe policy used to choose an unpinned page for replacement can affect the time\ntaken for database operations considerably. Of the man,Y alternative policies,\neach is suitable in different situations.\n\nSto7~ing Data: Disks and Files\nThe best-known replacement policy is least recently used (LRU). This can\nbe implemented in the buffer manager using a queue of pointers to frames with\npin_count O.\nA frame is added to the end of the queue when it becomes a\ncandidate for replacement (that is, when the p'irLco'unt goes to 0). The page\nchosen for replacement is the one in the frame at the head of the queue.\nA variant of LRU, called clock replacement, has similar behavior but less\noverhead. The idea is to choose a page for replacement using a current variable\nthat takes on values 1 through N, where N is the number of buffer frames, in\ncircular order. Vie can think of the frames being arranged in a circle, like a\nclock's face, and current as a clock hand moving across the face. To approximate\nLRU behavior, each frame also has an associated referenced bit, which is turned\non when the page p'in~count goes to O.\nThe current frame is considered for replacement. If the frame is not chosen for\nreplacement, current is incremented and the next frame is considered; this pro-\ncess is repeated until some frame is chosen. If the current frame has pin_count\ngreater than 0, then it is not a candidate for replacement and current is in-\ncremented. If the current frame has the referenced bit turned on, the clock\nalgorithm turns the referenced bit off and increments cm'rent-this way, a re-\ncently referenced page is less likely to be replaced. If the current frame has\np'irLcount 0 and its referenced bit is off, then the page in it is chosen for re-\nplacement. If all frames are pinned in some sweep of the clock hand (that is,\nthe value of current is incremented until it repeats), this means that no page\nin the buffer pool is a replacement candidate.\nThe LRU and clock policies are not always the best replacement strategies for a\ndatabase system, particularly if many user requests require sequential scans of\nthe data. Consider the following illustrative situation. Suppose the buffer pool\nh<4'3 10 frames, and the file to be scanned has 10 or fewer pages.\nAssuming,\nfor simplicity, that there are no competing requests for pages, only the first\nscan of the file does any I/O. Page requests in subsequent scans always find the\ndesired page in the buffer pool. On the other hand, suppose that the file to be\nscanned has 11 pages (which is one more than the number of available pages\nin the buffer pool).\nUsing LRU, every scan of the file will result in reading\nevery page of the file! In this situation, called sequential flooding, LRU is\nthe worst possible replacement strategy.\nOther replacement policies include first in first out (FIFO) and most re-\ncently used (MRU), which also entail overhead similar to LRU, and random,\narnong others. The details of these policies should be evident from their names\nand the preceding discussion of LRU and clock.\n\n,----------------\n_ _.._._-_..----~~------------ -··-~~l\nBuffer Management in Practice: IBM DB2 and Sybase ASE allow\n!\nbuffers to be partitioned into named pools. Each database, table, or in-\nI\ndex can be bound to one of these pools. Each pool can be configured to\nI\nuse either LRU or clock replacement in ASE; DB2 uses a variant of clock\n!\nreplacement, with the initial clock value based on the nature of the page\n!\n(e.g., index non-leaves get a higher starting clock value, which delays their\nreplacement). Interestingly, a buffer pool client in DB2 can explicitly indi-\ncate that it hates a page, making the page the next choice for replacement.\nAs a special case, DB2 applies MRU for the pages fetched in some utility\noperations (e.g., RUNSTATS), and DB2 V6 also supports FIFO. Informix\nand Oracle 7 both maintain a single global buffer pool using LRU; Mi-\ncrosoft SQL Server has a single pool using clock replacement. In Oracle\n8, tables can be bound to one of two pools; one has high priority, and the\nsystem attempts to keep pages in this pool in memory.\nBeyond setting a maximum number of pins for a given transaction, there\nare typically no features for controlling buffer pool usage on a per-\ntransaction basis. Microsoft SQL Server, however, supports a reservation of\nbuffer pages by queries that require large amounts of memory (e.g., queries\ninvolving sorting or hashing).\n9.4.2\nBuffer Management in DBMS versus OS\nObvious similarities exist between virtual memory in operating systems and\nbuffer management in database management systems. In both cases, the goal\nis to provide access to more data than will fit in main memory, and the basic\nidea is to bring in pages from disk to main memory a.<.; needed, replacing pages\nno longer needed in main memory.\nWhy can't we build a DBMS using the\nvirtual memory capability of an OS? A DBMS can often predict the order\nin which pages will be accessed, or page reference patterns, much more\naccurately than is typical in an as environment, and it is desirable to utilize\nthis property. Further, a DBMS needs more control over when a page is written\nto disk than an as typically provides.\nA DBMS can often predict reference patterns because most page references\nare generated by higher-level operations (such as sequential scans or particular\nimplementations of various relational algebra opera.tors) with a. known pattern\nof page accesses. This ability to predict reference patterns allows for a better\nchoice of pages to replace and makes the idea of specialized buffer replacmnent\npolicies more attractive in the DBMS environment.\nEven more important, being able to predict reference patterns enables the usc\nof a simple and very effective strategy called prefetching of pages.\nThe\n\nStoTing Data: Disks and Files\n~\"\n~--~--\"------------------------~\nPrefetching:\nIBM DB2 supports both sequential alld list prefeteh\n(prefetching a list of pages). In general, the prefeteh size is 32 4KB· pages,\nbut this can be set by the user. £tor some sequential type datahaseutilities\n(e.g., COPY, RUNSTATS), DB2 prefetches up to 64 4KB pages,·!'cJr a\nsmaller buffer pool (i.e., less than 1000 buffers), the prefetch quantity is\nadjusted downward to 16 or 8 pages. The prefetch size can be configured by\nthe user; for certain environments, it may be best to prefetch 1000 pages at\na time! Sybase ASE supports asynchronous prefetching of up to 256 pages,\nand uses this capability to reduce latency during indexed access to a table\nin a range scan. Oracle 8 uses prefetching for sequential scan, retrieving\nlarge objects, and certain index scans.\nMicrosoft SQL Server supports\nprefetching for sequential scan and for scarlS along the leaf level ofa B+\ntree index, and the prefetch size can be adjusted a<; a scan progresses. SQL\nServer also uses asynchronous prefetching extensively. Informix supports\nprefetching with a user-defined prefetch size.\nbuffer manager can anticipate the next several page requests and fetch the\ncorresponding pages into memory before the pages are requested. This strategy\nhas two benefits. First, the pages are available in the buffer pool when they\nare requested. Second, reading in a contiguous block of pages is much faster\nthan reading the same pages at different times in response to distinct requests.\n(Review the discussion of disk geometry to appreciate why this is so.) If the\npages to be prcfetched are not contiguous, recognizing that several pages need\nto be fetched can nonetheless lead to faster I/O because an order of retrieval\ncan be chosen for these pages that minimizes seek times and rotational delays.\nIncidentally, note that the I/O can typically be done concurrently with CPU\ncomputation.\nOnce the prefetch request is issued to the disk, the disk is re-\nsponsible for reading the requested pages into memory pages and the CPU can\ncontinue to do other work.\nA DBMS also requires the ability to explicitly force a page to disk, that is, to\nensure that the copy of the page on disk is updated with the copy in memory.\nAs a related point, a DBMS must be able to ensure that certain pages in the\nbuffer pool are written to disk before certain other pages to implement the ';VAL\nprotocol for cra,<;h recovery, as we saw in Section 1.7. Virtual memory imple-\nmentations in operating systems cannot be relied on to provide such control\nover when pages are written to disk; the OS command to write a page to disk\nmay be implemented by essentially recording the write request and deferring\nthe actual modification of the disk copy. If the systern crashes in the interim,\nthe effects can be catastrophic for a DBMS. (Crash recovery is discllssed further\nin Chapter 18.)",
          "pages": [
            355,
            356,
            357,
            358
          ],
          "relevance": {
            "score": 0.8,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "CHAPTER~9\nIndexes as Files: In Chapter 8, we presented indexes as a way of 6rga11i~~--··w·l\ning data records for efficient search. From an implementation standpoint,\nI\nI\ni\nindexes are just another kind of file, containing records that dil'ect traffic\non requests for data records. For example, a tree index is a collection of\nrecords organized into one page per node in the tree. It is convenient to\nactually think of a tree index as two files, because it contains two kinds\nof records: (1) a file of index entries, which are records with fields for the\nindex's search key, and fields pointing to a child node, and (2) a file of data\nentries, whose structure depends on the choice of data entry alternative.\n9.5\nFILES OF RECORDS\nWe now turn our attention from the way pages are stored on disk and brought\ninto main memory to the way pages are used to store records and organized\ninto logical collections or files. Higher levels of the DBMS code treat a page as\neffectively being a collection of records, ignoring the representation and storage\ndetails.\nIn fact, the concept of a collection of records is not limited to the\ncontents of a single page; a file can span several pages.\nIn this section, we\nconsider how a collection of pages can be organized as a file. We discuss how\nthe space on a page can be organized to store a collection of records in Sections\n9.6 and 9.7.\n9.5.1\nImplementing Heap Files\nThe data in the pages of a heap file is not ordered in any way, and the only\nguarantee is that one can retrieve all records in the file by repeated requests\nfor the next record. Every record in the file has a unique rid, and every page\nin a file is of the same size.\nSupported operations on a heap file include CTeatc and destmy files, 'insert a\nrecord, delete a record with a given rid, get a record with a given rid, and scan\nall records in the file. To get or delete a record with a given rid, note that we\nmust be able to find the id of the page containing the record, given the id of\nthe record.\nvVe must keep track of the pages in each heap file to support scans, and we must\nkeep track of pages that contain free space to implement insertion efficiently.\n\\Ve discuss two alternative ways to rnaintain this information. In each of these\nalternatives, pages must hold two pointers (which are page ids) for file-level\nbookkeeping in addition to the data.\n\nStoring Data: Disks aTul File,s\n32~\nLinked List of Pages\nOne possibility is to maintain a heap file as a doubly linked list of pages. The\nDBMS can remember where the first page is located by maintaining a table\ncontaining pairs of (heap_file_name, page_Laddr) in a known location on disk.\nWe call the first page of the file the header page.\nAn important task is to maintain information about empty slots created by\ndeleting a record from the heap file. This task has two distinct parts: how to\nkeep track of free space within a page and how to keep track of pages that have\nsome free space. We consider the first part in Section 9.6. The second part can\nbe addressed by maintaining a doubly linked list of pages with free space and\na doubly linked list of full pages; together, these lists contain all pages in the\nheap file. This organization is illustrated in Figure 9.4; note that each pointer\nis really a page id.\nData\npage\nData\npage\nLinked list of\nfull pages\nLinked list of pages\nwith free space\npage\nData\nData\npage\nHeap File Organization with a Linked List\nIf a new page is required, it is obtained by making a request to the disk space\nmanager and then added to the list of pages in the file (probably as a page\nwith free space, because it is unlikely that the new record will take up all the\nspace on the page). If a page is to be deleted from the heap file, it is removed\nfrom the list and the disk space Inanager is told to deallocate it. (Note that the\nscheme can easily be generalized to allocate or deallocate a sequence of several\npages and maintain a doubly linked list of these page sequences.)\nOne disadvantage of this scheIue is that virtually all pages in a file will be on\nthe free list if records are of variable length, because it is likely that every page\nha\",,, at least a few free bytes. To insert a typical record, we must retrieve and\nexaInine several pages on the free list before we find one with enough free space.\nThe directory-based heap file organization that we discuss next addresses this\nproblem.\n\nDirectory of Pages\nCHAPTER,g\nAn alternative to a linked list of pages is to maintain a directory of pages.\nThe DBMS must remember where the first directory page of each heap file is\nlocated. The directory is itself a collection of pages and is shown as a linked\nlist in Figure 9.5. (Other organizations are possible for the directory itself, of\ncourse.)\nHeader page\nData\npage 2\nData\npage N\nDIRECTORY\nHeap File Organization with a Directory\nEach directory entry identifies a page (or a sequence of pages) in the heap file.\nAs the heap file grows or shrinks, the number of entries in the directory-and\npossibly the number of pages in the directory itself--grows or shrinks corre-\nspondingly. Note that since each directory entry is quite small in comparison to\na typical page, the size of the directory is likely to be very small in comparison\nto the size of the heap file.\nFree space can be managed by maintaining a bit per entry, indicating whether\nthe corresponding page has any free space, or a count per entry, indicating the\namount of free space on the page. If the file contains variable-length records,\nwe can examine the free space count for an entry to determine if the record\nfits on the page pointed to by the entry. Since several entries fit on a directory\npage, we can efficiently search for a data page with enough space to hold a\nrecord to be inserted.\n9.6\nPAGE FORMATS\nThe page abstraction is appropriate when dealing with I/O issue-s, but higher\nlevels of the DBMS see data a..<; a collection of records.\nIn this section, we\n\nStoTing Data: D'i.5ks and Files\n327.\nRids in COInmercial Systems:\nIBM DB2 l Informix, Microsoft SQL\nServer l Oracle 8, and Sybase ASE all implement record ids as a page id\nand slot number. Syba..c;e ASE uses the following page organization, which\nis typical: Pages contain a header followed by the rows and a slot array.\nThe header contains the page identity, its allocation state, page free space\nstate, and a timestamp. The slot array is simply a mapping of slot number\nto page offset.\nOracle 8 and SQL Server use logical record ids rather than page id and slot\nnumber in one special case: If a table has a clustered index, then records in\nthe table are identified using the key value for the clustered index. This has\nthe advantage that secondary indexes need not be reorganized if records\nare moved ac~oss pages.\nconsider how a collection of records can be arranged on a page. We can think\nof a page as a collection of slots, each of which contains a record. A record is\nidentified by using the pair (page id, slot number); this is the record id (rid).\n(We remark that an alternative way to identify records is to assign each record\na unique integer as its rid and maintain a table that lists the page and slot of\nthe corresponding record for each rid. Due to the overhead of maintaining this\ntable, the approach of using (page id, slot number) as an rid is more common.)\nWe now consider some alternative approaches to managing slots on a page.\nThe main considerations are how these approaches support operations such as\nsearching, inserting, or deleting records on a page.\n9.6.1\nFixed-Length Records\nIf all records on the page are guaranteed to be of the same length, record slots\narc uniform and can be arranged consecutively within a page. At any instant,\nsome slots are occupied by records and others are unoccupied. When a record\nis inserted into the page, we must locate an empty slot and place the record\nthere. The main issues are how we keep track of empty slots and how we locate\nall records on a page. The alternatives hinge on how we handle the deletion of\na record.\nThe first alternative is to store records in the first N slots (where N is the\nnumber of records on the page); whenever a record is deleted, we move the last\nrecord on the page into the vacated slot. This format allows us to locate the\nith record on a page by a simple offset calculation, and all empty slots appear\ntogether at the end of the page. However, this approach docs not work if there\n\nCHAPTER.9\nare external references to the record that is moved (because the rid contains\nthe slot number, which is now changed).\nThe second alternative is to handle deletions by using an array of bits, one per\nslot, to keep track of free slot information. Locating records on the page requires\nscanning the bit array to find slots whose bit is on; when a record is deleted,\nits bit is turned off. The two alternatives for storing fixed-length records are\nillustrated in Figure 9.6. Note that in addition to the information about records\non the page, a page usually contains additional file-level information (e.g., the\nid of the next page in the file).\nThe figure does not show this additional\ninformation.\nPacked\nUnpacked, Bitmap\nM\n1J\nNumber of slots\nSlot 1\nSlot 2\nSlot 3\no\n~\nSlot M\nI I 1011\\ M\nFree ~\nSpace\nJ\n'-S\"Page ~\nJ\nHeader\nNumber of records\no\n...\nIN\nSlot N\nSlot 1\nSlot 2\nAlternative Page Organizations for Fixed-Length Recorcls\nThe slotted page organization described for variable-length records in Section\n9.6.2 can also be used for fixed-length records. It becomes attractive if we need\nto move records around on a page for reasons other than keeping track of space\nfreed by deletions. A typical example is that we want to keep the records on a\npage sorted (according to the value in some field).\n9.6.2\nVariable-Length Records\nIf records are of variable length, then we cannot divide the page into a fixed\ncollection of slots. The problem is that, when a new record is to be inserted,\nwe have to find an empty slot of just the right length----if we use a slot that\nis too big, we waste space, ancl obviously we cannot use a slot that is smaller\nthan the record length. Therefore, when a record is inserted, we must allocate\njust the right amount of space for it, and when a record is deleted, we must\nmove records to fill the hole created by the deletion, to ensure that all the free\nspace on the page is contiguous. Therefore, the ability to move records on a\npage becomes very important.\n\nSt011ng Data: Disks (l'nd Files\n3#29\nThe most flexible organization for variable-length records is to maintain a di-\nrectory of slots for each page, with a (record offset, recOT'd length) pair per\nslot. The first component (record offset) is a 'pointer' to the record, as shown\nin Figure 9.7; it is the ofl'set in bytes from the start of the data area on the\npage to the start of the record, Deletion is readily accomplished by setting the\nrecord ofl'set to -1. Records can be moved around on the page because the rid,\nwhich is the page number and slot number (that is, position in the directory),\ndoes not change when the record is moved; only the record ofl'set stored in the\nslot changes.\nDATA AREA\nrid = (i,N)\nPAGE i\n/\noffset of record from\n/\nstart of data area\nI\n\\\n\\\nRecord with rid = (i,l)\n\\~__II\nlength =24\nPage Organization for Variable-Length R.ecords\nThe space available for new records must be managed carefully because the page\nis not preformatted into slots. One way to manage free space is to maintain a\npointer (that is, ofl'set from the start of the data area on the page) that indicates\nthe start of the free space area. vVhen a new record is too large to fit into the\nremaining free space, we have to move records on the page to reclairn the space\nfreed by records deleted earlier. The idea is to ensure that, after reorganization,\nall records appear in contiguous order, followed by the available free space.\nA subtle point to be noted is that the slot for a deleted record cannot always\nbe removed from the slot directory, because slot numbers are used to identify\nrecords-by deleting a slot, we change (decrement) the slot number of subse-\nquent slots in the slot directory, and thereby change the rid of records pointed\nto by subsequent slots. The only way to remove slots from the slot directory is\nto remove the last slot if the record that it points to is deleted. However, when\n\nCHAPTER .9\na record is inserted, the slot directory should be scanned for an element that\ncurrently does not point to any record, and this slot should be used for the new\nrecord. A new slot is added to the slot directory only if all existing slots point\nto records. If inserts are much more common than deletes (as is typically the\ncase), the number of entries in the slot directory is likely to be very close to\nthe actual number of records on the page.\nThis organization is also useful for fixed-length records if we need to move\nthem around frequently; for example, when we want to maintain them in some\nsorted order. Indeed, when all records are the same length, instead of storing\nthis common length information in the slot for each record, we can store it once\nin the system catalog.\nIn some special situations (e.g., the internal pages of a B+ tree, which we\ndiscuss in Chapter 10), we lIlay not care about changing the rid of a record. In\nthis case, the slot directory can be compacted after every record deletion; this\nstrategy guarantees that the number of entries in the slot directory is the same\nas the number of records on the page. If we do not care about modifying rids,\nwe can also sort records on a page in an efficient manner by simply moving slot\nentries rather than actual records, which are likely to be much larger than slot\nentries.\nA simple variation on the slotted organization is to maintain only record offsets\nin the slots.\nfor variable-length records, the length is then stored with the\nrecord (say, in the first bytes). This variation makes the slot directory structure\nfor pages with fixed-length records the salIle a..s for pages with variab1e~length\nrecords.\n9.7\nRECORD FORMATS\nIn this section, we discuss how to organize fields within a record. While choosing\na way to organize the fields of a record, we must take into account whether the\nfields of the record are of fixed or variable length and consider the cost of various\noperations on the record, including retrieval and modification of fields.\nBefore discussing record fonnats, we note that in addition to storing individual\nrecords,\ninforlI1(~tion conllnon to all records of a given record type (such a.'3 the\nnumber of fields and field types) is stored in the system catalog, which can\nbe thought of as a description of the contents of a database, maintained by the\nDBMS (Section 12.1).\nThis avoids repeated storage of the same information\nwith each record of a given type.",
          "pages": [
            359,
            360,
            361,
            362,
            363,
            364,
            365
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "StoTing Data: Disks and Files\n:331,\nRecord Formats in Commercial Aystems: In IBM DB2, fixed-length\nfields are at fixed offsets from the beginning of the record. Variable-length\nfields have ofIset and length in the fixed offset part of the record, and\nthe fields themselves follow the fixed-length part of the record. Informix,\nMicrosoft SQL Server, and Sybase ASE use the same organization with\nminor variations.\nIn Oracle 8, records are structured as if all fields are\npotentially of variable length; a record is a sequence of length-data pairs,\nwith a special length value used to denote a null value.\n9.7.1\nFixed-Length Records\nIn a fixed-length record, each field h&<; a fixed length (that is, the value in this\nfield is of the same length in all records), and the number of fields is also fixed.\nThe fields of such a record can be stored consecutively, and, given the address of\nthe record, the address of a particular field can be calculated using information\nabout the lengths of preceding fields, which is available in the system catalog.\nThis record organization is illustrated in Figure 9.8.\nBase address (B)\nAddress =B+L1+L2\nFi = Field i\nLi = Length of\nfield i\nOrgani'lation of Records with Fixed-Length Fields\n9.7.2\nVariable-Length Records\nIn the relational model, every record in a relation contains the same number\nof fields. If the number of fields is fixed, a record is of variable length only\nbecause some of its fields are of variable length.\nOne possible orga,nizatioll is to store fields consecutively, separated by delim-\niters (which are special characters that do not appear in the data itself). This\norganization requires a scan of the record to locate a desired field.\nAn alternative is to reserve some space at the beginning of a record for use 1:LS\nan array of integer offsets-the ith integer in this array is the starting address\nof the ith field value relative to the start of the record. Note that we also store\nan offset to the end of the record; this offset is needed to recognize where the\nlast held ends. Both alternatives are illustrated in Figure 9.9.",
          "pages": [
            366
          ],
          "relevance": {
            "score": 0.3,
            "sql_score": 0.5,
            "concept_score": 0.5,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "ACID properties are essential for ensuring data integrity and reliability in database management systems. They stand for Atomicity, Consistency, Isolation, and Durability.",
        "explanation": "The ACID properties ensure that transactions (sets of operations) within a database system are processed reliably and consistently. Here's how they work together:\n\n1. **Atomicity**: This property ensures that a transaction is treated as a single unit of work. If any part of the transaction fails, the entire transaction is rolled back, maintaining data consistency.\n\n2. **Consistency**: A transaction must change the database from one valid state to another. It cannot leave the system in an inconsistent state.\n\n3. **Isolation**: This property ensures that concurrent transactions do not interfere with each other. Each transaction sees a consistent snapshot of the database, and changes made by one transaction are not visible until it is committed.\n\n4. **Durability**: Once a transaction is committed, its effects are permanent. The data remains intact even if there is a system failure.",
        "key_points": [
          "Atomicity ensures that transactions are indivisible; either all operations succeed or none do.",
          "Consistency maintains the integrity of the database by ensuring that transactions result in valid states.",
          "Isolation prevents concurrent transactions from interfering with each other, providing data isolation.",
          "Durability guarantees that committed transactions remain permanent, even in the face of system failures."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Example of a transaction BEGIN TRANSACTION; UPDATE accounts SET balance = balance - 100 WHERE account_id = 123; UPDATE accounts SET balance = balance + 100 WHERE account_id = 456; COMMIT;",
            "explanation": "This example demonstrates a simple transaction that transfers money from one account to another. It uses BEGIN TRANSACTION, UPDATE statements, and COMMIT to ensure that the transfer is completed atomically.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Practical scenario for maintaining consistency BEGIN TRANSACTION; UPDATE orders SET status = 'Shipped' WHERE order_id = 789; INSERT INTO shipment (order_id, tracking_number) VALUES (789, '1234567890'); COMMIT;",
            "explanation": "This practical example shows how a transaction can be used to update the status of an order and record its shipment in a single, consistent operation.",
            "validation_note": "SQL auto-fixed: "
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not using transactions for operations that should be atomic",
            "incorrect_code": "-- Incorrect example without transaction UPDATE accounts SET balance = balance - 100 WHERE account_id = 123; INSERT INTO logs (user_id, action) VALUES (1, 'Transfer');",
            "correct_code": "-- Correct example with transaction BEGIN TRANSACTION; UPDATE accounts SET balance = balance - 100 WHERE account_id = 123; INSERT INTO logs (user_id, action) VALUES (1, 'Transfer'); COMMIT;",
            "explanation": "Failing to use transactions can lead to inconsistent data states. Always wrap operations that should be atomic within a transaction."
          }
        ],
        "practice": {
          "question": "Explain how the ACID properties ensure data integrity in a database system.",
          "solution": "The ACID properties ensure data integrity by guaranteeing that transactions are treated as single units of work (Atomicity), maintaining valid states (Consistency), preventing interference between concurrent transactions (Isolation), and ensuring that committed changes remain permanent (Durability). Together, these properties help prevent data corruption and maintain the reliability of the database system."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER ,,9\nThe buffer manager will not read another page into a frame until its pi'll-count\nbecomes 0, that is, until all requestors of the page have unpilln~d it.\nIf a requested page is not in the b",
      "content_relevance": {
        "score": 0.7,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "isolation-levels": {
      "id": "isolation-levels",
      "title": "Transaction Isolation Levels",
      "definition": "READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE",
      "difficulty": "advanced",
      "page_references": [
        365,
        366,
        367,
        368,
        369,
        370,
        371,
        372,
        373,
        374,
        375,
        376,
        377,
        378,
        379,
        380
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER .9\na record is inserted, the slot directory should be scanned for an element that\ncurrently does not point to any record, and this slot should be used for the new\nrecord. A new slot is added to the slot directory only if all existing slots point\nto records. If inserts are much more common than deletes (as is typically the\ncase), the number of entries in the slot directory is likely to be very close to\nthe actual number of records on the page.\nThis organization is also useful for fixed-length records if we need to move\nthem around frequently; for example, when we want to maintain them in some\nsorted order. Indeed, when all records are the same length, instead of storing\nthis common length information in the slot for each record, we can store it once\nin the system catalog.\nIn some special situations (e.g., the internal pages of a B+ tree, which we\ndiscuss in Chapter 10), we lIlay not care about changing the rid of a record. In\nthis case, the slot directory can be compacted after every record deletion; this\nstrategy guarantees that the number of entries in the slot directory is the same\nas the number of records on the page. If we do not care about modifying rids,\nwe can also sort records on a page in an efficient manner by simply moving slot\nentries rather than actual records, which are likely to be much larger than slot\nentries.\nA simple variation on the slotted organization is to maintain only record offsets\nin the slots.\nfor variable-length records, the length is then stored with the\nrecord (say, in the first bytes). This variation makes the slot directory structure\nfor pages with fixed-length records the salIle a..s for pages with variab1e~length\nrecords.\n9.7\nRECORD FORMATS\nIn this section, we discuss how to organize fields within a record. While choosing\na way to organize the fields of a record, we must take into account whether the\nfields of the record are of fixed or variable length and consider the cost of various\noperations on the record, including retrieval and modification of fields.\nBefore discussing record fonnats, we note that in addition to storing individual\nrecords,\ninforlI1(~tion conllnon to all records of a given record type (such a.'3 the\nnumber of fields and field types) is stored in the system catalog, which can\nbe thought of as a description of the contents of a database, maintained by the\nDBMS (Section 12.1).\nThis avoids repeated storage of the same information\nwith each record of a given type.\n\nStoTing Data: Disks and Files\n:331,\nRecord Formats in Commercial Aystems: In IBM DB2, fixed-length\nfields are at fixed offsets from the beginning of the record. Variable-length\nfields have ofIset and length in the fixed offset part of the record, and\nthe fields themselves follow the fixed-length part of the record. Informix,\nMicrosoft SQL Server, and Sybase ASE use the same organization with\nminor variations.\nIn Oracle 8, records are structured as if all fields are\npotentially of variable length; a record is a sequence of length-data pairs,\nwith a special length value used to denote a null value.\n9.7.1\nFixed-Length Records\nIn a fixed-length record, each field h&<; a fixed length (that is, the value in this\nfield is of the same length in all records), and the number of fields is also fixed.\nThe fields of such a record can be stored consecutively, and, given the address of\nthe record, the address of a particular field can be calculated using information\nabout the lengths of preceding fields, which is available in the system catalog.\nThis record organization is illustrated in Figure 9.8.\nBase address (B)\nAddress =B+L1+L2\nFi = Field i\nLi = Length of\nfield i\nOrgani'lation of Records with Fixed-Length Fields\n9.7.2\nVariable-Length Records\nIn the relational model, every record in a relation contains the same number\nof fields. If the number of fields is fixed, a record is of variable length only\nbecause some of its fields are of variable length.\nOne possible orga,nizatioll is to store fields consecutively, separated by delim-\niters (which are special characters that do not appear in the data itself). This\norganization requires a scan of the record to locate a desired field.\nAn alternative is to reserve some space at the beginning of a record for use 1:LS\nan array of integer offsets-the ith integer in this array is the starting address\nof the ith field value relative to the start of the record. Note that we also store\nan offset to the end of the record; this offset is needed to recognize where the\nlast held ends. Both alternatives are illustrated in Figure 9.9.\n\nCHAPTERr9\nFields delimited by special symbol $\nArray of field offsets\nAlternative Record Organizations for Variable-Length Fields\nThe second approach is typically superior. For the overhead of the offset array,\nwe get direct access to any field.\nWe also get a clean way to deal with null\nvalues. A mdl value is a special value used to denote that the value for a field\nis unavailable or inapplicable. If a field contains a null value, the pointer to the\nend of the field is set to be the same as the pointer to the beginning of the field.\nThat is, no space is used for representing the null value, and a comparison of\nthe pointers to the beginning and the end of the field is used to determine that\nthe value in the field is null.\nVariable~length record formats can obviously be used to store fixed-length\nrecords as well; sometimes, the extra overhead is justified by the added flexibil-\nity, because issues such as supporting n'ull values and adding fields to a recorcl\ntype arise with fixed-length records as well.\nI-laving variable-length fields in a record can raise some subtle issues, especially\nwhen a record is modified.\nIII\nModifying a field may cause it to grow, whieh requires us to shift all subse-\nquent fields to make space for the modification in all three record formats\njust presentcel.\nIII\nA modified record may no longer fit into the space remaining on its page.\nIf so, it may have to be moved to another page. If riels, which are used\nto 'point' to a record, include the page number (see Section 9.6), moving\na record to 'another page causes a problem. We may have to leave a 'for-\nwarding address' on this page identifying the ne\"v location of the record.\nAnd to ensure that space is ahvays available for this forwarding address,\nwe would have to allocate some minimum space for each record, regardless\nof its length.\n\nStoring Data: Disks and Files\nLarge Records in Real Systems: In Sybc1se ASE, a record can be at\nmost 1962 bytes. This limit is set by the 2KB log page size, since records\nare not allowed to be larger than a page. The exceptions to this rule. an~\nBLOBs and CLOBs, which consist of 1:1 set of bidirectionally linked pages.\nIBlvl DB2 and Microsoft SqL Server also do not allow records to span\npages, although large objects are allowed to span pages and are handled\nseparately from other data types. In DB2, record size is limited only by\nthe page size; in SQL Server, a record can be at most 8KB, excluding\nLOBs. Informix and Oracle 8 allow records to span pages. Informix allows\nrecords to be at most 32KB, while Oracle has no maximum record size;\nlarge records are organized as a singly directed list.\nIII\nA record may grow so large that it no longer fits on anyone page. We have\nto deal with this condition by breaking a record into smaller records. The\nsmaller records could be chained together-part of each smaller record is\na pointer to the next record in the chain---to enable retrieval of the entire\noriginal record.\n9.8\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\nExplain the term memory hierarchy.\nWhat are the differences between\nprimary, secondary, and tertiary storage? Give examples of each. Which\nof these is volatile, and which are pCf'sistenf?\nWhy is persistent storage\nmore important for a DBMS than, say, a program that generates prime\nnumbers? (Section 9.1)\nIII\nWhy are disks used so widely in a DBMS? What are their advantages\nover main memory and tapes?\n':Vhat are their relative disadvantages?\n(Section 9.1.1)\nIII\nWhat is a disk block or page?\nHow are blocks arranged in a disk?\nHow\ndoes this affect the time to access a block? Discuss seek tiTne, rotational\ndday, and transfer time. (Section 9.1.1)\nIII\nExplain how careful placement of pages on the disk to exploit the geometry\nof a disk can minimize the seek time and rotational delay when pages are\nread sequentially. (Section 9.1.2)\nIII\nExplain what a RAID systenl is and how it improves performance and\nreliability. Discuss str-iping and its impact on performance and nxlundancy\nand its irnpact on reliability.\nvVhat are the trade-offs between reliability\n\nCHAPTER.£)\nand performance in the different RAID organizations called RAID levels'?\n(Section 9.2)\n..\nWnat is the role of the DBMS d'isk space manager'?\nvVhy do database\nsystems not rely on the operating system instead? (Section 9.3)\n..\nWhy does every page request in a DBMS go through the buffer manager?\nWhat is the buffer poor? '\\That is the difference between a frame in a buffer\npool, a page in a file, and a block on a disk? (Section 9.4)\n..\nWhat information does the buffer manager maintain for each page in the\nbuffer pool?\n·What information is maintained for each frame?\nWhat is\nthe significance of p'in_count and the d'irty flag for a page?\nUnder what\nconditions can a page in the pool be replaced?\nUnder what conditions\nmust a replaced page be written back to disk? (Section 9.4)\n..\nWhy does the buffer manager have to replace pages in the buffer pool?\nHow is a page chosen for replacement? vVhat is sequent'ial flood'ing, and\nwhat replacement policy causes it? (Section 9.4.1)\n..\nA DBMS buffer manager can often predict the access pattern for disk pages.\nHow does it utilize this ability to minimize I/O costs? Discuss prefetch-\n'ing.\n\\iVhat is forc'ing, and why is it required to support the write-ahead\nlog protocol in a DBMS? In light of these points, explain why database\nsystems reimplement many services provided by operating systems. (Sec-\ntion 9.4.2)\n..\nWhy is the abstraction of a file of records important? How is the software\nin a DBMS layered to take advantage of this? (Section 9.5)\n..\nWhat is a heap file? How are pages organized in a heap file? Discuss list\nversus directory organizations. (Section 9.5.1)\nIII\nDescribe how records are arranged on a page.\n\\i\\That is a slot, and how\nare slots used to identify records? How do slots ena.ble us to move records\non a page withont altering the record's identifier?\n·What arc the differ-\nences in page organizations for fixed-length and variable-length records?\n(Section 9.6)\niii\n·What are the differences in how fields are arranged within fixed-length and\nvariable-length records? For variable-length records, explain how the array\nof offsets organization provides direct access to a specific field and supports\nnull values. (Section 9.7)",
          "pages": [
            365,
            366,
            367,
            368,
            369
          ],
          "relevance": {
            "score": 0.2,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.1,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Storing Data: Disks and Files\nEXERCISES\nExercise 9.1 \\-Vhat is the most important difference behveen a disk and a tape?\nExercise 9.2 Explain the terms seek time, mtat'ional delay, and transfer t'ime.\n33:)\nExercise 9.3 Both disks and main memory support direct access to any desired location\n(page). On average, main memory accesses are faster, of course. \\\\That is the other important\ndifference (from the perspective of the time required to access a desired page)?\nExercise 9.4 If you have a large file that is frequently scanned sequentially, explain how you\nwould store the pages in the file on a disk.\nExercise 9.5 Consider a disk with a sector size of 512 bytes, 2000 tracks per surface, 50\nsectors per track, five double-sided platters, and average seek time of 10 msec.\n1. What is the capacity of a track in bytes? What is the capacity of each surface? What is\nthe capacity of the disk?\n2. How many cylinders does the disk have?\n:~. Give examples of valid block sizes. Is 256 bytes a valid block size? 2048? 51,200?\n4. If the disk platters rotate at 5400 rpm (revolutions per minute), what is the maximum\nrotational delay?\n5. If one track of data can be transferred per revolution, what is the transfer rate?\nExercise 9.6 Consider again the disk specifications from Exercise 9.5 and suppose that a\nblock size of 1024 bytes is chosen. Suppose that a file containing 100,000 records of 100 bytes\neach is to be stored on such a disk and that no record is allowed to span two blocks.\nL How many records fit onto a block?\n2. How many blocks are required to store the entire file? If the file is arranged sequentially\non disk, how lllallY surfaces are needed?\n:3. How many records of 100 bytes each can be stored using this disk?\n4. If pages are stored sequentially on disk, with page 1 on block 1 of track 1, what page is\nstored on block 1 of track 1 on the next disk surface? How would your answer change if\nthe disk were capable of reading and writing from all heads in parallel?\n5. VVhat titne is required to read a file containing 100,000 records of 100 bytes each sequen-\ntially? Again, how \\vould your answer change if the disk were capable of reading/writing\nfrom all heads in parallel (and the data was arranged optimally)?\n6. \\\\That is the time required to read a file containing 100,000 records of 100 bytes each in a\nrandom order? To read a record, the block containing the recOl'd has to be fetched from\ndisk. Assume that each block request incurs the average seek time and rotational delay.\nExercise 9.7 Explain what. the buffer manager JIms! do to process a read request for a page.\n\\Vhat happens if the requested page is in the pool but not pinned?\nExercise 9.8 When does a buffer manager write a page to disk?\nExercise 9.9 What. does it mean to say that a page is p'inned in the buffer pool? Who is\nresponsible for pinning pages? \\Vho is responsible for unpinning pages?\n\nCHAPTERJ9\nExercise 9.10 'When a page in the bulTer pool is modified, how does the DBMS ensure that\nthis change is propagated to disk?\n(Explain the role of the buffer manager as well as the\nmodifier of the page.)\nExercise 9.11 \\Vhat happens if a page is requested when all pages in the buffer pool are\ndirty?\nExercise 9.12 \\Vhat is sequential flooding of the buffer pool?\nExercise 9.13 Name an important capability of a DBIVIS buffer manager that is not sup-\nported by a typical operating system's buffer manager.\nExercise 9.14 Explain the term prefetching. \\Vhy is it important?\nExercise 9.15 Modern disks often have their own main memory caches, typically about\n1 MB, and use this to prefetch pages.\nThe rationale for this technique is the empirical\nobservation that, if a disk page is requested by some (not necessarily database!) application,\n80% of the time the next page is requested as well. So the disk gambles by reading ahead.\n1. Give a nontechnical reason that a DBMS may not want to rely on prefetching controlled\nby the disk.\n2. Explain the impact on the disk's cache of several queries running concurrently, each\nscanning a different file.\n3. Is this problem addressed by the DBMS buffer manager prefetching pages? Explain.\n4. Modern disks support segmented caches, with about four to six segments, each of which\nis used to cache pages from a different file.\nDoes this technique help, with respect to\nthe preceding problem? Given this technique, does it matter whether the DBMS buffer\nmanager also does prefetching?\nExercise 9.16 Describe two possible record formats. What are the trade-offs between them?\nExercise 9.17 Describe two possible page formats. What are the trade-offs between them?\nExercise 9.18 Consider the page format for variable-length records that uses a slot directory.\n1. One approach to managing the slot directory is to use a maximum size (i.e., a maximum\nnumber of slots) and allocate the directory array when the page is created. Discuss the\npros and cons of this approach with respect to the approach discussed in the text.\n2. Suggest a modification to this page format that would allow us to sort records (according\nto the value in some field) without moving records and without changing the record ids.\nExercise 9.19 Consider the two internal organizations for heap files (using lists of pages and\na directory of pages) discussed in the text.\n1. Describe them briefly and explain the trade-offs. \\Vhich organization would you choose\nif records are variable in length?\n2. Can you suggest a single page format to implement both internal file organizations'?\nExercise 9.20 Consider a list-based organizat.ion of the pages in a heap file in which two\nlists are maintained: a list of all pages in the file and a list of all pages with free space. In\ncontrast, the list-based organizatioll discussed in the text maintains a list of full pages and a\nlist of pages with free space.\n\nStoring Data: Disks and Files\n3&7\n1. VVhat are the trade-offs, if any'? Is one of them clearly superior?\n2. For each of these organizations, describe a suitable page format.\nExercise 9.21 Modern disk drives store more sectors on the outer tracks than the inner\ntracks. Since the rotation speed is constant, the sequential data transfer rate is also higher on\nthe outer tracks. The seek time and rotational delay are unchanged. Given this information,\nexplain good strategies for placing files with the following kinds of access patterns:\n1. rrequent, random accesses to a small file (e.g., catalog relations).\n2. Sequential scans of a large file (e.g., selection from a relation with no index).\n3. Random accesses to a large file via an index (e.g., selection from a relation via the index).\n4. Sequential scans of a small file.\nExercise 9.22 Why do frames in the buffer pool have a pin count instead of a pin flag?\nPROJECT-BASED EXERCISES\nExercise 9.23 Study the public interfaces for the disk space manager, the buffer manager,\nand the heap file layer in Minibase.\n1. Are heap files with variable-length records supported?\n2. What page format is used in Minibase heap files?\n3. What happens if you insert a record whose length is greater than the page size?\n4. How is free space handled in Minibase?\nBIBLIOGRAPHIC NOTES\nSalzberg [648] and Wiederhold [776] discuss secondary storage devices and file organizations\nin detail.\nRAID wa.s originally proposed by Patterson, Gibson, and Katz [587]. The article by Chen\net al. provides an excellent survey of RAID [171] .\nBooks about RAID include Gibson's\ndissertation [.317] and the publications from the RAID Advisory Board [605].\nThe design and implementation of storage managers is discussed in [65, 1:33, 219, 477, 718].\nWith the exception of [219], these systems emphasize el:tensibili.ty, anel the papers contain\nmuch of interest from that stanelpoint as well. Other papers that cover storage management\nissues in the context of significant implemented prototype systems are [480] and [588]. The\nDali storage Inanager, which is optimized for main memory databases, is described in [406].\nThree techniques for ilnplementing long fields are compared in [96]. The impact of processor\ncache misses 011 DBMS performallce ha.'i received attention lately, as complex queries have\nbecome increasingly CPU-intensive. [:33] studies this issue, and shows that performance can be\nsignificantly improved by using a new arrangement of records within a page, in which records\non a page are stored in a column~oriented format (all field values for the first attribute followed\nby values for the second attribute, etc.).\nStonebraker discusses operating systems issues in the context of databases in [715]. Several\nbuffer management policies for databa.se systems are compared in [181]. Buffer management\nis also studied in [119, 169, 2G1, 2:35].\n\nTREE-STRUCTURED\nINDEXING\n...\nWhat is the intuition behind tree-structured indexes? Why are they\ngood for range selections?\n...\nHow does an ISAM index handle search, insert, and delete?\ni\"-\nHow does a B+ tree index handle search, insert, and delete?\n...\nWhat is the impact of duplicate key values on index implementation'?\n...\nWhat is key compression, and why is it important?\n...\nWhat is bulk-loading, and why is it important?\n...\nWhat happens to record identifiers when dynamic indexes are up-\ndated? How does this affect clustered indexes?\nItt\nKey concepts: ISAM, static indexes, overflow pages, locking issues;\nB+ trees, dynamic indexes, balance, sequence sets, node format; B+\ntree insert operation, node splits, delete operation, merge versus redis-\ntribution, minimum occupancy; duplicates, overflow pages, including\nrids in search keys; key compression; bulk-loading; effects of splits on\nrids in clustered indexes.\nOne that would have the fruit must climb the tree.\nI'homas Fuller\nVVe now consider two index data structures, called ISAM and B+ trees, b<:h':led\non tree organizations.\nThese structures provide efficient support for range\nsearches, including sorted file scans as a special c<h'3e. Unlike sorted files, these\n\nTree-StTuctuTed Indel:ing\nindex structures support efficient insertion and deletion.\nThey also provide\nsupport for equality selections, although they are not &'3 efficient in this case as\nhash-b::l.'3ed indexes, which are discussed in Chapter 11.\nAn ISAJVI1 tree is a static index structure that is effective when the file is\nnot frequently updated, but it is unsuitable for files that grow and shrink a\nlot.\n\\Ve discuss ISAM in Section 10.2.\nThe B+ tree is a dynamic structure\nthat adjusts to changes in the file gracefully. It is the most widely used index\nstructure because it adjusts well to changes and supports both equality and\nrange queries. We introduce B+ trees in Section 10.3. We cover B+ trees in\ndetail in the remaining sections. Section 10.3.1 describes the format of a tree\nnode.\nSection lOA considers how to search for records by using a B+ tree\nindex. Section 10.5 presents the algorithm for inserting records into a B+ tree,\nand Section 10.6 presents the deletion algorithm.\nduplicates are handled. \\Ve conclude with a discussion of some practical issues\nconcerning B+ trees in Section 10.8.\nNotation: In the ISAM and B+ tree structures, leaf pages contain data entries,\naccording to the terminology introduced in Chapter 8.\nFor convenience, we\ndenote a data entry with search key value k as k*.\nNon-leaf pages conta.in\ninde:c entries of the form (search key 'Value., page id) and are used to direct the\nsea.rch for a desired data entry (which is stored in some leaf). We often simply\nuse entr'Y where the context makes the nature of the entry (index or data) clear.\n10.1\nINTUITION FOR TREE INDEXES\nConsider a file of Students recorcls sorted by gpa. To answer a range selection\nsuch as \"Find all students with a gpa higher than 3.0,\" we must identify the\nfirst such student by doing a binary search of the file and then scan the file\nfrom that point on. If the file is large, the initial binary search can be quite\nexpensive, since cost is proportional to the number of pages fetched; can we\nimprove upon this method'?\nOIle idea is to create a second file with OIle record per page in the original\n(data) file, of the form (first key on page, pointer to page), again sortecl by the\nkey attribute (which is gpa in our example). The format of a page in the second\ninde:c file is illustrated in Figure 10.1.\nWe refer to pairs of the form (key, pointer)\n~l.S indc:J: entries or just entries \\'\\'hen\nthe context is dear. Note that each index page contains OIle pointer more than\nI ISAM stands for Indexed Sequential Access Method.\n\nindex entry\nr·.. ········\nFigUl'e 10.1\nFormat of an Index Page\nCHAPTER 10,\nthe number of keys---each key serves as a separator- for the contents of the pages\npointed to by the pointers to its left and right.\nThe simple index file data structure is illustrated in Figure 10.2.\nI\nPafJe 1 ·11\nPage 2.· II\nPage 31\nIndex file\nData file\nOne-Level Index Structure\nWe can do a binary search of the index file to identify the page containing the\nfirst key (gpo.) value that satisfies the range selection (in our example, the first\nstudent with gpo. over 3.0) and follow the pointer to the page containing the first\ndata. record with that key value. We can then scan the data file sequentially\nfrom that point on to retrieve other qualifying records. This example uses the\nindex to find the first data page containing a Students record with gpo. greater\nthan 3.0, and the data file is scanned from that point on to retrieve other such\nStudents records.\nBecause the size of an entry in the index file (key value and page icl) is likely\nto be much smaller than the size of a page, and only one such entry exists per\npage of the data file, the index file is likely to be much smaller than the data\nfile; therefore, a binary search of the index file is much faster than a binary\nsearch of the data file.\nHowever, a binary search of the index file could still\nbe fairly expensive, and the index file is typically still large enough to make\ninserts and\nclelett~s expensive.\nThe potential large size of the index file motivates the tree indexing idea: Why\nnot apply the previous step of building an auxiliar:v structure all the collection\nof inde:l: records and so on recursively until the smallest auxiliary structure fits\nOIl one page? This repeated construction of a one-level index leads to a tree\nstructure with several levels of non-leaf pages.\n\nTrce-Stn.lduTed Inde:ring\nJ\nAs we observed in Section 8.3.2, the power of the approach comes from the fact\nthat locating a record (given a search key value) involves a traversal from the\nroot to a leaf, with one I/O (at most; SCHne pages, e.g.) the root, are likely to be\nin the buffer pool) per level. Given the typical fan-out value (over 100), trees\nrarely have more than 3-4 levels.\nThe next issue to consider is how the tree structure can handle inserts and\ndeletes of data entries. Two distinct approaches have been used, leading to the\nISAM and B+ tree data structures, which we discuss in subsequent sections.\n10.2\nINDEXED SEQUENTIAL ACCESS METHOD (ISAM)\nThe ISAM data structure is illustrated in Figure 10.3. The data entries of the\nISAM index are in the leaf pages of the tree and additional overflow pages\nchained to some leaf page. Database systems carefully organize the layout of\npages so that page boundaries correspond closely to the physical characteristics\nof the underlying storage device.\nThe ISAM structure is completely static\n(except for the overflow pages, of which it is hoped, there will be few) and\nfacilitates such low-level optimizations.\nNon-leaf\npages\nLeaf\npages\nOverjlow P{~c::::J1\n~\n~\nPrimary pages\nISAM Index Structure\nEach tree node is a disk page, and all the data resides in the leaf pages. This\ncorresponds to an index that uses Alternative (1) for data entries, in terms of\nthe alternatives described in Chapter 8; we can create an index with Alternative\n(2) by storing t.he data records in a separate file and storing (key, rid) pairs in\nthe leaf pages of the ISAM index. When the file is created, all leaf pages are\nallocated sequentially and sorted on the search key value. (If Alternative (2)\nor (3) is used, the data records are created and sorted before allocating the leaf\npages of the ISAM index.) The non-leaf level pages are then allocated. If there\nare several inserts to the file subsequently, so that more entries are inserted into\na leaf than will fit onto a single page, additional pages are needed because the",
          "pages": [
            370,
            371,
            372,
            373,
            374,
            375,
            376
          ],
          "relevance": {
            "score": 0.1,
            "sql_score": 1.0,
            "concept_score": 0.0,
            "non_sql_penalty": 0.2,
            "is_relevant": false,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER lO\nindex structure is static. These additional pages are allocated from an overflow\narea. The allocation of pages is illustrated in Figure 10.4.\nData Pages\nIndex Pages\nOverflow Pages\nPage Allocation in ISAM\nThe basic operations of insertion, deletion, and search are all quite straightfor-\nward. J;\"'or an equality selection search, we start at the root node and determine\nwhich subtree to search by comparing the value in the search field of the given\nrecord with the key values in the node. (The search algorithm is identical to\nthat for a B+ tree; we present this algorithm in more detail later.) For a range\nquery, the starting point in the data (or leaf) level is determined similarly, and\ndata pages are then retrieved sequentially. For inserts and deletes, the appro-\npriate page is determined as for a search, and the record is inserted or deleted\nwith overflow pages added if necessary.\nThe following example illustrates the ISAM index structure. Consider the tree\nshown in Figure 10.5. All searches begin at the root. For example, to locate a\nrecord with the key value 27, we start at the root and follow the left pointer,\nsince 27 < 40. We then follow the middle pointer, since 20 <= 27 < 33. For a\nrange sea,rch, we find the first qualifying data entry as for an equality selection\nand then retrieve primary leaf pages sequentially (also retrieving overflow pages\nas needed by following pointers from the primary pages).\nThe primary leaf\npages are cL..ssumed to be allocated sequentially\nthis a..ssumption is reasonable\nbecause the number of such pages is known when the tree is created and does\nnot change subsequently under inserts and deletes-and so no 'next leaf page'\npointers are needed.\nvVe assume that each leaf page can contain two entries.\nIf we now insert a\nrecord with key value 23, the entry 23* belongs in the second data page, which\nalready contains 20* and 27* and has no more space. We deal with this situation\nby adding an overflow page and putting 23* in. the overflow page. Chains of\noverflow pages can easily develop.\nF'or instance, inserting 48*, 41*, and 42*\nleads to an overflow chain of two pages. The tree of Figure 10.5 with all these\ninsertions is shown ill Figure 10.6.\n\nTree~StrLLctuTed Inde:ri:ng\n10*1\n15*IEEl B3 1\n40*1\n46*11\n51*l 55*1 1\n63*1 97*I\nSa.mple ISAM Tree\nNon-leaf\npages\nPrimary\nleaf\npages\nOverflow\npages\nISAM Tree a.fter Inserts\n\nCHAPTER Ii)\nThe deletion of an entry h\nis handled by simply removing the entry. If this\nentry is on an overflow page and the overflow page becomes empty, the page can\nbe removed. If the entry is on a primary page and deletion makes the primary\npage empty, the simplest approach is to simply leave the empty primary page\n~s it is; it serves as a placeholder for future insertions (and possibly lloll-empty\noverflow pages, because we do not move records from the overflow pages to the\nprimary page when deletions on the primary page create space).\nThus, the\nnumber of primary leaf pages is fixed at file creation time.\n10.2.1\nOverflow Pages, Locking Considerations\nNote that, once the ISAM file is created, inserts and deletes affect only the\ncontents of leaf pages. A consequence of this design is that long overflow chains\ncould develop if a number of inserts are made to the same leaf. These chains\ncan significantly affect the time to retrieve a record because the overflow chain\nhas to be searched as well when the search gets to this leaf. (Although data in\nthe overflow chain can be kept sorted, it usually is not, to make inserts fast.) To\nalleviate this problem, the tree is initially created so that about 20 percent of\neach page is free. However, once the free space is filled in with inserted records,\nunless space is freed again through deletes, overflow chains can be eliminated\nonly by a complete reorganization of the file.\nThe fact that only leaf pages are modified also has an important advantage with\nrespect to concurrent access. When a page is accessed, it is typically 'locked'\nby the requestor to ensure that it is not concurrently modified by other users\nof the page. To modify a page, it must be locked in 'exclusive' mode, which is\npermitted only when no one else holds a lock on the page. Locking can lead\nto queues of users (transactions, to be more precise) waiting to get access to a\npage. Queues can be a significant performance bottleneck, especially for heavily\naccessed pages near the root of an index structure.\nIn the ISAM structure,\nsince we know that index-level pages are never modified, we can safely omit\nthe locking step. Not locking index-level pages is an important advantage of\nISAM over a dynamic structure like a B+ tree. If the data distribution and\nsize are relatively static, which means overflow chains are rare, ISAM might be\npreferable to B+ trees due to this advantage.\n10.3\nB+ TREES: A DYNAMIC INDEX STRUCTURE\nA static structure such as the ISAI\\il index suffers from the problem that long\noverflow chains can develop a\"s the file grows, leading to poor performance. This\nproblem motivated the development of more flexible, dynamic structures that\nadjust gracefully to inserts and deletes. The B+ tree search structure, which\nis widely llsed, is a balanced tree in which the internal nodes direct the search",
          "pages": [
            377,
            378,
            379
          ],
          "relevance": {
            "score": 0.3,
            "sql_score": 1.0,
            "concept_score": 0.2,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Transaction isolation levels define how transactions interact with each other and with data that has been modified but not yet committed. They are crucial for maintaining data consistency and preventing issues like dirty reads, non-repeatable reads, and phantom reads.",
        "explanation": "Understanding transaction isolation levels is essential in database management because it ensures that concurrent transactions do not interfere with each other's work. There are four main isolation levels: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and SERIALIZABLE. Each level provides a different balance between performance and data integrity.\n\nREAD UNCOMMITTED allows a transaction to read data that has been modified but not yet committed by another transaction, which can lead to dirty reads. READ COMMITTED ensures that a transaction only sees data that has been committed, preventing dirty reads but allowing non-repeatable reads. REPEATABLE READ guarantees that a transaction will see the same data repeatedly during its execution, even if other transactions modify and commit that data. SERIALIZABLE is the highest isolation level, ensuring complete isolation by ordering transactions in a way that eliminates all concurrency issues.",
        "key_points": [
          "READ UNCOMMITTED allows dirty reads but provides the least overhead.",
          "READ COMMITTED prevents dirty reads but allows non-repeatable reads.",
          "REPEATABLE READ ensures repeatable reads but may cause phantom reads.",
          "SERIALIZABLE provides complete isolation but has the highest overhead."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Set transaction isolation level to READ COMMITTED SET TRANSACTION ISOLATION LEVEL READ COMMITTED;",
            "explanation": "This example demonstrates how to set the transaction isolation level in SQL. Setting the isolation level affects how transactions interact with each other and the data they read.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Simulate a scenario where two transactions might cause a dirty read\nBEGIN TRANSACTION;\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1;\n-- Another transaction reads the updated value before it is committed\nSELECT balance FROM accounts WHERE account_id = 1;",
            "explanation": "This practical example shows how two transactions might interact at different isolation levels, highlighting the importance of choosing the right isolation level for your application."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using READ UNCOMMITTED without considering the risk of dirty reads.",
            "incorrect_code": "-- Incorrectly set to READ UNCOMMITTED SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;",
            "correct_code": "-- Correctly set to a higher isolation level like READ COMMITTED SET TRANSACTION ISOLATION LEVEL READ COMMITTED;",
            "explanation": "Setting the isolation level too low can lead to data inconsistencies. It's important to understand the risks and choose an appropriate isolation level based on your application's requirements."
          }
        ],
        "practice": {
          "question": "Explain how SERIALIZABLE isolation level works and why it might be necessary in certain applications.",
          "solution": "SERIALIZABLE isolation level orders transactions as if they were executed sequentially, ensuring complete isolation. It prevents all types of concurrency issues but can significantly reduce performance due to the need for strict ordering. This isolation level is necessary in applications where data consistency is critical and concurrent modifications could lead to unpredictable results."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER .9\na record is inserted, the slot directory should be scanned for an element that\ncurrently does not point to any record, and this slot should be used for the new\nrecord. A new slot is added t",
      "content_relevance": {
        "score": 0.2,
        "sql_score": 1.0,
        "concept_score": 0.2,
        "non_sql_penalty": 0.2,
        "is_relevant": false,
        "analysis": "SQL content"
      }
    },
    "indexes": {
      "id": "indexes",
      "title": "Database Indexes",
      "definition": "Data structures that improve the speed of data retrieval operations",
      "difficulty": "intermediate",
      "page_references": [
        385,
        386,
        387,
        388,
        389,
        390,
        391,
        392,
        393,
        394,
        395,
        396,
        397,
        398,
        399,
        400
      ],
      "sections": {
        "definition": {
          "text": "CHAPTER jO\n/\n,_ - - Entry to be inserted in parent 11(.)de.\n[i]1\n<-- -\n(Note that 5 is 'copied up' and\n_-....... \"---\\\n,ontin.\", to ,ppcM;n the lenf.)\nEEf-rJ-~r\nSplit Leaf Pages during Insert of Entry 8*\nThe split pages in our example are shown in Figure 10.12. The index entry\npointing to the new non-leaf node is the pair (17, pointer to new index-level\npage); note that the key value 17 is 'pushed up' the tree, in contrast to the\nsplitting key value 5 in the leaf split, which was 'copied up.'\n/\nEntry to be inserted in parent node.\n~\n..£~\n_ :' -\n(Note that 17 is 'pushed up' and\nand appears once In the index. Contrast\nthIS with a leaf spILt.)\n)EffJD HPJ\nSplit Index Pages during Insert of Entry 8*\nThe difference in handling leaf-level and index-level splits arises from the B+\ntree requirement that all data entries h\nmust reside in the leaves.\nThis re-\nquirement prevents us from 'pushing up' 5 and leads to the slight redundancy\nof having some key values appearing in the leaf level as well as in some index\nleveL However, range queries can be efficiently answered by just retrieving the\nsequence of leaf pages; the redundancy is a small price to pay for efficiency. In\ndealing with the index levels, we have more flexibility, and we 'push up' 17 to\navoid having two copies of 17 in the index levels.\nNow, since the split node was the old root, we need to create a new root node\nto hold the entry that distinguishes the two split index pages. The tree after\ncompleting the insertion of the entry 8* is shown in Figure 10.13.\nOne variation of the insert algorithm tries to redistribute entries of a node N\nwith a sibling before splitting the node; this improves average occupancy. The\nsibling of a node N, in this context, is a node that is immediately to the left\nor right of N and has the same pare'nt as N.\n\nTree-Structured Index'ing\nB+ Tree after Inserting Entry 8*\nTo illustrate redistribution, reconsider insertion of entry 8* into the tree shown\nin Figure 10.9. The entry belongs in the left-most leaf, which is full. However,\nthe (only) sibling of this leaf node contains only two entries and can thus\naccommodate more entries. We can therefore handle the insertion of 8* with a\nredistribution. Note how the entry in the parent node that points to the second\nleaf has a new key value; we 'copy up' the new low key value on the second\nleaf. This process is illustrated in Figure 10.14.\nB+ Tree after Inserting Entry 8* Using Redistribution\nTo determine whether redistribution is possible, we have to retrieve the sibling.\nIf the sibling happens to be full, we have to split the node anyway. On average,\nchecking whether redistribution is possible increases I/O for index node splits,\nespecially if we check both siblings. (Checking whether redistribution is possible\nmay reduce I/O if the redistribution succeeds whereas a split propagates up the\ntree, but this case is very infrequent.) If the file is growing, average occupancy\nwill probably not be affected much even if we do not redistribute. Taking these\nconsiderations ,into account, not redistributing entries at non-leaf levels usually\npays off.\nIf a split occurs at the leaf level, however, we have to retrieve a neighbor\nto adjust the previous and next-neighbor pointers with respect to the newly\ncreated leaf node. Therefore, a limited form of redistribution makes sense: If a\nleaf node is full, fetch a neighbor node; if it ha.'3 space and has the same parent,\n\nCHAPTER :40\nredistribute the entries. Othenvise (the neighbor has diflerent parent, Le., it is\nnot a sibling, or it is also full) split the leaf node and a,djust the previous and\nnext-neighbor pointers in the split node, the newly created neighbor, and the\nold neighbor.\n10.6\nDELETE\nThe algorithm for deletion takes an entry, finds the leaf node where it belongs,\nand deletes it.\nPseudocode for the B+ tree deletion algorithm is given in\nthe entry by calling the delete algorithm on the appropriate child node. We\nusually go down to the leaf node where the entry belongs, remove the entry\nfrom there, and return all the way back to the root node.\nOccasionally a\nnode is at minimum occupancy before the deletion, and the deletion causes\nit to go below the occupancy threshold. When this happens, we must either\nredistribute entries from an adjacent sibling or merge the node with a sibling to\nmaintain minimum occupancy. If entries are redistributed between two nodes,\ntheir parent node must be updated to reflect this; the key value in the index\nentry pointing to the second node must be changed to be the lowest search key\nin the second node. If two nodes are merged, their parent must be updated to\nreflect this by deleting the index entry for the second node; this index entry is\npointed to by the pointer variable oldchildentry when the delete call returns to\nthe parent node. If the last entry in the root node is deleted in this manner\nbecause one of its children was deleted, the height of the tree decreases by 1.\nTo illustrate deletion, let us consider the sample tree shown in Figure 10.13. To\ndelete entry 19*, we simply remove it from the leaf page on which it appears,\nand we are done because the leaf still contains two entries. If we subsequently\ndelete 20*, however, the leaf contains only one entry after the deletion. The\n(only) sibling of the leaf node that contained 20* has three entries, and we can\ntherefore deal with the situation by redistribution; we move entry 24* to the\nleaf page that contained 20* and copy up the new splitting key (27, which is\nthe new low key value of the leaf from which we borrowed 24*) into the parent.\nThis process is illustrated in Figure 10.16.\nSuppose that we now delete entry 24*. The affected leaf contains only one entry\n(22*) after the deletion, and the (only) sibling contains just two entries (27*\nand 29*). Therefore, we cannot redistribute entries. However, these two leaf\nnodes together contain only three entries and can be merged. \\Vhile merging,\nwe can 'tos::;' the entry ((27, pointer' to second leaf page)) in the parent, which\npointed to the second leaf page, because the second leaf page is elnpty after the\nmerge and can be discarded. The right subtree of Figure 10.16 after thi::; step\nin the deletion of entry 2!1* is shown in Figure 10.17.\n\nTree-Structured Inde:l:ing\n,\nproc delete (parentpointer, nodepointer, entry, oldchiIdentry)\n/ / Deletes entry from s'ubtree w'ith TOot '*nodepointer '; degree is d;\n/ /\n'oldchildentry' null initially, and null upon ret1lrn unless child deleted\nif *nodepointer is a non-leaf node, say N,\nfind i such that K i ::; entry's key value < K i+l;\n/ / choose subtree\ndelete(nodepointer, Pi, entry, oldchildentry);\n/ / recursive delete\nif oldchildentry is null, return;\n/ / usual case: child not deleted\nelse,\n/ / we discarded child node (see discussion)\nremove *oldchildentry from N,\n/ / next, check for underflow\nif N has entries to spare,\n/ / usual case\nset oldchildentry to null, return; / / delete doesn't go further\nelse,\n/ / note difference wrt merging of leaf pages!\nget a sibling S of N:\n/ / parentpointer arg used to find S\nif S has extra entries,\nredistribute evenly between Nand S through parent;\nset oldchildentry to null, return;\nelse, merge Nand S\n/ / call node on rhs 111\noldchildentry = & (current entry in parent for M);\npull splitting key from parent down into node on left;\nmove all entries from 1\\11 to node on left;\ndiscard empty node M, return;\nif *nodepointer is a leaf node, say L,\nif L h&<; entries to spare,\n/ / usual case\nremove entry, set oldchildentry to null, and return;\nelse,\n/ / once in a while, the leaf becomes underfull\nget a sibling S of L;\n/ / parentpointer used to find S\nif S has extra entries,\nredistribute evenly between Land S;\nfind entry in parent for node on right;\n/ / call it A;J\nreplace key value in parent entry by new low-key value in 1\\11;\nelse, merge Land S\n/ / call node on rhs 1\\11\ndiscard empty node AI, adjust sibling pointers, return;\nendproc\nAlgorithm for Deletion from B+ Tree of Order r1",
          "pages": [
            385,
            386,
            387,
            388
          ],
          "relevance": {
            "score": 0.4,
            "sql_score": 1.0,
            "concept_score": 0.4,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "B+ Tree after Deleting Entries 19* and 20*\nPartial B+ Tree during Deletion of Entry 24*\nDeleting the entry (27, pointer to second leaf page) has created a non-Ieaf-Ievel\npage with just one entry, which is below the minimum of d = 2. To fix this\nproblem, we must either redistribute or merge. In either case, we must fetch a\nsibling. The only sibling of this node contains just two entries (with key values\n5 and 13), and so redistribution is not possible; we must therefore merge.\nThe situation when we have to merge two non-leaf nodes is exactly the opposite\nof the situation when we have to split a non-leaf node. We have to split a non-\nleaf node when it contains 2d keys and 2d + 1 pointers, and we have to add\nanother key--pointer pair. Since we resort to merging two non-leaf nodes only\nwhen we cannot redistribute entries between them, the two nodes must be\nminimally full; that is, each must contain d keys and d + 1 pointers prior to\nthe deletion. After merging the two nodes and removing the key--pointer pair\nto be deleted, we have 2d - 1 keys and 2d + 1 pointers: Intuitively, the left-\nmost pointer on the second merged node lacks a key value. To see what key\nvalue must be combined with this pointer to create a complete index entry,\nconsider the parent of the two nodes being merged. The index entry pointing\nto one of the merged nodes must be deleted from the parent because the node\nis about to be discarded. The key value in this index entry is precisely the key\nvalue we need to complete the new merged node: The entries in the first node\nbeing merged, followed by the splitting key value that is 'pulled down' from the\nparent, followed by the entries in the second non-leaf node gives us a total of 2d\nkeys and 2d + 1 pointers, which is a full non-leaf node. Note how the splitting\n\nTree-Structured Indexing\nkey value in the parent is pulled down, in contrast to the case of merging two\nleaf nodes.\nConsider the merging of two non-leaf nodes in our example. Together, the non-\nleaf node and the sibling to be merged contain only three entries, and they have\na total of five pointers to leaf nodes. To merge the two nodes, we also need to\npull down the index entry in their parent that currently discriminates between\nthese nodes. This index entry has key value 17, and so we create a new entry\n(17, left-most child pointer in sibling). Now we have a total of four entries and\nfive child pointers, which can fit on one page in a tree of order d = 2. Note that\npulling down the splitting key 17 means that it will no longer appear in the\nparent node following the merge.\nAfter we merge the affected non-leaf node\nand its sibling by putting all the entries on one page and discarding the empty\nsibling page, the new node is the only child of the old root, which can therefore\nbe discarded. The tree after completing all these steps in the deletion of entry\n24* is shown in Figure 10.18.\nB+ Tree after Deleting Entry 24*\nThe previous examples illustrated redistribution of entries across leaves and\nmerging of both leaf-level and non-leaf-level pages. The remaining case is that\nof redistribution of entries between non-leaf-level pages.\nTo understand this\ncase, consider the intermediate right subtree shown in Figure 10.17. We would\narrive at the same intermediate right subtree if we try to delete 24* from a\ntree similar to the one shown in Figure 10.16 but with the left subtree and\nroot key value as shown in Figure 10.19. The tree in Figure 10.19 illustrates\nan intermediate stage during the deletion of 24*. (Try to construct the initial\ntree. )\nIn contrast to the caf.;e when we deleted 24* from the tree of Figure HUG, the\nnon-leaf level node containing key value :30 now ha..s a sibling that can spare\nentries (the entries with key values 17 and 20).\nvVe move these entries3 over\nfrom the sibling. Note that, in doing so, we essentially push them through the\n:11t is sufficient to move over just the entry with key value 20, hut we are moving over two entries\n°0 illustrate what happens when several entries are redistributed.\n\nCHAPTER ;'0\nA B+ Tree during a Deletion\nsplitting entry in their parent node (the root), which takes care of the fact that\n17 becomes the new low key value on the right and therefore must replace the\nold splitting key in the root (the key value 22). The tree with all these changes\nis shown in Figure 10.20.\nB+ Tree after Deletion\nIn concluding our discussion of deletion, we note that we retrieve only one\nsibling of a node. If this node has spare entries, we use redistribution; otherwise,\nwe merge. If the node has a second sibling, it may be worth retrieving that\nsibling as well to check for the possibility of redistribution. Chances are high\nthat redistribution is possible, and unlike merging, redistribution is guaranteed\nto propagate no further than the parent node.\nAlso, the pages have more\nspace on them, which reduces the likelihood of a split on subsequent insertions.\n(Remember, files typically grow, not shrink!)\nHowever, the number of times\nthat this case arises (the node becomes less than half-full and the first sibling\ncannot spare an entry) is not very high, so it is not essential to implement this\nrefinement of the bct.'3ic algorithm that we presented.\n10.7\nDUPLICATES\nThe search, insertion, and deletion algorithms that we presented ignore the\nissue of duplicate keys, that is, several data entries with the same key value.\nvVe now discuss how duplica.tes can be handled.\n\nTree-StT'llct'uTed Inde:ring\n35~\nDuplicate Handling in COlllmercial Systems: In a clustered index in\nSybase ASE, the data rows are maintained in sorted order onthe page and\nin the eollection of data pages. The data pages are bidireetionally linked\nin sort order. Rows with duplicate keys are inserted into (or deleted from}\nthe ordered set of rows. This may result in overflow pages of rows with\nduplieate keys being inserted into the page chain or empty overflow pages\nremoved from the page chain. Insertion or deletion of a duplicate key does\nnot affect the higher index level'> unless a split or. lIlergy ofa .non.-overflow\npage occurs. In IBM DB2, Oracle 8, and Miero§oft'SQL'Server; dupliclltes\nare handled by adding a row id if necessary to eliminate duplicate key\nvalues.\n.\nThe basic search algorithm assumes that all entries with a given key value reside\non a single leaf page.\nOne way to satisfy this assumption is to use overflow\npages to deal with duplicates. (In ISAM, of course, we have overflow pages in\nany case, and duplicates are easily handled.)\nTypically, however, we use an alternative approach for duplicates. We handle\nthem just like any other entries and several leaf pages may contain entries with\na given key value. To retrieve all data entries with a given key value, we must\nsearch for the left-most data entry with the given key value and then possibly\nretrieve more than one leaf page (using the leaf sequence pointers). Modifying\nthe search algorithm to find the left-most data entry in an index with duplicates\nis an interesting exercise (in fact, it is Exercise 10.11).\nOne problem with this approach is that, when a record is deleted, if we use\nAlternative (2) for data entries, finding the corresponding data entry to delete\nin the B+ tree index could be inefficient because we may have to check several\nduplicate entries (key, rid) with the same key value.\nThis problem can be\naddressed by considering the rid value in the data entry to be part of the\nsearch key, for purposes of positioning the data entry in the tree. This solution\neffectively turns the index into a uniq71,e index (i.e\" no duplicates), Remember\nthat a search key can be any sequence of fields\nin this variant, the rid of the\ndata record is essentially treated as another field while constructing the search\nkey.\nAlternative (3) f'or data entries leads to a natural solution for duplicates, but if\nwe have a large number of duplicates, a single data entry could span multiple\npages. And of course, when a data record is deleted, finding the rid to delete\nfrom the corresponding data entry can be inefficient,\nThe solution to this\nproblem is similar to the one discussed previously for Alternative (2): vVe can\n\nmaintain the list of rids within each data entry in sorted order (say, by page\nnumber and then slot number if a rid consists of a page id and a slot id).\n10.8\nB+ TREES IN PRACTICE\nIn this section we discuss several important pragmatic issues.\n10.8.1\nKey Compression\nThe height of a B+ tree depends on the number of data entries and the size of\nindex entries. The size of index entries determines the number of index entries\nthat will fit on a page and, therefore, the fan-out of the tree. Since the height\nof the tree is proportional to logfan-oud# of data entries), and the number of\ndisk l/Os to retrieve a data entry is equal to the height (unless some pages are\nfound in the buffer pool), it is clearly important to maximize the fan-out to\nminimize the height.\nAn index entry contains a search key value and a page pointer.\nHence the\nsize depends primarily on the size of the search key value.\nIf search key\nvalues are very long (for instance, the name Devarakonda Venkataramana\nSathyanarayana Seshasayee Yellamanchali Murthy, or Donaudampfschifffahrts-\nkapitansanwiirtersmiitze), not many index entries will fit on a page: Fan-out is\nlow, and the height of the tree is large.\nOn the other hand, search key values in index entries are used only to direct\ntraffic to the appropriate leaf.\nWhen we want to locate data entries with a\ngiven search key value, we compare this search key value with the search key\nvalues of index entries (on a path from the root to the desired leaf). During\nthe comparison at an index-level node, we want to identify two index entries\nwith search key values kl and k2 such that the desired search key value k falls\nbetween k1 and k2. To accomplish this, we need not store search key values in\ntheir entirety in index entries.\nFor example, suppose we have two adjacent index entries in a node, with search\nkey values 'David Smith' and 'Devarakonda ... ' To discriminate between these\ntwo values, it is sufficient to store the abbreviated forms 'Da' and 'De.' More\ngenerally, the lneaning of the entry 'David Smith' in the B+ tree is that every\nvalue in the subtree pointed to by the pointer to the left of 'David Smith' is less\nthan 'David Smith,' and every value in the subtree pointed to by the pointer\nto the right of 'David Smith' is (greater than or equal to 'David Smith' and)\nless than 'Devarakonda ... '\n\nTree-Struct'ured Indexing\nB+ Trees in Real Systems: IBM DB2, Infol:mLx, Microsoft SQL Server,\nOracle 8, and Sybase ASE all support clustered ~d unclustered B+ tree\nindexes, with some differencesin how they handle deletions and duplicate\nkey values. In Sybase ASE, depending on the concurrency control schelne\nbeing used for· the index, the deleted row is removed (with merging if\nthe page occupancy goes below threshold) or simply 111arkedas deleted; a\ngarbage collection scheme is used to recover space . in th~ latter case,\nIn\nOracle 8, deletions are handled by marking the row as deleted. 1'0 reclaim\nthe space occupied by deleted records, we can rebuild the index online (i.e.,\nwhile users continue to use the index) or coalesce underfull pages (which\ndoes not reduce tree height). Coalesce is in-place, rebuild creates a copy.\nInformix handles deletions by simply marking records as deleted. DB2 and\nSQL Server remove deleted records and merge pages when occupancy goes\nbelow threshold.\nOracle 8 also allows records from multiple relations to be co-clustered on\nthe same page. The co-clustering can be based on a B+ tree search key or\nstatic hashing and up to 32 relations can be stored together.\nTo ensure such semantics for an entry is preserved, while compressing the entry\nwith key 'David Smith,' we must examine the largest key value in the subtree to\nthe left of 'David Smith' and the smallest key value in the subtree to the right\nof 'David Smith,' not just the index entries ('Daniel Lee' and 'Devarakonda\n... ') that are its neighbors. This point is illustrated in Figure 10.21; the value\n'Davey Jones' is greater than 'Dav,' and thus, 'David Smith' can be abbreviated\nonly to 'Davi,' not to 'Dav.'\nDevarakonda ...\no\nExample Illustrating Prefix Key Compression\nThis technique.\ncalled prefix key compression or simply key compres-\nsion, is supported in many commercial implementations of B+ trees. It can\nsubstantially increCL')e the fan-out of a tree. We do not discuss the details of\nthe insertion and deletion algorithms in the presence of key compression.\n\n10.8.2\nBulk-Loading a B+ Tree\nEntries are added to a B+ tree in two ways. First, we may have an existing\ncollection of data records with a B+ tree index on it; whenever a record is\nadded to the collection, a corresponding entry must be added to the B+ tree\nas well. (Of course, a similar comment applies to deletions.) Second, we may\nhave a collection of data records for which we want to create a B+ tree index\non some key field(s). In this situation, we can start with an empty tree and\ninsert an entry for each data record, one at a time, using the standard insertion\nalgorithm. However, this approach is likely to be quite expensive because each\nentry requires us to start from the root and go down to the appropriate leaf\npage. Even though the index-level pages are likely to stay in the buffer pool\nbetween successive requests, the overhead is still considerable.\nFor this reason many systems provide a bulk-loading utility for creating a B+\ntree index on an existing collection of data records. The first step is to sort\nthe data entries k* to be inserted into the (to be created) B+ tree according to\nthe search key k. (If the entries are key-pointer pairs, sorting them does not\nmean sorting the data records that are pointed to, of course.) We use a running\nexample to illustrate the bulk-loading algorithm. We assume that each data\npage can hold only two entries, and that each index page can hold two entries\nand an additional pointer (i.e., the B+ tree is assumed to be of order d = 1).\nAfter the data entries have been sorted, we allocate an empty page to serve as\nthe root and insert a pointer to the first page of (sorted) entries into it. We\nillustrate this process in Figure 10.22, using a sample set of nine sorted pages\nof data entries.\n~!--==_~~~~\"\",L:-=o-s_c_,:_.te_d_p~a~g_e_s_()f_(_la_t_a_e_nt_rie_s~n~ot_Y_\"_il_l_B_+_t_re_e~~_----,\nffi EEJ 110*~~ 112j~\n120*[221\nInitial Step in B+ Tree Bulk-Loading\nvVe then add one entry to the root page for each page of the sorted data entries.\nThe new entry consists of \\low key value on page, pointer' to page). vVe proceed\nuntil the root page is full; see Figure 10.23.\nTo insert the entry for the next page of data entries, we must split the root and\ncreate a new root page. vVe show this step in Figure 10.2/1.\n\nTr'ee-8iruci'ured Index'ing\nData entry pages not yet in B+ tree\nRoot Page Fills up in B+ Tree Bulk-Loading\nData entry pages Ilot yet ill B+ tree\nPage Split during B+ 'fi'ee Bulk-Loading",
          "pages": [
            389,
            390,
            391,
            392,
            393,
            394,
            395,
            396
          ],
          "relevance": {
            "score": 0.3,
            "sql_score": 1.0,
            "concept_score": 0.4,
            "non_sql_penalty": 0.2,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "CHAPTER :FO\n\"We have redistributed the entries evenly between the two children of the root,\nin anticipation of the fact that the B+ tree is likely to grow. Although it is\ndifficult (!) to illustrate these options when at most two entries fit on a page,\nwe could also have just left all the entries on the old page or filled up some\ndesired fraction of that page (say, 80 percent). These alternatives are simple\nvariants of the basic idea.\nTo continue with the bulk-loading example, entries for the leaf pages are always\ninserted into the right-most index page just above the leaf level. 'When the right-\nmost index page above the leaf level fills up, it is split. This action may cause\na split of the right-most index page one step closer to the root, as illustrated\nin Figures 10.25 and 10.26.\nData entry pages\nnot yet in B+ tree\nBefore Adding Entry for Leaf Page Containing 38*\nData entry pages\nnot yet in B+ tree\nI\nI\nI\nITIf IT,fT ?'\nr---.----'i\n12113{j2'122:J123*EJ ~5*136*~ '141*1!f'*1 ]\nAfter Adding Entry for Leaf Page Containing :38*\n\nTiee-Structured Inde:ring\n36,3\nNote that splits occur only on the right-most path from the root to the leaf\nlevel. \\Ve leave the completion of the bulk-loading example as a simple exercise.\nLet us consider the cost of creating an index on an existing collection of records.\nThis operation consists of three steps: (1) creating the data entries to insert\nin the index, (2) sorting the data entries, and (3) building the index from the\nsorted entries. The first step involves scanning the records and writing out the\ncorresponding data entries; the cost is (R + E) I/Os, where R is the number of\npages containing records and E is the number of pages containing data entries.\nSorting is discussed in Chapter 13; you will see that the index entries can be\ngenerated in sorted order at a cost of about 3E I/Os. These entries can then be\ninserted into the index as they are generated, using the bulk-loading algorithm\ndiscussed in this section. The cost of the third step, that is, inserting the entries\ninto the index, is then just the cost of writing out all index pages.\n10.8.3\nThe Order Concept\nWe presented B+ trees using the parameter d to denote minimum occupancy. It\nis worth noting that the concept of order (i.e., the parameter d), while useful for\nteaching B+ tree concepts, must usually be relaxed in practice and replaced\nby a physical space criterion; for example, that nodes must be kept at lea..c;t\nhalf-full.\nOne reason for this is that leaf nodes and non-leaf nodes can usually hold\ndifferent numbers of entries.\nRecall that B+ tree nodes are disk pages and\nnon-leaf nodes contain only search keys and node pointers, while leaf nodes can\ncontain the actual data records. Obviously, the size of a data record is likely\nto be quite a bit larger than the size of a search entry, so many more search\nentries than records fit on a disk page.\nA second reason for relaxing the order concept is that the search key may\ncontain a character string field (e.g., the name field of Students) whose size\nvaries from record to record; such a search key leads to variable-size data entries\nand index entries, and the number of entries that will fit on a disk page becomes\nvariable.\nFinally, even i{ the index is built on a fixed-size field, several records may still\nhave the same search key value (e.g., several Students records may have the\nsame gpa or name value). This situation can also lead to variable-size leaf entries\n(if we use Alternative (3) for data entries). Because of all these complications,\nthe concept of order is typically replaced by a simple physical criterion (e.g.,\nmerge if possible when more than half of the space in the node is unused).\n\nCHAPTER 1()\n10.8.4\nThe Effect of Inserts and Deletes on Rids\nIf the leaf pages contain data records-that is, the B+ tree is a clustered index-\nthen operations such as splits, merges, and redistributions can change rids.\nRecall that a typical representation for a rid is some combination of (physical)\npage number and slot number. This scheme allows us to move records within\na page if an appropriate page format is chosen but not across pages, as is the\ncase with operations such as splits. So unless rids are chosen to be independent\nof page numbers, an operation such as split or merge in a clustered B+ tree\nmay require compensating updates to other indexes on the same data.\nA similar comment holds for any dynamic clustered index, regardless of whether\nit is tree-based or hash-based.\nOf course, the problem does not arise with\nnonclustered indexes, because only index entries are moved around.\n10.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhy are tree-structured indexes good for searches, especially range selec-\ntions? (Section 10.1)\n•\nDescribe how search, insert, and delete operations work in ISAM indexes.\nDiscuss the need for overflow pages, and their potential impact on perfor-\nmance. What kinds of update workloads are ISAM indexes most vulnerable\nto, and what kinds of workloads do they handle well? (Section 10.2)\n•\nOnly leaf pages are affected in updates in ISAM indexes.\nDiscuss the\nimplications for locking and concurrent access.\nCompare ISAM and B+\ntrees in this regard. (Section 10.2.1)\n•\nWhat are the main differences between ISAM and B+ tree indexes? (Sec-\ntion 10.3)\n•\nWhat is the order of a B+ tree?\nDescribe the format of nodes in a B+\ntree. Why are nodes at the leaf level linked? (Section 10.3)\n•\nHow rnany nodes must be examined for equality search in a B+ tree? How\nmany for a range selection? Compare this with ISAM. (Section 10.4)\n•\nDescribe the B+ tree insertion algorithm, and explain how it eliminates\noverflow pages. Under what conditions can an insert increase the height of\nthe tree? (Section 10.5)\n•\nDuring deletion, a node might go below the minimum occupancy threshold.\nHow is this handled? Under what conditions could a deletion decrease the\nheight of the tree? (Section 10.6)",
          "pages": [
            397,
            398,
            399
          ],
          "relevance": {
            "score": 0.5,
            "sql_score": 1.0,
            "concept_score": 0.4,
            "non_sql_penalty": 0.0,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "A database index is like a table of contents for your data, allowing you to quickly find specific records without scanning every row.",
        "explanation": "Imagine you have a library full of books. Without an index (like the card catalog), you'd have to read every book cover to find the one you want. An index helps you jump directly to the section where your book is located, saving time and effort. In databases, indexes work similarly by allowing quick access to data based on certain columns.",
        "key_points": [
          "Key point 1: Improves search speed",
          "Key point 2: Created on one or more columns",
          "Key point 3: Can be unique (no duplicate values)",
          "Key point 4: Reduces the need to scan entire tables",
          "Key point 5: Must balance between performance and storage"
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "CREATE INDEX idx_lastname ON employees(last_name);",
            "explanation": "This example creates an index named 'idx_lastname' on the 'last_name' column of the 'employees' table. This allows for faster searches based on last names.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "SELECT * FROM employees WHERE last_name = 'Smith';",
            "explanation": "With an index on 'last_name', this query will be much faster than without, as the database can quickly locate all records where the last name is 'Smith'."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Creating indexes on columns with high cardinality",
            "incorrect_code": "CREATE INDEX idx_full_name ON employees(first_name, last_name);",
            "correct_code": "CREATE INDEX idx_last_name ON employees(last_name);",
            "explanation": "Indexes should be created on columns that have low cardinality (i.e., few distinct values). Creating an index on 'first_name' and 'last_name' together might not be necessary if 'last_name' is already indexed."
          }
        ],
        "practice": {
          "question": "Which of the following statements is true about database indexes?",
          "solution": "An index improves search speed by allowing quick access to data based on certain columns. It should be created on columns with low cardinality, and it must balance between performance and storage."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "CHAPTER jO\n/\n,_ - - Entry to be inserted in parent 11(.)de.\n[i]1\n<-- -\n(Note that 5 is 'copied up' and\n_-....... \"---\\\n,ontin.\", to ,ppcM;n the lenf.)\nEEf-rJ-~r\nSplit Leaf Pages during Insert of Entry",
      "content_relevance": {
        "score": 0.3,
        "sql_score": 1.0,
        "concept_score": 0.4,
        "non_sql_penalty": 0.2,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    },
    "authorization": {
      "id": "authorization",
      "title": "SQL Authorization",
      "definition": "GRANT and REVOKE statements for controlling access to database objects",
      "difficulty": "intermediate",
      "page_references": [
        405,
        406,
        407,
        408,
        409,
        410,
        411,
        412,
        413,
        414,
        415,
        416,
        417,
        418,
        419,
        420
      ],
      "sections": {
        "definition": {
          "text": "HASH-BASED INDEXING\n...\nWhat is the intuition behind hash-structured indexes? Why are they\nespecially good for equality searches but useless for range selections?\n...\nWhat is Extendible Hashing? How does it handle search, insert, and\ndelete?\n...\nWhat is Linear Hashing?\nHow does it handle search, insert, and\ndelete?\n...\nWhat are the similarities and differences between Extendible and Lin-\near Hashing?\nItt\nKey concepts: hash function, bucket, primary and overflow pages,\nstatic versus dynamic hash indexes; Extendible Hashing, directory of\nbuckets, splitting a bucket, global and local depth, directory doubling,\ncollisions and overflow pages; Linear Hashing, rounds ofsplitting, fam-\nily of hash functions, overflow pages, choice of bucket to split and time\nto split; relationship between Extendible Hashing's directory and Lin-\near Hashing's family of hash functiolis, need for overflow pages in both\nschemes in practice, use of a directory for Linear Hashing.\nL.~~_~__\nNot chaos-like, together crushed and bruised,\nBut, as the wo~ld harmoniously confused:\nWhere order in variety we see.\n___ J\nAlexander Pope, Windsor Forest\nIn this chapter we consider file organizations that are excellent for equality\nselections.\nThe basic idea is to use a hashing function, which maps values\n\nHash-Based Indexing\nin a search field into a range of b'ucket numbers to find the page on which a\ndesired data entry belongs. \\Ve use a simple scheme called Static Hashing to\nintroduce the idea. This scheme, like ISAM, suffers from the problem of long\noverflow chains, which can affect performance. Two solutions to the problem\nare presented.\nThe Extendible Hashing scheme uses a directory to support\ninserts and deletes efficiently with no overflow pages.\nThe Linear Hashing\nscheme uses a clever policy for creating new buckets and supports inserts and\ndeletes efficiently without the use of a directory. Although overflow pages are\nused, the length of overflow chains is rarely more than two.\nHash-based indexing techniques cannot support range searches, unfortunately.\nn'ee-based indexing techniques, discussed in Chapter 10, can support range\nsearches efficiently and are almost as good as ha...,h-based indexing for equality\nselections. Thus, many commercial systems choose to support only tree-based\nindexes.\nNonetheless, hashing techniques prove to be very useful in imple-\nmenting relational operations such as joins, as we will see in Chapter 14. In\nparticular, the Index Nested Loops join method generates many equality se-\nlection queries, and the difference in cost between a hash-based index and a\ntree-based index can become significant in this context.\nThe rest of this chapter is organized as follows.\nHashing. Like ISAM, its drawback is that performance degrades as the data\ngrows and shrinks. We discuss a dynamic hashing technique, called Extendible\nHashing, in Section 11.2 and another dynamic technique, called Linear Hashing,\nin Section 11.3. vVe compare Extendible and Linear Hashing in Section 11.4.\n11.1\nSTATIC HASHING\nThe Static Hashing scheme is illustrated in Figure 11.1. The pages containing\nthe data can be viewed as a collection of buckets, with one primary page\nand possibly additional overflow pages per bucket.\nA file consists of buckets\na through N - 1, with one primary page per bucket initially. Buckets contain\ndata entTies, which can be any of the three alternatives discussed in Chapter\n8.\nTo search for a data entry, we apply a hash function h to identify the bucket\nto which it belongs and then search this bucket.\nTo speed the search of a\nbucket, we can maintain data entries in sorted order by search key value; in\nthis chapter, we do not sort entries, and the order of entries within a bucket\nhas no significance. To insert a data entry, we use the hash function to identify\nthe correct bucket and then put the data entry there. If there is no space for\nthis data entry, we allocate a new overflow page, put the data entry on this\npage, and add the page to the overflow chain of the bucket. To delete a data\n\nh(key) mod N /\n~-~-§=l~\n// __\n~L~--.-J~· . -\n~G\\---I\n...\n,INd-\nPrimary bucket pages\nOverflow pages\nStatic Hashing\n$\nentry, we use the hashing function to identify the correct bucket, locate the\ndata entry by searching the bucket, and then remove it. If this data entry is\nthe last in an overflow page, the overflow page is removed from the overflow\nchain of the bucket and added to a list of free pages.\nThe hash function is an important component of the hashing approach. It must\ndistribute values in the domain of the search field uniformly over the collection\nof buckets. If we have N buckets, numbered a through N ~ 1, a hash function\nh of the form h(value)\n=\n(a * value + b) works well in practice. (The bucket\nidentified is h(value) mod N.) The constants a and b can be chosen to 'tune'\nthe hash function.\nSince the number of buckets in a Static Hashing file is known when the file\nis created, the primary pages can be stored on successive disk pages. Hence,\na search ideally requires just one disk I/O, and insert and delete operations\nrequire two I/Os (read and write the page), although the cost could be higher\nin the presence of overflow pages. As the file grows, long overflow chains can\ndevelop. Since searching a bucket requires us to search (in general) all pages\nin its overflow chain, it is easy to see how performance can deteriorate.\nBy\ninitially keeping pages 80 percent full, we can avoid overflow pages if the file\ndoes not grow too IIluch, but in general the only way to get rid of overflow\nchains is to create a new file with more buckets.\nThe main problem with Static Hashing is that the number of buckets is fixed.\nIf a file shrinks greatly, a lot of space is wasted; more important, if a file grows\na lot, long overflow chains develop, resulting in poor performance. Therefore,\nStatic Hashing can be compared to the ISAM structure (Section 10.2), which\ncan also develop long overflow chains in case of insertions to the same leaf.\nStatic Hashing also has the same advantages as ISAM with respect to concur-\nrent access (see Section 10.2.1).\n\nHash-Based Inrle:ring\nOne simple alternative to Static Hashing is to periodically 'rehash' the file to\nrestore the ideal situation (no overflow chains, about 80 percent occupancy).\nHowever, rehashing takes time and the index cannot be used while rehashing\nis in progress.\nAnother alternative is to use dynamic hashing techniques\nsuch as Extendible and Linear Hashing, which deal with inserts and deletes\ngracefully. vVe consider these techniques in the rest of this chapter.\n11.1.1\nNotation and Conventions\nIn the rest of this chapter, we use the following conventions. As in the previous\nchapter, record with search key k, we denote the index data entry by k*. For\nhash-based indexes, the first step in searching for, inserting, or deleting a data\nentry with search key k is to apply a hash function h to k; we denote this\noperation by h(k), and the value h(k) identifies the bucket for the data entry\nh. Note that two different search keys can have the same hash value.\n11.2\nEXTENDIBLE HASHING\nTo understand Extendible Hashing, let us begin by considering a Static Hashing\nfile. If we have to insert a new data entry into a full bucket, we need to add\nan overflow page. If we do not want to add overflow pages, one solution is\nto reorganize the file at this point by doubling the number of buckets and\nredistributing the entries across the new set of buckets. This solution suffers\nfrom one major defect--the entire file has to be read, and twice (h') many pages\nhave to be written to achieve the reorganization. This problem, however, can\nbe overcome by a simple idea: Use a directory of pointers to bucket.s, and\ndouble t.he size of the number of buckets by doubling just the directory and\nsplitting only the bucket that overflowed.\nTo understand the idea, consider the sample file shown in Figure 11.2. The\ndirectory consists of an array of size 4, with each element being a point.er to\na bucket.. (The global depth and local depth fields are discussed shortly, ignore\nthem for now.) To locat.e a data entry, we apply a hash funct.ion to the search\nfield and take the last. 2 bit.s of its binary represent.ation t.o get. a number\nbetween 0 and\n~~. The pointer in this array position gives us t.he desired bucket.;\nwe assume that each bucket can hold four data ent.ries. Therefore, t.o locate a\ndata entry with hash value 5 (binary 101), we look at directory element 01 and\nfollow the pointer to the data page (bucket B in the figure).\nTo insert. a dat.a entry, we search to find the appropriate bucket.. For example,\nto insert a data entry with hash value 13 (denoted as 13*), we examine directory\nelement 01 and go to the page containing data ent.ries 1*, 5*, and 21*. Since\n\nBucketC\nBucket B\nBucket A\n1f L~--L----i----4.,J.--'\n~ Data entry r\nwith h(r)=32\nLOCAL DEPTH~\nGLOBAL DEPTH\nDIRECTORY\nBucket D\nDATA PAGES\nExample of an Extendible Ha.~hed File\nthe page has space for an additional data entry, we are done after we insert the\nentry (Figure 11.3).\nLOCAL DEPTH~\nGLOBAL DEPTH\nDIRECTORY\nBucket A\nBucket B\nBucket C\nBucketD\nDATA PAGES\nAfter Inserting Entry T with h(T) = 1:3\nNext, let us consider insertion of a data entry into a full bucket. The essence\nof the Extcndible Hashing idea lies in how we deal with this case. Consider the\ninsertion of data entry 20* (binary 10100). Looking at directory clement 00,\nwe arc led to bucket A, which is already full. We 111Ust first split the bucket",
          "pages": [
            405,
            406,
            407,
            408,
            409
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "examples": {
          "text": "Hash-Based Indexing\nby allocating a new bucketl and redistributing the contents (including the new\nentry to be inserted) across the old bucket and its 'split image.' To redistribute\nentries across the old bucket and its split image, we consider the last three bits\nof h(T); the last two bits are 00, indicating a data entry that belongs to one of\nthese two buckets, and the third bit discriminates between these buckets. The\nredistribution of entries is illustrated in Figure 11.4.\nLOCAL DEPTH~:>\nGLOBAL DEPTH\nDIRECTORY\nBucket D\nBucket A2 (split image of bucket A)\nWhile Inserting Entry r with h(r}=20\nNote a problem that we must now resolve\"\"\" \"\"we need three bits to discriminate\nbetween two of our data pages (A and A2), but the directory has only enough\nslots to store all two-bit patterns. The solution is to double the directory. El-\nements that differ only in the third bit from the end are said to 'correspond':\nCOT-r'esponding elements of the directory point to the same bucket with the\nexception of the elements corresponding to the split bucket. In our example,\nbucket awas split; so, new directory element 000 points to one of the split ver-\nsions and new element 100 points to the other. The sample file after completing\nall steps in the insertion of 20* is shown in Figure 11.5.\nTherefore, doubling the file requires allocating a new bucket page, writing both\nthis page and the old bucket page that is being split, and doubling the directory\narray.\nThe directory is likely to be much smaller than the file itself because\neach element is just a page-id, and can be doubled by simply copying it over\nlSince there are 'no overflow pages in Extendible Hashing, a bucket can be thought of a.~ a single\npage.\n\nLOCAL DEPTH~\nGLOBAL DEPTH\nBucket A\nBucket B\nBucket C\nBucket 0\nDIRECTORY\nBucket A2\n(split image of bucket A)\nAfter Inserting Entry r with h(r) = 20\n(and adjusting the elements for the split buckets). The cost of doubling is now\nquite acceptable.\nWe observe that the basic technique used in Extendible Hashing is to treat the\nresult of applying a hash function h a\" a binary number and interpret the last d\nbits, where d depends on the size of the directory, as an offset into the directory.\nIn our example, d is originally 2 because we only have four buckets; after the\nsplit, d becomes 3 because we now have eight buckets.\nA corollary is that,\nwhen distributing entries across a bucket and its split image, we should do so\non the basis of the dth bit. (Note how entries are redistributed in our example;\nsee Figure 11.5.) The number d, called the global depth of the hashed file, is\nkept as part of the header of the file. It is used every time we need to locate a\ndata entry.\nAn important point that arises is whether splitting a bucket necessitates a\ndirectory doubling. Consider our example, as shown in Figure 11.5. If we now\ninsert 9*, it belongs in bucket B; this bucket is already full. \\Ve can deal with\nthis situation by splitting the bucket and using directory elements 001 and 10]\nto point to the bucket and its split image, as shown in Figure 11.6.\nHence, a bucket split does not necessarily require a directory doubling. How-\never, if either bucket A or A2 grows full and an insert then forces a bucket split,\nwe are forced to double the directory again.\n\nHash-Based Inde:ring\nLOCAL DEPTH---L..-->\nGLOBAL DEPTH\nDIRECTORY\nBucket A\nBucket B\nBucket C\nBucket 0\nBucket A2 (split image of bucket A)\nBucket B2 (split image of bucket B)\nAfter Inserting Entry l' with h(r) = 9\nTo differentiate between these cases and determine whether a directory doubling\nis needed, we maintain a local depth for each bucket. If a bucket whose local\ndepth is equal to the global depth is split, the directory must be doubled. Going\nback to the example, when we inserted 9* into the index shown in Figure 11.5,\nit belonged to bucket B with local depth 2, whereas the global depth was 3.\nEven though the bucket was split, the directory did not have to be doubled.\nBuckets A and A2, on the other hand, have local depth equal to the global\ndepth, and, if they grow full and are split, the directory must then be doubled.\nInitially, all local depths are equal to the global depth (which is the number of\nbits needed to express the total number of buckets). vVe increment the global\ndepth by 1 each time the directory doubles, of course. Also, whenever a bucket\nis split (whether or not the split leads to a directory doubling), we increment\nby 1 the local depth of the split bucket and assign this same (incremented)\nlocal depth to its (newly created) split image. Intuitively, if a bucket has local\ndepth l, the hash values of data entries in it agree on the la.st l bits; further, no\ndata entry in any other bucket of the file has a hash value with the same last I\nbits. A total of 2dl directory elernents point to a bucket with local depth I; if\nd = l, exactly one directory element points to the bucket and splitting such a\nbucket requires directory doubling.\n\nCHAPTER hI\nA final point to note is that we can also use the first d bits (the most significant\nbits) instead of the last d (least s'ignificant bits), but in practice the last d bits\nare used. The reason is that a directory can then be doubled simply by copying\nit.\nIn summary, a data entry can be located by computing its hash value, taking\nthe last d bits, and looking in the bucket pointed to by this directory element.\nFor inserts, the data entry is placed in the bucket to which it belongs and the\nbucket is split if necessary to make space. A bucket split leads to an increase in\nthe local depth and, if the local depth becomes greater than the global depth\nas a result, to a directory doubling (and an increase in the global depth) as\nwell.\nFor deletes, the data entry is located and removed. If the delete leaves the\nbucket empty, it can be merged with its split image, although this step is\noften omitted in practice. Merging buckets decreases the local depth. If each\ndirectory element points to the same bucket as its split image (i.e., 0 and 2d- 1\npoint to the same bucket, namely, A; 1 and 2d- 1 + 1 point to the same bucket,\nnamely, B, which mayor may not be identical to A; etc.), we can halve the\ndirectory and reduce the global depth, although this step is not necessary for\ncorrectness.\nThe insertion examples can be worked out backwards as examples of deletion.\n(Start with the structure shown after an insertion and delete the inserted ele-\nment. In each case the original structure should be the result.)\nIf the directory fits in memory, an equality selection can be answered in a\nsingle disk access, as for Static Hashing (in the absence of overflow pages), but\notherwise, two disk I/Os are needed. As a typical example, a 100MB file with\n100 bytes per data entry and a page size of 4KB contains 1 million data entries\nand only about 25,000 elements in the directory. (Each page/bucket contains\nroughly 40 data entries, and we have one directory element per bucket.) Thus,\nalthough equality selections can be twice as slow as for Static Hashing files,\nchances are high that the directory will fit in memory and performance is the\nsame as for Static Ha.<;hing files.\nOn the other hand, the directory grows in spurts and can become large for\nskewed data distTibutions (where our assumption that data pages contain roughly\nequal numbers of data entries is not valid). In the context of hashed files, in a\nskewed data distribution the distribution of hash values of seaTch field values\n(rather than the distribution of search field values themselves) is skewed (very\n'bursty' or nonuniform). Even if the distribution of search values is skewed, the\nchoice of a good hashing function typically yields a fairly uniform distribution\nof lw\"sh va.lues; skew is therefore not a problem in practice.\n\nHash~Based Inde:cing\nF\\lrther, collisions, or data entries with the same hash value, cause a problem\nand must be handled specially: \\Vhen more data entries th311 \\vill fit on a page\nhave the same hash value, we need overflow pages.\n11.3\nLINEAR HASHING\nLinear Hashing is a dynamic hashing technique, like Extendible Hashing, ad-\njusting gracefully to inserts and deletes.\nIn contrast to Extendible Hashing,\nit does not require a directory, deals naturally with collisions, and offers a lot\nof flexibility with respect to the timing of bucket splits (allowing us to trade\noff slightly greater overflow chains for higher average space utilization). If the\ndata distribution is very skewed, however, overflow chains could cause Linear\nHashing performance to be worse than that of Extendible Hashing.\nThe scheme utilizes a family of hash functions ha, hI, h2, ... , with the property\nthat each function's range is twice that of its predecessor. That is, if hi maps\na data entry into one of M buckets, h i+I maps a data entry into one of 2lv!\nbuckets. Such a family is typically obtained by choosing a hash function hand\nan initial number N ofbuckets,2 and defining hi(value) \"'= h(value) mod (2i N).\nIf N is chosen to be a power of 2, then we apply h and look at the last di bits;\ndo is the number of bits needed to represent N, and di = da+ i. Typically we\nchoose h to be a function that maps a data entry to some integer.\nSuppose\nthat we set the initial number N of buckets to be 32. In this case do is 5, and\nha is therefore h mod 32, that is, a number in the range 0 to 31. The value of\nd l is do + 1 = 6, and hI is h mod (2 * 32), that is, a number in the range 0 to\n63. Then h2 yields a number in the range 0 to 127, and so OIl.\nThe idea is best understood in terms of rounds of splitting.\nDuring round\nnumber Level, only hash functions hLeud and hLevel+1 are in use. The buckets\nin the file at the beginning of the round are split, one by one from the first to\nthe last bucket, thereby doubling the number of buckets. At any given point\nwithin a round, therefore, we have buckets that have been split, buckets that\nare yet to be split, and buckets created by splits in this round, as illustrated in\nConsider how we search for a data entry with a given search key value.\n\\Ve\napply ha..:sh function h Level , and if this leads us to one of the unsplit buckets,\nwe simply look there. If it leads us to one of the split buckets, the entry may\nbe there or it may have been moved to the new bucket created earlier in this\nround by splitting this bucket; to determine which of the two buckets contains\nthe entry, we apply hLevel+I'\n2Note that 0 to IV - 1 is not the range of fl.!\n\nBucket to be split\nNext\nBuckets that existed at the\nbeginning of this round:\nthis is the range of\nh Level\nr\nI\nI\n-<-\nCHAPTER \\1\nBuckets split in this round:\nIf h Le~'el ( search key mil,e\nis in this range, must use\nh Level+1 (search key vallie\nto decide if entry is in\nsplit image bucket.\n'split image' buckets:\ncreated (tlrrough splitting\nof other buckets) in this round\nBuckets during a Round in Linear Hashing\nUnlike Extendible Hashing, when an insert triggers a split, the bucket into\nwhich the data entry is inserted is not necessarily the bucket that is split. An\noverflow page is added to store the newly inserted data entry (which triggered\nthe split), as in Static Hashing. However, since the bucket to split is chosen\nin round-robin fashion, eventually all buckets are split, thereby redistributing\nthe data entries in overflow chains before the chains get to be more than one\nor two pages long.\nWe now describe Linear Hashing in more detail.\nA counter Level is used to\nindicate the current round number and is initialized to O. The bucket to split\nis denoted by Next and is initially bucket °(the first bucket). We denote the\nnumber of buckets in the file at the beginning of round Level by N Level. We\ncan easily verify that N Level = N * 2Level. Let the number of buckets at the\nbeginning of round 0, denoted by No, be N.\nWe show a small linear hashed\nfile in Figure 11.8. Each bucket can hold four data entries, and the file initially\ncontains four buckets, as shown in the figure.\nWe have considerable flexibility in how to trigger a split, thanks to the use of\noverflow pages. We can split whenever a new overflow page is added, or we can\nimpose additional conditions based all conditions such as space utilization. For\nour examples, a split is 'triggered' when inserting a new data entry causes the\ncreation of an Qverftow page.\n\\Vhenever a split is triggered the Next bucket is split, and hash function hLevel+l\nredistributes entries between this bucket (say bucket number b) and its split\nimage; the split image is therefore bucket number b+ NLeve/. After splitting a\nbucket, the value of Next is incremented by 1. In the example file, insertion of\n\nHash-Based Indexing\n38J\nLevel=O. N=4\nPRIMARY\nPAGES\nNext=O\n\"'1 32'1 44'1 36'1 1\nI 9\" 1 2S\"i S'f~~\n_\nData entry r\nwith h{r)mS\n~\n4:\n18\"\n10\"\n30\"\n.\n,\nPrJ.mary\n_\nbucket page\n~\nThis information is\nThe actual contelJts\nfor illustratiolJ only\nofthe linear hashedjile\nExample of a Linear Hashed File\ndata entry 43* triggers a split. The file after completing the insertion is shown\nin Figure 11.9.\nLevel=O\nPRIMARY\nOVERFLOW\nh 1\nh O\nPAGES\nPAGES\n~\nNext=1\n\"'~\n~\nAfter Inserting Record T with h(T) = 43\nAt any time in .the middle of a round Level, all buckets above bucket Ne:rt have\nbeen split, and the file contains buckets that are their split images, as illustrated\nin Figure 11.7. Buckets Next through NLevcl have not yet been split. If we use\nhLevel on a data entry and obtain a number b in the range Next through NLevel,\nthe data entry belongs to bucket b. For example, ho(18) is 2 (binary 10); since\nthis value is between the current values of Ne:r:t (= 1) and N 1 (=,-:': 4), this bucket\nhas not been split. However, if we obtain a number b in the range 0 through",
          "pages": [
            410,
            411,
            412,
            413,
            414,
            415,
            416
          ],
          "relevance": {
            "score": 0.7,
            "sql_score": 1.0,
            "concept_score": 1.0,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        },
        "commonMistakes": {
          "text": "Next, the data entry may be in this bucket or in its split image (which is bucket\nnumber b+NLevet}; we have to use hLevel+1 to determine to which of these two\nbuckets the data entry belongs. In other words, we have to look at one more\nbit of the data entry's hash value. For example, ho(32) and ho(44) are both a\n(binary 00). Since Next is currently equal to 1, which indicates a bucket that\nhas been split, we have to apply hI'\nWe have hI (32) = 0 (binary 000) and\nh1(44) = 4 (binary 100). Therefore, 32 belongs in bucket A and 44 belongs in\nits split image, bucket A2.\nNot all insertions trigger a split, of course. If we insert 37* into the file shown\nin Figure 11.9, the appropriate bucket has space for the new data entry. The\nfile after the insertion is shown in Figure 11.10.\nLevel=O\nPRIMARY\nOVERFLOW\nh1\nho\nPAGES\nPAGES\n~\nNext=1\n~~\n~\nEEITl\nAfter Inserting Record r with h(r) = 37\nSometimes the bucket pointed to by Next (the current candidate for splitting)\nis full, and a new data entry should be inserted in this bucket. In this case, a\nsplit is triggered, of course, but we do not need a new overflow bucket. This\nsituation is illustrated by inserting 29* into the file shown in Figure 11.10. The\nresult is shown in Figure 11.11.\nWhen Next is equal to NLevel - 1 and a split is triggered, we split the last of\nthe buckets present in the file at the beginning of round Level. The number\nof buckets after the split is twice the number at the beginning of the round,\nand we start a new round with Level incremented by 1 and Next reset to O.\nIncrementing Level amounts to doubling the effective range into which keys are\nhashed. Consider the example file in Figure 11.12, which was obtained from the\nfile of Figure 11.11 by inserting 22*, 66*, and 34*. (The reader is encouraged to\ntry to work out the details of these insertions.) Inserting 50* causes a split that\n\nHash-Based l'ndex'ing\nLevel=O\nPRIMARY\nOVERFLOW\nh 1\nho\nPAGES\nPAGES\n132'1\n025'\\\nNext\",2\n\"'114'\\18'110'13°1\n~EITQ\nE8Il\n~\nAfter Inserting Record r with h(r\") = 29\nleads to incrementing Level, as discussed previously; the file after this insertion\nis shown in Figure 11.13.\nLevel=O\nPRIMARY\nh 1\nh O\nPAGES\n~C[l\n~[J=l\n~\nNext=3\n~17'll1'1\n~J.:~lI{\nEL~1i?I=]\n~\n~\n~\nOVERFLOW\nPAGES\nAfter Inserting Records with h(r) = 22,66,and34\nIn summary, an equality selection costs just one disk I/O unless the bucket has\noverflow pages; in practice, the cost on average is about 1.2 disk accesses for\n\nLevel:1\nPRIMARY\nOVERFLOW\nh 1\nh o\nPAGES\nPAGES\nNext=O\nLd~I~]~\n~~8]2or;~!\n. ~l~j\n~5J11I-1\n'---=--L_~_\ni\n[44'!3sTT-l\n1..~13~J2::L~l\nr-i4I30:@I1\nAfter Inserting Record r with h(r) = 50\nreasonably uniform data distributions. (The cost can be considerably worse--\nlinear in the number of data entries in the file----if the distribution is very skewed.\nThe space utilization is also very poor with skewed data distributions.) Inserts\nrequire reading and writing a single page, unless a split is triggered.\n'We not discuss deletion in detail, but it is essentially the inverse of insertion.\nIf the last bucket in the file is empty, it can be removed and Next can be\ndecremented. (If Next is 0 and the last bucket becomes empty, Next is made to\npoint to bucket (AI/2) ~ 1, where !vI is the current number of buckets, Level is\ndecremented, and the empty bucket is removed.) If we wish, we can combine the\nlast bucket with its split image even when it is not empty, using some criterion\nto trigger this merging in essentially the same way. The criterion is typically\nbased on the occupancy of the file, and merging can be done to improve space\nutilization.\n11.4\nEXTENDIBLE VS. LINEAR HASHING\nTo understand the relationship between Linear Hashing and Extendible Hash-\ning, imagine that we also have a directory in Linear Hashing with elements 0\nto N - 1. The first split is at bucket 0, and so we add directory element N. In\nprinciple, we may imagine that the entire directory has been doubled at this\npoint; however, because element 1 is the same as element N + 1, elernent 2 is",
          "pages": [
            417,
            418,
            419
          ],
          "relevance": {
            "score": 0.62,
            "sql_score": 0.9,
            "concept_score": 0.9,
            "non_sql_penalty": 0.1,
            "is_relevant": true,
            "analysis": "SQL content"
          }
        }
      },
      "educational_notes": {
        "definition": "Hash-based indexing is a technique used to speed up data retrieval operations on database tables by using a hash function to map keys to specific buckets where the actual data entries are stored.",
        "explanation": "Hash-based indexing works by creating a mapping between search keys and bucket numbers. When you want to find an entry, you apply a hash function to the key to determine which bucket it should be in. This allows for very fast lookups because you don't have to scan through all entries in the table. However, this method is not suitable for range queries, where you need to find all entries within a certain range of values.",
        "key_points": [
          "Key point 1: Hash-based indexing speeds up equality searches by mapping keys directly to buckets.",
          "Key point 2: It's less efficient for range searches because it doesn't support querying across multiple buckets.",
          "Key point 3: Common pitfall is not understanding the limitations of hash-based indexing for certain types of queries.",
          "Key point 4: Best practice is to use tree-based indexes or hybrid approaches for tables that require both equality and range searches.",
          "Key point 5: Connects to other concepts like SQL joins, where hash-based indices can be very useful for generating many equality selection queries."
        ],
        "examples": [
          {
            "title": "Basic Usage",
            "code": "-- Example of a simple SELECT statement using a hash INDEX SELECT * FROM employees WHERE employee_id = 123;",
            "explanation": "This example demonstrates how to use a hash-based index for an equality search. The hash function maps the employee_id value to a specific bucket, allowing for quick retrieval.",
            "validation_note": "SQL auto-fixed: "
          },
          {
            "title": "Practical Example",
            "code": "-- Practical scenario where hash indexing is beneficial\nSELECT * FROM orders WHERE customer_id BETWEEN 100 AND 200;",
            "explanation": "This example shows how hash-based indexing might not be the best choice for a range search. It would require scanning multiple buckets, which could be inefficient."
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Using hash-based indexing for range searches",
            "incorrect_code": "-- Incorrect way to query using hash index\nSELECT * FROM table WHERE column BETWEEN value1 AND value2;",
            "correct_code": "-- Correct way to handle range queries\nSELECT * FROM table WHERE column >= value1 AND column <= value2;",
            "explanation": "Mistake happens because hash-based indexing is not designed for range searches. The correct approach depends on the specific database system and its capabilities."
          }
        ],
        "practice": {
          "question": "Create a SQL query that demonstrates using a hash index for an equality search.",
          "solution": "SELECT * FROM products WHERE product_id = 456;\nExplanation: This query uses a hash-based index to quickly find the product with product_id 456."
        }
      },
      "llm_enhanced": true,
      "raw_text_preview": "HASH-BASED INDEXING\n...\nWhat is the intuition behind hash-structured indexes? Why are they\nespecially good for equality searches but useless for range selections?\n...\nWhat is Extendible Hashing? How d",
      "content_relevance": {
        "score": 0.7,
        "sql_score": 1.0,
        "concept_score": 1.0,
        "non_sql_penalty": 0.1,
        "is_relevant": true,
        "analysis": "SQL content"
      }
    }
  }
}