{
  "concepts": {
    "content": {
      "id": "content",
      "title": "Content",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        2,
        3,
        4,
        6,
        7,
        8,
        9,
        10,
        11,
        12,
        13,
        14,
        15,
        16,
        17,
        18,
        19,
        20,
        21,
        22,
        23,
        24,
        25,
        26,
        27,
        28,
        29,
        30,
        31,
        32,
        33,
        34,
        35,
        36,
        38,
        39,
        40,
        41,
        42,
        43,
        44,
        45,
        46,
        47,
        48,
        49,
        50,
        51,
        52,
        53,
        54,
        55,
        56,
        57,
        58,
        59,
        60,
        61,
        62,
        63,
        64,
        65,
        66,
        67,
        68,
        69,
        70,
        71,
        72,
        73,
        74,
        75,
        76,
        77,
        78,
        79,
        80,
        81,
        82,
        83,
        84,
        85,
        86,
        87,
        88,
        89,
        90,
        91,
        92,
        93,
        94,
        95,
        96,
        97,
        98,
        99,
        100,
        101,
        102,
        103,
        104,
        105,
        106,
        107,
        108,
        109,
        110,
        111,
        112,
        113,
        114,
        115,
        116,
        117,
        118,
        119,
        120,
        121,
        122,
        123,
        124,
        125,
        126,
        127,
        128,
        129,
        130,
        131,
        132,
        133,
        134,
        135,
        136,
        137,
        138,
        139,
        140,
        141,
        142,
        143,
        144,
        145,
        146,
        147,
        148,
        149,
        150,
        151,
        152,
        153,
        154,
        155,
        156,
        157,
        158,
        159,
        160,
        161,
        162,
        163,
        164,
        165,
        166,
        167,
        168,
        169,
        170,
        171,
        172,
        173,
        174,
        175,
        176,
        177,
        178,
        179,
        180,
        181,
        182,
        183,
        184,
        185,
        186,
        187,
        188,
        189,
        190,
        191,
        192,
        193,
        194,
        195,
        196,
        197,
        198,
        199,
        200,
        201,
        202,
        203,
        204,
        205,
        206,
        207,
        208,
        209,
        210,
        211,
        212,
        213,
        214,
        215,
        216,
        217,
        218,
        220,
        221,
        222,
        223,
        224,
        225,
        226,
        227,
        228,
        229,
        230,
        231,
        232,
        233,
        234,
        235,
        236,
        237,
        238,
        239,
        240,
        241,
        242,
        243,
        244,
        245,
        246,
        247,
        248,
        249,
        250,
        251,
        252,
        253,
        254,
        255,
        256,
        257,
        258,
        259,
        260,
        261,
        262,
        263,
        264,
        265,
        266,
        267,
        268,
        269,
        270,
        271,
        272,
        273,
        274,
        275,
        276,
        277,
        278,
        279,
        280,
        281,
        282,
        283,
        284,
        285,
        286,
        287,
        289,
        290,
        291,
        292,
        293,
        294,
        295,
        296,
        297,
        298,
        299,
        300,
        301,
        302,
        303,
        304,
        305,
        306,
        307,
        308,
        309,
        310,
        311,
        312,
        313,
        314,
        315,
        316,
        317,
        318,
        319,
        320,
        321,
        322,
        323,
        324,
        325,
        326,
        327,
        328,
        329,
        330,
        331,
        332,
        333,
        334,
        335,
        336,
        337,
        338,
        339,
        340,
        341,
        342,
        343,
        344,
        345,
        346,
        347,
        348,
        349,
        350,
        351,
        352,
        353,
        354,
        355,
        356,
        357,
        358,
        359,
        360,
        361,
        362,
        363,
        364,
        365,
        366,
        367,
        368,
        369,
        370,
        371,
        372,
        373,
        374,
        375,
        376,
        377,
        378,
        379,
        380,
        381,
        382,
        383,
        384,
        385,
        386,
        387,
        388,
        389,
        390,
        391,
        392,
        393,
        394,
        395,
        396,
        397,
        398,
        399,
        400,
        401,
        402,
        403,
        404,
        405,
        406,
        407,
        408,
        409,
        410,
        411,
        412,
        413,
        414,
        415,
        416,
        417,
        418,
        419,
        420,
        421,
        422,
        423,
        424,
        425,
        426,
        428,
        429,
        430,
        431,
        432,
        433,
        434,
        435,
        436,
        437,
        438,
        439,
        440,
        441,
        442,
        443,
        444,
        445,
        446,
        447,
        448,
        449,
        450,
        451,
        452,
        453,
        454,
        455,
        456,
        457,
        458,
        459,
        460,
        461,
        462,
        463,
        464,
        465,
        466,
        467,
        468,
        469,
        470,
        471,
        472,
        473,
        474,
        475,
        476,
        477,
        478,
        479,
        480,
        481,
        482,
        483,
        484,
        485,
        486,
        487,
        488,
        489,
        490,
        491,
        492,
        493,
        494,
        495,
        496,
        497,
        498,
        499,
        500,
        501,
        502,
        503,
        504,
        505,
        506,
        507,
        508,
        509,
        510,
        511,
        512,
        513,
        514,
        515,
        516,
        517,
        518,
        519,
        520,
        521,
        522,
        523,
        524,
        525,
        526,
        527,
        528,
        529,
        530,
        531,
        532,
        533,
        534,
        535,
        536,
        537,
        538,
        539,
        540,
        541,
        542,
        543,
        544,
        545,
        546,
        547,
        548,
        549,
        550,
        551,
        552,
        553,
        554,
        555,
        556,
        557,
        558,
        559,
        560,
        561,
        562,
        563,
        564,
        565,
        566,
        567,
        568,
        570,
        571,
        572,
        573,
        574,
        575,
        576,
        577,
        578,
        579,
        580,
        581,
        582,
        583,
        584,
        585,
        586,
        587,
        588,
        589,
        590,
        591,
        592,
        593,
        594,
        595,
        596,
        597,
        598,
        599,
        600,
        601,
        602,
        603,
        604,
        605,
        606,
        607,
        608,
        609,
        610,
        611,
        612,
        613,
        614,
        615,
        616,
        617,
        618,
        619,
        620,
        621,
        622,
        623,
        624,
        625,
        626,
        627,
        628,
        629,
        630,
        631,
        632,
        633,
        634,
        635,
        636,
        637,
        638,
        640,
        641,
        642,
        643,
        644,
        645,
        646,
        647,
        648,
        649,
        650,
        651,
        652,
        653,
        654,
        655,
        656,
        657,
        658,
        659,
        660,
        661,
        662,
        663,
        664,
        665,
        666,
        667,
        668,
        669,
        670,
        671,
        672,
        673,
        674,
        675,
        676,
        677,
        678,
        679,
        680,
        681,
        682,
        683,
        684,
        685,
        686,
        687,
        688,
        689,
        690,
        691,
        692,
        693,
        694,
        695,
        696,
        697,
        698,
        699,
        700,
        701,
        702,
        703,
        704,
        705,
        706,
        707,
        708,
        709,
        710,
        711,
        712,
        713,
        714,
        715,
        716,
        717,
        718,
        719,
        720,
        721,
        722,
        723,
        724,
        725,
        726,
        727,
        728,
        729,
        730,
        731,
        732,
        733,
        734,
        735,
        736,
        737,
        738,
        739,
        740,
        741,
        742,
        743,
        744,
        745,
        746,
        747,
        748,
        749,
        750,
        751,
        752,
        753,
        754,
        755,
        756,
        758,
        759,
        760,
        761,
        763,
        764,
        765,
        766,
        767,
        768,
        769,
        770,
        771,
        772,
        773,
        774,
        775,
        776,
        777,
        778,
        779,
        780,
        781,
        782,
        783,
        784,
        785,
        786,
        787,
        788,
        789,
        790,
        791,
        792,
        793,
        794,
        795,
        796,
        797,
        798,
        799,
        800,
        801,
        802,
        803,
        804,
        805,
        806,
        807,
        808,
        809,
        810,
        811,
        812,
        813,
        814,
        815,
        816,
        817,
        818,
        819,
        820,
        821,
        822,
        823,
        824,
        825,
        826,
        827,
        828,
        829,
        830,
        831,
        833,
        834,
        835,
        836,
        837,
        838,
        839,
        840,
        841,
        842,
        843,
        844,
        845,
        846,
        847,
        848,
        849,
        850,
        851,
        852,
        853,
        854,
        855,
        856,
        857,
        858,
        859,
        860,
        861,
        862,
        863,
        864,
        865,
        866,
        867,
        868,
        869,
        870,
        871,
        872,
        873,
        874,
        875,
        876,
        877,
        878,
        879,
        880,
        881,
        882,
        883,
        884,
        885,
        886,
        887,
        888,
        889,
        890,
        891,
        892,
        893,
        894,
        895,
        896,
        897,
        898,
        899,
        900,
        901,
        902,
        903,
        904,
        905,
        906,
        907,
        908,
        909,
        910,
        911,
        912,
        913,
        914,
        915,
        916,
        917,
        918,
        919,
        920,
        921,
        922,
        923,
        924,
        925,
        926,
        927,
        928,
        929,
        930,
        931,
        932,
        933,
        934,
        935,
        936,
        937,
        938,
        939,
        940,
        941,
        942,
        943,
        944,
        945,
        946,
        947,
        948,
        949,
        950,
        951,
        952,
        953,
        954,
        955,
        956,
        957,
        958,
        959,
        960,
        961,
        962,
        963,
        964,
        965,
        966,
        967,
        968,
        969,
        970,
        971,
        972,
        973,
        974,
        975,
        976,
        977,
        978,
        979,
        980,
        981,
        982,
        983,
        984,
        985,
        986,
        987,
        988,
        989,
        990,
        991,
        992,
        993,
        994,
        995,
        996,
        997,
        998,
        999,
        1000,
        1001,
        1002,
        1003,
        1004,
        1005,
        1006,
        1007,
        1008,
        1009,
        1010,
        1011,
        1012,
        1013,
        1014,
        1015,
        1016,
        1017,
        1018,
        1019,
        1020,
        1021,
        1022,
        1023,
        1024,
        1025,
        1026,
        1027,
        1028,
        1029,
        1030,
        1031,
        1032,
        1033,
        1034,
        1035,
        1036,
        1037,
        1038,
        1039,
        1040,
        1041,
        1042,
        1043,
        1044,
        1045,
        1046,
        1047,
        1048,
        1049,
        1050,
        1051,
        1052,
        1053,
        1054,
        1055,
        1056,
        1057,
        1058,
        1059,
        1060,
        1061,
        1062,
        1063,
        1064,
        1065,
        1066,
        1067,
        1068,
        1069,
        1070,
        1071,
        1072,
        1073,
        1074,
        1075,
        1076,
        1077,
        1078,
        1079,
        1080,
        1081,
        1082,
        1083,
        1084,
        1085,
        1086,
        1087,
        1088,
        1089,
        1090,
        1091,
        1092,
        1093,
        1094,
        1095,
        1096
      ],
      "sections": {
        "content": {
          "text": "It's your choice!\nNew Modular Organization!\n3\nRelational Model\nSQLDDL\n27\nInfonnation Retrieval\nand XML Data\nManagement\n2\nER Model\nConceptual Design\nAppncatirms emphasis: A course that covers the principles of database systems and emphasizes\nhow they are used in developing data-intensive applications.\n.\nf,;~tY'W';Yl~t';;:;,~7' A course that has a strong systems emphasis and assumes that students have\ngood programming skills in C and C++.\nHybrid course: Modular organization allows you to teach the course with the emphasis you want.\n......-\n:= Dependencies\n~~~\nI\nv\nI\nII\nIV\nVIr\nIII\n\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\nj\n\nDATABASE MANAGEMENT\nSYSTEMS\n\nDATABASE MANAGEMENT\nSYSTEMS\nThird Edition\nRaghu Ramakrishnan\nUniversity of Wisconsin\nMadison, Wisconsin, USA\n•\nJohannes Gehrke\nCornell University\nIthaca, New York, USA\nBoston\nBurr Ridge, IL\nDubuque, IA\nMadison, WI\nNew York\nSan Francisco\nSt. Louis\nBangkok\nBogota\nCaracas\nKuala Lumpur\nLisbon\nLondon\nMadrid\nMexico City\nMilan\nMontreal\nNew Delhi\nSantiago\nSeoul\nSingapore\nSydney\nTaipei\nToronto\n\nMcGraw-Hill Higher Education tz\nA Lhvision of The McGraw-Hill Companies\nDATABASE MANAGEMENT SYSTEMS, THIRD EDITION\nInternational Edition 2003\nExclusive rights by McGraw-Hill Education (Asia), for manufacture and export. This\nbook cannot be re-exported from the country to which it is sold by McGraw-Hill. The\nInternational Edition is not available in North America.\nPublished by McGraw-Hili, a business unit of The McGraw-Hili Companies, Inc., 1221\nAvenue of the Americas, New York, NY 10020. Copyright © 2003, 2000, 1998 by The\nMcGraw-Hill Companies, Inc. All rights reserved. No part of this publication may be\nreproduced or distributed in any form or by any means, or stored in a database or retrieval\nsystem, without the prior written consent of The McGraw-Hill Companies, Inc.,\nincluding, but not limited to, in any network or other electronic storage or transmission,\nor broadcast for distance learning.\nSome ancillaries, including electronic and print components, may not be available to\ncustomers outside the United States.\n10\n09\n08\n07\n06\n05\n04\n03\n20\n09\n08\n07\n06\n05\n04\nCTF\nBJE\nLibrary of Congress Cataloging-in-Publication Data\nRamakrishnan, Raghu\nDatabase management systems / Raghu Ramakrishnan, Johannes Gehrke.~3rd ed.\np.\ncm.\nIncludes index.\nISBN 0-07-246563-8-ISBN 0-07-115110-9 (ISE)\n1.\nDatabase management. 1. Gehrke, Johannes. II. Title.\nQA76.9.D3 R237 2003\n005.74--Dc21\n2002075205\nCIP\nWhen ordering this title, use ISBN 0-07-123151-X\nPrinted in Singapore\nwww.mhhe.com\n\nTo Apu, Ketan, and Vivek with love\nTo Keiko and Elisa\n\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\nPREFACE\nPart I\nFOUNDATIONS\nCONTENTS\nXXIV\n1\n1\n2\nOVERVIEW OF DATABASE SYSTEMS\n1.1\nManaging Data\n1.2\nA Historical Perspective\n1.3\nFile Systems versus a DBMS\n1.4\nAdvantages of a DBMS\n1.5\nDescribing and Storing Data in a DBMS\n1.5.1\nThe Relational Model\n1.5.2\nLevels of Abstraction in a DBMS\n1.5.3\nData Independence\n1.6\nQueries in a DBMS\n1.7\nTransaction Management\n1.7.1\nConcurrent Execution of Transactions\n1.7.2\nIncomplete Transactions and System Crashes\n1.7.3\nPoints to Note\n1.8\nStructure of a DBMS\n1.9\nPeople Who Work with Databases\n1.10\nReview Questions\nINTRODUCTION TO DATABASE DESIGN\n2.1\nDatabase Design and ER Diagrams\n2.1.1\nBeyond ER Design\n2.2\nEntities, Attributes, and Entity Sets\n2.3\nRelationships and Relationship Sets\n2.4\nAdditional Features of the ER Model\n2.4.1\nKey Constraints\n2.4.2\nParticipation Constraints\n2.4.3\nWeak Entities\n2.4.4\nClass Hierarchies\n2.4.5\nAggregation\nvii\n3\n4\n6\n8\n9\n10\n11\n12\n15\n16\n17\n17\n18\n19\n19\n21\n22\n25\n26\n27\n28\n29\n32\n32\n34\n35\n37\n39\n\nVlll\nDATABASE \"NIANAGEMENT SYSTEivlS\nPreliminaries\nRelational Algebra\n4.2.1\nSelection and Projection\n4.2.2\nSet Operations\n3\n4\n2.5\nConceptual Design With the ER Model\n2..5.1\nEntity versus Attribute\n2.5.2\nEntity versus Relationship\n2.5.3\nBinary versus Ternary Relationships\n2..5.4\nAggregation versus Ternary Relationships\n2.6\nConceptual Design for Large Enterprises\n2.7\nThe Unified Modeling Language\n2.8\nCase Study: The Internet Shop\n2.8.1\nRequirements Analysis\n2.8.2\nConceptual Design\n2.9\nReview Questions\nTHE RELATIONAL MODEL\n3.1\nIntroduction to the Relational Model\n3.1.1\nCreating and Modifying Relations Using SQL\n3.2\nIntegrity Constraints over Relations\n3.2.1\nKey Constraints\n:3.2.2\nForeign Key Constraints\n3.2.3\nGeneral Constraints\n3.3\nEnforcing Integrity Constraints\n3.3.1\nTransactions and Constraints\n3.4\nQuerying Relational Data\n3.5\nLogical Database Design: ER to Relational\n3.5.1\nEntity Sets to Tables\n3.5.2\nRelationship Sets (without Constraints) to Tables\n3.5.3\nTranslating Relationship Sets with Key Constraints\n3.5.4\nTranslating Relationship Sets with Participation Constraints\n3.5.5\nTranslating Weak Entity Sets\n3.5.6\ncn'anslating Class Hierarchies\n3.5.7\nTranslating ER Diagrams with Aggregation\n3.5.8\nER to Relational: Additional Examples\n:3.6\nIntroduction to Views\n3.6.1\nViews, Data Independence, Security\n3.6.2\nUpdates on Views\n:3.7\nDestroying/Altering Tables and Views\n:3.8\nCase Study: The Internet Store\n:3.9\nReview Questions\nRELATIONAL ALGEBRA AND CALCULUS\n4.1\n4.2\n40\n41\n42\n43\n45\n46\n47\n49\n49\n50\n51\n57\n59\n62\n63\n64\n66\n68\n69\n72\n73\n74\n75\n76\n78\n79\n82\n83\n84\n85\n86\n87\n88\n91\n92\n94\n100\n101\n102\n103\n104\n\nContents\nlX\n~\n4.2.3\nRenaming\n106\n4.2.4\nJoins\n107\n4.2.5\nDivision\n109\n4.2.6\n1\\'lore Examples of Algebra Queries\n110\n4.3\nRelational Calculus\n116\n4.3.1\nTuple Relational Calculus\n117\n4.3.2\nDomain Relational Calculus\n122\n4.4\nExpressive Power of Algebra and Calculus\n124\n4.5\nReview Questions\n126\n5\nSQL: QUERIES, CONSTRAINTS, TRIGGERS\n130\n5.1\nOverview\n131\n5.1.1\nChapter Organization\n132\n.5.2\nThe Form of a Basic SQL Query\n133\n5.2.1\nExamples of Basic SQL Queries\n138\n5.2.2\nExpressions and Strings in the SELECT Command\n139\n5.3\nUNION, INTERSECT, and EXCEPT\n141\n5.4\nNested Queries\n144\n5.4.1\nIntroduction to Nested Queries\n145\n5.4.2\nCorrelated Nested Queries\n147\n5.4.3\nSet-Comparison Operators\n148\n5.4.4\nMore Examples of Nested Queries\n149\n5.5\nAggregate Operators\n151\n5.5.1\nThe GROUP BY and HAVING Clauses\n154\n5.5.2\nMore Examples of Aggregate Queries\n158\n5.6\nNull Values\n162\n5.6.1\nComparisons Using Null Values\n163\n5.6.2\nLogical Connectives AND, OR, and NOT\n163\n5.6.3\nImpact 011 SQL Constructs\n163\n5.6.4\nOuter Joins\n164\n5.6.5\nDisallowing Null Values\n165\n5.7\nComplex Integrity Constraints in SQL\n165\n5.7.1\nConstraints over a Single Table\n165\n5.7.2\nDomain Constraints and Distinct Types\n166\n5.7.3\nAssertions: ICs over Several Tables\n167\n5.8\nTriggers and Active Databases\n168\n5.8.1\nExamples of Triggers in SQL\n169\n5.9\nDesigning Active Databases\n171\n5.9.1\nWhy Triggers Can Be Hard to Understand\n171\n5.9.2\nConstraints versus Triggers\n172\n5.9.:3\nOther Uses of Triggers\n172\n5.10\nReview Questions\n17:3\n\nx\nDATABASE J\\;1ANAGEMENT SYSTEMS\nPart II\nAPPLICATION DEVELOPMENT\n183\n6\nDATABASE APPLICATION DEVELOPMENT\n185\n6.1\nAccessing Databases from Applications\n187\n6.1.1\nEmbedded SQL\n187\n6.1.2\nCursors\n189\n6.1.3\nDynamic SQL\n194\n6.2\nAn Introduction to JDBC\n194\n6.2.1\nArchitecture\n196\n6.3\nJDBC Classes and Interfaces\n197\n6.3.1\nJDBC Driver Management\n197\n6.3.2\nConnections\n198\n6.3.3\nExecuting SQL Statements\n200\n6.3.4\nResultSets\n201\n6.3.5\nExceptions and Warnings\n203\n6.3.6\nExamining Database Metadata\n204\n6.4\nSQLJ\n206\n6.4.1\nWriting SQLJ Code\n207\n6.5\nStored Procedures\n209\n6.5.1\nCreating a Simple Stored Procedure\n209\n6.5.2\nCalling Stored Procedures\n210\n6.5.3\nSQL/PSM\n212\n6.6\nCase Study: The Internet Book Shop\n214\n6.7\nReview Questions\n216\n7\nINTERNET APPLICATIONS\n220\n7.1\nIntroduction\n220\n7.2\nInternet Concepts\n221\n7.2.1\nUniform Resource Identifiers\n221\n7.2.2\nThe Hypertext Transfer Protocol (HTTP)\n223\n7.3\nHTML Documents\n226\n7.4\nXML Documents\n227\n7.4.1\nIntroduction to XML\n228\n7.4.2\nXML DTDs\n231\n7.4.3\nDomain-Specific DTDs\n234\n7.5\nThe Three-Tier Application Architecture\n236\n7.5.1\nSingle-Tier and Client-Server Architectures\n236\n7.5.2\nThree-Tier Architectures\n239\n7.5.3\nAdvantages of the Three-Tier Architecture\n241\n7.6\nThe Presentation Layer\n242\n7.6.1\nHTrvlL Forms\n242\n7.6.2\nJavaScript\n245\n7.6.3\nStyle Sheets\n247\n\nContents\n:»:i\n7.7\nThe Middle Tier\n7.7.1\nCGI: The Common Gateway Interface\n7.7.2\nApplication Servers\n7.7.3\nServlets\n7.7.4\nJavaServer Pages\n7.7.5\nMaintaining State\n7.8\nCase Study: The Internet Book Shop\n7.9\nReview Questions\n251\n251\n252\n254\n256\n258\n261\n264\nPart III\nSTORAGE AND INDEXING\n271\n273\n274\n275\n277\n277\n278\n279\n280\n282\n283\n284\n285\n287\n288\n289\n290\n291\n292\n292\n295\n299\n299\nData on External Storage\nFile Organizations and Indexing\n8.2.1\nClustered Indexes\n8.2.2\nPrimary and Secondary Indexes\nIndex Data Structures\n8.3.1\nHash-Based Indexing\n8.3.2\nTree-Based Indexing\nComparison of File Organizations\n8.4.1\nCost Model\n8.4.2\nHeap Files\n8.4.3\nSorted Files\n8.4.4\nClustered Files\n8.4.5\nHeap File with Unclustered Tree Index\n8.4.6\nHeap File With Unclustered Hash Index\n8.4.7\nComparison of I/O Costs\nIndexes and Performance Tuning\n8..5.1\nImpact of the Workload\n8.5.2\nClustered Index Organization\n8.5.3\nComposite Search Keys\n8.5.4\nIndex Specification in SQL:1999\nReview Questions\n8.6\n8.5\n8.4\n8.3\nOVERVIEW OF STORAGE AND INDEXING\n8.1\n8.2\n8\n9\nSTORING DATA: DISKS AND FILES\n9.1\nThe Memory Hierarchy\n9.1.1\nMagnetic Disks\n9.1.2\nPerformance Implications of Disk Structure\n9.2\nRedundant Arrays of Independent Disks\n9.2.1\nData Striping\n9.2.2\nRedundancy\n9.2.3\nLevels of Redundancy\n9.2.4\nChoice of RAID Levels\n304\n305\n306\n308\n309\n310\n311\n312\n316\n\nXll\nDATABASE\n~/IANAGE1'vIENT SYSTEMS\n9.3\nDisk Space Management\n9.3.1\nKeeping Track of Free Blocks\n9.3.2\nUsing as File Systems to il/ranage Disk Space\n9.4\nBuffer Manager\n9.4.1\nBuffer Replacement Policies\n9.4.2\nBuffer Management in DBMS versus OS\n9.5\nFiles of Records\n9.5.1\nImplementing Heap Files\n9.6\nPage Formats\n9.6.1\nFixed-Length Records\n9.6.2\nVariable-Length Records\n9.7\nRecord Formats\n9.7.1\nFixed-Length Records\n9.7.2\nVariable-Length Records\n9.8\nReview Questions\n10\nTREE-STRUCTURED INDEXING\n10.1\nIntuition For Tree Indexes\n10.2\nIndexed Sequential Access Method (ISAM)\n10.2.1\nOverflow Pages, Locking Considerations\n10.3\nB+ Trees: A Dynamic Index Structure\n10.3.1\nFormat of a Node\n10.4\nSearch\n10.5\nInsert\n10.6\nDelete\n10.7\nDuplicates\n10.8\nB+ Trees in Practice\n10.8.1\nKey Compression\n10.8.2\nBulk-Loading a B+ Tl'ee\n10.8.3\nThe Order Concept\n10.8.4\nThe Effect of Inserts and Deletes on Rids\n10.9\nReview Questions\n11\nHASH-BASED INDEXING\n11.1\nStatic Hashing\n11.1.1\nNotation and Conventions\n11.2\nExtendible HCkshing\n11.3\nLine~r Hashing\n11.4\nExtendible vs. Linear Ha\"lhing\nn.5\nReview Questions\nPart IV\nQUERY EVALUATION\n:316\n317\n317\n318\n320\n322\n324\n324\n326\n327\n328\n330\n331\n331\n333\n338\n339\n341\n344\n344\n346\n347\n348\n352\n356\n358\n358\n360\n363\n364\n364\n370\n371\n373\n373\n379\n384\n385\n391\n\nContents\n12\nOVERVIEW OF QUERY EVALUATION\n12.1\nThe System Catalog\n12.1.1\nInformation in the Catalog\n12.2\nIntroduction to Operator Evaluation\n12.2.1\nThree Common Techniques\n12.2.2\nAccess Paths\n12.3\nAlgorithms for Relational Operations\n12.3.1\nSelection\n12.3.2\nProjection\n12.3.3\nJoin\n12.3.4\nOther Operations\n12.4\nIntroduction to Query Optimization\n12.4.1\nQuery Evaluation Plans\n12.4.2\nMulti-operator Queries: Pipelined Evaluation\n12.4.3\nThe Iterator Interface\n12.5\nAlternative Plans: A Motivating Example\n12.5.1\nPushing Selections\n12.5.2\nUsing Indexes\n12.6\nWhat a Typical Optimizer Does\n12.6.1\nAlternative Plans Considered\n12.6.2\nEstimating the Cost of a Plan\n12.7\nReview Questions\n13\nEXTERNAL SORTING\n13.1\nWhen Does a DBMS Sort Data?\n13.2\nA Simple Two-Way Merge Sort\n13.3\nExternal Merge Sort\n13.3.1\nMinimizing the Number of Runs\n13.4\nMinimizing I/O Cost versus Number of I/Os\n13.4.1\nBlocked I/O\n13.4.2\nDouble Buffering\n13.5\nUsing B+ Trees for Sorting\n13.5.1\nClustered Index\n1:3.5.2\nUnclustered Index\n13.6\nReview Questions\n14\nEVALUATING RELATIONAL OPERATORS\n14.1\nThe' Selection Operation\n14.1.1\nNo Index, Unsorted Data\n14.1.2\nNo Index, Sorted Data\n14.1.:3\nB+ Tree Index\n14.1.4\nHash Index, Equality Selection\n14.2\nGeneral Selection Conditions\n393\n:394\n:39.5\n397\n398\n398\n400\n401\n401\n402\n404\n404\n405\n407\n408\n409\n409\n411\n414\n414\n416\n417\n421\n422\n423\n424\n428\n430\n430\n432\n4:33\n433\n434\n436\n439\n441\n441\n442\n442\n444\n444\n\nXIV\nDATABASE ~11ANAGEMENT SYSTEMS\n14.2.1\nCNF and Index Matching\n14.2.2\nEvaluating Selections without Disjunction\n14.2.3\nSelections with Disjunction\n14.3\nThe Projection Operation\n14.3.1\nProjection Based on Sorting\n14.3.2\nProjection Based on Hashing\n14.3.3\nSorting Versus Hashing for Projections\n14.3.4\nUse of Indexes for Projections\n14.4\nThe Join Operation\n14.4.1\nNested Loops Join\n14.4.2\nSort-Merge Join\n14.4.3\nHash Join\n14.4.4\nGeneral Join Conditions\n14.5\nThe Set Operations\n14.5.1\nSorting for Union and Difference\n14.5.2\nHashing for Union and Difference\n14.6\nAggregate Operations\n14.6.1\nImplementing Aggregation by Using an Index\n14.7\nThe Impact of Buffering\n14.8\nReview Questions\n445\n445\n446\n447\n448\n449\n451\n452\n452\n454\n458\n463\n467\n468\n469\n469\n469\n471\n471\n472\n15\nA TYPICAL RELATIONAL QUERY OPTIMIZER\n478\n15.1\nTranslating SQL Queries into Algebra\n479\n15.1.1\nDecomposition of a Query into Blocks\n479\n15.1.2\nA Query Block as a Relational Algebra Expression\n481\n15.2\nEstimating the Cost of a Plan\n482\n15.2.1\nEstimating Result Sizes\n483\n15.3\nRelational Algebra Equivalences\n488\n15.3.1\nSelections\n488\n15.3.2\nProjections\n488\n15.3.3\nCross-Products and Joins\n489\n15.3.4\nSelects, Projects, and Joins\n490\n15.3.5\nOther Equivalences\n491\n15.4\nEnumeration of Alternative Plans\n492\n15.4.1\nSingle-Relation Queries\n492\n15.4.2\nMultiple-Relation Queries\n496\nIS.5\nNested Subqueries\n504\n15.6\nThe System R Optimizer\n506\n15.7\nOther Approaches to Query Optimization\nS07\n15.8\nReview Questions\n507\nPart V\nTRANSACTION MANAGEMENT\n517\n\nContents\nXfV\n16\nOVERVIEW OF TRANSACTION MANAGEMENT\n519\n16.1\nThe ACID Properties\n520\n16.1.1\nConsistency and Isolation\n521\n16.1.2\nAtomicity and Durability\n522\n16.2\nTransactions and Schedules\n523\n16.3\nConcurrent Execution of Transactions\n524\n16.3.1\nrvlotivation for Concurrent Execution\n524\n16.3.2\nSerializability\n525\n16.3.3\nAnomalies Due to Interleaved Execution\n526\n16.3.4\nSchedules Involving Aborted Transactions\n529\n16.4\nLock-Based Concurrency Control\n530\n16.4.1\nStrict Two-Phase Locking (Strict 2PL)\n531\n16.4.2\nDeadlocks\n533\n16.5\nPerformance of Locking\n533\n16.6\nTransaction Support in SQL\n535\n16.6.1\nCreating and Terminating Transactions\n535\n16.6.2\nWhat Should We Lock?\n537\n16.6.3\nTransaction Characteristics in SQL\n538\n16.7\nIntroduction to Crash Recovery\n540\n16.7.1\nStealing Frames and Forcing Pages\n541\n16.7.2\nRecovery-Related Steps during Normal Execution\n542\n16.7.3\nOverview of ARIES\n543\n16.7.4\nAtomicity: Implementing Rollback\n543\n16.8\nReview Questions\n544\n17 CONCURRENCY CONTROL\n549\n17.1\n2PL, Serializability, and Recoverability\n550\n17.1.1\nView Serializability\n553\n17.2\nIntroduction to Lock Management\n553\n17.2.1\nImplementing Lock and Unlock Requests\n554\n17.3\nLock Conversions\n555\n17.4\nDealing With Deadlocks\n556\n17.4.1\nDeadlock Prevention\n558\n17.5\nSpecialized Locking Techniques\n559\n17.5.1\nDynamic Databases and the Phantom Problem\n560\n17.5.2\nConcurrency Control in B+ Trees\n561\n17.5.3\nMultiple-Granularity Locking\n564\n17.6\nConClurency Control without Locking\n566\n17.6.1\nOptimistic Concurrency Control\n566\n17.6.2\nTimestamp-Based Concurrency Control\n569\n17.6.3\nMultiversion Concurrency Control\n572\n17.7\nReviev Questions\n57:3\n\nXVI\nDATABASE rvlANAGEMENT SYSTEMS\n18\nCRASH RECOVERY\n18.1\nIntroduction to ARIES\n18.2\nThe Log\n18.3\nOther Recovery-Related Structures\n18.4\nThe Write-Ahead Log Protocol\n18.5\nCheckpointing\n18.6\nRecovering from a System Crash\n18.6.1\nAnalysis Phase\n18.6.2\nRedo Phase\n18.6.3\nUndo Phase\n18.7\nMedia Recovery\n18.8\nOther Approaches and Interaction with Concurrency Control\n18.9\nReview Questions\nPart VI\nDATABASE DESIGN AND TUNING\n579\n580\n582\n585\n586\n587\n587\n588\n590\n592\n595\n596\n597\n603\n19\nSCHEMA REFINEMENT AND NORMAL FORMS\n605\n19.1\nIntroduction to Schema Refinement\n606\n19.1.1\nProblems Caused by Redundancy\n606\n19.1.2\nDecompositions\n608\n19.1.3\nProblems Related to Decomposition\n609\n19.2\nFunctional Dependencies\n611\n19.3\nReasoning about FDs\n612\n19.3.1\nClosure of a Set of FDs\n612\n19.3.2\nAttribute Closure\n614\n19.4\nNormal Forms\n615\n19.4.1\nBoyce-Codd Normal Form\n615\n19.4.2\nThird Normal Form\n617\n19.5\nProperties of Decompositions\n619\n19.5.1\nLossless-Join Decomposition\n619\n19.5.2\nDependency-Preserving Decomposition\n621\n19.6\nNormalization\n622\n19.6.1\nDecomposition into BCNF\n622\n19.6.2\nDecomposition into 3NF\n625\n19.7\nSchema Refinement in Database Design\n629\n19.7.1\nConstraints on an Entity Set\n630\n19.7.2\nConstraints on a Relationship Set\n630\n19.7.3\nIdentifying Attributes of Entities\n631\n19.7.4\nIdentifying Entity Sets\n6:33\n19.8\nOther Kinds of Dependencies\n6:33\n19.8.1\nMultivalued Dependencies\n6:34\n19.8.2\nFourth Normal Form\n6:36\n19.8.:3\nJoin Dependencies\n(1:38\n\nContents\nXVll\n19.8.4\nFifth Normal Form\n6:38\n19.8.5\nInclusion Dependencies\n639\n19.9\nCase Study: The Internet Shop\n640\n19.10 Review Questions\n642\n20\nPHYSICAL DATABASE DESIGN AND TUNING\n649\n20.1\nIntroduction to Physical Database Design\n650\n20.1.1\nDatabase Workloads\n651\n20.1.2\nPhysical Design and Tuning Decisions\n652\n20.1.3\nNeed for Database Tuning\n653\n20.2\nGuidelines for Index Selection\n653\n20.3\nBasic Examples of Index Selection\n656\n20.4\nClustering and Indexing\n658\n20.4.1\nCo-clustering Two Relations\n660\n20.5\nIndexes that Enable Index-Only Plans\n662\n20.6\nTools to Assist in Index Selection\n663\n20.6.1\nAutomatic Index Selection\n663\n20.6.2\nHow Do Index Tuning Wizards Work?\n664\n20.7\nOverview of Database Tuning\n667\n20.7.1\nTuning Indexes\n667\n20.7.2\nTuning the Conceptual Schema\n669\n20.7.3\nTuning Queries and Views\n670\n20.8\nChoices in Tuning the Conceptual Schema\n671\n20.8.1\nSettling for a Weaker Normal Form\n671\n20.8.2\nDenormalization\n672\n20.8.3\nChoice of Decomposition\n672\n20.8.4\nVertical Partitioning of BCNF Relations\n674\n20.8.5\nHorizontal Decomposition\n674\n20.9\nChoices in Tuning Queries and Views\n675\n20.10 Impact of Concurrency\n678\n20.10.1 Reducing Lock Durations\n678\n20.10.2 Reducing Hot Spots\n679\n20.11 Case Study: The Internet Shop\n680\n20.11.11\\ming the Datab~'ie\n682\n20.12 DBMS Benchmarking\n682\n20.12.1 Well-Known DBMS Benchmarks\n683\n20.12.2 Using a Benchmark\n684\n20.13 Review Questions\n685\n21\nSECURITY AND AUTHORIZATION\n692\n21.1\nIntroduction to Datab~\"e Security\n693\n21.2\nAccess Control\n694\n21.3\nDiscretionary Access Control\n695\n\nxviii\nDATABASE ~/IANAGEMENT SYSTEMS\n21.3.1\nGrant and Revoke on Views and Integrity Constraints\n21.4\nMandatory Access Control\n21.4.1\nMultilevel Relations and Polyinstantiation\n21.4.2\nCovert Channels, DoD Security Levels\n21.5\nSecurity for Internet Applications\n21.5.1\nEncryption\n21.5.2\nCertifying Servers: The SSL Protocol\n21.5.3\nDigital Signatures\n21.6\nAdditional Issues Related to Security\n21.6.1\nRole of the Database Administrator\n21.6.2\nSecurity in Statistical Databases\n21.7\nDesign Case Study: The Internet Store\n21.8\nReview Questions\nPart VII\nADDITIONAL TOPICS\n22\nPARALLEL AND DISTRIBUTED DATABASES\n22.1\nIntroduction\n22.2\nArchitectures for Parallel Databases\n22.3\nParallel Query Evaluation\n22.3.1\nData Partitioning\n22.3.2\nParallelizing Sequential Operator Evaluation Code\n22.4\nParallelizing Individual Operations\n22.4.1\nBulk Loading and Scanning\n22.4.2\nSorting\n22.4.3\nJoins\n22.5\nParallel Query Optimization\n22.6\nIntroduction to Distributed Databases\n22.6.1\nTypes of Distributed Databases\n22.7\nDistributed DBMS Architectures\n22.7.1\nClient-Server Systems\n22.7.2\nCollaborating Server Systems\n22.7.3\nMidclleware Systems\n22.8\nStoring Data in a Distributed DBMS\n22.8.1\nFragmentation\n22.8.2\nReplication\n22.9\nDistributed Catalog Management\n22.9.1\nNaming Objects\n22.9.2\nCatalog Structure\n22.9.3\nDistributed Data Independence\n22.10 Distributed Query Processing\n22.1.0.1 Nonjoin Queries in a Distributed DBMS\n22.10.2 Joins in a Distributed DBMS\n704\n705\n707\n708\n709\n709\n712\n713\n714\n714\n715\n716\n718\n723\n725\n726\n727\n728\n730\n730\n731\n731\n732\n732\n735\n736\n737\n737\n738\n738\n739\n739\n739\n741\n741\n741\n742\n743\n743\n744\n745\n\nContents\nJ6x\n22.10.3 Cost-Based Query Optimization\n749\n22.11 Updating Distributed Data\n750\n22.11.1 Synchronous Replication\n750\n22.11.2 Asynchronous Replication\n751\n22.12 Distributed Transactions\n755\n22.13 Distributed Concurrency Control\n755\n22.13.1 Distributed Deadlock\n756\n22.14 Distributed Recovery\n758\n22.14.1 Normal Execution and Commit Protocols\n758\n22.14.2 Restart after a Failure\n760\n22.14.3 Two-Phase Commit Revisited\n761\n22.14.4 Three-Phase Commit\n762\n22.15 Review Questions\n763\n23\nOBJECT-DATABASE SYSTEMS\n772\n23.1\nMotivating Example\n774\n23.1.1\nNew Data Types\n775\n23.1.2\nManipulating the New Data\n777\n23.2\nStructured Data Types\n779\n23.2.1\nCollection Types\n780\n23.3\nOperations on Structured Data\n781\n23.3.1\nOperations on Rows\n781\n23.3.2\nOperations on Arrays\n781\n23.3.3\nOperations on Other Collection Types\n782\n23.3.4\nQueries Over Nested Collections\n783\n23.4\nEncapsulation and ADTs\n784\n23.4.1\nDefining Methods\n785\n23.5\nInheritance\n787\n23.5.1\nDefining Types with Inheritance\n787\n23.5.2\nBinding Methods\n788\n23.5.3\nCollection Hierarchies\n789\n23.6\nObjects, aIDs, and Reference Types\n789\n23.6.1\nNotions of Equality\n790\n23.6.2\nDereferencing Reference Types\n791\n23.6.3\nURLs and OIDs in SQL:1999\n791\n23.7\nDatabase Design for an ORDBJ\\'IS\n792\n23.7.1\nCollection Types and ADTs\n792\n2~).7.2\nObject Identity\n795\n23.7.3\nExtending the ER Model\n796\n23.7.4\nUsing Nested Collections\n798\n2:3.8\nORDBMS Implementation Challenges\n799\n23.8.]\nStorage and Access Methods\n799\n23.8.2\nQuery Processing\n801\n\nDATABASE ~/IANAGEMENT SYSTEl\\,fS\n23.8.3\nQuery Optimization\n23.9\nOODBMS\n23.9.1\nThe ODMG Data Model and ODL\n23.9.2\nOQL\n23.10 Comparing RDBMS, OODBl'vlS, and ORDBMS\n23.10.1 RDBMS versus ORDBMS\n23.10.2 OODBMS versus ORDBMS: Similarities\n23.10.3 OODBMS versus ORDBMS: Differences\n23.11 Review Questions\n80;3\n805\n805\n807\n809\n809\n809\n810\n811\n24\nDEDUCTIVE DATABASES\n817\n24.1\nIntroduction to Recursive Queries\n818\n24.1.1\nDatalog\n819\n24.2\nTheoretical Foundations\n822\n24.2.1\nLeast Model Semantics\n823\n24.2.2\nThe Fixpoint Operator\n824\n24.2.3\nSafe Datalog Programs\n825\n24.2.4\nLeast Model = Least Fixpoint\n826\n24.3\nRecursive Queries with Negation\n827\n24.3.1\nStratification\n828\n24.4\nFrom Datalog to SQL\n831\n24.5\nEvaluating Recursive Queries\n834\n24.5.1\nFixpoint Evaluation without Repeated Inferences\n835\n24.5.2\nPushing Selections to Avoid Irrelevant Inferences\n837\n24.5.3\nThe Magic Sets Algorithm\n838\n24.6\nReview Questions\n841\n25\nDATA WAREHOUSING AND DECISION SUPPORT\n846\n25.1\nIntroduction to Decision Support\n848\n25.2\nOLAP: Multidimensional Data Model\n849\n25.2.1\nMultidimensional Database Design\n853\n25.:3\nMultidimensional Aggregation Queries\n854\n25.3.1\nROLLUP and CUBE in SQL:1999\n856\n25.4\nWindow Queries in SQL:1999\n859\n25.4.1\nFraming a Window\n861\n25.4.2\nNew Aggregate Functions\n862\n25.5\nFindipg Answers Quickly\n862\n25.5.1\nTop N Queries\n863\n25.5.2\nOnline Aggregation\n864\n25.6\nImplementation Techniques for OLAP\n865\n25.6.1\nBitmap Indexes\n866\n25.6.2\nJoin Indexes\n868\n25.6.3\nFile Organizations\n869\n\nContents\n25.7\nData 'Warehousing\n25.7.1\nCreating and Ivlaintaining a Warehouse\n25.8\nViews and Decision Support\n25.8.1\nViews, OLAP, and \\Varehousing\n25.8.2\nQueries over Views\n25.9\nView Materialization\n25.9.1\nIssues in View Materialization\n25.10 Maintaining Materialized Views\n2.5.10.1 Incremental View Maintenance\n25.10.2 Maintaining Warehouse Views\n25.10.3 When Should We Synchronize Views?\n25.11 Review Questions\n870\n871\n872\n872\n873\n873\n874\n876\n876\n879\n881\n882\n26\nDATA MINING\n889\n26.1\nIntroduction to Data Mining\n890\n26.1.1\nThe Knowledge Discovery Process\n891\n26.2\nCounting Co-occurrences\n892\n26.2.1\nFrequent Itemsets\n892\n26.2.2\nIceberg Queries\n895\n26.3\nMining for Rules\n897\n26.3.1\nAssociation Rules\n897\n26.3.2\nAn Algorithm for Finding Association Rules\n898\n26.3.3\nAssociation Rules and ISA Hierarchies\n899\n26.3.4\nGeneralized Association Rules\n900\n26.3.5\nSequential Patterns\n901\n26.3.6\nThe Use of Association Rules for Prediction\n902\n26.3.7\nBayesian Networks\n903\n26.3.8\nClassification and Regression Rules\n904\n26.4\nTree-Structured Rules\n906\n26.4.1\nDecision Trees\n907\n26.4.2\nAn Algorithm to Build Decision Trees\n908\n26.5\nClustering\n911\n26.5.1\nA Clustering Algorithm\n912\n26.6\nSimilarity Search over Sequences\n913\n26.6.1\nAn Algorithm to Find Similar Sequences\n915\n26.7\nIncremental Mining and Data Streams\n916\n26.7.1\nIncremental Maintenance of Frequent Itemsets\n918\n26.8\nAdditional Data Mining Tasks\n920\n26.9\nReview Questions\n920\n27\nINFORMATION RETRIEVAL AND XML DATA\n926\n27.1\nColliding Worlds: Databa'3es, IR, and XML\n27.1.1\nDBMS versus IR Systems\n927\n928\n\nxxii\nDATABASE l\\1ANAGEMENT SYSTEMS\n27.2\nIntroduction to Information Retrieval\n929\n27.2.1\nVector Space Model\n930\n27.2.2 TFjIDF Weighting of Terms\n931\n27.2.3\nRanking Document Similarity\n932\n27.2.4\n:Measuring Success: Precision and Recall\n934\n27.3\nIndexing for Text Search\n934\n27.3.1\nInverted Indexes\n935\n27.3.2\nSignature Files\n937\n27.4\nWeb Search Engines\n939\n27.4.1\nSearch Engine Architecture\n939\n27.4.2\nUsing Link Information\n940\n27.5\nManaging Text in a DBMS\n944\n27.5.1\nLoosely Coupled Inverted Index\n945\n27.6\nA Data Model for XML\n945\n27.6.1\nMotivation for Loose Structure\n945\n27.6.2\nA Graph Model\n946\n27.7\nXQuery: Querying XML Data\n948\n27.7.1\nPath Expressions\n948\n27.7.2\nFLWR Expressions\n949\n27.7.3\nOrdering of Elements\n951\n27.7.4\nGrouping and Generation of Collection Values\n951\n27.8\nEfficient Evaluation of XML Queries\n952\n27.8.1\nStoring XML in RDBMS\n952\n27.8.2\nIndexing XML Repositories\n956\n27.9\nReview Questions\n959\n28\nSPATIAL DATA MANAGEMENT\n968\n28.1\nTypes of Spatial Data and Queries\n969\n28.2\nApplications Involving Spatial Data\n971\n28.3\nIntroduction to Spatial Indexes\n973\n28.3.1\nOverview of Proposed Index Structures\n974\n28.4\nIndexing Based on Space-Filling Curves\n975\n28.4.1\nRegion Quad Trees and Z-Ordering: Region Data\n976\n28.4.2\nSpatial Queries Using Z-Ordering\n978\n28.5\nGrid Files\n978\n28..5.1\nAdapting Grid Files to Handle Regions\n981\n28.6\nR Trees: Point and Region Data\n982\n28.6~1\nQueries\n983\n28.6.2\nInsert and Delete Operations\n984\n28.6.3\nConcurrency Control\n986\n28.6.4\nGeneralized Search Trees\n987\n28.7\nIssues in High-Dimensional Indexing\n988\n28.8\nReview Questions\n988\n\nContents\n29\nFURTHER READING\n29.1\nAdvanced Tl\"ansaction Processing\n29.1.1\nTransaction Processing Monitors\n29.1.2\nNew Transaction Models\n29.1.3\nReal-Time DBlvISs\n29.2\nData Integration\n29.3\nMobile Databases\n29.4\nMain Memory Databases\n29.5\nMultimedia Databases\n29.6\nGeographic Information Systems\n29.7\nTemporal Databases\n29.8\nBiological Databases\n29.9\nInformation Visualization\n29.10 Summary\n30\nTHE MINIBASE SOFTWARE\n30.1\nWhat Is Available\n30.2\nOverview of Minibase Assignments\n30.3\nAcknowledgments\nREFERENCES\nAUTHOR INDEX\nSUBJECT INDEX\nxxm\n992\n993\n993\n994\n994\n995\n995\n996\n997\n998\n999\n999\n1000\n1000\n1002\n1002\n1003\n1004\n1005\n1045\n1054\n\nPREFACE\nThe advantage of doing one's praising for oneself is that one can lay it on so thick\nand exactly in the right places.\n--Samuel Butler\nDatabase management systems are now an indispensable tool for managing\ninformation, and a course on the principles and practice of database systems\nis now an integral part of computer science curricula.\nThis book covers the\nfundamentals of modern database management systems, in particular relational\ndatabase systems.\nWe have attempted to present the material in a clear, simple style. A quantita-\ntive approach is used throughout with many detailed examples. An extensive\nset of exercises (for which solutions are available online to instructors) accom-\npanies each chapter and reinforces students' ability to apply the concepts to\nreal problems.\nThe book can be used with the accompanying software and programming as-\nsignments in two distinct kinds of introductory courses:\n1. Applications Emphasis: A course that covers the principles of database\nsystems, and emphasizes how they are used in developing data-intensive ap-\nplications. Two new chapters on application development (one on database-\nbacked applications, and one on Java and Internet application architec-\ntures) have been added to the third edition, and the entire book has been\nextensively revised and reorganized to support such a course. A running\ncase-study and extensive online materials (e.g., code for SQL queries and\nJava applications, online databases and solutions) make it easy to teach a\nhands-on application-centric course.\n2. Systems Emphasis: A course that has a strong systems emphasis and\nassumes that students have good programming skills in C and C++. In\nthis case the accompanying Minibase software can be llsed as the basis\nfor projects in which students are asked to implement various parts of a\nrelational DBMS. Several central modules in the project software (e.g.,\nheap files, buffer manager, B+ trees, hash indexes, various join methods)\nxxiv\n\nPTeface\nXKV\nare described in sufficient detail in the text to enable students to implement\nthem, given the (C++) class interfaces.\nr..,1any instructors will no doubt teach a course that falls between these two\nextremes. The restructuring in the third edition offers a very modular orga-\nnization that facilitates such hybrid courses. The also book contains enough\nmaterial to support advanced courses in a two-course sequence.\nOrganization of the Third Edition\nThe book is organized into six main parts plus a collection of advanced topics, as\nshown in Figure 0.1. The Foundations chapters introduce database systems, the\n(1) Foundations\nBoth\n(2) Application Development\nApplications emphasis\n(3) Storage and Indexing\nSystems emphasis\n(4) Query Evaluation\nSystems emphasis\n(5) Transaction Management\nSystems emphasis\n(6) Database Design and Tuning\nApplications emphasis\n(7) Additional Topics\nBoth\nFigure 0.1\nOrganization of Parts in the Third Edition\nER model and the relational model. They explain how databases are created\nand used, and cover the basics of database design and querying, including an\nin-depth treatment of SQL queries. While an instructor can omit some of this\nmaterial at their discretion (e.g., relational calculus, some sections on the ER\nmodel or SQL queries), this material is relevant to every student of database\nsystems, and we recommend that it be covered in as much detail as possible.\nEach of the remaining five main parts has either an application or a systems\nempha.sis. Each of the three Systems parts has an overview chapter, designed to\nprovide a self-contained treatment, e.g., Chapter 8 is an overview of storage and\nindexing. The overview chapters can be used to provide stand-alone coverage\nof the topic, or as the first chapter in a more detailed treatment. Thus, in an\napplication-oriented course, Chapter 8 might be the only material covered on\nfile organizations and indexing, whereas in a systems-oriented course it would be\nsupplemented by a selection from Chapters 9 through 11. The Database Design\nand Tuning part contains a discussion of performance tuning and designing for\nsecure access. These application topics are best covered after giving students\na good grasp of database system architecture, and are therefore placed later in\nthe chapter sequence.\n\nXXVI\nSuggested Course Outlines\nDATABASE ~1ANAGEMENT SYSTEMS\nThe book can be used in two kinds of introductory database courses, one with\nan applications emphasis and one with a systems empha..':iis.\nThe introductory applications-oriented course could cover the :Foundations chap-\nters, then the Application Development chapters, followed by the overview sys-\ntems chapters, and conclude with the Database Design and Tuning material.\nChapter dependencies have been kept to a minimum, enabling instructors to\neasily fine tune what material to include. The Foundations material, Part I,\nshould be covered first, and within Parts III, IV, and V, the overview chapters\nshould be covered first.\nThe only remaining dependencies between chapters\nin Parts I to VI are shown as arrows in Figure 0.2.\nThe chapters in Part I\nshould be covered in sequence. However, the coverage of algebra and calculus\ncan be skipped in order to get to SQL queries sooner (although we believe this\nmaterial is important and recommend that it should be covered before SQL).\nThe introductory systems-oriented course would cover the Foundations chap-\nters and a selection of Applications and Systems chapters. An important point\nfor systems-oriented courses is that the timing of programming projects (e.g.,\nusing Minibase) makes it desirable to cover some systems topics early. Chap-\nter dependencies have been carefully limited to allow the Systems chapters to\nbe covered as soon as Chapters 1 and 3 have been covered.\nThe remaining\nFoundations chapters and Applications chapters can be covered subsequently.\nThe book also has ample material to support a multi-course sequence. Obvi-\nously, choosing an applications or systems emphasis in the introductory course\nresults in dropping certain material from the course; the material in the book\nsupports a comprehensive two-course sequence that covers both applications\nand systems a.spects. The Additional Topics range over a broad set of issues,\nand can be used as the core material for an advanced course, supplemented\nwith further readings.\nSupplementary Material\nThis book comes with extensive online supplements:\n..\nOnline Chapter: To make space for new material such a.'3 application\ndevelopment, information retrieval, and XML, we've moved the coverage\nof QBE to an online chapter.\nStudents can freely download the chapter\nfrom the book's web site, and solutions to exercises from this chapter are\nincluded in solutions manual.\n\nPreface\nxxvii;\n(\n, (\n(\n3\n2\n4\nH\n5 J\nI\nI\n1~\n!---i\nRelational Model\nRelational Algebra\nI Introduction,\n:\nERModel\n1--1\nSQLDM~\n\"---~~~\n. Conceptual Design\nl\nSQLDDL\nand Calculus\nII\n6\nDatabase Application\n~\nDevelopment\n7\nDatabase-Backed\nInternet Applications\n8\n9\n]\n10\n] [\n11\n]\nIII\nOverview of\nJ\\\nStorage and Indexing\nData Storage\nTree Indexes\nHash Indexes\n\\\n12\n13\n14\n15\nIV\nOverview of\nEvaluation of\nI--\nA Typical\nQuery Evaluation 1\\\nExternal Sorting\nRelational Operators\nRelational Optimizer\n\\\n16\n17\n18\nV\nOverview of\nConcurrency r--\nCrash\nTransaction Management 1\\\nControl\nRecovery\n\\ \\\n\\\n19\n20\n21\nVI\nSchema Refinement,\nPhysical DB\nSecurity and\nFDs, Normalization\nDesign, Tuning\nAuthorization\n22\n23\n24\n25\nParallel and\nObject-Database\nDeductive\nData Warehousing\nDistributed DBs\nSystems\nDatabases\nand Decision Support\nVII\nC\n26\n27\n28\n29\nData\nInformation Retrieval\nSpatial\nFurther\nMining\nand XML Data\nDatabases\nReading\nFigure 0.2\nChapter Organization and Dependencies\nlIII\nLecture Slides:\nLecture slides are freely available for all chapters in\nPostscript, and PDF formats.\nCourse instructors can also obtain these\nslides in Microsoft Powerpoint format, and can adapt them to their teach-\ning needs. Instructors also have access to all figures llsed in the book (in\nxfig format), and can use them to modify the slides.\n\nxxviii\nDATABASE IVIANAGEMENT SVSTErvIS\n•\nSolutions to Chapter Exercises: The book has an UnUS1H:l,lly extensive\nset of in-depth exercises. Students can obtain solutioIls to odd-numbered\nchapter exercises and a set of lecture slides for each chapter through the\nvVeb in Postscript and Adobe PDF formats. Course instructors can obtain\nsolutions to all exercises.\n•\nSoftware: The book comes with two kinds of software.\nFirst, we have\nJ\\!Iinibase, a small relational DBMS intended for use in systems-oriented\ncourses.\nMinibase comes with sample assignments and solutions, as de-\nscribed in Appendix 30. Access is restricted to course instructors. Second,\nwe offer code for all SQL and Java application development exercises in\nthe book, together with scripts to create sample databases, and scripts for\nsetting up several commercial DBMSs. Students can only access solution\ncode for odd-numbered exercises, whereas instructors have access to all\nsolutions.\n•\nInstructor's Manual: The book comes with an online manual that of-\nfers instructors comments on the material in each chapter. It provides a\nsummary of each chapter and identifies choices for material to emphasize\nor omit.\nThe manual also discusses the on-line supporting material for\nthat chapter and offers numerous suggestions for hands-on exercises and\nprojects. Finally, it includes samples of examination papers from courses\ntaught by the authors using the book. It is restricted to course instructors.\nFor More Information\nThe home page for this book is at URL:\nhttp://www.cs.wisc.edu/-dbbook\nIt contains a list of the changes between the 2nd and 3rd editions, and a fre-\nquently updated link to all known erTOT8 in the book and its accompanying\nsupplements.\nInstructors should visit this site periodically or register at this\nsite to be notified of important changes by email.\nAcknowledgments\nThis book grew out of lecture notes for CS564, the introductory (senior/graduate\nlevel) database course at UvV-Madison. David De\\Vitt developed this course\nand the Minirel project, in which students wrote several well-chosen parts of\na relational DBMS. My thinking about this material was shaped by teaching\nCS564, and Minirel was the inspiration for Minibase, which is more compre-\nhensive (e.g., it has a query optimizer and includes visualization software) but\n\nPreface\nXXIX\ntries to retain the spirit of MinireL lVEke Carey and I jointly designed much of\nMinibase. My lecture notes (and in turn this book) were influenced by Mike's\nlecture notes and by Yannis Ioannidis's lecture slides.\nJoe Hellerstein used the beta edition of the book at Berkeley and provided\ninvaluable feedback, assistance on slides, and hilarious quotes.\nvVriting the\nchapter on object-database systems with Joe was a lot of fun.\nC. Mohan provided invaluable assistance, patiently answering a number of ques-\ntions about implementation techniques used in various commercial systems, in\nparticular indexing, concurrency control, and recovery algorithms. Moshe Zloof\nanswered numerous questions about QBE semantics and commercial systems\nbased on QBE. Ron Fagin, Krishna Kulkarni, Len Shapiro, Jim Melton, Dennis\nShasha, and Dirk Van Gucht reviewed the book and provided detailed feedback,\ngreatly improving the content and presentation. Michael Goldweber at Beloit\nCollege, Matthew Haines at Wyoming, Michael Kifer at SUNY StonyBrook,\nJeff Naughton at Wisconsin, Praveen Seshadri at Cornell, and Stan Zdonik at\nBrown also used the beta edition in their database courses and offered feedback\nand bug reports. In particular, Michael Kifer pointed out an error in the (old)\nalgorithm for computing a minimal cover and suggested covering some SQL\nfeatures in Chapter 2 to improve modularity. Gio Wiederhold's bibliography,\nconverted to Latex format by S. Sudarshan, and Michael Ley's online bibliogra-\nphy on databases and logic programming were a great help while compiling the\nchapter bibliographies. Shaun Flisakowski and Uri Shaft helped me frequently\nin my never-ending battles with Latex.\nlowe a special thanks to the many, many students who have contributed to\nthe Minibase software. Emmanuel Ackaouy, Jim Pruyne, Lee Schumacher, and\nMichael Lee worked with me when I developed the first version of Minibase\n(much of which was subsequently discarded, but which influenced the next\nversion). Emmanuel Ackaouy and Bryan So were my TAs when I taught CS564\nusing this version and went well beyond the limits of a TAship in their efforts\nto refine the project.\nPaul Aoki struggled with a version of Minibase and\noffered lots of useful eomments as a TA at Berkeley. An entire class of CS764\nstudents (our graduate database course) developed much of the current version\nof Minibase in a large class project that was led and coordinated by Mike Carey\nand me. Amit Shukla and Michael Lee were my TAs when I first taught CS564\nusing this vers~on of Minibase and developed the software further.\nSeveral students worked with me on independent projects, over a long period\nof time, to develop Minibase components. These include visualization packages\nfor the buffer manager and B+ trees (Huseyin Bekta.'3, Harry Stavropoulos, and\nWeiqing Huang); a query optimizer and visualizer (Stephen Harris, Michael Lee,\nand Donko Donjerkovic); an ER diagram tool based on the Opossum schema\n\nxxx\nDATABASE NIANAGEMENT SYSTEMS\n~\neditor (Eben Haber); and a GUI-based tool for normalization (Andrew Prock\nand Andy Therber). In addition, Bill Kimmel worked to integrate and fix a\nlarge body of code (storage manager, buffer manager, files and access methods,\nrelational operators, and the query plan executor) produced by the CS764 class\nproject. Ranjani Ramamurty considerably extended Bill's work on cleaning up\nand integrating the various modules. Luke Blanshard, Uri Shaft, and Shaun\nFlisakowski worked on putting together the release version of the code and\ndeveloped test suites and exercises based on the Minibase software. Krishna\nKunchithapadam tested the optimizer and developed part of the Minibase GUI.\nClearly, the Minibase software would not exist without the contributions of a\ngreat many talented people. With this software available freely in the public\ndomain, I hope that more instructors will be able to teach a systems-oriented\ndatabase course with a blend of implementation and experimentation to com-\nplement the lecture material.\nI'd like to thank the many students who helped in developing and checking\nthe solutions to the exercises and provided useful feedback on draft versions of\nthe book. In alphabetical order: X. Bao, S. Biao, M. Chakrabarti, C. Chan,\nW. Chen, N. Cheung, D. Colwell, C. Fritz, V. Ganti, J. Gehrke, G. Glass, V.\nGopalakrishnan, M. Higgins, T. Jasmin, M. Krishnaprasad, Y. Lin, C. Liu, M.\nLusignan, H. Modi, S. Narayanan, D. Randolph, A. Ranganathan, J. Reminga,\nA. Therber, M. Thomas, Q. Wang, R. Wang, Z. Wang, and J. Yuan. Arcady\nGrenadeI', James Harrington, and Martin Reames at Wisconsin and Nina Tang\nat Berkeley provided especially detailed feedback.\nCharlie Fischer, Avi Silberschatz, and Jeff Ullman gave me invaluable advice\non working with a publisher. My editors at McGraw-Hill, Betsy Jones and Eric\nMunson, obtained extensive reviews and guided this book in its early stages.\nEmily Gray and Brad Kosirog were there whenever problems cropped up. At\nWisconsin, Ginny Werner really helped me to stay on top of things.\nFinally, this book was a thief of time, and in many ways it was harder on my\nfamily than on me. My sons expressed themselves forthrightly. From my (then)\nfive-year-old, Ketan: \"Dad, stop working on that silly book. You don't have\nany time for me.\" Two-year-old Vivek: \"You working boook? No no no come\nplay basketball me!\" All the seasons of their discontent were visited upon my\nwife, and Apu nonetheless cheerfully kept the family going in its usual chaotic,\nhappy way all the many evenings and weekends I was wrapped up in this book.\n(Not to mention the days when I was wrapped up in being a faculty member!)\nAs in all things, I can trace my parents' hand in much of this; my father,\nwith his love of learning, and my mother, with her love of us, shaped me. My\nbrother Kartik's contributions to this book consisted chiefly of phone calls in\nwhich he kept me from working, but if I don't acknowledge him, he's liable to\n\nPreface\nbe annoyed. I'd like to thank my family for being there and giving meaning to\neverything I do. (There! I knew I'd find a legitimate reason to thank Kartik.)\nAcknowledgments for the Second Edition\nEmily Gray and Betsy Jones at 1tfcGraw-Hill obtained extensive reviews and\nprovided guidance and support as we prepared the second edition. Jonathan\nGoldstein helped with the bibliography for spatial databases.\nThe following\nreviewers provided valuable feedback on content and organization: Liming Cai\nat Ohio University, Costas Tsatsoulis at University of Kansas, Kwok-Bun Vue\nat University of Houston, Clear Lake, William Grosky at Wayne State Univer-\nsity, Sang H. Son at University of Virginia, James M. Slack at Minnesota State\nUniversity, Mankato, Herman Balsters at University of Twente, Netherlands,\nKaren C. Davis at University of Cincinnati, Joachim Hammer at University of\nFlorida, Fred Petry at Tulane University, Gregory Speegle at Baylor Univer-\nsity, Salih Yurttas at Texas A&M University, and David Chao at San Francisco\nState University.\nA number of people reported bugs in the first edition. In particular, we wish\nto thank the following: Joseph Albert at Portland State University, Han-yin\nChen at University of Wisconsin, Lois Delcambre at Oregon Graduate Institute,\nMaggie Eich at Southern Methodist University, Raj Gopalan at Curtin Univer-\nsity of Technology, Davood Rafiei at University of Toronto, Michael Schrefl at\nUniversity of South Australia, Alex Thomasian at University of Connecticut,\nand Scott Vandenberg at Siena College.\nA special thanks to the many people who answered a detailed survey about how\ncommercial systems support various features: At IBM, Mike Carey, Bruce Lind-\nsay, C. Mohan, and James Teng; at Informix, M. Muralikrishna and Michael\nUbell; at Microsoft, David Campbell, Goetz Graefe, and Peter Spiro; at Oracle,\nHakan Jacobsson, Jonathan D. Klein, Muralidhar Krishnaprasad, and M. Zi-\nauddin; and at Sybase, Marc Chanliau, Lucien Dimino, Sangeeta Doraiswamy,\nHanuma Kodavalla, Roger MacNicol, and Tirumanjanam Rengarajan.\nAfter reading about himself in the acknowledgment to the first edition, Ketan\n(now 8) had a simple question: \"How come you didn't dedicate the book to us?\nWhy mom?\"\nK~tan, I took care of this inexplicable oversight. Vivek (now 5)\nwas more concerned about the extent of his fame: \"Daddy, is my name in evvy\ncopy of your book? Do they have it in evvy compooter science department in\nthe world'?\"\nVivek, I hope so. Finally, this revision would not have made it\nwithout Apu's and Keiko's support.\n\nxx.,xii\nDATABASE l\\IANAGEl'vIENT SYSTEMS\nAcknowledgments for the Third Edition\n\\rYe thank Raghav Kaushik for his contribution to the discussion of XML, and\nAlex Thomasian for his contribution to the coverage of concurrency control. A\nspecial thanks to Jim JVlelton for giving us a pre-publication copy of his book\non object-oriented extensions in the SQL: 1999 standard, and catching several\nbugs in a draft of this edition. Marti Hearst at Berkeley generously permitted\nus to adapt some of her slides on Information Retrieval, and Alon Levy and\nDan Sueiu were kind enough to let us adapt some of their lectures on X:NIL.\nMike Carey offered input on Web services.\nEmily Lupash at McGraw-Hill has been a source of constant support and en-\ncouragement. She coordinated extensive reviews from Ming Wang at Embry-\nRiddle Aeronautical University, Cheng Hsu at RPI, Paul Bergstein at Univ. of\nMassachusetts, Archana Sathaye at SJSU, Bharat Bhargava at Purdue, John\nFendrich at Bradley, Ahmet Ugur at Central Michigan, Richard Osborne at\nUniv.\nof Colorado, Akira Kawaguchi at CCNY, Mark Last at Ben Gurion,\nVassilis Tsotras at Univ. of California, and Ronald Eaglin at Univ. of Central\nFlorida. It is a pleasure to acknowledge the thoughtful input we received from\nthe reviewers, which greatly improved the design and content of this edition.\nGloria Schiesl and Jade Moran dealt cheerfully and efficiently with last-minute\nsnafus, and, with Sherry Kane, made a very tight schedule possible. Michelle\nWhitaker iterated many times on the cover and end-sheet design.\nOn a personal note for Raghu, Ketan, following the canny example of the\ncamel that shared a tent, observed that \"it is only fair\" that Raghu dedicate\nthis edition solely to him and Vivek, since \"mommy already had it dedicated\nonly to her.\" Despite this blatant attempt to hog the limelight, enthusiastically\nsupported by Vivek and viewed with the indulgent affection of a doting father,\nthis book is also dedicated to Apu, for being there through it all.\nFor Johannes, this revision would not have made it without Keiko's support\nand inspiration and the motivation from looking at Elisa's peacefully sleeping\nface.\n\nPART I\nFOUNDATIONS\n\n1\nOVERVIEW OF\nDATABASE SYSTEMS\n--\nWhat is a DBMS, in particular, a relational DBMS?\n..\nWhy should we consider a DBMS to manage data?\n..\nHow is application data represented in a DBMS?\n--\nHow is data in a DBMS retrieved and manipulated?\n..\nHow does a DBMS support concurrent access and protect data during\nsystem failures?\n..\nWhat are the main components of a DBMS?\n..\nWho is involved with databases in real life?\n..\nKey concepts: database management, data independence, database\ndesign, data model; relational databases and queries; schemas, levels\nof abstraction; transactions, concurrency and locking, recovery and\nlogging; DBMS architecture; database administrator, application pro-\ngrammer, end user\nHas everyone noticed that all the letters of the word database are typed with\nthe left hand? Now the layout of the QWEHTY typewriter keyboard was designed,\namong other things, to facilitate the even use of both hands. It follows, therefore,\nthat writing about databases is not only unnatural, but a lot harder than it appears.\n---Anonymous\nThe alIlount of information available to us is literally exploding, and the value\nof data as an organizational asset is widely recognized. To get the most out of\ntheir large and complex datasets, users require tools that simplify the tasks of\n3\n\n4\nCHAPTER If\nThe area of database management systenls is a microcosm of computer sci-\nence in general. The issues addressed and the techniques used span a wide\nspectrum, including languages, object-orientation and other progTamming\nparadigms, compilation, operating systems, concurrent programming, data\nstructures, algorithms, theory, parallel and distributed systems, user inter-\nfaces, expert systems and artificial intelligence, statistical techniques, and\ndynamic programming. \\Ve cannot go into all these &<;jpects of database\nmanagement in one book, but we hope to give the reader a sense of the\nexcitement in this rich and vibrant discipline.\nmanaging the data and extracting useful information in a timely fashion. Oth-\nerwise, data can become a liability, with the cost of acquiring it and managing\nit far exceeding the value derived from it.\nA database is a collection of data, typically describing the activities of one or\nmore related organizations. For example, a university database might contain\ninformation about the following:\n•\nEntities such as students, faculty, courses, and classrooms.\n•\nRelationships between entities, such as students' enrollment in courses,\nfaculty teaching courses, and the use of rooms for courses.\nA database management system, or DBMS, is software designed to assist\nin maintaining and utilizing large collections of data. The need for such systems,\nas well as their use, is growing rapidly. The alternative to using a DBMS is\nto store the data in files and write application-specific code to manage it. The\nuse of a DBMS has several important advantages, as we will see in Section 1.4.\n1.1\nMANAGING DATA\nThe goal of this book is to present an in-depth introduction to database man-\nagement systems, with an empha.sis on how to design a database and\n'li8C a\nDBMS effectively. Not surprisingly, many decisions about how to use a DBIvIS\nfor a given application depend on what capabilities the DBMS supports effi-\nciently. Therefore, to use a DBMS well, it is necessary to also understand how\na DBMS work8.\nMany kinds of database management systems are in use, but this book concen-\ntrates on relational database systems (RDBMSs), which are by far the\ndominant type of DB~'IS today. The following questions are addressed in the\ncorc chapters of this hook:\n\nOverview of Databa8e SY8tem8\n5\n1. Database Design and Application Development: How can a user\ndescribe a real-world enterprise (e.g., a university) in terms of the data\nstored in a DBMS? \\Vhat factors must be considered in deciding how to\norganize the stored data? How can ,ve develop applications that rely upon\na DBMS? (Chapters 2, 3, 6, 7, 19, 20, and 21.)\n2. Data Analysis: How can a user answer questions about the enterprise by\nposing queries over the data in the DBMS? (Chapters 4 and 5.)1\n3. Concurrency and Robustness: How does a DBMS allow many users to\naccess data concurrently, and how does it protect the data in the event of\nsystem failures? (Chapters 16, 17, and 18.)\n4. Efficiency and Scalability: How does a DBMS store large datasets and\nanswer questions against this data efficiently? (Chapters 8, 9, la, 11, 12,\n13, 14, and 15.)\nLater chapters cover important and rapidly evolving topics, such as parallel and\ndistributed database management, data warehousing and complex queries for\ndecision support, data mining, databases and information retrieval, XML repos-\nitories, object databases, spatial data management, and rule-oriented DBMS\nextensions.\nIn the rest of this chapter, we introduce these issues. In Section 1.2, we be-\ngin with a brief history of the field and a discussion of the role of database\nmanagement in modern information systems. We then identify the benefits of\nstoring data in a DBMS instead of a file system in Section 1.3, and discuss\nthe advantages of using a DBMS to manage data in Section 1.4. In Section\n1.5, we consider how information about an enterprise should be organized and\nstored in a DBMS. A user probably thinks about this information in high-level\nterms that correspond to the entities in the organization and their relation-\nships, whereas the DBMS ultimately stores data in the form of (rnany, many)\nbits. The gap between how users think of their data and how the data is ul-\ntimately stored is bridged through several levels of abstract1:on supported by\nthe DBMS. Intuitively, a user can begin by describing the data in fairly high-\nlevel terms, then refine this description by considering additional storage and\nrepresentation details as needed.\nIn Section 1.6, we consider how users can retrieve data stored in a DBMS and\nthe need for techniques to efficiently compute answers to questions involving\nsuch data. In Section 1.7, we provide an overview of how a DBMS supports\nconcurrent access to data by several users and how it protects the data in the\nevent of system failures.\n1An online chapter on Query-by-Example (QBE) is also available.\n\n6\nCHAPTERrl\nvVe then briefly describe the internal structure of a DBMS in Section 1.8, and\nmention various groups of people associated with the development and use of\na DBMS in Section 1.9.\n1.2\nA HISTORICAL PERSPECTIVE\nFrom the earliest days of computers, storing and manipulating data have been a\nmajor application focus. The first general-purpose DBMS, designed by Charles\nBachman at General Electric in the early 1960s, was called the Integrated Data\nStore. It formed the basis for the network data model, which was standardized\nby the Conference on Data Systems Languages (CODASYL) and strongly in-\nfluenced database systems through the 1960s. Bachman was the first recipient\nof ACM's Turing Award (the computer science equivalent of a Nobel Prize) for\nwork in the database area; he received the award in 1973.\nIn the late 1960s, IBM developed the Information Management System (IMS)\nDBMS, used even today in many major installations. IMS formed the basis for\nan alternative data representation framework called the hierarchical data model.\nThe SABRE system for making airline reservations was jointly developed by\nAmerican Airlines and IBM around the same time, and it allowed several people\nto access the same data through a computer network. Interestingly, today the\nsame SABRE system is used to power popular Web-based travel services such\nas Travelocity.\nIn 1970, Edgar Codd, at IBM's San Jose Research Laboratory, proposed a new\ndata representation framework called the relational data model. This proved to\nbe a watershed in the development of database systems: It sparked the rapid\ndevelopment of several DBMSs based on the relational model, along with a rich\nbody of theoretical results that placed the field on a firm foundation.\nCodd\nwon the 1981 Turing Award for his seminal work. Database systems matured\nas an academic discipline, and the popularity of relational DBMSs changed the\ncommercial landscape. Their benefits were widely recognized, and the use of\nDBMSs for managing corporate data became standard practice.\nIn the 1980s, the relational model consolidated its position as the dominant\nDBMS paradigm, and database systems continued to gain widespread use. The\nSQL query language for relational databases, developed as part of IBM's Sys-\ntem R project, is now the standard query language.\nSQL was standardized\nin the late 1980s, and the current standard, SQL:1999, was adopted by the\nAmerican National Standards Institute (ANSI) and International Organization\nfor Standardization (ISO). Arguably, the most widely used form of concurrent\nprogramming is the concurrent execution of database programs (called trans-\nactions).\nUsers write programs a.\" if they are to be run by themselves, and\n\nOverview of Database Systems\n7\nthe responsibility for running them concurrently is given to the DBl\\/IS. James\nGray won the 1999 Turing award for his contributions to database transaction\nmanagement.\nIn the late 1980s and the 1990s, advances were made in many areas of database\nsystems. Considerable research was carried out into more powerful query lan-\nguages and richer data models, with emphasis placed on supporting complex\nanalysis of data from all parts of an enterprise. Several vendors (e.g., IBM's\nDB2, Oracle 8, Informix2 UDS) extended their systems with the ability to store\nnew data types such as images and text, and to ask more complex queries. Spe-\ncialized systems have been developed by numerous vendors for creating data\nwarehouses, consolidating data from several databases, and for carrying out\nspecialized analysis.\nAn interesting phenomenon is the emergence of several enterprise resource\nplanning (ERP) and management resource planning (MRP) packages,\nwhich add a substantial layer of application-oriented features on top of a DBMS.\nWidely used. packages include systems from Baan, Oracle, PeopleSoft, SAP,\nand Siebel.\nThese packages identify a set of common tasks (e.g., inventory\nmanagement, human resources planning, financial analysis) encountered by a\nlarge number of organizations and provide a general application layer to carry\nout these ta.'3ks. The data is stored in a relational DBMS and the application\nlayer can be customized to different companies, leading to lower overall costs\nfor the companies, compared to the cost of building the application layer from\nscratch.\nMost significant, perhaps, DBMSs have entered the Internet Age. While the\nfirst generation of websites stored their data exclusively in operating systems\nfiles, the use of a DBMS to store data accessed through a Web browser is\nbecoming widespread.\nQueries are generated through Web-accessible forms\nand answers are formatted using a markup language such as HTML to be\neasily displayed in a browser. All the database vendors are adding features to\ntheir DBMS aimed at making it more suitable for deployment over the Internet.\nDatabclse management continues to gain importance as more and more data is\nbrought online and made ever more accessible through computer networking.\nToday the field is being driven by exciting visions such a'S multimedia databases,\ninteractive video, streaming data, digital libraries, a host of scientific projects\nsuch as the human genome mapping effort and NASA's Earth Observation Sys-\ntem project, and the desire of companies to consolidate their decision-making\nprocesses and mine their data repositories for useful information about their\nbusinesses. Commercially, database management systems represent one of the\n2Informix was recently acquired by IBM.\n\n8\nlargest and most vigorous market segments. Thus the study of database sys-\ntems could prove to be richly rewarding in more ways than one!\n1.3\nFILE SYSTEMS VERSUS A DBMS\nTo understand the need for a\nDB:~,,1S, let us consider a motivating scenario: A\ncompany has a large collection (say, 500 GB3 ) of data on employees, depart-\nments, products, sales, and so on. This data is accessed concurrently by several\nemployees. Questions about the data must be answered quickly, changes made\nto the data by different users must be applied consistently, and access to certain\nparts of the data (e.g., salaries) must be restricted.\nWe can try to manage the data by storing it in operating system files.\nThis\napproach has many drawbacks, including the following:\n•\nWe probably do not have 500 GB of main memory to hold all the data.\nWe must therefore store data in a storage device such as a disk or tape and\nbring relevant parts into main memory for processing as needed.\n•\nEven if we have 500 GB of main memory, on computer systems with 32-bit\naddressing, we cannot refer directly to more than about 4 GB of data. We\nhave to program some method of identifying all data items.\n•\nWe have to write special programs to answer each question a user may want\nto ask about the data. These programs are likely to be complex because\nof the large volume of data to be searched.\n•\nWe must protect the data from inconsistent changes made by different users\naccessing the data concurrently. If applications must address the details of\nsuch concurrent access, this adds greatly to their complexity.\n•\nWe must ensure that data is restored to a consistent state if the system\ncrac;hes while changes are being made.\n•\nOperating systems provide only a password mechanism for security. This is\nnot sufficiently flexible to enforce security policies in which different users\nhave permission to access different subsets of the data.\nA DBMS is a piece of software designed to make the preceding tasks easier. By\nstoring data in.a DBNIS rather than as a collection of operating system files,\nwe can use the DBMS's features to manage the data in a robust and efficient\nrnanner.\nAs the volume of data and the number of users grow hundreds of\ngigabytes of data and thousands of users are common in current corporate\ndatabases DBMS support becomes indispensable.\n------,-\n.\n3 A kilobyte (KB) is 1024 bytes, a megabyte (MB) is 1024 KBs, a gigabyte (GB) is 1024 MBs, a\nterabyte ('1'B) is 1024 CBs, and a petabyte (PB) is 1024 terabytes.\n\nOverv'iew of Database Systems\n1.4\nADVANTAGES OF A DBMS\nUsing a DBMS to manage data h3..'3 many advantages:\n9\nII\nData Independence: Application programs should not, ideally, be ex-\nposed to details of data representation and storage, The DBJVIS provides\nan abstract view of the data that hides such details.\nII\nEfficient Data Access: A DBMS utilizes a variety of sophisticated tech-\nniques to store and retrieve data efficiently. This feature is especially im-\npOl'tant if the data is stored on external storage devices.\nII\nData Integrity and Security: If data is always accessed through the\nDBMS, the DBMS can enforce integrity constraints. For example, before\ninserting salary information for an employee, the DBMS can check that\nthe department budget is not exceeded. Also, it can enforce access contmls\nthat govern what data is visible to different classes of users.\nII\nData Administration: When several users share the data, centralizing\nthe administration of data can offer sig11ificant improvements. Experienced\nprofessionals who understand the nature of the data being managed, and\nhow different groups of users use it, can be responsible for organizing the\ndata representation to minimize redundancy and for fine-tuning the storage\nof the data to make retrieval efficient.\nII\nConcurrent Access and Crash Recovery: A DBMS schedules concur-\nrent accesses to the data in such a manner that users can think of the data\nas being accessed by only one user at a time. Further, the DBMS protects\nusers from the effects of system failures.\nII\nReduced Application Development Time: Clearly, the DBMS sup-\nports important functions that are common to many applications accessing\ndata in the DBMS. This, in conjunction with the high-level interface to the\ndata, facilitates quick application development.\nDBMS applications are\nalso likely to be more robust than similar stand-alone applications because\nmany important tasks are handled by the DBMS (and do not have to be\ndebugged and tested in the application).\nGiven all these advantages, is there ever a reason not to use a DBMS? Some-\ntimes, yes. A DBMS is a complex piece of software, optimized for certain kinds\nof workloads (e.g., answering complex queries or handling many concurrent\nrequests), and its performance may not be adequate for certain specialized ap-\nplications.\nExamples include applications with tight real-time constraints or\njust a few well-defined critical operations for which efficient custom code must\nbe written. Another reason for not using a DBMS is that an application may\nneed to manipulate the data in ways not supported by the query language. In\n\n10\nCHAPTER:l\nsuch a situation, the abstract view of the datet presented by the DBlVIS does\nnot match the application's needs and actually gets in the way. As an exam-\nple, relational databa.'3es do not support flexible analysis of text data (although\nvendors are now extending their products in this direction).\nIf specialized performance or data manipulation requirements are central to an\napplication, the application may choose not to use a DBMS, especially if the\nadded benefits of a DBMS (e.g., flexible querying, security, concurrent access,\nand crash recovery) are not required. In most situations calling for large-scale\ndata management, however, DBlVISs have become an indispensable tool.\n1.5\nDESCRIBING AND STORING DATA IN A DBMS\nThe user of a DBMS is ultimately concerned with some real-world enterprise,\nand the data to be stored describes various aspects of this enterprise.\nFor\nexample, there are students, faculty, and courses in a university, and the data\nin a university database describes these entities and their relationships.\nA data model is a collection of high-level data description constructs that hide\nmany low-level storage details. A DBMS allows a user to define the data to be\nstored in terms of a data model. Most database management systems today\nare based on the relational data model, which we focus on in this book.\nWhile the data model of the DBMS hides many details, it is nonetheless closer\nto how the DBMS stores data than to how a user thinks about the underlying\nenterprise. A semantic data model is a more abstract, high-level data model\nthat makes it easier for a user to come up with a good initial description of\nthe data in an enterprise. These models contain a wide variety of constructs\nthat help describe a real application scenario.\nA DBMS is not intended to\nsupport all these constructs directly; it is typically built around a data model\nwith just a few bi:1Sic constructs, such as the relational model.\nA databa.se\ndesign in terms of a semantic model serves as a useful starting point and is\nsubsequently translated into a database design in terms of the data model the\nDBMS actually supports.\nA widely used semantic data model called the entity-relationship (ER) model\nallows us to pictorially denote entities and the relationships among them. vVe\ncover the ER model in Chapter 2.\n\nOverview of Database Systc'lns\n11\nJ\nAn Example of Poor Design: The relational schema for Students il-\nlustrates a poor design choice; you should neVCT create a field such as age,\nwhose value is constantly changing.\nA better choice would be DOB (for\ndate of birth); age can be computed from this. \\Ve continue to use age in\nour examples, however, because it makes them easier to read.\n1.5.1\nThe Relational Model\nIn this section we provide a brief introduction to the relational model.\nThe\ncentral data description construct in this model is a relation, which can be\nthought of as a set of records.\nA description of data in terms of a data model is called a schema.\nIn the\nrelational model, the schema for a relation specifies its name, the name of each\nfield (or attribute or column), and the type of each field. As an example,\nstudent information in a university database may be stored in a relation with\nthe following schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nThe preceding schema says that each record in the Students relation has five\nfields, with field names and types as indicated.\nAn example instance of the\nStudents relation appears in Figure 1.1.\nI sid\n[ name\nIZogin\n53666\nJones\njones@cs\n18\n3.4\n53688\nSmith\nsmith@ee\n18\n3.2\n53650\nSmith\nsmith@math\n19\n3.8\n53831\nMadayan\nmadayan(gmusic\n11\n1.8\n53832\nGuldu\nguldui:Qhnusic\n12\n2.0\nFigure 1.1\nAn Instance of the Students Relation\nEach row in the Students relation is a record that describes a student. The\ndescription is rlOt completeo----for example, the student's height is not included---\nbut is presumably adequate for the intended applications in the university\ndatabase. Every row follows the schema of the Students relation. The schema\ncall therefore be regarded as a template for describing a student.\nvVe can make the description of a collection of students more precise by specify-\ning integrity constraints, which are conditions that the records in a relation\n\n12\nCHAPTER? 1\nmust satisfy. for example, we could specify that every student has a unique\nsid value. Observe that we cannot capture this information by simply adding\nanother field to the Students schema. Thus, the ability to specify uniqueness\nof the values in a field increases the accuracy with which we can describe our\ndata.\nThe expressiveness of the constructs available for specifying integrity\nconstraints is an important ar;;pect of a data model.\nOther Data Models\nIn addition to the relational data model (which is used in numerous systems,\nincluding IBM's DB2, Informix, Oracle, Sybase, Microsoft's Access, FoxBase,\nParadox, Tandem, and Teradata), other important data models include the\nhierarchical model (e.g., used in IBM's IMS DBMS), the network model (e.g.,\nused in IDS and IDMS), the object-oriented model (e.g., used in Objectstore\nand Versant), and the object-relational model (e.g., used in DBMS products\nfrom IBM, Informix, ObjectStore, Oracle, Versant, and others). While many\ndatabases use the hierarchical and network models and systems based on the\nobject-oriented and object-relational models are gaining acceptance in the mar-\nketplace, the dominant model today is the relational model.\nIn this book, we focus on the relational model because of its wide use and im-\nportance. Indeed, the object-relational model, which is gaining in popularity, is\nan effort to combine the best features of the relational and object-oriented mod-\nels, and a good grasp of the relational model is necessary to understand object-\nrelational concepts. (We discuss the object-oriented and object-relational mod-\nels in Chapter 23.)\n1.5.2\nLevels of Abstraction in a DBMS\nThe data in a DBMS is described at three levels of abstraction, ar;; illustrated\nin Figure 1.2. The database description consists of a schema at each of these\nthree levels of abstraction: the conceptual, physical, and external.\nA data definition language (DDL) is used to define the external and coneep-\ntual schemas. \\;Ye discuss the DDL facilities of the Inost wid(~ly used database\nlanguage, SQL, in Chapter 3. All DBMS vendors also support SQL commands\nto describe aspects of the physical schema, but these commands are not part of\nthe SQL language standard. Information about the conceptual, external, and\nphysical schemas is stored in the system catalogs (Section 12.1). vVe discuss\nthe three levels of abstraction in the rest of this section.\n\nOucTlJ'iew of Database SyslcTns\nExternal Schema 1\nExternal Schema 2\nExternal Schema 3\nFigure 1.2\nLevels of Abstraction in a DBMS\nConceptual Schema\nThe conceptual schema (sometimes called the logical schema) describes the\nstored data in terms of the data model of the DBMS. In a relational DBMS,\nthe conceptual schema describes all relations that are stored in the database.\nIn our sample university databa..'3e, these relations contain information about\nentities, such as students and faculty, and about relationships, such as students'\nenrollment in courses.\nAll student entities can be described using records in\na Students relation, as we saw earlier. In fact, each collection of entities and\neach collection of relationships can be described as a relation, leading to the\nfollowing conceptual schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nFaculty(fid: string, fname: string, sal: real)\nCourses( cid: string, cname: string, credits: integer)\nRooms(nw: integer, address: string, capacity: integer)\nEnrolled(sid: string, cid: string, grade: string)\nTeaches(fid: string, cid: string)\nMeets_In( cid: string, rno: integer, ti'fne: string)\nThe choice of relations, and the choice of fields for each relation, is not always\nobvious, and the process of arriving at a good conceptual schema is called\nconceptual database design.\nvVe discuss conceptual databa..se design in\nChapters 2 and 19.\n\n14\nPhysical Schema\nCHAPTER»1\nThe physical schema specifies additional storage details.\nEssentially, the\nphysical schema summarizes how the relations described in the conceptual\nschema are actually stored on secondary storage devices such as disks and\ntapes.\nWe must decide what file organizations to use to store the relations and create\nauxiliary data structures, called indexes, to speed up data retrieval operations.\nA sample physical schema for the university database follows:\n•\nStore all relations as unsorted files of records. (A file in a DBMS is either\na collection of records or a collection of pages, rather than a string of\ncharacters as in an operating system.)\n•\nCreate indexes on the first column of the Students, Faculty, and Courses\nrelations, the sal column of Faculty, and the capacity column of Rooms.\nDecisions about the physical schema are based on an understanding of how the\ndata is typically accessed. The process of arriving at a good physical schema\nis called physical database design. We discuss physical database design in\nChapter 20.\nExternal Schema\nExternal schemas, which usually are also in terms of the data model of\nthe DBMS, allow data access to be customized (and authorized) at the level\nof individual users or groups of users.\nAny given database has exactly one\nconceptual schema and one physical schema because it has just one set of\nstored relations, but it may have several external schemas, each tailored to a\nparticular group of users. Each external schema consists of a collection of one or\nmore views and relations from the conceptual schema. A view is conceptually\na relation, but the records in a view are not stored in the DBMS. Rather, they\nare computed using a definition for the view, in terms of relations stored in the\nDBMS. \\iVe discuss views in more detail in Chapters 3 and 25.\nThe external schema design is guided by end user requirements. For exalnple,\nwe might want to allow students to find out the names of faculty members\nteaching courses as well as course enrollments. This can be done by defining\nthe following view:\nCourseinfo( rid: string, fname: string, enTollment: integer)\nA user can treat a view just like a relation and ask questions about the records\nin the view.\nEven though the records in the view are not stored explicitly,\n\nOverview of Database Systems\n).5\nthey are computed as needed. vVe did not include Courseinfo in the conceptual\nschema because we can compute Courseinfo from the relations in the conceptual\nschema, and to store it in addition would be redundant. Such redundancy, in\naddition to the wasted space, could lead to inconsistencies.\nFor example, a\ntuple may be inserted into the Enrolled relation, indicating that a particular\nstudent has enrolled in some course, without incrementing the value in the\nenrollment field of the corresponding record of Courseinfo (if the latter also is\npart of the conceptual schema and its tuples are stored in the DBMS).\nL5.3\nData Independence\nA very important advantage of using a DBMS is that it offers data indepen-\ndence. That is, application programs are insulated from changes in the way\nthe data is structured and stored. Data independence is achieved through use\nof the three levels of data abstraction; in particular, the conceptual schema and\nthe external schema provide distinct benefits in this area.\nRelations in the external schema (view relations) are in principle generated\non demand from the relations corresponding to the conceptual schema.4\nIf\nthe underlying data is reorganized, that is, the conceptual schema is changed,\nthe definition of a view relation can be modified so that the same relation is\ncomputed as before.\nFor example, suppose that the Faculty relation in our\nuniversity database is replaced by the following two relations:\nFaculty_public(fid: string, fname: string, office: integer)\nFaculty_private(J£d: string, sal: real)\nIntuitively, some confidential information about faculty has been placed in a\nseparate relation and information about offices has been added. The Courseinfo\nview relation can be redefined in terms of Faculty_public and Faculty_private,\nwhich together contain all the information in Faculty, so that a user who queries\nCourseinfo will get the same answers as before.\nThus, users can be shielded from changes in the logical structure of the data, or\nchanges in the choice of relations to be stored. This property is called logical\ndata independence.\nIn turn, the conceptual schema insulates users from changes in physical storage\ndetails. This property is referred to as physical data independence. The\nconceptual schema hides details such as how the data is actually laid out on\ndisk, the file structure, and the choice of indexes. As long as the conceptual\n4In practice, they could be precomputed and stored to speed up queries on view relations, but the\ncomputed view relations must be updated whenever the underlying relations are updated.\n\n16\nCHAPTE~ 1\nschema remains the same, we can change these storage details without altering\napplications. (Of course, performance might be affected by such changes.)\n1.6\nQUERIES IN A DBMS\nThe ease \\vith which information can be obtained from a database often de-\ntermines its value to a user. In contrast to older database systems, relational\ndatabase systems allow a rich class of questions to be posed easily; this feature\nhas contributed greatly to their popularity.\nConsider the sample university\ndatabase in Section 1.5.2. Here are some questions a user might ask:\n1. What is the name of the student with student ID 1234567\n2. What is the average salary of professors who teach course CS5647\n3. How many students are enrolled in CS5647\n4. What fraction of students in CS564 received a grade better than B7\n5. Is any student with a CPA less than 3.0 enrolled in CS5647\nSuch questions involving the data stored in a DBMS are called queries.\nA\nDBMS provides a specialized language, called the query language, in which\nqueries can be posed.\nA very attractive feature of the relational model is\nthat it supports powerful query languages. Relational calculus is a formal\nquery language based on mathematical logic, and queries in this language have\nan intuitive, precise meaning.\nRelational algebra is another formal query\nlanguage, based on a collection of operators for manipulating relations, which\nis equivalent in power to the calculus.\nA DBMS takes great care to evaluate queries as efficiently as possible.\nvVe\ndiscuss query optimization and evaluation in Chapters 12, Vl, and 15.\nOf\ncourse, the efficiency of query evaluation is determined to a large extent by\nhow the data is stored physically.\nIndexes can be used to speed up many\nqueries----in fact, a good choice of indexes for the underlying relations can speed\nup each query in the preceding list. \\Ve discuss data storage and indexing in\nChapters 8, 9, 10, and 11.\nA DBMS enables users to create, modify, and query data through a data\nmanipulation language (DML). Thus, the query language is only one part\nof the Dl\\ilL, which also provides constructs to insert, delete, and modify data,.\nvVe will discuss the DML features of SQL in Chapter 5. The DML and DDL\nare collectively referred to cl.s the data sublanguage when embedded within\na host language (e.g., C or COBOL).\n\nOverview of Database Systems\n1.7\nTRANSACTION MANAGEMENT\nConsider a database that holds information about airline reservations. At any\ngiven instant, it is possible (and likely) that several travel agents are look-\ning up information about available seats OIl various flights and making new\nseat reservations. When several users access (and possibly modify) a database\nconcurrently, the DBMS must order their requests carefully to avoid conflicts.\nFor example, when one travel agent looks up Flight 100 on some given day\nand finds an empty seat, another travel agent may simultaneously be making\na reservation for that seat, thereby making the information seen by the first\nagent obsolete.\nAnother example of concurrent use is a bank's database.\nWhile one user's\napplication program is computing the total deposits, another application may\ntransfer money from an account that the first application has just 'seen' to an\naccount that has not yet been seen, thereby causing the total to appear larger\nthan it should be.\nClearly, such anomalies should not be allowed to occur.\nHowever, disallowing concurrent access can degrade performance.\nFurther, the DBMS must protect users from the effects of system failures by\nensuring that all data (and the status of active applications) is restored to a\nconsistent state when the system is restarted after a crash. For example, if a\ntravel agent asks for a reservation to be made, and the DBMS responds saying\nthat the reservation has been made, the reservation should not be lost if the\nsystem crashes.\nOn the other hand, if the DBMS has not yet responded to\nthe request, but is making the necessary changes to the data when the crash\noccurs, the partial changes should be undone when the system comes back up.\nA transaction is anyone execution of a user program in a DBMS. (Executing\nthe same program several times will generate several transactions.) This is the\nbasic unit of change as seen by the DBMS: Partial transactions are not allowed,\nand the effect of a group of transactions is equivalent to some serial execution\nof all transactions.\nvVe briefly outline how these properties are guaranteed,\ndeferring a detailed discussion to later chapters.\n1.7.1\nConcurrent Execution of Transactions\nAn important task of a DBMS is to schedule concurrent accesses to data so\nthat each user can safely ignore the fact that others are accessing the data\nconcurrently. The importance of this ta.sk cannot be underestimated because\na database is typically shared by a large number of users, who submit their\nrequests to the DBMS independently and simply cannot be expected to deal\nwith arbitrary changes being made concurrently by other users.\nA DBMS\n\n18\nCHAPTER a.\nallows users to think of their programs &'3 if they were executing in isolation,\none after the other in some order chosen by the DBJ\\;:IS. For example, if a\nprogTam that deposits cash into an account is submitted to the DBMS at the\nsame time as another program that debits money from the same account, either\nof these programs could be run first by the DBMS, but their steps will not be\ninterleaved in such a way that they interfere with each other.\nA locking protocol is a set of rules to be followed by each transaction (and en-\nforced by the DBMS) to ensure that, even though actions of several transactions\nmight be interleaved, the net effect is identical to executing all transactions in\nsome serial order. A lock is a mechanism used to control access to database\nobjects.\nTwo kinds of locks are commonly supported by a DBMS: shared\nlocks on an object can be held by two different transactions at the same time,\nbut an exclusive lock on an object ensures that no other transactions hold\nany lock on this object.\nSuppose that the following locking protocol is followed: Every transaction be-\ngins by obtaining a shared lock on each data object that it needs to read and an\nexclusive lock on each data object that it needs to\nrnod~fy, then releases all its\nlocks after completing all actions. Consider two transactions T1 and T2 such\nthat T1 wants to modify a data object and T2 wants to read the same object.\nIntuitively, if T1's request for an exclusive lock on the object is granted first,\nT2 cannot proceed until T1 relea..':les this lock, because T2's request for a shared\nlock will not be granted by the DBMS until then. Thus, all of T1's actions will\nbe completed before any of T2's actions are initiated. We consider locking in\nmore detail in Chapters 16 and 17.\n1.7.2\nIncomplete Transactions and System Crashes\nTransactions can be interrupted before running to completion for a va,riety of\nreasons, e.g., a system crash. A DBMS must ensure that the changes made by\nsuch incomplete transactions are removed from the database. For example, if\nthe DBMS is in the middle of transferring money from account A to account\nB and has debited the first account but not yet credited the second when the\ncrash occurs, the money debited from account A must be restored when the\nsystem comes back up after the crash.\nTo do so, the DBMS maintains a log of all writes to the database. A crucial\nproperty of the log is that each write action must be recorded in the log (on disk)\nbefore the corresponding change is reflected in the database itself--otherwise, if\nthe system crcLShes just after making the change in the datab(Lse but before the\nchange is recorded in the log, the DBIVIS would be unable to detect and undo\nthis change. This property is called Write-Ahead Log, or WAL. To ensure\n\nOverview of Database By.stems\n19\nthis property, the DBMS must be able to selectively force a page in memory to\ndisk.\nThe log is also used to ensure that the changes made by a successfully com-\npleted transaction are not lost due to a system crash, as explained in Chapter\n18.\nBringing the database to a consistent state after a system crash can be\na slow process, since the DBMS must ensure that the effects of all transac-\ntions that completed prior to the crash are restored, and that the effects of\nincomplete transactions are undone. The time required to recover from a crash\ncan be reduced by periodically forcing some information to disk; this periodic\noperation is called a checkpoint.\n1.7.3\nPoints to Note\nIn summary, there are three points to remember with respect to DBMS support\nfor concurrency control and recovery:\n1. Every object that is read or written by a transaction is first locked in shared\nor exclusive mode, respectively.\nPlacing a lock on an object restricts its\navailability to other transactions and thereby affects performance.\n2. For efficient log maintenance, the DBMS must be able to selectively force\na collection of pages in main memory to disk. Operating system support\nfor this operation is not always satisfactory.\n3. Periodic checkpointing can reduce the time needed to recover from a crash.\nOf course, this must be balanced against the fact that checkpointing too\noften slows down normal execution.\n1.8\nSTRUCTURE OF A DBMS\nFigure 1.3 shows the structure (with some simplification) of a typical DBMS\nbased on the relational data model.\nThe DBMS accepts SQL comma,nels generated from a variety of user interfaces,\nproduces query evaluation plans, executes these plans against the databc4'le, and\nreturns the answers. (This is a simplification: SQL commands can be embedded\nin host-language application programs, e.g., Java or COBOL programs.\nvVe\nignore these issues to concentrate on the core DBl\\ilS functionality.)\nvVhen a user issues a query, the parsed query is presented to a query opti-\nmizer, which uses information about how the data is stored to produce an\nefficient execution plan for evaluating the query.\nAn execution plan is a\n\n20\nCHAPTER 1\nUnsophisticated users (customers, travel agents, etc.)\nSophisticated users. application\nprogrammers, DB administrators\nDBMS\n-\nshows references\nDATABASE\nRecovery\nManager\nshov.':$ command now\nSQL Interla<:e\n\\\nSystem Catalog\nData Files .--/\nPlan Executor\nOperator Evaluator\n[C-l\nInd,\"'I~---\"\"\n'----------,-~-~~-~ - --- ---,.--------.--\nQuery\nEvaluation\nL::::=======~=========::',J'Engine\nFigure 1.3\nArchitecture of a DBMS\nblueprint for evaluating a query, usually represented as a tree of relational op-\nerators (with annotations that contain additional detailed information about\nwhich access methods to use, etc.). We discuss query optimization in Chapters\n12 and 15.\nRelational operators serve as the building blocks for evaluating\nqueries posed against the data. The implementation of these operators is dis-\ncussed in Chapters 12 and 14.\nThe code that implements relational operators sits on top of the file and access\nmethods layer. This layer supports the concept of a file, which, in a DBMS, is a\ncollection of pages or a collection of records. Heap files, or files of unordered\npages, a:s well as indexes are supported.\nIn addition to keeping track of the\npages in a file, this layer organizes the information within a page.\nFile and\npage level storage issues are considered in Chapter 9. File organizations and\nindexes are cQIlsidered in Chapter 8.\nThe files and access methods layer code sits on top of the buffer manager,\nwhich brings pages in from disk to main memory ct.\" needed in response to read\nrequests. Buffer management is discussed in Chapter 9.\n\nOve1'Fie'll} of Database SY.'3te'171S\n2).\nThe lowest layer of the DBMS software deals with management of space on\ndisk, where the data is stored.\nHigher layers allocate, deallocate, read, and\nwrite pages through (routines provided by) this layer, called the disk space\nmanager. This layer is discussed in Chapter 9.\nThe DBMS supports concurrency and crash recovery by carefully scheduling\nuser requests and maintaining a log of all changes to the database. DBNIS com-\nponents associated with concurrency control and recovery include the trans-\naction manager, which ensures that transactions request and release locks\naccording to a suitable locking protocol and schedules the execution transac-\ntions; the lock manager, which keeps track of requests for locks and grants\nlocks on database objects when they become available; and the recovery man-\nager, which is responsible for maintaining a log and restoring the system to a\nconsistent state after a crash. The disk space manager, buffer manager, and\nfile and access method layers must interact with these components. We discuss\nconcurrency control and recovery in detail in Chapter 16.\n1.9\nPEOPLE WHO WORK WITH DATABASES\nQuite a variety of people are associated with the creation and use of databases.\nObviously, there are database implementors, who build DBMS software,\nand end users who wish to store and use data in a DBMS. Dat,abase imple-\nmentors work for vendors such as IBM or Oracle. End users come from a diverse\nand increasing number of fields. As data grows in complexity ant(volume, and\nis increasingly recognized as a major asset, the importance of maintaining it\nprofessionally in a DBMS is being widely accepted. Many end user.s simply use\napplications written by database application programmers (see below) and so\nrequire little technical knowledge about DBMS software. Of course, sophisti-\ncated users who make more extensive use of a DBMS, such as writing their own\nqueries, require a deeper understanding of its features.\nIn addition to end users and implementors, two other cla.'3ses of people are\nassociated with a DBMS: application programmer-s and database administrators.\nDatabase application programmers develop packages that facilitate data\naccess for end users, who are usually not computer professionals, using the\nhost or data languages and software tools that DBMS vendors provide. (Such\ntools include report writers, spreadsheets, statistical packages, and the like.)\nApplication programs should ideally access data through the external schema.\nIt is possible to write applications that access data at a lower level, but such\napplications would comprornise data independence.\n\n22\nCHAPTEI~ 1\nA personal databa'3e is typically maintained by the individual who owns it and\nuses it. However, corporate or enterprise-wide databases are typically impor-\ntant enough and complex enough that the task of designing and maintaining the\ndatabase is entrusted to a professional, called the database administrator\n(DBA). The DBA is responsible for many critical tasks:\nIII\nDesign of the Conceptual and Physical Schemas: The DBA is re-\nsponsible for interacting with the users of the system to understand what\ndata is to be stored in the DBMS and how it is likely to be used. Based on\nthis knowledge, the DBA must design the conceptual schema (decide what\nrelations to store) and the physical schema (decide how to store them).\nThe DBA may also design widely used portions of the external schema, al-\nthough users probably augment this schema by creating additional views.\nIII\nSecurity and Authorization: The DBA is responsible for ensuring that\nunauthorized data access is not permitted. In general, not everyone should\nbe able to access all the data. In a relational DBMS, users can be granted\npermission to access only certain views and relations.\nFor example, al-\nthough you might allow students to find out course enrollments and who\nteaches a given course, you would not want students to see faculty salaries\nor each other's grade information.\nThe DBA can enforce this policy by\ngiving students permission to read only the Courseinfo view.\nIII\nData Availability and Recovery from Failures: The DBA must take\nsteps to ensure that if the system fails, users can continue to access as much\nof the uncorrupted data as possible. The DBA must also work to restore\nthe data to a consistent state. The DB.I\\!IS provides software support for\nthese functions, but the DBA is responsible for implementing procedures\nto back up the data periodically and maintain logs of system activity (to\nfacilitate recovery from a crash).\nl'il\nDatabase Tuning: Users' needs are likely to evolve with time. The DBA\nis responsible for modifying the database, in particular the conceptual and\nphysical schemas, to ensure adequate performance as requirements change.\n1.10\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\nvVhat are the main benefits of using a DBMS to manage data in applica-\ntions involving extensive data access? (Sections 1.1, 1.4)\nIII\nvVhen would you store data in a DBMS instead of in operating system files\nand vice-versa? (Section 1.3)\n\nOver-view of Database Systems\n23\n9\n•\nWhat is a data model? \\Vhat is the relational data model? What is data\nindependence and how does a DBNIS support it? (Section 1.5)\n•\nExplain the advantages of using a query language instead of custom pro-\ngrams to process data. (Section 1.6)\n•\nWhat is a transaction? \\Vhat guarantees does a DBMS offer with respect\nto transactions? (Section 1.7)\n•\nWhat are locks in a DBMS, and why are they used? What is write-ahead\nlogging, and why is it used? What is checkpointing and why is it used?\n(Section 1.7)\n•\nIdentify the main components in a DBMS and briefly explain what they\ndo. (Section 1.8)\n•\nExplain the different roles of database administrators, application program-\nmers, and end users of a database. Who needs to know the most about\ndatabase systems? (Section 1.9)\nEXERCISES\nExercise 1.1 Why would you choose a database system instead of simply storing data in\noperating system files? When would it make sense not to use a database system?\nExercise 1.2 What is logical data independence and why is it important?\nExercise 1.3 Explain the difference between logical and physical data independence.\nExercise 1.4 Explain the difference between external, internal, and conceptual schemas.\nHow are these different schema layers related to the concepts of logical and physical data\nindependence?\nExercise 1.5 What are the responsibilities of a DBA? If we assume that the DBA is never\ninterested in running his or her own queries, does the DBA still need to understand query\noptimization? Why?\nExercise 1.6 Scrooge McNugget wants to store information (names, addresses, descriptions\nof embarrassing moments, etc.) about the many ducks on his payroll. Not surprisingly, the\nvolume of data compels him to buy a database system. To save money, he wants to buy one\nwith the fewest possible features, and he plans to run it as a stand-alone application on his\nPC clone. Of course, Scrooge does not plan to share his list with anyone. Indicate which of\nthe following DBMS features Scrooge should pay for; in each case, also indicate why Scrooge\nshould (or should not) pay for that feature in the system he buys.\n1. A security facility.\n2. Concurrency control.\n3. Crash recovery.\n4. A view mechanism.\n\n24\nCHAPTER 1\n5. A query language.\nExercise 1.1 Which of the following plays an important role in representing information\nabout the real world in a database'? Explain briefly.\n1. The data definition language.\n2. The data manipulation language.\n3. The buffer manager.\n4. The data model.\nExercise 1.8 Describe the structure of a DBMS. If your operating system is upgraded to\nsupport some new functions on as files (e.g., the ability to force some sequence of bytes to\ndisk), which layer(s) of the DBMS would you have to rewrite to take advantage of these new\nfunctions?\nExercise 1.9 Answer the following questions:\n1. What is a transaction?\n2. Why does a DBMS interleave the actions of different transactions instead of executing\ntransactions one after the other?\n3. What must a user guarantee with respect to a transaction and database consistency?\nWhat should a DBMS guarantee with respect to concurrent execution of several trans-\nactions and database consistency'?\n4. Explain the strict two-phase locking protocol.\n5. What is the WAL property, and why is it important?\nPROJECT-BASED EXERCISES\nExercise 1.10 Use a Web browser to look at the HTML documentation for Minibase. Try\nto get a feel for the overall architecture.\nBIBLIOGRAPHIC NOTES\nThe evolution of database management systems is traced in [289]. The use of data models\nfor describing real-world data is discussed in [423], and [425] contains a taxonomy of data\nmodels.\nThe three levels of abstraction were introduced in [186, 712].\nThe network data\nmodel is described in [186], and [775] discusses several commercial systems based on this\nmodel.\n[721] contains a good annotated collection of systems-oriented papers on database\nmanagement.\nOther texts covering database management systems include [204, 245, 305,\n3;~9, 475, 574,\n689, 747, 762].\n[204] provides a detailed discussion of the relational model from a concep-\ntual standpoint and is notable for its extensive annotated bibliography.\n[574] presents a\nperformance-oriented perspective, with references to several commercial systems.\n[245] and\n[689] offer broad coverage of databa,se system concepts, including a discussion of the hierar-\nchical and network data models.\n[339] emphasizes the connection between database query\nlanguages and logic programming. [762] emphasizes data models. Of these texts, [747] pro-\nvides the most detailed discussion of theoretical issues. Texts devoted to theoretical aspects\ninclude [3, 45, 501]. Handbook [744] includes a section on databases that contains introductory\nsurvey articles on a number of topics.\n\n2\nINTRODUCTION TO\nDATABASE DESIGN\n..\nWhat are the steps in designing a database?\n..\nWhy is the ER model used to create an initial design?\n..\nWhat are the main concepts in the ER model?\n..\nWhat are guidelines for using the ER model effectively?\n..\nHow does database design fit within the overall design framework for\ncomplex software within large enterprises?\n..\nWhat is UML and how is it related to the ER model?\n..\nKey concepts:\ndatabase design, conceptual, logical, and physical\ndesign; entity-relationship (ER) model, entity set, relationship set,\nattribute, instance, key; integrity constraints, one-to-many and many-\nto-many relationships, participation constraints; weak entities, class\nhierarchies, aggregation; UML, class diagrams, clataba,se diagrams,\ncomponent diagrams.\nThe great successful men of the \\vorld have used their imaginations.\nThey\nthink ahead and create their mental picture. and then go to work materializing that\npicture in all its details, filling in here, adding a little there, altering this bit and\nthat bit, but steadily building, steadily building.\nRobert Collier\nThe (~ntitY-T'd(ltion8hip(ER) data 'model allows us to describe the data involved\nin a real-world enterprise in terms of objects and their relationships and is\nwidely used to (levelop an initial databa.'3e design. It provides useful eoncepts\nthat allow us to move fronl an informal description of what users we:mt 1'rorn\n25\n\n26\nCHAPTEij, 2\ntheir database to a more detailed, precise description that can be implemented\nin a DBMS. In this chapter, we introduce the ER model and discuss how its\nfeatures allow us to model a wide range of data faithfully.\n\\Ve begin with an overview of databa...')e design in Section 2.1 in order to motivate\nour discussion of the ER model. \\Vithin the larger context of the overall design\nprocess, the ER model is used in a phase called conceptual database design.\n\\Ve then introduce the ER model in Sections 2.2, 2.3, and 2.4. In Section 2.5,\nwe discuss database design issues involving the ER model. We briefly discuss\nconceptual database design for large enterprises in Section 2.6. In Section 2.7,\nwe present an overview of UML, a design and modeling approach that is more\ngeneral in its scope than the ER model.\nIn Section 2.8, we introduce a case study that is used as a running example\nthroughout the book. The case study is an end-to-end database design for an\nInternet shop. We illustrate the first two steps in database design (requirements\nanalysis and conceptual design) in Section 2.8. In later chapters, we extend this\ncase study to cover the remaining steps in the design process.\nWe note that many variations of ER diagrams are in use and no widely accepted\nstandards prevail.\nThe presentation in this chapter is representative of the\nfamily of ER models and includes a selection of the most popular features.\n2.1\nDATABASE DESIGN AND ER DIAGRAMS\nWe begin our discussion of database design by observing that this is typically\njust one part, although a central part in data-intensive applications, of a larger\nsoftware system design. Our primary focus is the design of the database, how-\never, and we will not discuss other aspects of software design in any detail. We\nrevisit this point in Section 2.7.\nThe database design process can be divided into six steps. The ER model is\nmost relevant to the first three steps.\n1. Requirements Analysis:\nThe very first step in designing a database\napplication is to understand what data is to be stored in the database,\nwhat applications must be built on top of it, and what operations are\nmost frequent and subject to performance requirements. In other words,\nwe must find out what the users want from the database. This is usually\nan informal process that involves discussions with user groups, a study\nof the current operating environment and how it is expected to change,\nanalysis of any available documentation on existing applications that are\nexpected to be replaced or complemented by the database, and so OIl.\n\nIntToduct'ion to Database Design\n27.\nDatabase Design Tools: Design tools are available from RDBwiS ven-\ndors as well as third-party vendors. For example! see the following link for\ndetails on design and analysis tools from Sybase:\nhttp://www.sybase.com/products/application_tools\nThe following provides details on Oracle's tools:\nhttp://www.oracle.com/tools\nSeveral methodologies have been proposed for organizing and presenting\nthe information gathered in this step, and some automated tools have been\ndeveloped to support this process.\n2. Conceptual Database Design: The information gathered in the require-\nments analysis step is used to develop a high-level description of the data\nto be stored in the database, along with the constraints known to hold over\nthis data. This step is often carried out using the ER model and is dis-\ncussed in the rest of this chapter. The ER model is one of several high-level,\nor semantic, data models used in database design. The goal is to create\na simple description of the data that closely matches how users and devel-\nopers think of the data (and the people and processes to be represented in\nthe data). This facilitates discussion among all the people involved in the\ndesign process, even those who have no technical background. At the same\ntime, the initial design must be sufficiently precise to enable a straightfor-\nward translation into a data model supported by a commercial database\nsystem (which, in practice, means the relational model).\n3. Logical Database Design:\nWe must choose a DBMS to implement\nour databctse design, and convert the conceptual database design into a\ndatabase schema in the data model of the chosen DBMS. We will consider\nonly relational DBMSs, and therefore, the task in the logical design step\nis to convert an ER schema into a relational database schema.\nWe dis-\ncuss this step in detail in Chapter 3; the result is a conceptual schema,\nsometimes called the logical schema, in the relational data model.\n2.1.1\nBeyond ER Design\nThe ER diagram is just an approximate description of the data, constructed\nthrough a subjective evaluation of the information collected during require-\nments analysis.\nA more careful analysis can often refine the logical schema\nobtained at the end of Step 3. Once we have a good logical schema, we must\nconsider performance criteria and design the physical schema. Finally, we must\naddress security issues and ensure that users are able to access the data they\nneed, but not data that we wish to hide from them. The remaining three steps\nof clatabase design are briefly described next:\n\n28\nCHAPTER. 2\n4. Schema Refinement: The fourth step ill databa')e design is to analyze\nthe collection of relations in our relational database schema to identify po-\ntential problems, and to refine it. In contrast to the requirements analysis\nand conceptual design steps, which are essentially subjective, schema re-\nfinement can be guided by some elegant and powerful theory. \\Ve discuss\nthe theory of normalizing relations-restructuring them to ensure some\ndesirable properties-in Chapter 19.\n5. Physical Database Design: In this step, we consider typical expected\nworkloads that our database must support and further refine the database\ndesign to ensure that it meets desired performance criteria. This step may\nsimply involve building indexes on some tables and clustering some tables,\nor it may involve a substantial redesign of parts of the database schema\nobtained from the earlier design steps.\nWe discuss physical design and\ndatabase tuning in Chapter 20.\n6. Application and Security Design: Any software project that involves\na DBMS must consider aspects of the application that go beyond the\ndatabase itself.\nDesign methodologies like UML (Section 2.7) try to ad-\ndress the complete software design and development cycle. Briefly, we must\nidentify the entities (e.g., users, user groups, departments) and processes\ninvolved in the application. We must describe the role of each entity in ev-\nery process that is reflected in some application task, as part of a complete\nworkflow for that task.\nFor each role, we must identify the parts of the\ndatabase that must be accessible and the parts of the database that must\nnot be accessible, and we must take steps to ensure that these access rules\nare enforced. A DBMS provides several mechanisms to assist in this step,\nand we discuss this in Chapter 21.\nIn the implementation phase, we must code each task in an application lan-\nguage (e.g., Java), using the DBlVIS to access data.\nWe discuss application\ndevelopment in Chapters 6 and 7.\nIn general, our division of the design process into steps should be seen as a\nclassification of the kinds of steps involved in design.\nRealistically, although\nwe might begin with the six step process outlined here, a complete database\ndesign will probably require a subsequent tuning phase in which all six kinds\nof design steps are interleaved and repeated until the design is satisfactory.\n2.2\nENTITIES, ATTRIBUTES, AND ENTITY SETS\nAn entity is an object in the real world that is distinguishable frQm other\nobjects.\nExamples include the following: the Green Dragonzord toy, the toy\ndepartment, the manager of the toy department, the home address of the rnan-\n\nIntrod'lJ.ctioTt to Database De8'(qn\nagel' of the toy department. It is often useful to identify a collection of similar\nentities. Such a collection is called an entity set. Note that entity sets need\nnot be disjoint; the collection of toy department employees and the collection\nof appliance department employees may both contain employee John Doe (who\nhappens to work in both departments). \\Ve could also define an entity set called\nEmployees that contains both the toy and appliance department employee sets.\nAn entity is described using a set of attributes. All entities in a given entity\nset have the same attributes; this is what we mean by similar. (This statement\nis an oversimplification, as we will see when we discuss inheritance hierarchies\nin Section 2.4.4, but it suffices for now and highlights the main idea.)\nOur\nchoice of attributes reflects the level of detail at which we wish to represent\ninformation about entities.\nFor example, the Employees entity set could use\nname, social security number (ssn), and parking lot (lot) as attributes. In this\ncase we will store the name, social security number, and lot number for each\nemployee. However, we will not store, say, an employee's address (or gender or\nage).\nFor each attribute associated with an entity set, we must identify a domain of\npossible values. For example, the domain associated with the attribute name\nof Employees might be the set of 20-character strings. 1 As another example, if\nthe company rates employees on a scale of 1 to 10 and stores ratings in a field\ncalled mting, the associated domain consists of integers 1 through 10. FUrther,\nfor each entity set, we choose a key. A key is a minimal set of attributes whose\nvalues uniquely identify an entity in the set. There could be more than one\ncandidate key; if so, we designate one of them as the primary key. For now we\nassume that each entity set contains at least one set of attributes that uniquely\nidentifies an entity in the entity set; that is, the set of attributes contains a key.\nWe revisit this point in Section 2.4.3.\nThe Employees entity set with attributes ssn, name, and lot is shown in Figure\n2.1. An entity set is represented by a rectangle, and an attribute is represented\nby an oval.\nEach attribute in the primary key is underlined.\nThe domain\ninformation could be listed along with the attribute name, but we omit this to\nkeep the figures compact. The key is s.m.\n2.3\nREL~TIONSHIPS AND RELATIONSHIP SETS\nA relationship is an association among two or more entities. For example, we\nmay have the relationship that Attishoo works in the pharmacy department.\niTo avoid confusion, we assume that attribute names do not repeat across entity sets. This is not\na real limitation because we can always use the entity set name to resolve ambiguities if the same\nattribute name is used in more than one entity set.\n\n30\nCHAPTER 2\nFigure 2.1\nThe Employees Entity Set\nAs with entities, we may wish to collect a set of similar relationships into a\nrelationship set. A relationship set can be thought of as a set of n-tuples:\nEach n-tuple denotes a relationship involving n entities el through en, where\nentity ei is in entity set Ei . In Figure 2.2 we show the relationship set Works_In,\nin which each relationship indicates a department in which an employee works.\nNote that several relationship sets might involve the same entity sets.\nFor\nexample, we could also have a Manages relationship set involving Employees\nand Departments.\nFigure 2.2\nThe Works-ln Relationship Set\nA relationship can also have descriptive attributes. Descriptive attributes\nare used to record information about the relationship, rather than about any\none of the participating entities; for example, we may wish to record that At-\ntishoo works in the pharmacy department as of January 1991. This information\nis captured in Figure 2.2 by adding an attribute, since, to Works_In. A relation-\nship must be uniquely identified by the participating entities, without reference\nto the descriptive attributes. In the Works_In relationship set, for example, each\nWorks_In relationship must be uniquely identified by the combination of em-\nployee ssn and department d'id. Thus, for a given employee-department pair,\nwe cannot have more than one associated since value.\nAn instance of a relationship set is a set of relationships.\nIntuitively, an\ninstance can be thought of &'3 a 'snapshot' of the relationship set at some instant\n\nIntroduction to Database Design\n;31\nin time. An instance of the vVorks.ln relationship set is shown in Figure 2.3.\nEach Employees entity is denoted by its ssn, and each Departments entity\nis denoted by its did, for simplicity.\nThe since value is shown beside each\nrelationship. (The 'many-te-many' and 'total participation' comments in the\nfigure are discussed later, when we discuss integrity constraints.)\n~__I---\\-~---r-~5J\n---IIL---t----j-----W ~\n--.__---t-----\\I-:::::~~\nEMPLOYEES\nTotal participation\nWORKS_IN\nMany to Many\nDEPARTMENTS\nTotal participation\nFigure 2.3\nAn Instance of the Works_In Relationship Set\nAs another example of an ER diagram, suppose that each department has offices\nin several locations and we want to record the locations at which each employee\nworks.\nThis relationship is ternary because we must record an association\nbetween an employee, a department, and a location. The ER diagram for this\nvariant of Works_In, which we call Works.ln2, is shown in Figure 2.4.\nFigure 2.4\nA Ternary Relationship Set\nThe entity sets that participate in a relationship set need not be distinct; some-\ntimes a relationship might involve two entities in the same entity set. For ex-\nample, consider the Reports_To relationship set shown in Figure 2.5.\nSince\n\n32\nCHAPTER 2\nemployees report. to other employees, every relationship in Reports_To is of\nthe form (emlJ1. emp2) , where both empl and empz are entities in Employees.\nHowever, they play different roles:\nernpl reports to the managing employee\nemp2, which is reflected in the role indicators supervisor and subordinate in\nFigure 2.5. If an entity set plays more than one role, the role indicator concate-\nnated with an attribute name from the entity set gives us a unique name for\neach attribute in the relationship set. For example, the Reports_To relation-\nship set has attributes corresponding to the ssn of the supervisor and the ssn\nof the subordinate, and the names of these attributes are supcrvisoLssn and\nsubordinate-ssn.\nFigure 2.5\nThe Reports_To Relationship Set\n2.4\nADDITIONAL FEATURES OF THE ER MODEL\nWe now look at some of the constructs in the ER model that allow us to describe\nsome subtle properties of the data. The expressiveness of the ER model is a\nbig reason for its widespread lise.\n2.4.1\nKey Constraints\nConsider the Works-.In relationship shown in Figure 2.2.\nAn employee can\nwork in several departments, and a department can have several employees, &.,\nillustrated in the vVorks_In instance shown in Figure 2.3. Employee 231-31-5368\nh&., worked in Department 51 since 3/3/93 and in Department 56 since 2/2/92.\nDepartment 51 h&'3 two employees.\nNow consider another relationship set called Manages between the Employ-\nees and Departments entity sets such that each department h&') at most one\nmanager, although a single employee is allowed to manage more than one de-\npartment. The restriction that each department h&,> at most one manager is\n\nIntroduction to Database Des'ign\n33\nan example of a key constraint, and it implies that each Departments entity\nappears in at most one 1Jlanages relationship in any allowable instance of Man-\nages. This restriction is indicated in the ER diagram of Figure 2.6 by using an\narrow from Departments to Manages. Intuitively, the arrow states that given\na Departments entity, we can uniquely determine the Manages relationship in\nwhich it appears.\nFigure 2.6\nKey Constraint on Manages\nAn instance of the Manages relationship set is shown in Figure 2.7. While this\nis also a potential instance for the WorksJn relationship set, the instance of\nWorks_In shown in Figure 2.3 violates the key constraint on Manages.\n1123-22-36661.\n!231-31-53681\n[223-32-6316\\\n--..----t-------;'-------a~\n~\n~\nEMPLOYEES\nPartial participation\nMANAGES\nOne to Many\nDEPARTMENTS\nTotal participation\nFigure 2.7\nAn Instance of the Manages Relationship Set\nA relationship set like Manages is sometimes said to be one-to-many, to\nindicate that one employee can be associated with many departments (in the\ncapacity of a manager), whereas each department can be associated with at\nmost one employee as its manager. In contrast, the \\Vorks-.In relationship set, in\nwhich an employee is allowed to work in several departments and a department\nis allowed to have several employees, is said to be many-to-many.\n\n34\nCHAPTER 2\nIf we add the restriction that each employee can manage at most one depl:1J't-\nment to the Manages relationship set, which would be indicated by adding\nan arrow from Employees to lVlanages in Figure 2.6, we have a one-to-one\nrelationship set.\nKey Constraints for Ternary Relationships\nWe can extend this convention-and the underlying key constraint concept-to\nrelationship sets involving three or more entity sets: If an entity set E has a\nkey constraint in a relationship set R, each entity in an instance of E appears\nin at most one relationship in (a corresponding instance of) R. To indicate a\nkey constraint on entity set E in relationship set R, we draw an arrow from E\nto R.\nIn Figure 2.8, we show a ternary relationship with key constraints. Each em-\nploy~e works in at most one department and at a single location. An instance\nof the Works_In3 relationship set is shown in Figure 2.9. Note that each depart-\nment can be associated with several employees and locations and each location\ncan be associated with several departments and employees; however, each em-\nployee is associated with a single department and location.\nlot\nEmployees\nWorksJn3\nDepartments\nFigure 2.8\nA Ternary Relationship Set with Key Constraints\n2.4.2\nParticipation Constraints\nThe key constraint on Manages tells us that a department ha:s at most one\nmanager. A natural question to ask is whether every department ha.'3 a Inan-\nagel'. Let us say that every department is required to have a manager. This\nrequirement is an example of a participation constraint; the particip::ltion of\nthe entity set Departments in the relationship set Manages is said to be total.\nA participation that is not total is said to be partial.\nAs an example, the\n\nIntroduction to Database Design\n•\nI Paris\nI\nEMPLOYEES\n!223-32-63161\n/~~\n//;123-22-3666) a..:~---~-r---'\"\n( ~~\n1131-24-36501\nKey constraint\nLOCATIONS\nFigure 2.9\nAn Instance of Works_In3\nparticipation of the entity set Employees in Manages is partial, since not every\nemployee gets to manage a department.\nRevisiting the Works..ln relationship set, it is natural to expect that each em-\nployee works in at least one department and that each department has at least\none employee. This means that the participation of both Employees and De-\npartments in Works..ln is total.\nThe ER diagram in Figure 2.10 shows both\nthe Manages and Works..ln relationship sets and all the given constraints. If\nthe participation of an entity set in a relationship set is total, the two are con-\nnected by a thick line; independently, the presence of an arrow indicates a key\nconstraint. The instances of Works_In and Manages shown in Figures 2.3 and\n2.7 satisfy all the constraints in Figure 2.10.\n2.4.3\nWeak Entities\nThus far, we have assumed that the attributes associated with an entity set\ninclude a key. This assumption does not always hold. For example, suppose\nthat employees can purchase insurance policies to cover their dependents. \"Ve\nwish to record information about policies, including who is covered by each\npolicy, but this information is really our only interest in the dependents of an\nemployee. If an employee quits, any policy owned by the employee is terminated\nand we want to delete all the relevant policy and dependent information from\nthe database.\n\n36\nCHAPTETh 2\nFigure 2.10\nManages and Works_In\nWe might choose to identify a dependent by name alone in this situation, since\nit is reasonable to expect that the dependents of a given employee have different\nnames. Thus the attributes of the Dependents entity set might be pname and\nage.\nThe attribute pname does not identify a dependent uniquely.\nRecall\nthat the key for Employees is ssn; thus we might have two employees called\nSmethurst and each might have a son called Joe.\nDependents is an example of a weak entity set. A weak entity can be iden-\ntified uniquely only by considering some of its attributes in conjunction with\nthe primary key of another entity, which is called the identifying owner.\nThe following restrictions must hold:\n11'I\nThe owner entity set and the weak entity set must participate in a one-\nto-many relationship set (one owner entity is associated with one or more\nweak entities, but each weak entity has a single owner). This relationship\nset is called the identifying relationship set of the weak entity set.\nIII\nThe weak entity set must have total participation in the identifying rela-\ntionship set.\nFor example, a Dependents entity can be identified uniquely only if we take the\nkey of the owning Employees entity and the pname of the Dependents entity.\nThe set of attributes of a weak entity set that uniquely identify a weak entity\nfor a given owner entity is called a partial key of the weak entity set. In our\nexample, pname is a partial key for Dependents.\n\nIntrod'uction to Database Design\nThe Dependents weak entity set and its relationship to Employees is shown in\nFigure 2.1.1. The total participation of Dependents in Policy is indicated by\nlinking them with a dark line. The arrow from Dependents to Policy indicates\nthat each Dependents entity appears in at most one (indeed, exactly one, be-\ncause of the participation constraint) Policy relationship.\nTo underscore the\nfact that Dependents is a weak entity and Policy is its identifying relationship,\nwe draw both with dark lines.\nTo indicate that pname is a partial key for\nDependents, we underline it using a broken line. This means that there may\nwell be two dependents with the same pname value.\nEmployees\nFigure 2.11\nA Weak Entity Set\n2.4.4\nClass Hierarchies\nSometimes it is natural to classify the entities in an entity set into subclasses.\nFor example, we might want to talk about an Hourly-Emps entity set and a\nContracLEmps entity set to distinguish the basis on which they are paid. We\nmight have attributes hours_worked and hourly_wage defined for Hourly_Emps\nand an attribute contractid defined for ContracLEmps.\nWe want the semantics that every entity in one of these sets is also an Em-\nployees entity and, as such, must have all the attributes of Employees defined.\nTherefore, the attributes defined for an Hourly_Emps entity are the attributes\nfor Employees plus Hourly~mps. \\Ve say that the attributes for the entity set\nEmployees are inherited by the entity set Hourly_Emps and that Hourly-Emps\nISA (read is a) Employees. In addition-and in contrast to class hierarchies\nin programming languages such &'3 C++~~~there is a constraint on queries over\ninstances of these entity sets: A query that asks for all Employees entities\nmust consider all Hourly_Emps and ContracLEmps entities as well.\nFigure\n2.12 illustrates,the cl&ss hierarchy.\nThe entity set Employees may also be classified using a different criterion. For\nexample, we might identify a subset of employees &'3 SenioLEmps.\nWe can\nrnodify Figure 2.12 to reflect this change by adding a second ISA node &'3 a\nchild of Employees and making SenioLEmps a child of this node. Each of these\nentity sets might be classified further, creating a multilevel ISA hierarchy.\n\n38\nCHAPTEJl; 2\nhourly-wages\nFigure 2.12\nClass Hierarchy\nA class hierarchy can be viewed in one of two ways:\n•\nEmployees is specialized into subclasses.\nSpecialization is the process\nof identifying subsets of an entity set (the superclass) that share some\ndistinguishing characteristic. Typically, the superclass is defined first, the\nsubclasses are defined next, and subclass-specific attributes and relation-\nship sets are then added.\n•\nHourly-Emps and ContracLEmps are generalized by Employees. As an-\nother example, two entity sets Motorboats and Cars may be generalized\ninto an entity set MotoLVehicles.\nGeneralization consists of identifying\nsome common characteristics of a collection of entity sets and creating a\nnew entity set that contains entities possessing these common character-\nistics. Typically, the subclasses are defined first, the superclass is defined\nnext, and any relationship sets that involve the superclass are then defined.\nWe can specify two kinds of constraints with respect to ISA hierarchies, namely,\noverlap and covering constraints.\nOverlap constraints determine whether\ntwo subclasses are allowed to contain the same entity. For example, can At-\ntishoo be both an Hourly_Emps entity and a ContracLEmps entity? Intuitively,\nno. Can he be both a ContracLEmps entity and a Senior-Emps entity? Intu-\nitively, yes. We denote this by writing 'ContracLE;mps OVERLAPS Senior-Emps.'\nIn the absence of such a statement, we assume by default that entity sets are\nconstrained to have no overlap.\nCovering constraints determine whether the entities in the subclasses collec-\ntively include all entities in the superclass. For example, does every Employees\n\nIntroduction to Database Design\nentity have to belong to one of its subclasses?\nIntuitively, no.\nDoes every\n~'IotoLVehicles entity have to be either a Motorboats entity or a Cars entity?\nIntuitively, yes; a characteristic property of generalization hierarchies is that\nevery instance of a superclass is an instance of a subclass. vVe denote this by\nwriting 'Motorboats AND Cars COVER Motor-Vehicles.' In the absence of such a\nstatement, we assume by default that there is no covering constraint; we can\nhave motor vehicles that are not motorboats or cars.\nThere are two basic reasons for identifying subclasses (by specialization or\ngeneralization):\n1. We might want to add descriptive attributes that make sense only for the\nentities in a subclass. For example, hourly_wages does not make sense for a\nContracLEmps entity, whose pay is determined by an individual contract.\n2. We might want to identify the set of entities that participate in some rela-\ntionship. For example, we might wish to define the Manages relationship\nso that the participating entity sets are Senior-Emps and Departments,\nto ensure that only senior employees can be managers. As another exam-\nple, Motorboats and Cars may have different descriptive attributes (say,\ntonnage and number of doors), but as Motor_Vehicles entities, they must\nbe licensed. The licensing information can be captured by a Licensed_To\nrelationship between Motor_Vehicles and an entity set called Owners.\n2.4.5\nAggregation\nAs defined thus far, a relationship set is an association between entity sets.\nSometimes, we have to model a relationship between a collection of entities\nand relationships. Suppose that we have an entity set called Projects and that\neach Projects entity is sponsored by one or more departments.\nThe Spon-\nsors relationship set captures this information. A department that sponsors a\nproject might assign employees to monitor the sponsorship. Intuitively, Moni-\ntors should be a relationship set that associates a Sponsors relationship (rather\nthan a Projects or Departments entity) with an Employees entity.\nHowever,\nwe have defined relationships to &'3sociate two or more entities.\nTo define a relationship set such &'3 Monitors, we introduce a new feature of\nthe ER model, called aggregation.\nAggregation allows us to indicate that\na relationship set (identified through a dashed box) participates in another\nrelationship set. This is illustrated in Figure 2.13, with a dashed box around\nSponsors (and its participating entity sets) used to denote aggregation. This\neffectively allows us to treat Sponsors as an entity set for purposes of defining\nthe Monitors relationship set.\n\n40\nCHAPTER 2\nMonitors /~\nI\n._.\n_.\n-\n-\n-\n-\n.-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n~\n-~\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n~\n-\n-\n-\n-\n-\n-\n-\n--\n-\n-\n-\n-\n-\n-\n~\n-\n-\n-\n-\n-\n-\n-\n-\n--\n-\n-- -;\nI\nI\n~:\n-\nI\nDepartments\nSponsors\nI\nI\nI\nI\nI\n----------------------------------------\n1\nI\n------~~~~~-------\nFigure 2.13\nAggregation\nWhen should we use aggregation? Intuitively, we use it when we need to ex-\npress a relationship among relationships. But can we not express relationships\ninvolving other relationships without using aggregation? In our example, why\nnot make Sponsors a ternary relationship? The answer is that there are really\ntwo distinct relationships, Sponsors and Monitors, each possibly with attributes\nof its own. For instance, the Monitors relationship has an attribute 1tntil that\nrecords the date until when the employee is appointed as the sponsorship mon-\nitor. Compare this attribute with the attribute since of Sponsors, which is the\ndate when the sponsorship took effect. The use of aggregation versus a ternary\nrelationship may also be guided by certain integrity constraints, as explained\nin Section 2.5.4.\n2.5\nCONCEPTUAL DESIGN WITH THE ER MODEL\nDeveloping an ER diagram presents several choices, including the following:\n..\nShould a concept be modeled as an entity or an attribute?\n..\nShould a concept be modeled &'3 an entity or a relationship?\nII\n\"Vhat arc the relationship sets and their participating entity sets? Should\nwe use binary or ternary relationships?\nII\nShould we use aggregation?\n\nIntrod'lLct'ion to Database Design\n\\Ve now discuss the issues involved in making these choices.\n2.5.1\nEntity versus Attribute\n41\n\\Vhile identifying the attributes of an entity set, it is sometimes not clear\nwhether a property should be modeled as an attribute or as an entity set (and\nrelated to the first entity set using a relationship set). For example, consider\nadding address information to the Employees entity set. One option is to use\nan attribute address.\nThis option is appropriate if we need to record only\none address per employee, and it suffices to think of an address as a string. An\nalternative is to create an entity set called Addresses and to record associations\nbetween employees and addresses using a relationship (say, Has_Address). This\nmore complex alternative is necessary in two situations:\n•\nWe have to record more than one address for an employee.\n•\nWe want to capture the structure of an address in our ER diagram. For\nexample, we might break down an address into city, state, country, and\nZip code, in addition to a string for street information. By representing an\naddress as an entity with these attributes, we can support queries such as\n\"Find all employees with an address in Madison, WI.\"\nFor another example of when to model a concept as an entity set rather than\nan attribute, consider the relationship set (called WorksJ:n4) shown in Figure\n2.14.\nFigure 2.14\nThe \\Vorks_In4 Relationship Set\nIt differs from the \\Vorks_In relationship set of Figure 2.2 only in that it has\nattributes JTOtn and to, instead of since.\nIntuitively, it records the interval\nduring which an employee works for a department.\nNow suppose that it is\npossible for an employee to work in a given department over more than one\nperiod.\nThis possibility is ruled out by the ER diagram's semantics, because a rela-\ntionship is uniquely identified by the participating entities (recall from Section\n\n42\nCHAPTER' 2\n2.3). The problem is that we want to record several values for the descriptive\nattributes for each instance of the vVorks-ln2 relationship.\n(This situation is\nanalogous to wanting to record several addresses for each employee.) vVe can\naddress this problem by introducing an entity set called, say, Duration, with\nattributes from and to, as shown in Figure 2.15.\n(~~-T:~~~)\nI\nEmployees\nI\nWorksJn4\nDepartments\nfrom\nto\nFigure 2.15\nThe Works-ln4 Relationship Set\nIn some versions of the' ER model, attributes are allowed to take on sets as\nvalues. Given this feature, we could make Duration an attribute of Works_In,\nrather than an entity set; associated with each Works_In relationship, we would\nhave a set of intervals. This approach is perhaps more intuitive than model-\ning Duration as an entity set.\nNonetheless, when such set-valued attributes\nare translated into the relational model, which does not support set-valued\nattributes, the resulting relational schema is very similar to what we get by\nregarding Duration as an entity set.\n2.5.2\nEntity versus Relationship\nConsider the relationship set called Manages in Figure 2.6. Suppose that each\ndepartment manager is given a discretionary budget (dbudget) , as shown in\nFigure 2.16, in which we have also renamed the relationship set to Manages2.\nFigure 2.16\nEntity versus Relationship\n\nIntroduction to Database Design\nGiven a department, we know the manager, as well &'3 the manager's starting\ndate and budget for that department. This approach is natural if we t'l\"ssume\nthat a manager receives a separate discretionary budget for each department\nthat he or she manages.\nBut what if the discretionary budget is a sum that covers all departments\nmanaged by that employee?\nIn this case, each Manages2 relationship that\ninvolves a given employee will have the same value in the db1Ldget field, leading\nto redundant storage of the same information.\nAnother problem with this\ndesign is that it is misleading; it suggests that the budget is associated with\nthe relationship, when it is actually associated with the manager.\nWe can address these problems by introducing a new entity set called Managers\n(which can be placed below Employees in an ISA hierarchy, to show that every\nmanager is also an employee). The attributes since and dbudget now describe\na manager entity, as intended.\nAs a variation, while every manager has a\nbudget, each manager may have a different starting date (as manager) for each\ndepartment. In this case dbudget is an attribute of Managers, but since is an\nattribute of the relationship set between managers and departments.\nThe imprecise nature of ER modeling can thus make it difficult to recognize\nunderlying entities, and we might associate attributes with relationships rather\nthan the appropriate entities.\nIn general, such mistakes lead to redundant\nstorage of the same information and can cause many problems.\nWe discuss\nredundancy and its attendant problems in Chapter 19, and present a technique\ncalled normalization to eliminate redundancies from tables.\n2.5.3\nBinary versus Ternary Relationships\nConsider the ER diagram shown in Figure 2.17. It models a situation in which\nan employee can own several policies, each policy can be owned by several\nemployees, and each dependent can be covered by several policies.\nSuppose that we have the following additional requirements:\nIII\nA policy cannot be owned jointly by two or more employees.\n11II\nEvery policy must be owned by some employee.\nlIII\nDependents is a weak entity set, and each dependent entity is uniquely\nidentified by taking pname in conjunction with the policyid of a policy\nentity (which, intuitively, covers the given dependent).\nThe first requirement suggests that we impose a key constraint on Policies with\nrespect to Covers, but this constraint has the unintended side effect that a\n\n44\nC~~C~T~\n~I\nI\nEmployees\nlot\nCovers\nFigure 2.17\nPolicies as an Entity Set\nCHAPTERf 2\npolicy can cover only one dependent. The second requirement suggests that we\nimpose a total participation constraint on Policies. This solution is acceptable\nif each policy covers at least one dependent. The third requirement forces us\nto introduce an identifying relationship that is binary (in our version of ER\ndiagrams, although there are versions in which this is not the case).\nEven ignoring the third requirement, the best way to model this situation is to\nuse two binary relationships, as shown in Figure 2.18.\nFigure 2.18\nPolicy Revisited\n\nIntTod'U(~t\"ion to Database Des'ign\n45.\nThis example really has two relationships involving Policies, and our attempt\nto use a single ternary relationship (Figure 2.17) is inappropriate. There are\nsituations, however, \"vhere a relationship inherently a.'3sociates more than two\nentities. vVe have seen such an example in Figures 2,4 and 2.15.\nAs a typical example of a ternary relationship, consider entity sets Parts, Sup-\npliers, and Departments, and a relationship set Contracts (with descriptive\nattribute qty) that involves all of them. A contract specifies that a supplier will\nsupply (some quantity of) a part to a department.\nThis relationship cannot\nbe adequately captured by a collection of binary relationships (without the use\nof aggregation). With binary relationships, we can denote that a supplier 'can\nsupply' certain parts, that a department 'needs' some parts, or that a depart-\nment 'deals with' a certain supplier.\nNo combination of these relationships\nexpresses the meaning of a contract adequately, for at least two reasons:\n•\nThe facts that supplier S can supply part P, that department D needs part\nP, and that D will buy from S do not necessarily imply that department D\nindeed buys part P from supplier S!\n•\nWe cannot represent the qty attribute of a contract cleanly.\n2.5.4\nAggregation versus Ternary Relationships\nAs we noted in Section 2.4.5, the choice between using aggregation or a ternary\nrelationship is mainly determined by the existence of a relationship that relates\na relationship set to an entity set (or second relationship set). The choice may\nalso be guided by certain integrity constraints that we want to express.\nFor\nexample, consider the ER diagram shown in Figure 2.13. According to this dia-\ngram, a project can be sponsored by any number of departments, a department\ncan sponsor one or more projects, and each sponsorship is monitored by one\nor more employees. If we don't need to record the unt-il attribute of Monitors,\nthen we might reasonably use a ternal'Y relationship, say, Sponsors2, as shown\nin Figure 2.19.\nConsider the constraint that each sponsorship (of a project by a department)\nbe monitored by at most one employee.\nVVe cannot express this constraint\nin terms of the Sponsors2 relationship set. On the other hand, we can easily\nexpress the cOnstraint by drawing an arrow from the aggregated relationship\nSponsors to the relationship Monitors in Figure 2.13.\nThus, the presence of\nsuch a constraint serves &s another reason for using aggregation rather than a\nternary relationship set.\n\n46\nCHAPTERt2\nEmployees\nstarted_on\nG:(:P\nProjects\nSponsors2\ndname\nG:(?\n>-------11\nDepartment<\n1\nFigure 2.19\nUsing a Ternary Relationship instead of Aggregation\n2.6\nCONCEPTUAL DESIGN FOR LARGE ENTERPRISES\nWe have thus far concentrated on the constructs available in the ER model\nfor describing various application concepts and relationships. The process of\nconceptual design consists of more than just describing small fragments of the\napplication in terms of ER diagrams. For a large enterprise, the design may re-\nquire the efforts of more than one designer and span data and application code\nused by a number of user groups.\nUsing a high-level, semantic data model,\nsuch as ER diagrams, for conceptual design in such an environment offers the\nadditional advantage that the high-level design can be diagrammatically rep-\nresented and easily understood by the many people who must provide input to\nthe design process.\nAn important aspect of the design process is the methodology used to structure\nthe development of the overall design and ensure that the design takes into\naccount all user requirements and is consistent. The usual approach is that the\nrequirements of various user groups are considered, any conflicting requirements\nare somehow resolved, and a single set of global requirements is generated at\nthe end of the.requirements analysis phase. Generating a single set of global\nrequirements is a difficult task, but it allows the conceptual design phase to\nproceed with the development of a logical schema that spans all the data and\napplications throughout the enterprise.\nAn alternative approach is to develop separate conceptual scherna.'-l for different\nuser groups and then integTate these conceptual schemas. To integrate\nmulti~\n\nIntmduction to Database De.s'ign\n47\npIe conceptual schemas, we must €'Btablish correspondences between entities,\nrelationships, and attributes, and we must resolve numerous kinds of conflicts\n(e.g., naming conflicts, domain mismatches, differences in measurement units).\nThis task is difficult in its own right. In some situations, schema integration\ncannot be avoided; for example, when one organization merges with another,\nexisting databases may have to be integrated. Schema integration is also in-\ncreasing in importance as users demand access to heterogeneous data sources,\noften maintained by different organizations.\n2.7\nTHE UNIFIED MODELING LANGUAGE\nThere are many approaches to end-to-end software system design, covering all\nthe steps from identifying the business requirements to the final specifications\nfor a complete application, including workflow, user interfaces, and many as-\npects of software systems that go well beyond databases and the data stored in\nthem. In this section, we briefly discuss an approach that is becoming popular,\ncalled the unified modeling language (UML) approach.\nUML, like the ER model, has the attractive feature that its constructs can be\ndrawn as diagrams. It encompasses a broader spectrum of the software design\nprocess than the ER model:\nIII\nBusiness Modeling: In this phase, the goal is to describe the business\nprocesses involved in the software application being developed.\nIII\nSystem Modeling: The understanding of business processes is used to\nidentify the requirements for the software application.\nOne part of the\nrequirements is the database requirements.\nIII\nConceptual Database Modeling: This step corresponds to the creation\nof the ER design for the database. For this purpose, UML provides many\nconstructs that parallel the ER constructs.\nIII\nPhysical Database Modeling: Ul\\IL also provides pictorial represen-\ntations for physical database design choices, such &'3 the creation of table\nspaces and indexes. (\\\\1e discuss physical databa\"se design in later chapters,\nbut not the corresponding UML constructs.)\nIII\nHardware System Modeling: UML diagrams can be used to describe\nthe hardware configuration used for the application.\nTh(~re are many kinds of diagrams in UML. Use case diagrams describe the\nactions performed by the system in response to user requests, and the people\ninvolved in these actions.\nThese diagrams specify the external functionality\n<-hat the system is expected to support.\n\n48\nCHAPTER;2\nActivity diagrams 8hmv the flow of actions in a business process. Statechart\ndiagrams describe dynamic interactions between system objects.\nThese dia-\ngrams, used in busine.c;s and systern modeling, describe how the external func-\ntionality is to be implemented, consistent with the business rules and processes\nof the enterprise.\nClass diagrams are similar to ER diagrams, although they are more general\nin that they are intended to model application entities (intuitively, important\nprogram components) and their logical relationships in addition to data entities\nand their relationships.\nBoth entity sets and relationship sets can be represented as classes in UML,\ntogether with key constraints, weak entities, and class hierarchies. The term\nrelationship is used slightly differently in UML, and UML's relationships are\nbinary.\nThis sometimes leads to confusion over whether relationship sets in\nan ER diagram involving three or more entity sets can be directly represented\nin UML. The confusion disappears once we understand that all relationship\nsets (in the ER sense) are represented as classes in UML; the binary UML\n'relationships' are essentially just the links shown in ER diagrams between\nentity sets and relationship sets.\nRelationship sets with key constraints are usually omitted from UML diagrams,\nand the relationship is indicated by directly linking the entity sets involved.\nFor example, consider Figure 2.6. A UML representation of this ER diagram\nwould have a class for Employees, a class for Departments, and the relationship\nManages is shown by linking these two classes. The link can be labeled with\na name and cardinality information to show that a department can have only\none manager.\nAs we will see in Chapter 3, ER diagrams are translated into the relational\nmodel by mapping each entity set into a table and each relationship set into\na table. FUrther, as we will see in Section 3.5.3, the table corresponding to a\none-to-many relationship set is typically omitted by including some additional\ninformation about the relationship in the table for one of the entity sets in-\nvolved. Thus, UML class diagrams correspond closely to the tables created by\nmapping an ER diagram.\nIndeed, every class in a U1I1L class diagram is mapped into a table in the cor-\nresponding U]\\'1L database diagram.\nUML's database diagrams show how\nclasses are represented in the database and contain additional details about\nthe structure of the database such as integrity constraints and indexes. Links\n(UML's 'relationships') between UML classes lead to various integrity con-\nstraints between the corresponding tables.\nMany details specific to the re-\nlational model (e.g., views, fOTe'ign keys, null-allowed fields) and that reflect\n\nIntroduction to Dutaba8C Design\n49\nphysical design choices (e.g., indexed fields) can be modeled ill UN[L database\ndiagrams.\nUML's component diagrams describe storage aspects of the database, such\nas tablespaces and database pa,titions) , as well as interfaces to applications\nthat access the database. Finally, deployment diagrams show the hardware\naspects of the system.\nOur objective in this book is to concentrate on the data stored in a database\nand the related design issues.\nTo this end, we deliberately take a simplified\nview of the other steps involved in software design and development. Beyond\nthe specific discussion of UlIIL, the material in this section is intended to place\nthe design issues that we cover within the context of the larger software design\nprocess. \\Ve hope that this will assist readers interested in a more comprehen-\nsive discussion of software design to complement our discussion by referring to\nother material on their preferred approach to overall system design.\n2.8\nCASE STUDY: THE INTERNET SHOP\nWe now introduce an illustrative, 'cradle-to-grave' design case study that we\nuse as a running example throughout this book. DBDudes Inc., a well-known\ndatabase consulting firm, has been called in to help Barns and Nobble (B&N)\nwith its database design and implementation. B&N is a large bookstore special-\nizing in books on horse racing, and it has decided to go online. DBDudes first\nverifies that B&N is willing and able to pay its steep fees and then schedules a\nlunch meeting--billed to B&N, naturally~to do requirements analysis.\n2.8.1\nRequirements Analysis\nThe owner of B&N, unlike many people who need a database, has thought\nextensively about what he wants and offers a concise summary:\n\"I would like my customers to be able to browse my catalog of books and\nplace orders over the Internet. Currently, I take orders over the phone. I have\nmostly corporate customers who call me and give me the ISBN number of a\nbook and a quantity; they often pay by credit card. I then prepare a shipment\nthat contains the books they ordered. If I don't have enough copies in stock,\nI order additional copies and delay the shipment until the new copies arrive;\nI want to ship a customer's entire order together. My catalog includes all the\nbooks I sell. For each book, the catalog contains its ISBN number, title, author,\npurcha.se price, sales price, and the year the book was published. Most of my\nsustomers are regulars, and I have records with their names and addresses.\n\n50\nyear-published\nOrders\nFigure 2.20\nER Diagram of the Initial Design\nCHAPTER¢2\nNew customers have to call me first and establish an account before they can\nuse my website.\nOn my new website, customers should first identify themselves by their unique\ncustomer identification number. Then they should be able to browse my catalog\nand to place orders online.\"\nDBDudes's consultants are a little surprised by how quickly the requirements\nphase is completed--it usually takes weeks of discussions (and many lunches\nand dinners) to get this done~~but return to their offices to analyze this infor-\nmation.\n2.8.2\nConceptual Design\nIn the conceptual design step, DBDudes develops a high level description of\nthe data in terms of the ER model.\nThe initial design is shown in Figure\n2.20. Books and customers are modeled as entities and related through orders\nthat customers place.\nOrders is a relationship set connecting the Books and\nCustomers entity sets.\nFor each order, the following attributes are stored:\nquantity, order date, and ship date. As soon as an order is shipped, the ship\ndate is set; until then the ship date is set to null, indicating that this order has\nnot been shipped yet.\nDBDudes has an internal design review at this point, and several questions are\nraised.\nTo protect their identities, we will refer to the design team leader as\nDude 1 and the design reviewer as Dude 2.\nDude 2: \\\\That if a. customer places two orders for the same book in one day?\nDude 1: The first order is ha,ndlecl by crea.ting a new Orders relationship and\n\nIntroduct'ion to Database Design\n51,\nthe second order is handled by updating the value of the quantity attribute in\nthis relationship.\nDude 2: \\\\That if a customer places two orders for different books in one day?\nDude 1: No problem. Each instance of the Orders relationship set relates the\ncustomer to a different book.\nDude 2: Ah, but what if a customer places two orders for the same book on\ndifferent days?\nDude 1: \\Ve can use the attribute order date of the orders relationship to\ndistinguish the two orders.\nDude 2: Oh no you can't. The attributes of Customers and Books must jointly\ncontain a key for Orders.\nSo this design does not allow a customer to place\norders for the same book on different days.\nDude 1: Yikes, you're right. Oh well, B&N probably won't care; we'll see.\nDBDudes decides to proceed with the next phase, logical database design; we\nrejoin them in Section 3.8.\n2.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nName the main steps in database design. What is the goal of each step?\nIn which step is the ER model mainly used? (Section 2.1)\n•\nDefine these terms: entity, entity set, attribute, key. (Section 2.2)\n•\nDefine these terms:\nrelationship, relationship set, descriptive attributes.\n(Section 2.3)\n•\nDefine the following kinds of constraints, and give an example of each: key\nconstraint, participation constraint. What is a weak entity? What are class\nhierarchies'? What is aggregation?\nGive an example scenario motivating\nthe use of each of these ER model design constructs. (Section 2.4)\n•\nWhat guidelines would you use for each of these choices when doing ER\ndesign: \\Vhether to use an attribute or an entity set, an entity or a relation-\nship set, a binary or ternary relationship, or aggregation. (Section 2.5)\nIII'l\nWhy is designing a database for a large enterprise especially hard? (Sec-\ntion 2.6)\n•\nWhat is UML? How does databa\"se design fit into the overall design of\na data-intensive software system? How is UML related to ER diagrams?\n(Section 2.7)\n\n52\nEXERCISES\nCHAPTERJ2\nExercise 2.1 Explain the following terms briefly:\nattribute, domain, entity, relationship,.\nentity set, relationship set, one-to-many relat'ionship, many-to-many 1'elationship. pan·tcipa-\ntion constmint. overlap constraint, covering constraint, weak entity set,. aggregat'ion, and role\nindicator.\nExercise 2.2 A university database contains information about professors (identified by so-\ncial security number, or SSN) and courses (identified by courseid). Professors teach courses;\neach of the following situations concerns the Teaches relationship set.\nFor each situation,\ndraw an ER diagram that describes it (assuming no further constraints hold).\n1. Professors can teach the same course in several semesters, and each offering must be\nrecorded.\n2. Professors can teach the same course in several semesters, and only the most recent\nsuch offering needs to be recorded.\n(Assume this condition applies in all subsequent\nquestions.)\n3. Every professor must teach some course.\n4. Every professor teaches exactly one course (no more, no less).\n5. Every professor teaches exactly one course (no more, no less), and every course must be\ntaught by some professor.\n6. Now suppose that certain courses can be taught by a team of professors jointly, but it\nis possible that no one professor in a team can teach the course. Model this situation,\nintroducing additional entity sets and relationship sets if necessary.\nExercise 2.3 Consider the following information about a university database:\nII\nProfessors have an SSN, a name, an age, a rank, and a research specialty.\nII\nProjects have a project number, a sponsor name (e.g., NSF), a starting date, an ending\ndate, and a budget.\nII\nGraduate students have an SSN, a name, an age, and a degree program (e.g., M.S. or\nPh.D.).\nII\nEach project is managed by one professor (known as the project's principal investigator).\nII\nEach project is worked on by one or more professors (known as the project's co-investigators).\nIII\nProfessors can manage and/or work on multiple projects.\nII\nEach project is worked on by one or more graduate students (known as the project's\nresearch assistants).\nII\nWhen graduate students >'lark on a project, a professor must supervise their work on the\nproject. Graduate students can work on multiple projects, in which case they will have\na (potentially different) supervisor for each one.\nII\nDepartments have a department number, a department name, and a main office.\nII\nDepartments have a professor (known as the chairman) who runs the department.\nII\nProfessors work in one or more departments, and for each department that they work\nin, a time percentage is associated with their job.\nII\nGraduate students have one major department in which they are working OIl their degree.\n\nIntroduction to Database Design\n~3\nIII\nEach graduate student has another, more senior graduate student (known as a student\nadvisor) who advises him or her OIl what courses to take.\nDesign and dra\\v an ER diagram that captures the information about the university. Use only\nthe basic ER model here; that is, entities, relationships, and attributes. Be sure to indicate\nany key and participation constraints.\nExercise 2.4 A company database needs to store information about employees (identified\nby ssn, with salary and phone as attributes), departments (identified by dna, with dname and\nbudget as attributes), and children of employees (with name and age as attributes). Employees\nwork in departments; each department is managed by an employee; a child must be identified\nuniquely by name when the parent (who is an employee; assume that only one parent works\nfor the company) is known.\nWe are not interested in information about a child once the\nparent leaves the company.\nDraw an ER diagram that captures this information.\nExercise 2.5 Notown Records has decided to store information about musicians who perform\non its albums (as well as other company data) in a database. The company has wisely chosen\nto hire you as a database designer (at your usual consulting fee of $2500jday).\nIII\nEach musician that records at Notown has an SSN, a name, an address, and a phone\nnumber. Poorly paid musicians often share the same address, and no address has more\nthan one phone.\nIII\nEach instrument used in songs recorded at Notown has a name (e.g., guitar, synthesizer,\nflute) and a musical key (e.g., C, B-flat, E-flat).\nIII\nEach album recorded on the Notown label has a title, a copyright date, a format (e.g.,\nCD or MC), and an album identifier.\nIII\nEach song recorded at Notown has a title and an author.\nIII\nEach musician may play several instruments, and a given instrument may be played by\nseveral musicians.\nIII\nEach album has a number of songs on it, but no song may appear on more than one\nalbum.\nIII\nEach song is performed by one or more musicians, and a musician may perform a number\nof songs.\nIII\nEach album has exactly one musician who acts as its producer. A musician may produce\nseveral albums, of course.\nDesign' a conceptual schema for Notown and draw an ER diagram for your schema.\nThe\npreceding information describes the situation that the Notown database must model. Be sure\nto indicate all key and cardinality constraints and any assumptions you make. Identify any\nconstraints you are unable to capture in the ER diagram and briefly explain why you could\nnot express them.\nExercise 2.6 Computer Sciences Department frequent fliers have been complaining to Dane\nCounty Airport officials about the poor organization at the airport. As a result, the officials\ndecided that all information related to the airport should be organized using a DBMS, and\nyou have been hired to design the database.\nYour first task is to organize the information\nabout all the airplanes stationed and maintainecl at the airport. The relevant information is\nas follows:\n\n54\nCHAPTER 12\n•\nEvery airplane has a registration number, and each airplane is of a specific model.\n•\nThe airport accommodates a number of airplane models, and each model is identified by\na model number (e.g., DC-lO) and has a capacity and a weight.\n•\nA number of technicians work at the airport. You need to store the name, SSN, address,\nphone number, and salary of each technician.\n•\nEach technician is an expert on one or more plane model(s), and his or her expertise may\noverlap with that of other technicians. This information about technicians must also be\nrecorded.\n•\nTraffic controllers must have an annual medical examination. For each traffic controller,\nyou must store the date of the most recent exam.\n•\nAll airport employees (including technicians) belong to a union.\nYou must store the\nunion membership number of each employee.\nYou can assume that each employee is\nuniquely identified by a social security number.\n•\nThe airport has a number of tests that are used periodically to ensure that airplanes are\nstill airworthy. Each test has a Federal Aviation Administration (FAA) test number, a\nname, and a maximum possible score.\n•\nThe FAA requires the airport to keep track of each time a given airplane is tested by a\ngiven technician using a given test. For each testing event, the information needed is the\ndate, the number of hours the technician spent doing the test, and the score the airplane\nreceived on the test.\n1. Draw an ER diagram for the airport database. Be sure to indicate the various attributes\nof each entity and relationship set; also specify the key and participation constraints for\neach relationship set. Specify any necessary overlap and covering constraints a.s well (in\nEnglish).\n2. The FAA passes a regulation that tests on a plane must be conducted by a technician\nwho is an expert on that model.\nHow would you express this constraint in the ER\ndiagram? If you cannot express it, explain briefly.\nExercise 2.7 The Prescriptions-R-X chain of pharmacies ha.s offered to give you a free life-\ntime supply of medicine if you design its database. Given the rising cost of health care, you\nagree. Here's the information that you gather:\n11II\nPatients are identified by an SSN, and their names, addresses, and ages must be recorded.\n11II\nDoctors are identified by an SSN. For each doctor, the name, specialty, and years of\nexperience must be recorded.\nIII\nEach pharmaceutical company is identified by name and has a phone number.\nIII\nFor each drug, the trade name and formula must be recorded.\nEach drug is sold by\na given pharmaceutical company, and the trade name identifies a drug uniquely from\namong the pJ;oducts of that company. If a pharmaceutical company is deleted, you need\nnot keep track of its products any longer.\nIII\nEach pharmacy has a name, address, and phone number.\nIII\nEvery patient has a primary physician. Every doctor has at least one patient.\n•\nEach pharmacy sells several drugs and has a price for each.\nA drug could be sold at\nseveral pharmacies, and the price could vary from one pharmacy to another.\n\nIntToduction to DatabaBe Design\n55\n•\nDoctors prescribe drugs for patients.\nA doctor could prescribe one or more drugs for\nseveral patients, and a patient could obtain prescriptions from several doctors.\nEach\nprescription has a date and a quantity associated with it.\nYou can assume that, if a\ndoctor prescribes the same drug for the same patient more than once, only the last such\nprescription needs to be stored.\n•\nPharmaceutical companies have long-term contracts with pharmacies. A pharmaceutical\ncompany can contract with several pharmacies, and a pharmacy can contract with several\npharmaceutical companies. For each contract, you have to store a start date, an end date,\nand the text of the contract.\n•\nPharmacies appoint a supervisor for each contract. There must always be a supervisor\nfor each contract, but the contract supervisor can change over the lifetime of the contract.\n1. Draw an ER diagram that captures the preceding information. Identify any constraints\nnot captured by the ER diagram.\n2. How would your design change if each drug must be sold at a fixed price by all pharma-\ncies?\n3. How would your design change if the design requirements change as follows: If a doctor\nprescribes the same drug for the same patient more than once, several such prescriptions\nmay have to be stored.\nExercise 2.8 Although you always wanted to be an artist, you ended up being an expert on\ndatabases because you love to cook data and you somehow confused database with data baste.\nYour old love is still there, however, so you set up a database company, ArtBase, that builds a\nproduct for art galleries. The core of this product is a database with a schema that captures\nall the information that galleries need to maintain. Galleries keep information about artists,\ntheir names (which are unique), birthplaces, age, and style of art. For each piece of artwork,\nthe artist, the year it was made, its unique title, its type of art (e.g., painting, lithograph,\nsculpture, photograph), and its price must be stored. Pieces of artwork are also classified into\ngroups of various kinds, for example, portraits, still lifes, works by Picasso, or works of the\n19th century; a given piece may belong to more than one group. Each group is identified by\na name (like those just given) that describes the group. Finally, galleries keep information\nabout customers. For each customer, galleries keep that person's unique name, address, total\namount of dollars spent in the gallery (very important!), and the artists and groups of art\nthat the customer tends to like.\nDraw the ER diagram for the database.\nExercise 2.9 Answer the following questions.\n•\nExplain the following terms briefly: UML, use case diagrams, statechart diagrams, class\ndiagrams, database diagrams, component diagrams, and deployment diagrams.\n•\nExplain the relationship between ER diagrams and UML.\nBffiLIOGRAPHIC NOTES\nSeveral books provide a good treatment of conceptual design; these include [63J (which also\ncontains a survey of commercial database design tools) and [730J.\nThe ER model wa..<; proposed by Chen [172], and extensions have been proposed in a number\nof subsequent papers. Generalization and aggregation were introduced in [693]. [390, 589]\n\n56\nCHAPTER ,;2\ncontain good surveys of semantic data models. Dynamic and temporal aspects of semantic\ndata models are discussed in [749].\n[731] discusses a design methodology based on developing an ER diagram and then translating\nit to the relational model. Markowitz considers referential integrity in the context of ER to\nrelational mapping and discusses the support provided in some commercial systems (a..<; of\nthat date) in [513, 514].\nThe entity-relationship conference proceedings contain numerous papers on conceptual design,\nwith an emphasis on the ER model; for example, [698].\nThe OMG home page (www. omg. org) contains the specification for UML and related modeling\nstandards.\nNumerous good books discuss UML; for example [105, 278, 640] and there is a\nyearly conference dedicated to the advancement of UML, the International Conference on the\nUnified Modeling Language.\nView integration is discussed in several papers, including [97, 139, 184, 244, 535, 551, 550,\n685, 697, 748]. [64] is a survey of several integration approaches.\n\n3\nTHE RELATIONAL MODEL\n....\nHow is data represented in the relational model?\n....\nWhat integrity constraints can be expressed?\n....\nHow can data be created and modified?\n....\nHow can data be manipulated and queried?\n....\nHow can we create, modify, and query tables using SQL?\n....\nHow do we obtain a relational database design from an ER diagram?\n....\nWhat are views and why are they used?\n..\nKey concepts:\nrelation, schema, instance, tuple, field, domain,\ndegree,\ncardinality;\nSQL DDL,\nCREATE TABLE,\nINSERT, DELETE,\nUPDATE; integrity constraints, domain constraints, key constraints,\nPRIMARY KEY, UNIQUE, foreign key constraints, FOREIGN KEY; refer-\nential integrity maintenance, deferred and immediate constraints; re-\nlational queries; logical database design, translating ER diagrams to\nrelations, expressing ER constraints using SQL; views, views and log:-\nical independence, security; creating views in SQL, updating views,\nquerying views, dropping views\nTABLE: An arrangement of words, numbers, or signs, or combinations of them,\n&s in parallel columns, to exhibit a set of facts or relations in a definite, compact,\nand comprehensive form; a synopsis or scheme.\n-----vVebster's Dictionary of the English Language\nCodd proposed the relational data model in 1970. At that time, most databa,,'Se\nsystems were based on one of two older data models (the hierarchical model\n57\n\n58\nCHAPTER ~\nSQL. Originally developed as the query language of the pioneering\nSystem-R relational DBl\\1S at IBIYl, structured query language (SQL)\nhas become the most widely used language for creating, manipulating,\nand querying relational DBMSs. Since many vendors offer SQL products,\nthere IS a need for a standard that defines \\official SQL.' The existence of\na standard allows users to measure a given vendor's version of SQL for\ncompleteness. It also allows users to distinguish SQLfeatures specific to\none product from those that are standard; an application that relies on\nnonstandard features is less portable.\nThe first SQL standard was developed in 1986 by the American National\nStandards Institute (ANSI) and was called SQL-86. There was a minor\nrevision in 1989 called SQL-89 and a major revision in 1992 called SQL-\n92.\nThe International Standards Organization (ISO) collaborated with\nANSI to develop SQL-92. Most commercial DBMSs currently support (the\ncore subset of) SQL-92 and are working to support the recently adopted\nSQL:1999 version of the standard, a major extension of SQL-92.\nOur\ncoverage of SQL is based on SQL:1999, but is applicable to SQL-92 as\nwell; features unique to SQL:1999 are explicitly noted.\nand the network model); the relational model revolutionized the database field\nand largely supplanted these earlier models.\nPrototype relational databa.'3e\nmanagement systems were developed in pioneering research projects at IBM\nand DC-Berkeley by the mid-197Gs, and several vendors were offering relational\ndatabase products shortly thereafter.\nToday, the relational model is by far\nthe dominant data model and the foundation for the leading DBMS products,\nincluding IBM's DB2 family, Informix, Oracle, Sybase, Microsoft's Access and\nSQLServer, FoxBase, and Paradox. Relational database systems are ubiquitous\nin the marketplace and represent a multibillion dollar industry.\nThe relational model is very simple and elegant: a database is a collection of\none or more relations, where each relation is a table with rows and columns.\nThis simple tabular representation enables even novice users to understand the\ncontents of a database, and it permits the use of simple, high-level languages\nto query the data. The major advantages of the relational model over the older\ndata models are its simple data representation and the ease with which even\ncomplex queries can be expressed.\n\\Vhile we concentrate on the underlying concepts, we also introduce the Data\nDefinition Language (DDL) features of SQL, the standard language for\ncreating, manipulating, and querying data in a relational DBMS. This allows\nus to ground the discussion firmly in terms of real databa.se systems.\n\nThe Relational 1\\1odd\n59\nvVe discuss the concept of a relation in Section\n~t1 and show how to create\nrelations using the SQL language. An important component of a data model is\nthe set of constructs it provides for specifying conditions that must be satisfied\nby the data.\nSuch conditions, called 'integrity constraints (lGs), enable the\nDBIviS to reject operations that might corrupt the data. We present integrity\nconstraints in the relational model in Section 3.2, along with a discussion of\nSQL support for les. \\Ve discuss how a DBMS enforces integrity constraints\nin Section 3.3.\nIn Section 3.4, we turn to the mechanism for accessing and retrieving data\nfrom the database, query languages, and introduce the querying features of\nSQL, which we examine in greater detail in a later chapter.\nWe then discuss converting an ER diagram into a relational database schema\nin Section 3.5. We introduce views, or tables defined using queries, in Section\n3.6. Views can be used to define the external schema for a database and thus\nprovide the support for logical data independence in the relational model. In\nSection 3.7, we describe SQL commands to destroy and alter tables and views.\nFinally, in Section 3.8 we extend our design case study, the Internet shop in-\ntroduced in Section 2.8, by showing how the ER diagram for its conceptual\nschema can be mapped to the relational model, and how the use of views can\nhelp in this design.\n3.1\nINTRODUCTION TO THE RELATIONAL MODEL\nThe main construct for representing data in the relational model is a relation.\nA relation consists of a relation schema and a relation instance.\nThe\nrelation instance is a table, and the relation schema describes the column heads\nfor the table.\nWe first describe the relation schema and then the relation\ninstance. The schema specifies the relation's name, the name of each field (or\ncolumn, or attribute), and the domain of each field. A domain is referred to\nin a relation schema by the domain name and has a set of associated values.\n\\Ve use the example of student information in a university database from Chap-\nter 1 to illustrate the parts of a relation schema:\nStudents(sid: string, name: string, login: string,\nage: integer, gpa: real)\nThis says, for instance, that the field named sid has a domain named string.\nThe set of values associated with domain string is the set of all character\nstrings.\n\n60\nCHAPTER 3\nWe now turn to the instances of a relation. An instance of a relation is a set\nof tuples, also called records, in which each tuple has the same number of\nfields as the relation schema. A relation instance can be thought of as a table\nin which each tuple is a row, and all rows have the same number of fields. (The\nterm relation instance is often abbreviated to just relation, when there is no\nconfusion with other aspects of a relation such as its schema.)\nAn instance of the Students relation appears in Figure 3.1.\nThe instance 81\nField names\nTUPLES\n(RECORDS,\nROWS)\nFIELDS (ATTRIBUTES, COLUMNS)\n-~\nname\nI---/o'-gz-'n--\n50000\nDave\ndave@cs\n19\n3.3\n53666\nJones\njones@cs\n18\n3.4\n53688\nSmith\nsmith@ee\n18\n3.2\n53650\nSmith\nsmith@math\n19\n3.8\n\"'\\ 53831\nMadayan\nmadayan@music\n11\n1.8\n53832\nGuldu\nguldu@music\n12\n2.0\nFigure 3.1\nAn Instance 81 of the Students Relation\ncontains six tuples and has, as we expect from the schema, five fields. Note that\nno two rows are identical. This is a requirement of the relational model-each\nrelation is defined to be a set of unique tuples or rows.\nIn practice, commercial systems allow tables to have duplicate rows, but we\nassume that a relation is indeed a set of tuples unless otherwise noted. The\norder in which the rows are listed is not important. Figure 3.2 shows the same\nrelation instance. If the fields are named, as in our schema definitions and\nI s'id\nI name\n[.login\n53831\nMadayan\nmadayan@music\n11\n1.8\n53832\nGuldu\ngllldll@music\n12\n2.0\n53688\nSmith\nsmith@ee\n18\n3.2\n53650\nSmith\nsmith@math\n19\n3.8\n53666\nJOI;es\njones@cs\n18\n3.4\n50000\nDave\ndave@cs\n19\n3.3\nFigure 3.2\nAn Alternative Representation of Instance 81 of Students\nfigures depicting relation instances, the order of fields does not matter either.\nHowever, an alternative convention is to list fields in a specific order and refer\n\nThe Relat'ional lvfodel\n61,\nto a field by its position.\nThus, s'id is field 1 of Students, login is field\n:~,\nand so on. If this convention is used, the order of fields is significant. Most\ndatabase systems use a combination of these conventions. For example, in SQL,\nthe named fields convention is used in statements that retrieve tuples and the\nordered fields convention is commonly used when inserting tuples.\nA relation schema specifies the domain of each field or column in the relation\ninstance.\nThese domain constraints in the schema specify an important\ncondition that we want each instance of the relation to satisfy: The values\nthat appear in a column must be drawn from the domain associated with that\ncolumn.\nThus, the domain of a field is essentially the type of that field, in\nprogramming language terms, and restricts the values that can appear in the\nfield.\nMore formally, let R(fI:Dl, ..., In:Dn) be a relation schema, and for each Ii,\n1 :::; i :::; n, let Dami be the set of values associated with the domain named Di.\n.An instance of R that satisfies the domain constraints in the schema is a set of\ntuples with n fields:\n{ (fI : dl ,\n,In: dn)\nI dl E Daml' ... ,dn E Damn}\nThe angular brackets (\n) identify the fields of a tuple. Using this notation,\nthe first Students tuple shown in Figure 3.1 is written as (sid: 50000, name:\nDave, login: dave@cs, age: 19, gpa: 3.3). The curly brackets {...} denote a set\n(of tuples, in this definition). The vertical bar I should be read 'such that,' the\nsymbol E should be read 'in,' and the expression to the right of the vertical\nbar is a condition that must be satisfied by the field values of each tuple in the\nset. Therefore, an instance of R is defined as a set of tuples. The fields of each\ntuple must correspond to the fields in the relation schema.\nDomain constraints are so fundamental in the relational model that we hence-\nforth consider only relation instances that satisfy them; therefore, relation\ninstance means relation instance that satisfies the domain constraints in the\nrelation schema.\nThe degree, also called arity, of a relation is the number of fields. The car-\ndinality of a relation instance is the number of tuples in it. In Figure 3.1, the\ndegree of the relation (the number of columns) is five, and the cardinality of\nthis instance is six.\nA relational database is a collection of relations with distinct relation names.\nThe relational database schema is the collection of schemas for the relations\nin the database. 'For example, in Chapter 1, we discllssed a university database\nwith relations called Students, Faculty, Courses, Rooms, Enrolled, Teaches,\nand Meets~In. An instance of a relational databa..'3e is a collection of relation\n\n62\nCHAPTER~3\ninstances, one per relation schema in the database schema; of course, each\nrelation instance must satisfy the domain constraints in its schema.\n3.1.1\nCreating and Modifying Relations Using SQL\nThe SQL language standard uses the word table to denote relation, and we often\nfollow this convention when discussing SQL. The subset of SQL that supports\nthe creation, deletion, and modification of tables is called the Data Definition\nLanguage (DDL). Further, while there is a command that lets users define new\ndomains, analogous to type definition commands in a programming language,\nwe postpone a discussion of domain definition until Section 5.7. For now, we\nonly consider domains that are built-in types, such as integer.\nThe CREATE TABLE statement is used to define a new table. 1\nTo create the\nStudents relation, we can use the following statement:\nCREATE TABLE Students ( sid\nname\nlogin\nage\ngpa\nCHAR(20) ,\nCHAR(30) ,\nCHAR(20) ,\nINTEGER,\nREAL)\nTuples are inserted ,using the INSERT command. We can insert a single tuple\ninto the Students table as follows:\nINSERT\nINTO\nStudents\n(sid, name, login, age, gpa)\nVALUES (53688, 'Smith', 'smith@ee', 18, 3.2)\nWe can optionally omit the list of column names in the INTO clause and list\nthe values in the appropriate order, but it is good style to be explicit about\ncolumn names.\nWe can delete tuples using the DELETE command. We can delete all Students\ntuples with name equal to Smith using the command:\nDELETE\nFROM\nWHERE\nStudents S\nS.name = 'Smith'\n1SQL also provides statements to destroy tables and to change the columns associated with a table;\nwe discuss these in Section 3.7.\n\nThe Relational Pr10del\nvVe can modify the column values in an existing row using the UPDATE com-\nmand. For example, we can increment the age and decrement the gpa of the\nstudent with sid 53688:\nUPDATE Students S\nSET\nS.age = S.age + 1, S.gpa = S.gpa -\n1\nWHERE\nS.sid = 53688\nThese examples illustrate some important points. The WHERE clause is applied\nfirst and determines which rows are to be modified.\nThe SET clause then\ndetermines how these rows are to be modified. If the column being modified is\nalso used to determine the new value, the value used in the expression on the\nright side of equals (=) is the old value, that is, before the modification. To\nillustrate these points further, consider the following variation of the previous\nquery:\nUPDATE Students S\nSET\nS.gpa = S.gpa - 0.1\nWHERE\nS.gpa >= 3.3\nIf this query is applied on the instance 81 of Students shown in Figure 3.1, we\nobtain the instance shown in Figure 3.3.\nI sid\nI name\nI login\n50000\nDave\ndave@cs\n19\n3.2\n53666\nJones\njones@cs\n18\n3.3\n53688\nSmith\nsmith@ee\n18\n3.2\n53650\nSmith\nsmith@math\n19\n3.7\n53831\nMadayan\nmadayan@music\n11\n1.8\n53832\nGuldu\nguldu@music\n12\n2.0\nFigure 3.3\nStudents Instance 81 after Update\n3.2\nINTEGRITY CONSTRAINTS OVER RELATIONS\nA database is only as good as the information stored in it, and a DBMS must\ntherefore help prevent the entry of incorrect information. An integrity con-\nstraint (Ie) is a condition specified on a database schema and restricts the\ndata that can be stored in an instance of the databa'3e. If a database instance\nsatisfies all the integrity constraints specified on the database schema, it is a\nlegal instance. A DBMS enforces integrity constraints, in that it permits only\nlegal instances to be stored in the database.\nIntegrity constraints are specified and enforced at different times:\n\n64\nCHAPTER 3\n1. \\\\Then the DBA or end user defines a database schema, he or she specifies\nthe rcs that must hold on any instance of this database.\n2. \"Vhen a database application is run, the DBMS checks for violations and\ndisallows changes to the data that violate the specified ICs.\n(In some\nsituations, rather than disallow the change, the DBMS might make some\ncompensating changes to the data to ensure that the database instance\nsatisfies all ICs. In any case, changes to the database are not allowed to\ncreate an instance that violates any IC.) It is important to specify exactly\nwhen integrity constraints are checked relative to the statement that causes\nthe change in the data and the transaction that it is part of. We discuss\nthis aspect in Chapter 16, after presenting the transaction concept, which\nwe introduced in Chapter 1, in more detail.\nMany kinds of integrity constraints can be specified in the relational model.\nWe have already seen one example of an integrity constraint in the domain\nconstraints associated with a relation schema (Section 3.1). In general, other\nkinds of constraints can be specified as well; for example, no two students\nhave the same sid value. In this section we discuss the integrity constraints,\nother than domain constraints, that a DBA or user can specify in the relational\nmodel.\n3.2.1\nKey Constraints\nConsider the Students relation and the constraint that no two students have the\nsame student id. This IC is an example of a key constraint. A key constraint\nis a statement that a certain minimal subset of the fields of a relation is a\nunique identifier for a tuple.\nA set of fields that uniquely identifies a tuple\naccording to a key constraint is called a candidate key for the relation; we\noften abbreviate this to just key. In the case of the Students relation, the (set\nof fields containing just the) sid field is a candidate key.\nLet us take a closer look at the above definition of a (candidate) key. There\nare two parts to the definition: 2\n1. Two distinct tuples in a legal instance (an instance that satisfies all Ies,\nincluding the key constraint) cannot have identical values in all the fields\nof a key.\n2. No subset of the set of fields in a key is a unique identifier for a tuple.\n2The term key is rather overworked.\nIn the context of access methods, we speak of sear'ch keys.\nwhich are quite different.\n\nThe Relational ltdodel\nThe first part of the definition means that, in any legal instance, the values in\nthe key fields uniquely identify a tuple in the instance. \\Vhen specifying a key\nconstraint, the DBA or user must be sure that this constraint will not prevent\nthem from storing a 'correct' set of tuples. (A similar comment applies to the\nspecification of other kinds of les as well.)\nThe notion of •correctness' here\ndepends on the nature of the data being stored. For example, several students\nmay have the same name, although each student has a unique student id. If\nthe name field is declared to be a key, the DBMS will not allow the Students\nrelation to contain two tuples describing different students with the same name!\nThe second part of the definition means, for example, that the set of fields\n{sid, name} is not a key for Students, because this set properly contains the\nkey {sid}. The set {sid, name} is an example of a superkey, which is a set of\nfields that contains a key.\nLook again at the instance of the Students relation in Figure 3.1. Observe that\ntwo different rows always have different sid values; sid is a key and uniquely\nidentifies a tuple. However, this does not hold for nonkey fields. For example,\nthe relation contains two rows with Smith in the name field.\nNote that every relation is guaranteed to have a key. Since a relation is a set of\ntuples, the set of all fields is always a superkey. If other constraints hold, some\nsubset of the fields may form a key, but if not, the set of all fields is a key.\nA relation may have several candidate keys.\nFor example, the login and age\nfields of the Students relation may, taken together, also identify students uniquely.\nThat is, {login, age} is also a key. It may seem that login is a key, since no\ntwo rows in the example instance have the same login value. However, the key\nmust identify tuples uniquely in all possible legal instances of the relation. By\nstating that {login, age} is a key, the user is declaring that two students may\nhave the same login or age, but not both.\nOut of all the available candidate keys, a database designer can identify a\nprimary key.\nIntuitively, a tuple can be referred to from elsewhere in the\ndatabase by storing the values of its primary key fields. For example, we can\nrefer to a Students tuple by storing its sid value. As a consequence of referring\nto student tuples in this manner, tuples are frequently accessed by specifying\ntheir sid value.\nIn principle, we can use any key, not just the primary key,\nto refer to a tuple.\nHowever, using the primary key is preferable because it\nis what the DBMS expects this is the significance of designating a particular\ncandidate key as a primary key\nand optimizes for. For example, the DBMS\nmay create an index with the primary key fields 8..'3 the search key, to make the\nretrieval of a tuple given its primary key value efficient. The idea of referring\nto a tuple is developed further in the next section.\n\n66\nSpecifying Key Constraints in SQL\nCHAPTER,3\nIn SQL, we can declare that a subset of the columns of a table constitute a key\nby using the UNIQUE constraint. At most one of these candidate keys can be\ndeclared to be a primary key, using the PRIMARY KEY constraint.\n(SQL does\nnot require that such constraints be declared for a table.)\nLet us revisit our example table definition and specify key information:\nCREATE TABLE Students ( sid\nCHAR(20) ,\nname CHAR (30) ,\nlogin\nCHAR(20) ,\nage\nINTEGER,\ngpa\nREAL,\nUNIQUE (name, age),\nCONSTRAINT StudentsKey PRIMARY KEY (sid) )\nThis definition says that sid is the primary key and the combination of name\nand age is also a key.\nThe definition of the primary key also illustrates how\nwe can name a constraint by preceding it with CONSTRAINT constraint-name.\nIf the constraint is violated, the constraint name is returned and can be used\nto identify the error.\n3.2.2\nForeign Key Constraints\nSometimes the information stored in a relation is linked to the information\nstored in another relation. If one of the relations is modified, the other must be\nchecked, and perhaps modified, to keep the data consistent. An IC involving\nboth relations must be specified if a DBMS is to make such checks. The most\ncommon IC involving two relations is a foreign key constraint.\nSuppose that, in addition to Students, we have a second relation:\nEnrolled(studid: string, cid: string, gTade: string)\nTo ensure that only bona fide students can enroll in courses, any value that\nappears in the studid field of an instance of the Enrolled relation should also\nappear in the sid field of some tuple in the Students relation. The st'udid field\nof Enrolled is called a foreign key and refers to Students. The foreign key in\nt~l~referencil1grel~tio~~(Enrolled,inour. exalIlpl~)!nll~tl~latcht~le~)l:lirl~l~y~key:_:-- -\n()f -the JCferC11(;ed relation (Students); that jS,-jtIn~lstJUly(iU\"lhe'-s<lnie-~l~~i;;~_\n_.. ,'--\"\n.\n\".\n...•.. ....\n........••.....\n....-._--....\n--~-----~-\nof columns and cornpatible dCita types,\naltl1()u~h the column\nnanl(~S can be\nalffei'cii£.'\n-\n.----\n'~'.\n_N~\n~_-~\n\nThe Relational A10del\n67\nThis constraint is illustrated in Figure 3.4. As the figure shows, there may well\nbe some Students tuples that are not referenced from Enrolled (e.g., the student\nwith sid=50000). However, every studid value that appears in the instance of\nthe Enrolled table appears in the primary key column of a row in the Students\ntable.\nForeign key\nr---I\nPrimary key\n~\ncid\ngrade studid ~ -- sid\nname\nlogin\nage\ngpa\n~===:====:==~\n~==i====*==========*====:::*~\nCarnatic101\nC\n53831,\n50000\nDave\ndave@cs\nI---~~~~~+-~--t\nReggae203\nB\n53832, '-\n,-f\n53666\nJones\njones@cs\nI---..::::::...~~~~+-~--t\nTopology112\nA\n5365(}-~' ,\\'\n53688\nSmith\nsmith@ee\n1---~-=~~~+-~---1 ,'\\- ,\nHistory105\nB\n53666\"\n\\'\"\",~\n53650\nSmith\nsmith@math\n~~-=--~-'----~...L-~-J\n\\\"'\\\n53831\nMadayan\nmadayan@music\n\"'\\\n53832\nGuldu\ngu1du@music\n19\n18\n18\n19\n11\n12\n3.3\n3.4\n3.2\n3.8\n1.8\n2.0\nEnrolled (Referencing relation)\nStudents (Referenced relation)\nFigure 3.4\nReferential Integrity\nIf we try to insert the tuple (55555, Artl04, A) into E1, the Ie is violated be-\ncause there is no tuple in 51 with sid 55555; the database system should reject\nsuch an insertion. Similarly, if we delete the tuple (53666, Jones, jones@cs, 18,\n3.4) from 51, we violate the foreign key constraint because the tuple (53666,\nHistoryl05, B) in El contains studid value 53666, the sid of the deleted Stu-\ndents tuple. The DBMS should disallow the deletion or, perhaps, also delete\nthe Enrolled tuple that refers to the deleted Students tuple. We discuss foreign\nkey constraints and their impact on updates in Section 3.3.\nFinally, we note that a foreign key could refer to the same relation. For example,\nwe could extend the Students relation with a column called partner and declare\nthis column to be a foreign key referring to Students. Intuitively, every student\ncould then have a partner, and the partner field contains the partner's sid. The\nobservant reader will no doubt ask, \"y\\That if a student does not (yet) have\na partnerT' This situation is handled in SQL by using a special value called\nnull. The use of nun in a field of a tuple rneans that value in that field is either\nunknown or not applicable (e.g., we do not know the partner yet or there is\nno partner). The appearanC(~ of null in a foreign key field does not violate the\nforeign key constraint.\nHowever, null values are not allowed to appear in a\nprimary key field (because the primary key fields are used to identify a tuple\nuniquely). \\Ve discuss null values further in Chapter 5.\n\n68\nSpecifying Foreign Key Constraints in SQL\nCHAPTERp3\nLet us define Enrolled(studid: string, cid: string, grade: string):\nCREATE TABLE Enrolled ( studid CHAR(20) ,\ncid\nCHAR(20),\ngrade CHAR(10),\nPRIMARY KEY (studid, cid),\nFOREIGN KEY (studid) REFERENCES Students)\nThe foreign key constraint states that every st'udid value in Enrolled must also\nappear in Students, that is, studid in Enrolled is a foreign key referencing Stu-\ndents. Specifically, every studid value in Enrolled must appear as the value in\nthe primary key field, sid, of Students. Incidentally, the primary key constraint\nfor Enrolled states that a student has exactly one grade for each course he or\nshe is enrolled in. If we want to record more than one grade per student per\ncourse, we should change the primary key constraint.\n3.2.3\nGeneral Constraints\nDomain, primary key, and foreign key constraints are considered to be a fun-\ndamental part of the relational data model and are given special attention in\nmost commercial systems. Sometimes, however, it is necessary to specify more\ngeneral constraints.\nFor example, we may require that student ages be within a certain range of\nvalues; given such an IC specification, the DBMS rejects inserts and updates\nthat violate the constraint. This is very useful in preventing data entry errors.\nIf we specify that all students must be at least 16 years old, the instance of\nStudents shown in Figure 3.1 is illegal because two students are underage. If\nwe disallow the insertion of these two tuples, we have a legal instance, as shown\nin Figure 3.5.\ns'id\nI na'me\nlogin\nI age I gpa I\n53666\nJones\njones@cs\n18\n3.4\n53688\nSmith\nsmithCQ)ee\n18\n~).2\nI\n53650\nSmith\nsmith@math\n19\n3.8 I\n. -\nFigure 3.5\nAn Instance 82 of the Students Relation\nThe IC that students must be older than 16 can be thought of as an extended\ndomain constraint, since we are essentially defining the set of permissible age\n\nThe Relational lV/ode!\n69\nvalues more stringently than is possible by simply using a standard domain\nsuch :'1.S integer. In general, however, constraints that go well beyond domain,\nkey, or foreign key constraints can be specified. For example, we could require\nthat every student whose age is greater than 18 must have a gpa greater than\n3.\nCurrent relational database systems support such general constraints in the\nform of table constraints and assertions. Table constraints are associated with a\nsingle table and checked whenever that table is modified. In contrast, assertions\ninvolve several tables and are checked whenever any of these tables is modified.\nBoth table constraints and assertions can use the full power of SQL queries to\nspecify the desired restriction.\nWe discuss SQL support for table constraints\nand assertions in Section 5.7 because a full appreciation of their power requires\na good grasp of SQL's query capabilities.\n3.3\nENFORCING INTEGRITY CONSTRAINTS\nAs we observed earlier, ICs are specified when a relation is created and enforced\nwhen a relation is modified. The impact of domain, PRIMARY KEY, and UNIQUE\nconstraints is straightforward: If an insert, delete, or update command causes\na violation, it is rejected. Every potential Ie violation is generally checked at\nthe end of each SQL statement execution, although it can be deferred until the\nend of the transaction executing the statement, as we will see in Section 3.3.1.\nConsider the instance 51 of Students shown in Figure 3.1. The following inser-\ntion violates the primary key constraint because there is already a tuple with\nthe s'id 53688, and it will be rejected by the DBMS:\nINSERT\nINTO\nStudents\n(sid, name, login, age, gpa)\nVALUES (53688, 'Mike', 'mike@ee', 17,3.4)\nThe following insertion violates the constraint that the primary key cannot\ncontain null:\nINSERT\nINTO\nStudents\n(sid, name, login, age, gpa)\nVALUES (null, 'Mike', 'mike@ee', 17,3.4)\nOf course, a similar problem arises whenever we try to insert a tuple with a\nvalue in a field that is not in the domain associated with that field, that is,\nwhenever we violate a domain constraint. Deletion does not cause a violation\nof clornain, primary key or unique constraints. However, an update can cause\nviolations, sirnilar to an insertion:\n\n70\nUPDATE Students S\nSET\nS.sid = 50000\nWHERE\nS.sid = 53688\nCHAPTER,3\nThis update violates the primary key constraint because there is already a tuple\nwith sid 50000.\nThe impact of foreign key constraints is more complex because SQL sometimes\ntries to rectify a foreign key constraint violation instead of simply rejecting the\nchange. We discuss the referential integrity enforcement steps taken by\nthe DBMS in terms of our Enrolled and Students tables, with the foreign key\nconstraint that Enrolled.sid is a reference to (the primary key of) Students.\nIn addition to the instance 81 of Students, consider the instance of Enrolled\nshown in Figure 3.4.\nDeletions of Enrolled tuples do not violate referential\nintegrity, but insertions of Enrolled tuples could.\nThe following insertion is\nillegal because there is no Students tuple with sid 51111:\nINSERT\nINTO\nEnrolled\n(cid, grade, studid)\nVALUES ('Hindi101', 'B', 51111)\nOn the other hand, insertions of Students tuples do not violate referential\nintegrity, and deletions of Students tuples could cause violations.\nFurther,\nupdates on either Enrolled or Students that change the studid (respectively,\nsid) value could potentially violate referential integrity.\nSQL provides several alternative ways to handle foreign key violations.\nWe\nmust consider three basic questions:\n1. What should we do if an Enrolled row is inserted, with a studid column\nvalue that does not appear in any row of the Students table?\nIn this case, the INSERT command is simply rejected.\n2. What should we do if a Students row is deleted?\nThe options are:\n•\nDelete all Enrolled rows that refer to the deleted Students row.\n•\nDisallow the deletion of the Students row if an Enrolled row refers to\nit.\n•\nSet the studid column to the sid of some (existing) 'default' student,\nfor every Enrolled row that refers to the deleted Students row.\n\nThe Relational l'lfodel\n71\n•\nFor every Enrolled row that refers to it, set the studid column to null.\nIn our example, this option conflicts with the fact that stud'id is part\nof the primary key of Enrolled and therefore cannot be set to mtll.\nTherefore, we are limited to the first three options in our example,\nalthough this fourth option (setting the foreign key to null) is available\nin general.\n3. What should we do if the primary key val'ue of a Students row is updated?\nThe options here are similar to the previous case.\nSQL allows us to choose any of the four options on DELETE and UPDATE. For\nexample, we can specify that when a Students row is deleted, all Enrolled rows\nthat refer to it are to be deleted as well, but that when the sid column of a\nStudents row is modified, this update is to be rejected if an Enrolled row refers\nto the modified Students row:\nCREATE TABLE Enrolled ( studid CHAR(20) ,\ncid\nCHAR(20) ,\ngrade CHAR(10),\nPRIMARY KEY (studid, dd),\nFOREIGN KEY (studid) REFERENCES Students\nON DELETE CASCADE\nON UPDATE NO ACTION)\nThe options are specified as part of the foreign key declaration. The default\noption is NO ACTION, which means that the action (DELETE or UPDATE) is to be\nrejected, Thus, the ON UPDATE clause in our example could be omitted, with\nthe same effect. The CASCADE keyword says that, if a Students row is deleted,\nall Enrolled rows that refer to it are to be deleted as well. If the UPDATE clause\nspecified CASCADE, and the sid column of a Students row is updated, this update\nis also carried out in each Enrolled row that refers to the updated Students row.\nIf a Students row is deleted, we can switch the enrollment to a 'default' student\nby using ON DELETE SET DEFAULT. The default student is specified 3.'3 part of\nthe definition of the sid field in Enrolled; for example, sid CHAR(20) DEFAULT\n'53666 '. Although the specification of a default value is appropriate in some\nsituations (e.g\"\na default parts supplier if a particular supplier goes out of\nbusiness), it is really not appropriate to switch enrollments to a default student.\nThe correct solution in this example is to also delete all enrollment tuples for\nthe deleted student (that is, CASCADE) or to reject the update.\nSQL also allows the use of null as the default value by specifying ON DELETE\nSET NULL.\n\n72\nCHAPTERf 3\n3.3.1\nTransactions and Constraints\nAs we saw in Chapter 1, a program that runs against a database is called a\ntransaction, and it can contain several statements (queries, inserts, updates,\netc.) that access the database. If (the execution of) a statement in a transac-\ntion violates an integrity constraint, should the DBMS detect this right away\nor should all constraints be checked together just before the transaction com-\npletes?\nBy default, a constraint is checked at the end of every SQL statement that\ncould lead to a violation, and if there is a violation, the statement is rejected.\nSometimes this approach is too inflexible. Consider the following variants of\nthe Students and Courses relations; every student is required to have an honors\ncourse, and every course is required to have a grader, who is some student.\nCREATE TABLE Students ( sid\nCHAR(20) ,\nname CHAR(30),\nlogin\nCHAR(20) ,\nage\nINTEGER,\nhonorsCHAR(10) NOT NULL,\ngpa\nREAL)\nPRIMARY KEY (sid),\nFOREIGN KEY (honors) REFERENCES Courses (cid))\nCREATE TABLE Courses (cid\nCHAR(10),\ncname CHAR (10) ,\ncredits INTEGER,\ngrader CHAR(20) NOT NULL,\nPRIMARY KEY (dd)\nFOREIGN KEY (grader) REFERENCES Students (sid))\nvVhenever a Students tuple is inserted, a check is made to see if the\"honors\ncourse is in the Courses relation, and whenever a Courses tuple is inserted,\na check is made to see that the grader is in the Students relation.\nHow are\nwe to insert the very first course or student tuple?\nOne cannot be inseited\nwithout the other. The only way to accomplish this insertion is to defer the\nconstraint che~king that would normally be carried out at the end of an INSERT\nstatement.\nSQL allows a constraint to be in DEFERRED or IMMEDIATE mode.\nSET CONSTRAINT ConstntintFoo DEFERRED\n\nThe Relational 1\\11odel\nA constraint in deferred mode is checked at commit time.\nIn our example,\nthe foreign key constraints on Boats and Sailors can both be declared to be in\ndeferred mode. \"VVe can then insert? boat with a nonexistent sailor as the cap-\ntain (temporarily making the database inconsistent), insert the sailor (restoring\nconsistency), then commit and check that both constraints are satisfied.\n3.4\nQUERYING RELATIONAL DATA\nA relational database query (query, for short) is a question about the data,\nand the answer consists of a new relation containing the result. For example,\nwe might want to find all students younger than 18 or all students enrolled in\nReggae203. A query language is a specialized language for writing queries.\nSQL is the most popular commercial query language for a relational DBMS.\nWe now present some SQL examples that illustrate how easily relations can be\nqueried.\nConsider the instance of the Students relation shown in Figure 3.1.\nWe can retrieve rows corresponding to students who are younger than 18 with\nthe following SQL query:\nSELECT *\nFROM\nStudents S\nWHERE\nS.age < 18\nThe symbol ,*, means that we retain all fields of selected tuples in the result.\nThink of S as a variable that takes on the value of each tuple in Students, one\ntuple after the other. The condition S. age < 18 in the WHERE clause specifies\nthat we want to select only tuples in which the age field has a value less than\n18. This query evaluates to the relation shown in Figure 3.6.\nI··sid\nj . name\nI login\n53831\nMadayan\nmadayan@music\n11\nI 1.8\n53832\nGuldu\nguldu@music\n12 I 2.0\nFigure 3.6\nStudents with age < 18 OIl Instance 51\nThis example illustrates that the domain of a field restricts the operations\nthat are permitted on field values, in addition to restricting the values that can\nappear in the field. The condition S. age < 18 involves an arithmetic comparison\nof an age value with an integer and is permissible because the domain of age\nis the set of integers.\nOn the other hand, a condition such as S.age = S.\"id\ndoes not make sense because it compares an integer value with a string value,\nand this comparison is defined to fail in SQL; a query containing this condition\nproduces no answer tuples.\n\n74\nCHAPTERJ 3\nIn addition to selecting a subset of tuples, a query can extract a subset of the\nfields of each selected tuple. vVe can compute the names and logins of students\nwho are younger than 18 with the following query:\nSELECT S.name, S.login\nFROM\nStudents S\nWHERE\nS.age < 18\nFigure 3.7 shows the answer to this query; it is obtained by applying the se-\nlection to the instance 81 of Students (to get the relation shown in Figure\n3.6), followed by removing unwanted fields. Note that the order in which we\nperform these operations does matter-if we remove unwanted fields first, we\ncannot check the condition S. age < 18, which involves one of those fields.\nI name\nMadayan\nGuldu\nmadayan@music\nguldu@music\nFigure 3.7\nNames and Logins of Students under 18\nWe can also combine information in the Students and Enrolled relations. If we\nwant to obtain the names of all students who obtained an A and the id of the\ncourse in which they got an A, we could write the following query:\nSELECT S.name, E.cid\nFROM\nStudents S, Enrolled E\nWHERE\nS.sid = E.studid AND E.grade = 'A'\nThis query can be understood as follows: \"If there is a Students tuple Sand\nan Enrolled tuple E such that S.sid = E.studid (so that S describes the student\nwho is enrolled in E) and E.grade = 'A', then print the student's name and\nthe course id.\" When evaluated on the instances of Students and Enrolled in\nFigure 3.4, this query returns a single tuple, (Smith, Topology112).\nWe cover relational queries and SQL in more detail in subsequent chapters.\n3.5\nLOGICAL DATABASE DESIGN: ER TO\nRELATIONAL\nThe ER model is convenient for representing an initial, high-level databi'lse\ndesign. Given an ER diagram describing a databa'3e, a standard approach is\ntaken to generating a relational database schema that closely approximates\n\nThe Relational !'viodel\nthe ER design. (The translation is approximate to the extent that we cannot\ncapture all the constraints implicit in the ER design using SQL, unless we use\ncertain SQL constraints that are costly to check.)\nWe now describe how to\ntranslate an ER diagram into a collection of tables with associated constraints,\nthat is, a relational database schema.\n3.5.1\nEntity Sets to Tables\nAn entity set is mapped to a relation in a straightforward way: Each attribute\nof the entity set becomes an attribute of the table. Note that we know both\nthe domain of each attribute and the (primary) key of an entity set.\nConsider the Employees entity set with attributes ssn, name, and lot shown in\nFigure 3.8. A possible instance of the Employees entity set, containing three\nFigure 3.8\nThe Employees Entity Set\nEmployees entities, is shown in Figure 3.9 in a tabular format.\nI ssn\nI name\nI lot I\n123-22-3666\nAttishoo\n48\n231-31-5368\nSmiley\n22\n131-24-3650\nSmethurst\n35\nFigure 3.9\nAn Instance of the Employees Entity Set\nThe following SQL statement captures the preceding information, including the\ndomain constraints and key information:\nCREATE TABLE Employees ( ssn\nCHAR(11),\nname\nCHAR(30) ,\nlot\nINTEGER,\nPRIMARY KEY (ssn) )\n\n76\nCHAPTER~3\n3.5.2\nRelationship Sets (without Constraints) to Tables\nA relationship set, like an entity set, is mapped to a relation in the relational\nmodel. Vve begin by considering relationship sets without key and participa-\ntion constraints, and we discuss how to handle such constraints in subsequent\nsections. To represent a relationship, we must be able to identify each partic-\nipating entity and give values to the descriptive attributes of the relationship.\nThus, the attributes of the relation include:\n•\nThe primary key attributes of each participating entity set, as foreign key\nfields.\n•\nThe descriptive attributes of the relationship set.\nThe set of nondescriptive attributes is a superkey for the relation. If there are\nno key constraints (see Section 2.4.1), this set of attributes is a candidate key.\nConsider the Works_In2 relationship set shown in Figure 3.10. Each department\nhas offices in several locations and we want to record the locations at which\neach employee works.\nC~~~T:~:~~Cf)(~~fT3~~\nI\nEmployees\nI\nWorksJn2\nr Departments\nI\n~ddress~\ncapacity\nFigure 3.10\nA Ternary Relationship Set\nAll the available information about the Works-ln2 table is captured by the\nfollowing SQL definition:\nCREATE TABLE \\iVorks_In2 ( ssn\nCHAR(11),\ndid\nINTEGER,\naddress\nCHAR(20) ,\nsince\nDATE,\nPRIMARY KEY (8sn, did, address),\nFOREIGN KEY (ssn) REFERENCES Employees,\n\nThe Relational Iv!odel\nFOREIGN KEY (address) REFERENCES Locations,\nFOREIGN KEY (did) REFERENCES Departments)\nNote that the address, did. and ssn fields cannot take on n'ull values. Because\nthese fields are part of the primary key for \\Vorks_In2, a NOT NULL constraint\nis implicit for each of these fields.\nThis constraint ensures that these fields\nuniquely identify a department, an employee, and a location in each tuple\nof WorksJn.\nvVe can also specify that a particular action is desired when a\nreferenced Employees, Departments, or Locations tuple is deleted, as explained\nin the discussion of integrity constraints in Section 3.2.\nIn this chapter, we\nassume that the default action is appropriate except for situations in which the\nsemantics of the ER diagram require some other action.\nFinally, consider the Reports_To relationship set shown in Figure 3.11.\nThe\nFigure 3.11\nThe Reports_To Relationship Set\nrole indicators supervisor and subordinate are used to create meaningful field\nnames in the CREATE statement for the Reports..To table:\nCREATE TABLE Reports_To (\nsupervisor...ssn\nCHAR (11),\nsubordinate...ssn\nCHAR (11) ,\nPRIMARY KEY\n(supervisor~'3sn, subordinate_\",,:>sn),\nFOREIGN KEY (supervisor...ssn) REFERENCES Employees(ssn),\nFOREIGN KEY (subordinate...ssn) REFERENCES Employees(ssn) )\nObserve that we need to explicitly name the referenced field of Employees\nbecause the field name differs from the name(s) of the referring field(s).\n\n78\nCHAPTER~3\n3.5.3\nTranslating Relationship Sets with Key Constraints\nIf a relationship set involves n entity sets and somem of them are linked via\narrows in the ER diagTam, the key for anyone of these m entity sets constitutes\na key for the relation to which the relationship set is mapped. Hence we have\nm candidate keys, and one of these should be designated as the primary key.\nThe translation discussed in Section 2.3 from relationship sets to a relation can\nbe used in the presence of key constraints, taking into account this point about\nkeys.\nConsider the relationship set Manages shown in Figure 3.12.\nThe table cor-\nManages\n>4I\"f--~~-\nFigure 3.12\nKey Constraint on Manages\nresponding to Manages has the attributes ssn, did, since.\nHowever, because\neach department has at most one manager, no two tuples can have the same\ndid value but differ on the ssn value. A consequence of this observation is that\ndid is itself a key for Manages; indeed, the set did, ssn is not a key (because it\nis not minimal). The Manages relation can be defined using the following SQL\nstatement:\nCREATE TABLE Manages (ssn\nCHAR (11) ,\ndid\nINTEGER,\nsince\nDATE,\nPRIMARY KEY (did),\nFOREIGN KEY (ssn) REFERENCES Employees,\nFOREIGN KEY\n(did~REFERENCES\nDepartments)\nA second approach to translating a relationship set with key constraints is\noften superior because it avoids creating a distinct table for the relationship\nset.\nThe idea is to include the information about the relationship set in the\ntable corresponding to the entity set with the key, taking adyantage of the\nkey constraint.\nIn the Manages example, because a departmerl~ has at most\none manager, we can add the key fields of the Employees tuple denoting the\nInanager and the since attribute to the Departments tuple.\n\nThe Relational 1\\1odel\n7~\nThis approach eliminates the need for a separate Manages relation, and queries\nasking for a department's manager can be answered without combining infor-\nmation from two relations. The only drawback to this approach is that space\ncould be wasted if several departments have no managers.\nIn this case the\nadded fields would have to be filled with null values. The first translation (us-\ning a separate table for Manages) avoids this inefficiency, but some important\nqueries require us to combine information from two relations, which can be a\nslow operation.\nThe following SQL statement, defining a DepLMgr relation that captures the\ninformation in both Departments and Manages, illustrates the second approach\nto translating relationship sets with key constraints:\nCREATE TABLE DepLMgr ( did\nINTEGER,\ndname\nCHAR(20),\nbudget\nREAL,\nssn\nCHAR (11) ,\nsince\nDATE,\nPRIMARY KEY (did),\nFOREIGN KEY (ssn) REFERENCES Employees)\nNote that ssn can take on null values.\nThis idea can be extended to deal with relationship sets involving more than\ntwo entity sets. In general, if a relationship set involves n entity sets and some\nTn of them are linked via arrows in the ER diagram, the relation corresponding\nto anyone of the m sets can be augmented to capture the relationship.\nWe discuss the relative merits of the two translation approaches further after\nconsidering how to translate relationship sets with participation constraints\ninto tables.\n3.5.4\nTranslating Relationship Sets with Participation\nConstraints\nConsider the ER diagram in Figure 3.13, which shows two relationship sets,\nManages and \"Vorks_In.\nEvery department is required to have a manager, due to the participation\nconstraint, and at most one manager, due to the key constraint. The following\nSQL statement reflects the second translation approach discussed in Section\n3.5.3, and uses the key constraint:\n\n80\nCHAPTER'3\nFigure 3.13\nManages and WorksJn\nCREATE TABLE DepLMgr ( did\nINTEGER,\ndname\nCHAR(20) ,\nbudget\nREAL,\nssn\nCHAR(11) NOT NULL,\nsince\nDATE,\nPRIMARY KEY (did),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE NO ACTION)\nIt also captures the participation constraint that every department must have\na manager: Because ssn cannot take on null values, each tuple of DepLMgr\nidentifies a tuple in Employees (who is the manager). The NO ACTION specifi-\ncation, which is the default and need not be explicitly specified, ensures that\nan Employees tuple cannot be deleted while it is pointed to by a Dept-Mgr\ntuple. If we wish to delete such an Employees tuple, we must first change the\nDepLMgr tuple to have a new employee &'3 manager. (vVe could have specified\nCASCADE instead of NO ACTION, but deleting all information about a department\njust because its manager has been fired seems a bit extreme!)\nThe constraint that every department must have a manager cannot be cap-\ntured using the first translation approach discussed in Section 3.5.3.\n(Look\nat the definition of lVIanages and think about what effect it would have if we\nadded NOT NULL constraints to the ssn and did fields.\nHint: The constraint\nwould prevent the firing of a manager, but does not ensure that a manager is\ninitially appointed for each department!) This situation is a strong argument\n\nThe Relational lvfodel\n8~\nin favor of using the second approach for one-to-many relationships such as\nManages, especially when the entity set with the key constraint also has a total\nparticipation constraint.\nUnfortunately, there are many participation constraints that we cannot capture\nusing SQL, short of using table constraints or assertions. Table constraints and\nassertions can be specified using the full power of the SQL query language\n(as discussed in Section 5.7) and are very expressive but also very expensive to\ncheck and enforce. For example, we cannot enforce the participation constraints\non the \\iVorks_In relation without using these general constraints. To see why,\nconsider the Works-ln relation obtained by translating the ER diagram into·\nrelations.\nIt contains fields ssn and did, which are foreign keys referring to\nEmployees and Departments. To ensure total participation of Departments in\nWorks_In, we have to guarantee that every did value in Departments appears\nin a tuple of Works_In. We could try to guarantee this condition by declaring\nthat did in Departments is a foreign key referring to Works_In, but this is not\na valid foreign key constraint because did is not a candidate key for Works_In.\nTo ensure total participation of Departments in Works_In using SQL, we need\nan assertion. We have to guarantee that every did value in Departments appears\nin a tuple of Works_In; further, this tuple of Works_In must also have non-null\nvalues in the fields that are foreign keys referencing other entity sets involved in\nthe relationship (in this example, the ssn field). We can ensure the second part\nof this constraint by imposing the stronger requirement that ssn in Works-ln\ncannot contain null values.\n(Ensuring that the participation of Employees in\nWorks_In is total is symmetric.)\nAnother constraint that requires assertions to express in SQL is the requirement\nthat each Employees entity (in the context of the Manages relationship set)\nmust manage at least one department.\nIn fact, the Manages relationship set exemplifies most of the participation con-\nstraints that we can capture using key and foreign key constraints. Manages is\na binary relationship set in which exactly one of the entity sets (Departments)\nhas a key constraint, and the total participation constraint is expressed on that\nentity set.\n\\Ve can also capture participation constraints using key and foreign key con-\nstraints in one other special situation: a relationship set in which all participat-\ning entity sets have key constraints and total participation. The best translation\napproach in this case is to map all the entities &'3 well as the relationship into\na single table; the details are straightforward.\n\n82\nCHAPTER~3\n3.5.5\nTranslating Weak Entity Sets\nA weak entity set always participates in a one-to-many binary relationship and\nhas a key constraint and total participation. The second translation approach\ndiscussed in Section 3.5.3 is ideal in this case, but we must take into account\nthat the weak entity has only a partial key.\nAlso, when an owner entity is\ndeleted, we want all owned weak entities to be deleted.\nConsider the Dependents weak entity set shown in Figure 3.14, with partial\nkey pname. A Dependents entity can be identified uniquely only if we take the\nkey of the owning Employees entity and the pname of the Dependents entity,\nand the Dependents entity must be deleted if the owning Employees entity is\ndeleted.\nEmployees\nFigure 3.14\nThe Dependents Weak Entity Set\nWe can capture the desired semantics with the following definition of the\nDep_Policy relation:\nCREATE TABLE Dep_Policy (pname\nCHAR(20) ,\nage\nINTEGER,\ncost\nREAL,\nssn\nCHAR (11) ,\nPRIMARY KEY (pname, ssn),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE CASCADE )\nObserve that the primary key is (pna:me, ssn) , since Dependents is a weak\nentity. This constraint is a change with respect to the translation discussed in\nSection 3.5.3.\n\\Ve have to ensure that every Dependents entity is associated\nwith an Employees entity (the owner), as per the total participation constraint\non Dependents. That is, ssn cannot be null.\nThis is ensured because\nSST/, is\npart of the primary key. The CASCADE option ensures that information about\nan employee's policy and dependents is deleted if the corresponding Employees\ntuple is deleted.\n\nThe Relational 1\"1,,1oriel\n3.5.6\nTranslating Class Hierarchies\n83\nWe present the two basic approaches to handling ISA hierarchies by applying\nthem to the ER diagram shown in Figure 3.15:\nFigure 3.15\nClass Hierarchy\n1. We can map each of the entity sets Employees, Hourly_Emps, and Con-\ntracLEmps to a distinct relation.\nThe Employees relation is created as\nin Section 2.2.\nWe discuss Hourly~mps here; ContracLEmps is han-\ndled similarly.\nThe relation for Hourly_Emps includes the hourly_wages\nand hours_worked attributes of Hourly_Emps. It also contains the key at-\ntributes of the superclass (ssn, in this example), which serve as the primary\nkey for Hourly_Emps, a.', well as a foreign key referencing the superclass\n(Employees).\nFor each Hourly_Emps entity, the value of the name and\nlot attributes are stored in the corresponding row of the supercla...,s (Em-\nployees). Note that if the superclass tuple is deleted, the delete must be\ncascaded to Hourly~mps.\n2. Alternatively, we can create just two relations, corresponding to Hourly_Emps\nand ContracLEmps.\nThe relation for Hourly~mps includes all the at-\ntributes of Hourly_Emps as well as all the attributes of Employees (i.e.,\nssn, name, lot, hO'l.1,rly_wages, hours_worked:).\nThe first approach is general and always applicable. Queries in which we want\nto (~xamine all employees and do not care about the attributes specific to the\nsubclasses are handled easily using the Employees relation. However, queries\nin which we want to examine, say, hourly employees, may require us to com-\nbine Hourly_Emps (or ContracLEmps, as the case may be) with Employees to\nretrieve name and lot.\n\n84\nCHAPTER··~\nThe second approach is not applicable if we have employees who are neither\nhourly employees nor contract employees, since there is no way to store such\nemployees. Also, if an employee is both an Hourly-.Emps and a ContracLEmps\nentity, then the name and lot: values are stored twice. This duplication can lead\nto some of the anomalies that we discuss in Chapter 19. A query that needs to\nexamine all employees must now examine two relations. On the other hand, a\nquery that needs to examine only hourly employees can now do so by examining\njust one relation. The choice between these approaches clearly depends on the\nsemantics of the data and the frequency of common operations.\nIn general, overlap and covering constraints can be expressed in SQL only by\nusing assertions.\n3.5.7\nTranslating ER Diagrams with Aggregation\nConsider the ER diagram shown in Figure 3.16.\nThe Employees, Projects,\nManilars\nDepartments\nI\n_______did~ fT:C~'~~)\nFigure 3.16\nAggregation\nand Departments entity sets and the Sponsors relationship set are mapped as\ndescribed in previous sections. For the Monitors relationship set, we create a\nrelation with the following attributes: the key attributes of Employees (88n), the\nkey attributes of Sponsors (d'id, p'id), and the descriptive attributes of Monitors\n('/.tnt:'il). This translation is essentially the standard mapping for a relationship\nset, as described in Section 3.5.2.\n\nThe Relational A!odd\n85\n~\nThere is a special case in which this translation can be refined by dropping the\nSponsors relation. Consicler the Sponsors relation. It has attributes pid, did,\nand since; and in general we need it (in addition to l\\rlonitors) for two reasons:\n1. \\Ve have to record the descriptive attributes (in our example, since) of the\nSponsors relationship.\n2. Not every sponsorship has a monitor, and thus some (p'id, did) pairs in the\nSponsors relation may not appear in the Monitors relation.\nHowever, if Sponsors has no descriptive attributes and has total participation\nin Monitors, every possible instance of the Sponsors relation can be obtained\nfrom the (pid, did) columns of Monitors; Sponsors can be dropped.\n3.5.8\nER to Relational: Additional Examples\nConsider the ER diagram shown in Figure 3.17. We can use the key constraints\nFigure 3.17\nPolicy Revisited\nto combine Purchaser information with Policies and Beneficiary information\nwith Dependents, and translate it into the relational model as follows:\nCREATE TABLE Policies ( policyid INTEGER,\ncost\nREAL,\nssn\nCHAR (11) NOT NULL,\nPRIMARY KEY (policyid),\nFOREIGN KEY (ssn) REFERENCES Employees\nON DELETE CASCADE )\n\n86\nCHAPTERB\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nage\nINTEGER,\npolicyid INTEGER,\nPRIMARY KEY (pname, policyid),\nFOREIGN KEY (policyid) REFERENCES Policies\nON DELETE CASCADE)\nNotice how the deletion of an employee leads to the deletion of all policies\nowned by the employee and all dependents who are beneficiaries of those poli-\ncies. Further, each dependent is required to have a covering policy-because\npolicyid is part of the primary key of Dependents, there is an implicit NOT NULL\nconstraint. This model accurately reflects the participation constraints in the\nER diagram and the intended actions when an employee entity is deleted.\nIn general, there could be a chain of identifying relationships for weak entity\nsets. For example, we assumed that policyid uniquely identifies a policy. Sup-\npose that policyid distinguishes only the policies owned by a given employee;\nthat is, policyid is only a partial key and Policies should be modeled as a weak\nentity set. This new assumption about policyid does not cause much to change\nin the preceding discussion.\nIn fact, the only changes are that the primary\nkey of Policies becomes (policyid, ssn) , and as a consequence, the definition of\nDependents changes-a field called ssn is added and becomes part of both the\nprimary key of Dependents and the foreign key referencing Policies:\nCREATE TABLE Dependents (pname\nCHAR(20) ,\nssn\nCHAR (11) ,\nage\nINTEGER,\npolicyid INTEGER NOT NULL,\nPRIMARY KEY (pname, policyid, ssn),\nFOREIGN KEY (policyid, ssn) REFERENCES Policies\nON DELETE CASCADE )\n3.6\nINTRODUCTION TO VIEWS\nA view is a table whose rows are not explicitly stored in the database but\nare computed as needed from a view definition. Consider the Students and\nEnrolled relations. Suppose we are often interested in finding the names and\nstudent identifiers of students who got a grade of B in some course, together\nwith the course identifier. \\Ne can define a view for this purpose. Using SQL\nnotation:\nCREATE VIEW B-Students (name, sid, course)\nAS SELECT S.sname, S.sid, E.cid\n\nThe Relational 1I1odel\nFROM\nWHERE\nStudents S, Enrolled E\nS.sid = E.studid AND E.grade = 'B'\n87\n$\nThe view B-Students has three fields called name, sid, and course with the\nsame domains as the fields sname and sid in Students and cid in Enrolled.\n(If the optional arguments name, sid, and course are omitted from the CREATE\nVIEW statement, the column names sname, sid, and cid are inherited.)\nThis view can be used just like a base table, or explicitly stored table, in\ndefining new queries or views. Given the instances of Enrolled and Students\nshown in Figure 3.4, B-Students contains the tuples shown in Figure 3.18.\nConceptually, whenever B-Students is used in a query, the view definition is\nfirst evaluated to obtain the corresponding instance of B-Students, then the rest\nof the query is evaluated treating B-Students like any other relation referred\nto in the query. (We discuss how queries on views are evaluated in practice in\nChapter 25.)\nsid\ncourse\nHistory105\nReggae203\nFigure 3.18\nAn Instance of the B-Students View\n3.6.1\nViews, Data Independence, Security\nConsider the levels of abstraction we discussed in Section 1.5.2. The physical\nschema for a relational database describes how the relations in the conceptual\nschema are stored, in terms of the file organizations and indexes used.\nThe\nconceptual schema is the collection of schemas of the relations stored in the\ndatabase. While some relations in the conceptual schema can also be exposed to\napplications, that is, be part of the exte'mal schema of the database, additional\nrelations in the external schema can be defined using the view mechanism.\nThe view mechanism thus provides the support for logical data independence\nin the relational model.\nThat is, it can be used to define relations in the\nexternal schema that mask changes in the conceptual schema of the database\nfrom applications. For example, if the schema of a stored relation is changed,\nwe can define a view with the old schema and applications that expect to see\nthe old schema can now use this view.\nViews are also valuable in the context of security: We can define views that\ngive a group of users access to just the information they are allowed to see. For\nexample, we can define a view that allows students to see the other students'\n\n88\nCHAPTER B\nname and age but not their gpa, and allows all students to access this view but\nnot the underlying Students table (see Chapter 21).\n3.6.2\nUpdates on Views\nThe motivation behind the view mechanism is to tailor how users see the data.\nUsers should not have to worry about the view versus base table distinction.\nThis goal is indeed achieved in the case of queries on views; a view can be used\njust like any other relation in defining a query. However, it is natural to want to\nspecify updates on views as well. Here, unfortunately, the distinction between\na view and a ba.se table must be kept in mind.\nThe SQL-92 standard allows updates to be specified only on views that are\ndefined on a single base table using just selection and projection, with no use of\naggregate operations.3 Such views are called updatable views. This definition\nis oversimplified, but it captures the spirit of the restrictions.\nAn update on\nsuch a restricted view can always be implemented by updating the underlying\nbase table in an unambiguous way. Consider the following view:\nCREATE VIEW GoodStudents (sid, gpa)\nAS SELECT S.sid, S.gpa\nFROM\nStudents S\nWHERE\nS.gpa> 3.0\nWe can implement a command to modify the gpa of a GoodStudents row by\nmodifying the corresponding row in Students. We can delete a GoodStudents\nrow by deleting the corresponding row from Students. (In general, if the view\ndid not include a key for the underlying table, several rows in the table could\n'correspond' to a single row in the view. This would be the case, for example,\nif we used S.sname instead of S.sid in the definition of GoodStudents. A com-\nmand that affects a row in the view then affects all corresponding rows in the\nunderlying table.)\nWe can insert a GoodStudents row by inserting a row into Students, using\nnull values in columns of Students that do not appear in GoodStudents (e.g.,\nsname, login). Note that primary key columns are not allowed to contain null\nvalues. Therefore, if we attempt to insert rows through a view that does not\ncontain the primary key of the underlying table, the insertions will be rejected.\nFor example, if GoodStudents contained snarne but not ,c;id, we could not insert\nrows into Students through insertions to GooclStudents.\n3There is also the restriction that the DISTINCT operator cannot be used in updatable vi(;w defi-\nnitions. By default, SQL does not eliminate duplicate copies of rows from the result of it query; the\nDISTINCT operator requires duplicate elimination. vVe discuss t.his point further in Chapt.er 5.\n\nThe Relational A10dd\n----------------~-\nUpdatable Views in SQL:1999 The Hew SQL standard has expanded\nthe class of view definitions that are\nupdatable~ taking primary . key\nconstraints into account.\nIn contra..')t·to SQL-92~ a· view definition that\ncontains more than OIle table in the FROM clause may be updatable under\nthe new definition.\nIntuitively~ we can update afield of a. view if it is\nobtained from exactly one of the underlying tables, and the primary key\nof that table is included in the fields of the view.\nSQL:1999 distinguishes between views whose rows can be modified (updat-\nable views) and views into which new rows can be inserted (insertable-\ninto views): Views defined using the SQL constructs UNION, INTERSECT,\nand EXCEPT (which we discuss in Chapter 5) cannot be inserted into, even\nif they are updatable. Intuitively, updatability ensures that an updated\ntuple in the view can be traced to exactly one tuple in one of the tables\nused to define the view. The updatability property, however, may still not\nenable us to decide into which table to insert a new tuple.\nAn important observation is that an INSERT or UPDATE may change the un-\nderlying base table so that the resulting (i.e., inserted or modified) row is not\nin the view! For example, if we try to insert a row (51234, 2.8) into the view,\nthis row can be (padded with null values in the other fields of Students and\nthen) added to the underlying Students table, but it will not appear in the\nGoodStudents view because it does not satisfy the view condition gpa > 3.0.\nThe SQL default action is to allow this insertion, but we can disallow it by\nadding the clause WITH CHECK OPTION to the definition of the view.\nIn this\ncase, only rows that will actually appear in the view are permissible insertions.\nWe caution the reader, that when a view is defined in terms of another view,\nthe interaction between these view definitions with respect to updates and the\nCHECK OPTION clause can be complex; we not go into the details.\nNeed to Restrict View Updates\nvVhile the SQL rules on updatable views are more stringent than necessary,\nthere are some fundamental problems with updates specified on views and good\nreason to limit the class of views that can be updated. Consider the Students\nrelation and a new relation called Clubs:\nClubs( cname: string, jyear: date, mnarne: string)\n\n90\n~\nSailing\n1996\nDave\nHiking\n1997\nSmith\nRowing\n1998\nSmith\nDave\nJones\nSmith\nSmith\ndave(gcs\njones~~cs\nsmith@ee\nsmith@math\nCHAPTER 8\nFigure 3.19\nAn Instance C of Clubs\nI name\n,. login\nFigure 3.20\nAn Instance 53 of Students\nI dub\nsince\nDave\ndave@cs\nSailing\n1996\nSmith\nsmith@ee\nHiking\n1997\nSmith\nsmith@ee\nRowing\n1998\nSmith\nsmith@math\nHiking\n1997\nSmith\nsmith@math\nRowing\n1998\nFigure 3.21\nInstance of ActiveStudents\nA tuple in Clubs denotes that the student called mname has been a member of\nthe club cname since the date jyear.4 Suppose that we are often interested in\nfinding the names and logins of students with a gpa greater than 3 who belong\nto at least one club, along with the club name and the date they joined the\nclub. We can define a view for this purpose:\nCREATE VIEW ActiveStudents (name, login, club, since)\nAS SELECT S.sname, S.login, C.cname, C.jyear\nFROM\nStudents S, Clubs C\nWHERE\nS.sname = C.mname AND S.gpa> 3\nConsider the instances of Students and Clubs shown in Figures 3.19 and 3.20.\nWhen evaluated using the instances C and S3, ActiveStudents contains the\nrows shown in Figure 3.21.\nNow suppose that we want to delete the row (Smith, smith@ee, Hiking, 1997)\nfrom ActiveStudents.\nHow are we to do this?\nActiveStudents rows are not\nstored explicitly but computed as needed from the Students and Clubs tables\nusing the view definition.\nSo we must change either Students or Clubs (or\nboth) in such a way that evaluating the view definition on the modified instance\ndoes not produce the row (Snrith, 8Tnith@ec, Hiking, 1997.) This ta.sk can be\nctccomplished in one of two ways:\nby either deleting the row (53688.. Sm'ith,\n8Tn'ith(iJ)ee, 18, ,g.2) from Students or deleting the row (Hiking, 1.997, 8m/ith)\nclvVe remark that Clubs has a poorly designed schema (chosen for the sake of our discussion of view\nupdates), since it identifies students by name, which is not a candidate key for Students.\n\nThe Relational tv!odel\n9J\nfrom Clubs. But neither solution is satisfactory. Removing the Students row\nhas the effect of also deleting the row (8m:ith, smith@ee, Rowing, 1998) from the\nview ActiveStudents. Removing the Clubs row h&'3 the effect of also deleting the\nrow (Smith, smith@math, Hiking, 1991) from the view ActiveStudents. Neither\nside effect is desirable. In fact, the only reasonable solution is to d'isallow such\nupdates on views.\nViews involving more than one base table can, in principle, be safely updated.\nThe B-Students view we introduced at the beginning of this section is an ex-\nample of such a view.\nConsider the instance of B-Students shown in Figure\n3.18 (with, of course, the corresponding instances of Students and Enrolled as\nin Figure 3.4). To insert a tuple, say (Dave, 50000, Reggae203) B-Students, we\ncan simply insert a tuple (Reggae203, B, 50000) into Enrolled since there is al-\nready a tuple for sid 50000 in Students. To insert (John, 55000, Reggae203), on\nthe other hand, we have to insert (Reggae203, B, 55000) into Enrolled and also\ninsert (55000, John, null, null, null) into Students. Observe how null values\nare used in fields of the inserted tuple whose value is not available. Fortunately,\nthe view schema contains the primary key fields of both underlying base tables;\notherwise, we would not be able to support insertions into this view. To delete\na tuple from the view B-Students, we can simply delete the corresponding tuple\nfrom Enrolled.\nAlthough this example illustrates that the SQL rules on updatable views are\nunnecessarily restrictive, it also brings out the complexity of handling view\nupdates in the general case. For practical reasons, the SQL standard has chosen\nto allow only updates on a very restricted class of views.\n3.7\nDESTROYING/ALTERING TABLES AND VIEWS\nIf we decide that we no longer need a base table and want to destroy it (i.e.,\ndelete all the rows and remove the table definition information), we can use\nthe DROP TABLE command. For example, DROP TABLE Students RESTRICT de-\nstroys the Students table unless some view or integrity constraint refers to\nStudents; if so, the command fails. If the keyword RESTRICT is replaced by\nCASCADE, Students is dropped and any referencing views or integrity constraints\nare (recursively) dropped as well; one of these t\\VO keyvlOrds must always be\nspecified. A vipw can be dropped using the DROP VIEW command, which is just\nlike DROP TABLE.\nALTER TABLE modifies the structure of an existing table.\nTo add a column\ncalled maiden-name to Students, for example, we would use the following com-\nmand:\n\n92\nALTER TABLE Students\nADD COLUMN maiden-name CHAR(10)\nCUAPTER·.'3\nThe definition of Students is modified to add this column, and all existing rows\nare padded with null values in this column.\nALTER TABLE can also be used\nto delete columns and add or drop integrity constraints on a table; we do not\ndiscuss these aspects of the command beyond remarking that dropping columns\nis treated very similarly to dropping tables or views.\n3.8\nCASE STUDY: THE INTERNET STORE\nThe next design step in our running example, continued from Section 2.8, is\nlogical database design. Using the standard approach discussed in Chapter 3,\nDBDudes maps the ER diagram shown in Figure 2.20 to the relational model,\ngenerating the following tables:\nCREATE TABLE Books ( isbn\nCHAR ( 10) ,\ntitle\nCHAR(80) ,\nauthor\nCHAR(80),\nqty_in-stock\nINTEGER,\nprice\nREAL,\nyeaLpublished\nINTEGER,\nPRIMARY KEY (isbn))\nCREATE TABLE Orders ( isbn\nCHAR (10) ,\nciel\nINTEGER,\ncarelnum\nCHAR (16) ,\nqty\nINTEGER,\norder_date\nDATE,\nship_date\nDATE,\nPRIMARY KEY (isbn,cid),\nFOREIGN KEY (isbn) REFERENCES Books,\nFOREIGN KEY (cid) REFERENCES Customers)\nCREATE TABLE Customers ( cid\nINTEGER,\ncname\nCHAR(80),\naddress\nCHAR(200),\nPRIMARY KEY (cid)\nThe design team leader, who is still brooding over the fact that the review\nexposed a flaw in the design, now has an inspiration. The Orders table contains\nthe field order_date and the key for the table contains only the fields isbn and\nc'id. Because of this, a customer cannot order the same book OIl different days,\n\nThe Relat'ional l1;lodel\n9~\na re.striction that was not intended. vVhy not add the order-date attribute to\nthe key for the Orders table? This would eliminate the unwanted restrietion:\nCREATE TABLE Orders (\nisbn\nCHAR(10) ,\nPRIMARY KEY (isbn,cid,ship_date),\n...)\nThe reviewer, Dude 2, is not entirely happy with this solution, which he calls\na 'hack'.\nHe points out that no natural ER diagram reflects this design and\nstresses the importance of the ER diagram\n&<; a design do·cument.\nDude 1\nargues that, while Dude 2 has a point, it is important to present B&N with\na preliminary design and get feedback; everyone agrees with this, and they go\nback to B&N.\nThe owner of B&N now brings up some additional requirements he did not\nmention during the initial discussions: \"Customers should be able to purchase\nseveral different books in a single order. For example, if a customer wants to\npurchase three copies of 'The English Teacher' and two copies of 'The Character\nof Physical Law,' the customer should be able to place a single order for both\nbooks.\"\nThe design team leader, Dude 1, asks how this affects the shippping policy.\nDoes B&N still want to ship all books in an order together?\nThe owner of\nB&N explains their shipping policy: \"As soon as we have have enough copies\nof an ordered book we ship it, even if an order contains several books. So it\ncould happen that the three copies of 'The English Teacher' are shipped today\nbecause we have five copies in stock, but that 'The Character of Physical Law'\nis shipped tomorrow, because we currently have only one copy in stock and\nanother copy arrives tomorrow.\nIn addition, my customers could place more\nthan one order per day, and they want to be able to identify the orders they\nplaced.\"\nThe DBDudes team thinks this over and identifies two new requirements: First,\nit must be possible to order several different books in a single order and sec-\nond, a customer must be able to distinguish between several orders placed the\nsame day. To accomodate these requirements, they introduce a new attribute\ninto the Orders table called ordernum, which uniquely identifies an order and\ntherefore the customer placing the order. However, since several books could be\npurchased in a single order, onleTnum and isbn are both needed to determine\nqt.y and ship_dat.e in the Orders table.\nOrders are assign(~d order numbers sequentially and orders that are placed later\nhave higher order numbers. If several orders are placed by the same customer\n\n94\nCHAPTER03\non a single day, these orders have different order numbers and can thus be\ndistinguished. The SQL DDL statement to create the modified Orders table\nfollows:\nCREATE TABLE Orders ( ordernum\nINTEGER,\nisbn\nCHAR(10),\ndd\nINTEGER,\ncardnum\nCHAR (16) ,\nqty\nINTEGER,\nordeLdate\nDATE,\nship~date\nDATE,\nPRIMARY KEY (ordernum, isbn),\nFOREIGN KEY (isbn) REFERENCES Books\nFOREIGN KEY (dd) REFERENCES Customers)\nThe owner of B&N is quite happy with this design for Orders, but has realized\nsomething else. (DBDudes is not surprised; customers almost always come up\nwith several new requirements as the design progresses.) While he wants all\nhis employees to be able to look at the details of an order, so that they can\nrespond to customer enquiries, he wants customers' credit card information to\nbe secure. To address this concern, DBDudes creates the following view:\nCREATE VIEW OrderInfo (isbn, cid, qty, order-date, ship_date)\nAS SELECT O.cid, O.qty, O.ordeLdate, O.ship_date\nFROM\nOrders 0\nThe plan is to allow employees to see this table, but not Orders; the latter is\nrestricted to B&N's Accounting division. We'll see how this is accomplished in\nSection 21.7.\n3.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhat is a relation? Differentiate between a relation schema and a relation\ninstance. Define the terms arity and degree of a relation. What are domain\nconstraints? (Section 3.1)\n•\nWhat SQL construct enables the definition of a relation? \\Vhat constructs\nallow modification of relation instances? (Section 3.1.1)\n•\n\\Vhat are integrity constraints? Define the terms primary key constTa'int\nand foreign key constraint. How are these constraints expressed in SQL?\nWhat other kinds of constraints can we express in SQL? (Section 3.2)\n\nThe Relational Alodel\n95\n•\n\\Vhat does the DBMS do when constraints are violated? What is referen-\ntial 'integr-ity? \\Vhat options does SQL give application programmers for\ndealing with violations of referential integrity? (Section 3.3)\n•\nWhen are integrity constraints enforced by a DBMS? How can an appli-\ncation programmer control the time that constraint violations are checked\nduring transaction execution? (Section 3.3.1)\n•\nWhat is a relational database query? (Section 3.4)\n•\nHow can we translate an ER diagram into SQL statements to create ta-\nbles?\nHow are entity sets mapped into relations?\nHow are relationship\nsets mapped? How are constraints in the ER model, weak entity sets, class\nhierarchies, and aggregation handled? (Section 3.5)\n•\nWhat is a view? How do views support logical data independence? How\nare views used for security?\nHow are queries on views evaluated? Why\ndoes SQL restrict the class of views that can be updated? (Section 3.6)\n•\nWhat are the SQL constructs to modify the structure of tables and de--\nstray tables and views? Discuss what happens when we destroy a view.\n(Section 3.7)\nEXERCISES\nExercise 3.1 Define the following terms: relation schema, relational database schema, do-\nmain, relation instance, relation cardinality, and relation degree.\nExercise 3.2 How many distinct tuples are in a relation instance with cardinality 22?\nExercise 3.3 Does the relational model, as seen by an SQL query writer, provide physical\nand logical data independence? Explain.\nExercise 3.4 \\\\That is the difference between a candidate key and the primary key for a given\nrelation? What is a superkey?\nExercise 3.5 Consider the instance of the Students relation shown in Figure 3.1.\n1. Give an example of an attribute (or set of attributes) that you can deduce is not a\ncandidate key, based on this instance being legaL\n2. Is there any example of an attribute (or set of attributes) that you can deduce is a\ncandidate key, based on this instance being legal?\nExercise 3.6 What is a foreign key constraint? Why are such constraints important? What\nis referential integrity?\nExercise 3.7 Consider the relations Students, Faculty, Courses, Rooms, Enrolled, Teaches,\n'Lnd Meets_In defined in Section 1.5.2.\n\n96\nCHAPTERr3\n1. List all the foreign key constraints among these relations.\n2. Give an example of a (plausible) constraint involving one or more of these relations that\nis not a primary key or foreign key constraint.\nExercise 3.8 Answer each of the following questions briefly. The questions are b&'>ed OIl the\nfollowing relational schema:\nEmp( eid: integer, ename: string, age: integer, sala1l1: real)\nWorks(eid: integer, did: integer, peLtime: integer)\nDept(did: integer, dname: string, budget: real, managerid: integer)\n1. Give an example of a foreign key constraint that involves the Dept relation. What are\nthe options for enforcing this constraint when a user attempts to delete a Dept tuple?\n2. Write the SQL statements required to create the preceding relations, including appro-\npriate versions of all primary and foreign key integrity constraints.\n3. Define the Dept relation in SQL so that every department is guaranteed to have a\nmanager.\n4. Write an SQL statement to add John Doe as an employee with eid = 101, age = 32 and\nsalary = 15,000.\n5. Write an SQL statement to give every employee a 10 percent raise.\n6. Write an SQL statement to delete the Toy department. Given the referential integrity\nconstraints you chose for this schema, explain what happens when this statement is\nexecuted.\nExercise 3.9 Consider the SQL query whose answer is shown in Figure 3.6.\n1. Modify this query so that only the login column is included in the answer.\n2. If the clause WHERE S.gpa >= 2 is added to the original query, what is the set of tuples\nin the answer?\nExercise 3.10 Explain why the addition of NOT NULL constraints to the SQL definition of\nthe Manages relation (in Section 3.5.3) would not enforce the constraint that each department\nmust have a manager. What, if anything, is achieved by requiring that the S8n field of Manages\nbe non-null?\nExercise 3.11 Suppose that we have a ternary relationship R between entity sets A, B,\nand C such that A has a key constraint and total participation and B has a key constraint;\nthese are the only constraints.\nA has attributes al and a2, with al being the key; Band\nC are similar.\nR has no descriptive attributes.\nWrite SQL statements that create tables\ncorresponding to this information so &s to capture as many of the constraints as possible. If\nyou cannot capt,).ue some constraint, explain why.\nExercise 3.12 Consider the scenario from Exercise 2.2, where you designed an ER diagram\nfor a university database. \\Vrite SQL staternents to create the corresponding relations and\ncapture as many of the constraints as possible. If you cannot: capture some constraints, explain\nwhy.\nExercise 3.13 Consider the university database from Exercise 2.:3 and the ER diagram you\ndesigned. Write SQL statements to create the corresponding relations and capture &'> many\nof the constraints as possible. If you cannot capture some constraints, explain why.\n\nThe RelatioTwl A10del\n97\nt\nExercise 3.14 Consider the scenario from Exercise 2.4, where you designed an ER diagram\nfor a company databa,c;e.\n\\~Trite SQL statements to create the corresponding relations and\ncapture as many of the constraints as possible.\nIf you cannot capture some constraints,\nexplain why.\nExercise 3.15 Consider the Notown database from Exercise 2.5. You have decided to rec-\nommend that Notown use a relational database system to store company data.\nShow the\nSQL statements for creating relations corresponding to the entity sets and relationship sets\nin your design. Identify any constraints in the ER diagram that you are unable to capture in\nthe SQL statements and briefly explain why you could not express them.\nExercise 3.16 Thanslate your ER diagram from Exercise 2.6 into a relational schema, and\nshow the SQL statements needed to create the relations, using only key and null constraints.\nIf your translation cannot capture any constraints in the ER diagram, explain why.\nIn Exercise 2.6, you also modified the ER diagram to include the constraint that tests on a\nplane must be conducted by a technician who is an expert on that model. Can you modify\nthe SQL statements defining the relations obtained by mapping the ER diagram to check this\nconstraint?\nExercise 3.17 Consider the ER diagram that you designed for the Prescriptions-R-X chain of\npharmacies in Exercise 2.7. Define relations corresponding to the entity sets and relationship\nsets in your design using SQL.\nExercise 3.18 Write SQL statements to create the corresponding relations to the ER dia-\ngram you designed for Exercise 2.8. If your translation cannot capture any constraints in the\nER diagram, explain why.\nExercise 3.19 Briefly answer the following questions based on this schema:\nEmp(e'id: integer, ename: string, age: integer, salary: real)\nWorks(eid: integer, did: integer, peLtime: integer)\nDept(did: integer, budget: real, managerid: integer)\n1. Suppose you have a view SeniorEmp defined as follows:\nCREATE VIEW SeniorEmp (sname, sage, salary)\nAS SELECT E.ename, Kage, E.salary\nFROM\nEmp E\nWHERE\nKage > 50\nExplain what the system will do to process the following query:\nSELECT S.sname\nFROM\nSeniorEmp S\nWHERE\nS.salary > 100,000\n2. Give an example of a view on Emp that could be automatically updated by updating\nEmp.\n3. Give an example of a view on Emp that would be impossible to update (automatically)\nand explain why your example presents the update problem that it does.\nExercise 3.20 C::onsider the following schema:\n\n98\nSuppliers(sid: integer, sname: string, address: string)\nParts(pid: integer, pname: string, color: string)\nCatalog(sid: integer, pid: integer, cost: real)\nCHAPTER,. 3\nThe Catalog relation lists the prices charged for parts by Suppliers.\nAnswer the following\nquestions:\n•\nGive an example of an updatable view involving one relation.\n•\nGive an example of an updatable view involving two relations.\n•\nGive an example of an insertable-into view that is updatable.\n•\nGive an example of an insertable-into view that is not updatable.\nPROJECT-BASED EXERCISES\nExercise 3.21 Create the relations Students, Faculty, Courses, Rooms, Enrolled, Teaches,\nand Meets_In in Minibase.\nExercise 3.22 Insert the tuples shown in Figures 3.1 and 3.4 into the relations Students and\nEnrolled. Create reasonable instances of the other relations.\nExercise 3.23 What integrity constraints are enforced by Minibase?\nExercise 3.24 Run the SQL queries presented in this chapter.\nBIBLIOGRAPHIC NOTES\nThe relational model was proposed in a seminal paper by Codd [187]. Childs [176] and Kuhns\n[454] foreshadowed some of these developments.\nGallaire and :WIinker's book [296] contains\nseveral papers on the use of logic in the context of relational databases. A system based on a\nvariation of the relational model in which the entire database is regarded abstractly as a single\nrelation, called the universal relation, is described in [746]. Extensions of the relational model\nto incorporate null values, which indicate an unknown or missing field value, are discussed by\nseveral authors; for example, [329, 396, 622, 754, 790].\nPioneering projects include System R [40, 150] at IBM San Jose Research Laboratory (now\nIBM Almaden Research Center), Ingres [717] at the University of California at Berkeley,\nPRTV [737] at the IBNI UK Scientific Center in Peterlee, and QBE [801] at IBM T. J.\nWatson Research Center.\nA rich theory underpins the field of relational databases. Texts devoted to theoretical aspects\ninclude those by··Atzeni and DeAntonellis [45]; Maier [501]; and Abiteboul, Hull, and Vianu\n[:3]. [415] is an excellent survey article.\nIntegrity constraints in relational databases have been discussed at length.\n[190] addresses\nsemantic extensions to the relational model, and integrity, in particular referential integrity.\nU~60] discusses semantic integrity constraints.\n[2()~3] contains papers that address various\naspects of integrity constraints, including in particular a detailed discussion of referential\nintegrity. A vast literature deals \\vith enforcing integrity constraints. [51] compares the cost\n\nThe Relational AIodel\n99\n.~\nof enforcing integrity constraints via compile-time, run-time, and post-execution checks. [145]\npresents an SQL-based language for specifying integrity constraints and identifies conditions\nunder which integrity rules specified in this language can be violated.\n[713] discusses the\ntechnique of integrity constraint checking by query modification.\n[180] discusses real-time\nintegrity constraints.\nOther papers on checking integrity constraints in databases include\n[82, 122, 138,517]. [681] considers the approach of verifying the correctness of programs that\naccess the database instead of run-time checks. Note that this list of references is far fTom\ncomplete; in fact, it does not include any of the many papers on checking recursively specified\nintegrity constraints. Some early papers in this widely studied area can be found in [296] and\n[295].\nFor references on SQL, see the bibliographic notes for Chapter 5. This book does not discuss\nspecific products based on the relational model, but many fine books discuss each of the major\ncommercial systems; for example, Chamberlin's book on DB2 [149], Date and McGoveran's\nbook on Sybase [206], and Koch and Loney's book on Oracle [443].\nSeveral papers consider the problem of translaping updates specified on views into updates\non the underlying table [59, 208, 422, 468, 778].\n[292] is a good survey on this topic.\nSee\nthe bibliographic notes for Chapter 25 for references to work querying views and maintaining\nmaterialized views.\n[731] discusses a design methodology based on developing an ER diagram and then translating\nto the relational model.\nMarkowitz considers referential integrity in the context of ER to\nrelational mapping and discusses the support provided in some commercial systems (as of\nthat date) in [513, 514].\n\n4\nRELATIONAL ALGEBRA\nAND CALCULUS\n..\nWhat is the foundation for relational query languages like SQL? What\nis the difference between procedural and declarative languages?\n...\nWhat is relational algebra, and why is it important?\n...\nWhat are the basic algebra operators, and how are they combined to\nwrite complex queries?\n...\nWhat is relational calculus, and why is it important?\n...\nWhat subset of mathematical logic is used in relational calculus, and\nhow is it used to write queries?\n..\nKey concepts: relational algebra, select, project, union, intersection,\ncross-product, join, division; tuple relational calculus, domain rela-\ntional calculus, formulas, universal and existential quantifiers, bound\nand free variables\n'--------------------\nStand finn in your refusal to remain conscious during algebra.\nIn real life, I\nassure you, there is no such thing as algebra.\n~·-Fran Lebowitz, Social Studies\nThis chapter presents two formal query languages associated with the relational\nmodel. Query 'languages are specialized languages for asking questions, or\nqueries, that involve the data in a database. After covering some preliminaries\nin Section 4.1, we discuss rdafional algebra in Section 4.2. Queries in relational\nalgebra are composed using a collection of operators, and each query describes\na step-by-step procedure for computing the desired answer; that is, queries are\n100\n\nRelat'ional Algebra and Calcullls\nun\nspecified in an operationa.l manner. In Section 4.3, we discuss Tela.l'ional calcu-\nlus, in which a query describes the desired ans\\ver without specifying how the\nanswer is to be computed; this nonprocedural style of querying is called declar-\nat'i'Ve. \\Ve usually refer to relational algebra and relational calculus as algebra\nand calculus, respectively.\nvVe compare the expressive power of algebra and\ncalculus in Section 4.4. These formal query languages have greatly influenced\ncommercial query languages such as SQL, which we discuss in later chapters.\n4.1\nPRELIMINARIES\nWe begin by clarifying some important points about relational queries. The\ninputs and outputs of a query are relations. A query is evaluated using instances\nof each input relation and it produces an instance of the output relation. In\nSection 3.4, we used field names to refer to fields because this notation makes\nqueries more readable.\nAn alternative is to always list the fields of a given\nrelation in the same order and refer to fields by position rather than by field\nname.\nIn defining relational algebra and calculus, the alternative of referring to fields\nby position is more convenient than referring to fields by name: Queries often\ninvolve the computation of intermediate results, which are themselves relation\ninstances; and if we use field names to refer to fields, the definition of query\nlanguage constructs must specify the names of fields for all intermediate relation\ninstances. This can be tedious and is really a secondary issue, because we can\nrefer to fields by position anyway. On the other hand, field names make queries\nmore readable.\nDue to these considerations, we use the positional notation to formally define\nrelational algebra and calculus.\nWe also introduce simple conventions that\nallow intermediate relations to 'inherit' field names, for convenience.\nvVe present a number of sample queries using the following schema:\nSailors(sid: integer, snarne: string, rating: integer, age: real)\nBoats( bid: integer, bnarne: string, coloT: string)\nReserves(sid: integer, bid: _~_r:teger, day: date)\nThe key fields are underlined, and the doma,in of each field is listed after the\nfield name. Thus, .sid is the key for Sailors, bid is the key for Boats, and all\nthree fields together form the key for Reserves.\nFields in an instance of one\nof these relations are referred to by name, or positionally, using the order in\nwhich they were just listed.\n\n102\nCHAPTER 4,\nIn several examples illustrating the relational algebra operators, we use the\ninstances 81 and 82 (of Sailors) and R1 (of Reserves) shown in Figures 4.1,\n4.2, and 4.3, respectively.\n1\",';-1\nJ\nI yv'l' 1\n22\nDustin\n7\n45.0\n31\nLubber\n8\n55.5\n58\nRusty\n10\n35.0\nFigure 4.1\nInstance Sl of Sailors\nt'Cc'Jdl\nl'l\"n1mn1Jlnl:JlPj\n28\nyuppy\n9\n35.0\n31\nLubber\n8\n55.5\n44\nguppy\n5\n35.0\n58\nRusty\n10\n35.0\nFigure 4.2\nInstance S2 of Sailors\nFigure 4.3\nInstance Rl of Reserves\n4.2\nRELATIONAL ALGEBRA\nRelational algebra is one of the two formal query languages associated with the\nrelational model. Queries in algebra are composed using a collection of oper-\nators.\nA fundamental property is that every operator in the algebra accepts\n(one or two) relation instances as arguments and returns a relation instance\nas the result.\nThis property makes it easy to compose operators to form a\ncomplex query-a relational algebra expression is recursively defined to be\na relation, a unary algebra operator applied to a single expression, or a binary\nalgebra operator applied to two expressions. We describe the basic operators of\nthe algebra (selection, projection, union, cross-product, and difference), as well\nas some additional operators that can be defined in terms of the basic opera-\ntors but arise frequently enough to warrant special attention, in the following\nsections.\nEach relational query describes a step-by-step procedure for computing the\ndesired answer, based on the order in which operators are applied in the query.\nThe procedural nature of the algebra allows us to think of an algebra expression\nas a recipe, or a plan, for evaluating a query, and relational systems in fact use\nalgebra expressions to represent query evaluation plans.\n\nRelational AlgebTa and Calculus\n4.2.1\nSelection and Projection\nRelational algebra includes operators to select rows from a relation (a) and to\nproject columns (7r). These operations allow us to manipulate data in a single\nrelation.\nConsider the instance of the Sailors relation shown in Figure 4.2,\ndenoted as 52. We can retrieve rows corresponding to expert sailors by using\nthe a operator. The expression\narating>8 (52)\nevaluates to the relation shown in Figure 4.4. The subscript rating> 8 specifies\nthe selection criterion to be applied while retrieving tuples.\nsname I rating I\nyuppy\n9\nRusty\n10\nFigure 4.4\nO\"r(lting>s(S2)\nyuppy\n9\nLubber\n8\nguppy\n5\nRusty\n10\nFigure 4.5\n7r,m(lT1lc,Tating(S2)\nThe selection operator a specifies the tuples to retain through a selection con-\ndition.\nIn general, the selection condition is a Boolean combination (i.e., an\nexpression using the logical connectives /\\ and V) of terms that have the form\nattribute op constant or attributel op attribute2, where op is one of the com-\nparison operators <, <=, =, ,#, >=, or >. The reference to an attribute can be\nby position (of the form .i or i) or by name (of the form .name or name). The\nschema of the result of a selection is the schema of the input relation instance.\nThe projection operator 7r allows us to extract columns from a relation; for\nexample, we can find out all sailor names and ratings by using 1f. The expression\n7rsname,rafing(52)\nevaluates to the relation shown in Figure 4.5.\nThe subscript 8na:me)rating\nspecifies the fields to be retained; the other fields are 'projected out.'\nThe\nschema of the result of a projection is determined by the fields that are projected\nin the obvious way.\nSuppose that we wanted to find out only the ages of sailors. The expression\nevaluates to the relation shown in Figure /1.6. The irnportant point to note is\nthat, although three sailors are aged 35, a single tuple with age=:J5.0 appears in\n\n104\nCHAPTER+!\nthe result of the projection. This follm\\'8 from the definition of a relation as a set\nof tuples. In practice, real systems often omit the expensive step of eliminating\nduplicate tuples, leading to relations that are multisets. However, our discussion\nof relational algebra and calculus a..-;sumes that duplicate elimination is always\ndone so that relations are always sets of tuples.\nSince the result of a relational algebra expression is always a relation, we can\nsubstitute an expression wherever a relation is expected. For example, we can\ncompute the names and ratings of highly rated sailors by combining two of the\npreceding queries. The expression\n7rsname,rating ((Jrati.ng>8 (82))\nproduces the result shown in Figure 4.7. It is obtained by applying the selection\nto 82 (to get the relation shown in Figure 4.4) and then applying the projection.\nI age\n·.1\nQBO\n~\nFigure 4.6\n1rage (82)\n4.2.2\nSet Operations\nFigure 4.7\n1rsname,rating(Urating>s(S2))\nThe following standard operations on sets are also available in relational al-\ngebra: un'ion (U), intersection (n), set-difference (-), and cmss-product (x).\nII\nUnion: R U 8 returns a relation instance containing aU tuples that occur\nin either relation instance R or relation instance 8 (or both). Rand 8\nmust be union-compatible, and the schema of the result is defined to be\nidentical to the schema of R.\nTwo relation instances are said to be union-compatible if the following\nconditions hold:\n~ they have the same number of the fields, and\n- corresponding fields, taken in order from left to right, have the same\ndomains.\nNote that\n~eld names are not used in defining union-compatibility.\nfor\nconvenience, we will assume that the fields of R U 5' inherit names from R,\nif the fields of R have names. (This assumption is implicit in defining the\nschema of R U 5' to be identical to the schema of R, as stated earlier.)\nIII\nIntersection: R n 5' returns a relation instance containing all tuples that\noccur in both Rand S. The relations Rand S must be union-compatible,\nand the schema of the result is defined to be identical to the schema of R.\n\nRelational Algebra and CalC'ul1L8\nH15\n•\nSet-difference: R- 8 returns a relation instance containing all tuples that\noccur in R but not in 8. The relations Rand 8 must be union-compatible,\nand the schema of the result is defined to be identical to the schema of R.\n•\nCross-product: R x 8 returns a relation instance whose schema contains\nall the fields of R (in the same order as they appear in R) followed by all\nthe fields of 8 (in the same order as they appear in 8). The result of R x 8\ncontains OIle tuple (1', s) (the concatenation of tuples rand s) for each pair\nof tuples l' E R,\nS E 8. The cross-product opertion is sometimes called\nCartesian product.\n\\\\Te use the convention that the fields of R x 8 inherit names from the\ncorresponding fields of Rand 8. It is possible for both Rand 8 to contain\none or more fields having the same name; this situation creates a naming\nconfi'ict. The corresponding fields in R x 8 are unnamed and are referred\nto solely by position.\nIn the preceding definitions, note that each operator can be applied to relation\ninstances that are computed using a relational algebra (sub)expression.\nWe now illustrate these definitions through several examples. The union of 81\nand 82 is shown in Figure 4.8. Fields are listed in order; field names are also\ninherited from 81. 82 has the same field names, of course, since it is also an\ninstance of Sailors. In general, fields of 82 may have different names; recall that\nwe require only domains to match. Note that the result is a set of tuples. TUples\nthat appear in both 81 and 82 appear only once in 81 U 82. Also, 81 uRI is\nnot a valid operation because the two relations are not union-compatible. The\nintersection of 81 and 82 is shown in Figure 4.9, and the set-difference 81- 82\nis shown in Figure 4.10.\n22\nDustin\n7\n45.0\n31\nLubber\n8\n55.5\n58\nRusty\n10\n35.0\n28\nyuppy\n9\n35.0\n44\nguppy\n5\n35.0\nFigure 4.8\n31 u 52\nThe result of the cross-product 81 x Rl is shown in Figure 4.11. Because Rl\nand 81 both have a field named sid, by our convention on field names, the\ncorresponding two fields in 81 x Rl are unnamed, and referred to solely by the\nposition in which they appear in Figure 4.11. The fields in 81 x Rl have the\nsame domains as the corresponding fields in Rl and 5'1. In Figure 4.11, sid is\n\n106\n.isifi\n\"\"\".h.~\"\n31\nLubber\n8\n55.5\n58\nRusty\n10\n35.0\nFigure 4.9\n81 n 82\nGHAPTER f4\nli~·iiJB1ff/fj,me\nIt,{4t~rf1l1f:4f1ei I\nI 22 I Dustin I 7\n[3fOJ\nFigure 4.10\n81 - 82\nlisted in parentheses to emphasize that it is not an inherited field name; only\nthe corresponding domain is inherited.\n(sid!)\nbid\naay\n22\nDustin\n7\n45.0\n22\n101\n10/10/96\n22\nDustin\n7\n45.0\n58\n103\n11/12/96\n31\nLubber\n8\n55.5\n22\n101\n10/10/96\n31\nLubber\n8\n55.5\n58\n103\n11/12/96\n58\nRusty\n10\n35.0\n22\n101\n10/10/96\n58\nRusty\n10\n35.0\n58\n103\n11/12/96\nFigure 4.11\n81 x R1\n4.2.3\nRenaming\nWe have been careful to adopt field name conventions that ensure that the result\nof a relational algebra expression inherits field names from its argument (input)\nrelation instances in a natural way whenever possible. However, name conflicts\ncan arise in some cases; for example, in 81 x Rl. It is therefore convenient\nto be able to give names explicitly to the fields of a relation instance that is\ndefined by a relational algebra expression. In fact, it is often convenient to give\nthe instance itself a name so that we can break a large algebra expression into\nsmaller pieces by giving names to the results of subexpressions.\nvVe introduce a renaming operator p for this purpose. The expression p(R(F), E)\ntakes an arbitrary relational algebra expression E and returns an instance of\na (new) relation called R. R contains the same tuples as the result of E and\nhas the same schema as E, but some fields are renamed. The field names in\nrelation R are the sarne as in E, except for fields renamed in the Tenaming list\nF, which is a list of terms having the form oldname ~, newnarne or position ~\nrW1llTlJLrne. For p to be well-defined, references to fields (in the form of oldnarnes\nor posit.ions in the renaming list) may be unarnbiguous and no two fields in the\nresult may have the same name. Sometimes we want to only renarne fields or\n(re)name the relation; we therefore treat both Rand F as optional in the use\nof p. (Of course, it is meaningless to omit both.)\n\nRelational AlgebTa and Calc\"Uh18\n107\nFor example, the expression p(C(l\n----7 s'id1,5\n----7 sid2), 81 x R1) returns a\nrelation that contains the tuples shown in Figure 4.11 and has the following\nschema: C(sidl: integer, ,marrw: string, mt'ing: integer, age: real, sid2:\ninteger, bid: integer, day: dates).\nIt is customary to include some additional operators in the algebra, but all of\nthem can be defined in terms of the operators we have defined thus far.\n(In\nfact, the renaming operator is needed only for syntactic convenience, and even\nthe n operator is redundant; R n 8 can be defined as R - (R - 8).) We consider\nthese additional operators and their definition in terms of the basic operators\nin the next two subsections.\n4.2.4\nJoins\nThe join operation is one of the most useful operations in relational algebra\nand the most commonly used way to combine information from two or more\nrelations. Although a join can be defined as a cross-product followed by selec-\ntions and projections, joins arise much more frequently in practice than plain\ncross-products. Further, the result of a cross-product is typically much larger\nthan the result of a join, and it is very important to recognize joins and imple-\nment them without materializing the underlying cross-product (by applying the\nselections and projections 'on-the-fly'). For these reasons, joins have received\na lot of attention, and there are several variants of the join operation. 1\nCondition Joins\nThe most general version of the join operation accepts a join condition c and\na pair of relation instances as arguments and returns a relation instance. The\njoin cond'it-ion is identical to a selection condition in form.\nThe operation is\ndefined as follows:\nR [:X)e S =\nO\"e(R X S)\nThus [:X) is defined to be a cross-product followed by a selection. Note that the\ncondition c can (and typically does) refer to attributes of both Rand S. The\nreference to an attribute of a relation, say, R, can be by positioll (of the form\nR.i) or by Ilame (of the form R.name).\nAs an example, the result of Sl\n[><JS1.8id<Rl.sid R1 is shown in Figure 4.12.\nBecause sid appears in both 81 and R1, the corresponding fields in the result\nof the cross-product 81 x R1 (and therefore in the result of 81 [:X)S1.sid<Rl.sid R1)\n1Several variants of joins are not discussed in this chapter.\nAn important c.la..'iS of joins, called\n01lter joins, is discussed in Chapter 5.\n\n108\nCHAPTER ..t1\nare unnamed. Domains are inherited from the corresponding fields of 81 and\nRl.\nI (sid) I snarne I rating I age\nI···{si41:l bid\nda/lj\nI 22\nI Dustin\n7\n45.0\n58\nI 103 I 11/12/96\nI 31\nI Lubber\n8\n.- 55.5\n58\nI 103 I 11/12/96\nFigure 4.12\n51 NSl. s id<R1.sid R1\nEquijoin\nA common special case of the join operation R [>(] 8 is when the join condition\nconsists solely of equalities (connected by 1\\) of the form R.name1 = 8.name2,\nthat is, equalities between two fields in Rand S. In this case, obviously, there is\nsome redundancy in retaining both attributes in the result. For join conditions\nthat contain only such equalities, the join operation is refined by doing an\nadditional projection in which 8.name2 is dropped. The join operation with\nthis refinement is called equijoin.\nThe schema of the result of an equijoin contains the fields of R (with the same\nnames and domains as in R) followed by the fields of 8 that do not appear\nin the join conditions. If this set of fields in the result relation includes two\nfields that inherit the same name from Rand 8, they are unnamed in the result\nrelation.\nWe illustrate 81l:<JR.sid=5.sid Rl in Figure 4.13. Note that only one field called\nsid appears in the result.\n~ ,marneI rating I age I· bid·1 day =tJ\nI 22\nDustIn I 7\nI 45.0\n101 I 10/10/96 I\nI 58\nRust}~ I 10\nI ~5.0\n103 I 11/12/96 I\nFigure 4.13\n81 MR ..,H1=S'.\"id HI\nNatural Join\nA further special ca.'3e of the join operation R\n[>(] S is an eqUlJom in which\nequalities arc specified on all fields having the same name in Rand S.\nIn\nthis case, we can simply omit the join condition; the default is that the join\ncondition is a collection of equalities on all common fields. We call this special\ncase a natumJ jo'in, and it has the nice property that the result is guaranteed\nnot to have two fields with the saIne name.\n\nRelai'ional Algelrra and Calculus\nlQ9\nThe equijoin expression 81 D<m..sid=s.sid R1 is actually a natural join al1d can\nsimply be denoted as 81 [Xl R1, since the only common field is sid. If the two\nrelations have no attributes in common, 81 [Xl Rl is simply the cross-product.\n4.2.5\nDivision\nThe division operator is useful for expressing certain kinds of queries for exam-\nple, \"Find the names of sailors who have reserved all boats.\"\nUnderstanding\nhow to use the basic operators of the algebra to define division is a useful exer-\ncise. However, the division operator does not have the same importance as the\nother operators-it is not needed as often, and database systems do not try to\nexploit the semantics of division by implementing it as a distinct operator (as,\nfor example, is done with the join operator).\nWe discuss division through an example.\nConsider two relation instances A\nand B in which A has (exactly) two fields x and y and B has just one field y,\nwith the same domain as in A. We define the division operation AlB as the\nset of all x values (in the form of unary tuples) such that for every y value in\n(a tuple of) B, there is a tuple (x,y) in A.\nAnother way to understand division is as follows. For each x value in (the first\ncolumn of) A, consider the set of y values that appear in (the second field of)\ntuples of A with that x value. If this set contains (all y values in) B, the x\nvalue is in the result of AlB.\nAn analogy with integer division may also help to understand division.\nFor\nintegers A and B, AlB is the largest integer Q such that Q * B\n::::; A.\n:For\nrelation instances A and B, AlB is the largest relation instance Q such that\nQ x B S:::A.\nDivision is illustrated in Figure 4.14. It helps to think of A as a relation listing\nthe parts supplied by suppliers and of the B relations as listing parts. AIB'i\ncomputes suppliers who supply all parts listed in rdation instance Bi.\nExpressing AlBin terms of the ba...sic algebra operators is an interesting ex-\nercise, and the reader should try to do this before reading further. The basic\nidea is to compute all :r values in A that are not disqualified.\nAn x value is\ndisqualified if lJy attaching a y value from B, we obtain a tuple (x,y) that is not\nin A. We can compute disqualified tuples using the algebra expression\nThus, we can define AlBa.....,\n\n110\nA\nI sno i pno I\n81\n\\\npI I\nsl\np2\n~._~~\n81\ni\np4\n82\n\\\npI\n~_P2\n-~\n83\n'\np2\n84\np2\ns4\np4\nCHAPTER$4\nBl\n[P.!!~J\nAlBl\n~;~J\n,....---,\n~\nLEU\nI 82 I\nB2\nI pno ]\n83\nBE\n0\np4\n~\nAlB2\nB3 ~\nBE\nB1J\n84\np2\n~\np4\nAlB3 [ill\nFigure 4.14\nExamples Illustrating Division\nTo understand the division operation in full generality, we have to consider the\ncase when both x and yare replaced by a set of attributes. The generalization is\nstraightforward and left as an exercise for the reader. We discuss two additional\nexamples illustrating division (Queries Q9 and Q10) later in this section.\n4.2.6\nMore Examples of Algebra Queries\nWe now present several examples to illustrate how to write queries in relational\nalgebra. We use the Sailors, Reserves, and Boats schema for all our examples\nin this section. We use parentheses as needed to make our algebra expressions\nunambiguous.\nNote that all the example queries in this chapter are given\na unique query number. The query numbers are kept unique across both this\nchapter and the SQL query chapter (Chapter 5). This numbering makes it easy\nto identify a query when it is revisited in the context of relational calculus and\nSQL and to compare different ways of writing the same query. (All references\nto a query can be found in the subject index.)\nIn the rest of this chapter (and in Chapter 5), we illustrate queries using the\ninstances 83 of Sailors, R2 of Reserves, and B1 of Boats, shown in Figures\n4.15, 4.16, and 4.17, respectively.\n(Q1) Find the names of sailors who have rcscT'ucd boat lOS.\nThis query can be written as follows:\n\".marne (( (Jbid=1O:~Re.5erve8)[XJ 8ailoT.5)\n\nRelational Algebra and Calcul1ls\n~l\n»>\n.....\"\"\"'.+A.\".,'\"\nhAh\n22\nDustin\n7\n45.0\n29\nBrutus\n1\n33.0\n31\nLubber\n8\n55.5\n32\nAndy\n8\n25.5\n58\nRusty\n10\n35.0\n64\nHoratio\n7\n35.0\n71\nZorba\n10\n16.0\n74\nHoratio\n9\n35.0\n85\nArt\n3\n25.5\n95\nBob\n3\n63.5\nFigure 4.15\nAn Instance 83 of Sailors\n111,\n22\n101\n10/10/98\n22\n102\n10/10/98\n22\n103\n10/8/98\n22\n104\n10/7/98\n31\n102\n11/10/98\n31\n103\n11/6/98\n31\n104\n11/12/98\n64\n101\n9/5/98\n64\n102\n9/8/98\n74\n103\n9/8/98\nFigure 4.16\nAn Instance R2 of Reserves\nWe first compute the set of tuples in Reserves with bid = 103 and then take the\nnatural join of this set with Sailors. This expression can be evaluated on in-\nstances of Reserves and Sailors. Evaluated on the instances R2 and S3, it yields\na relation that contains just one field, called sname, and three tuples (Dustin),\n(Horatio), and (Lubber). (Observe that two sailors are called Horatio and only\none of them has reserved a red boat.)\n[~]bname\nI color· I\n101\nInterlake\nblue\n102\nInterlake\nred\n103\nClipper\ngreen\n104\nMarine\nred\nFigure 4.17\nAn Instance HI of Boats\nWe can break this query into smaller pieces llsing the renaming operator p:\np(Temp1, IJbir1=103 ReseTves)\np(Temp2, Temp11XJ Sailor's)\n1Tsname(Temp2)\nNotice that because we are only llsing p to give names to intermediate relations,\nthe renaming list is optional and is omitted. TempI denotes an intermediate\nrelation that identifies reservations of boat 103. Temp2 is another intermediate\nrelation, and it denotes sailors who have mad(~ a reservation in the set Templ.\nThe instances of these relations when evaluating this query on the instances R2\nand S3 are illustrated in Figures 4.18 and 4.19. Finally, we extract the sname\ncolumn from Temp2.\n\n112\n22\n10~~\n10/8/98\n31\n103\n11/6/98\n74\n103\n9/8/98\nDustin\n31\nLubber\n8\n74\nHoratio\n9\nCHAPTER;l\n10/8/98\n11/6/98--\n9/8/98\nFigure 4.18\nInstance of TempI\nFigure 4.19\nInstance of Temp2\nThe version of the query using p is essentially the same as the original query;\nthe use of p is just syntactic sugar. However, there are indeed several distinct\nways to write a query in relational algebra. Here is another way to write this\nquery:\nJrsname(CJbid=103(Reserves IX! Sailors))\nIn this version we first compute the natural join of Reserves and Sailors and\nthen apply the selection and the projection.\nThis example offers a glimpse of the role played by algebra in a relational\nDBMS. Queries are expressed by users in a language such as SQL. The DBMS\ntranslates an SQL query into (an extended form of) relational algebra and\nthen looks for other algebra expressions that produce the same answers but are\ncheaper to evaluate. If the user's query is first translated into the expression\n7fsname (CJbid=103 (Reserves IX! Sailors))\na good query optimizer will find the equivalent expression\n7rsname ((CJb·id=103Reserves) IX! Sailors)\nFurther, the optimizer will recognize that the second expression is likely to\nbe less expensive to compute because the sizes of intermediate relations are\nsmaller, thanks to the early use of selection.\n(Q2) Find the names of sailors who ha've reserved a red boat.\n7f.marne ((CJcolor='red'Boats) IX! Reserves !><J SailoI's)\nThis query involves a series of two joins. First, we choose (tuples describing)\nred boats.\nThen, we join this set with Reserves (natural join, with equality\nspecified on thE) bid column) to identify reservations of red boats.\nNext, we\njoin the resulting intermediate relation with Sailors (natural join, with equality\nspecified on the sid column) to retrieve the names of sailors who have rnade\nreservations for red boats. Finally, we project the sailors' names. The answer,\nwhen evaluated on the instances B1, R2, and S3, contains the names Dustin,\nHoratio, and Lubber.\n\nRelational Algebra and Calculus\nAn equivalent expression is:\nB.3\nThe reader is invited to rewrite both of these queries by using p to make the\nintermediate relations explicit and compare the schema.<=; of the intermediate\nrelations.\nThe second expression generates intermediate relations with fewer\nfields (and is therefore likely to result in intermediate relation instances with\nfewer tuples as well). A relational query optimizer would try to arrive at the\nsecond expression if it is given the first.\n(Q3) Find the colors of boats reserved by Lubber.\nJrcolor((asname='Lubber,Sa'ilors) [XJ Reserves [XJ Boats)\nThis query is very similar to the query we used to compute sailors who reserved\nred boats. On instances Bl, R2, and S3, the query returns the colors green\nand red.\n(Q4) Find the names of sailors who have reserved at least one boat.\nJrsname(Sailors [XJ Reserves)\nThe join of Sailors and Reserves creates an intermediate relation in which tuples\nconsist of a Sailors tuple 'attached to' a Reserves tuple. A Sailors tuple appears\nin (some tuple of) this intermediate relation only if at least one Reserves tuple\nhas the same sid value, that is, the sailor has made some reservation.\nThe\nanswer, when evaluated on the instances Bl, R2 and S3, contains the three\ntuples (Dustin), (HoTatio) , and (LubbeT).\nEven though two sailors called\nHoratio have reserved a boat, the answer contains only one copy of the tuple\n(HoTatio) , because the answer is a relation, that is, a set of tuples, with no\nduplicates.\nAt this point it is worth remarking on how frequently the natural join operation\nis used in our examples. This frequency is more than just a coincidence based\non the set of queries we have chosen to discuss; the natural join is a very\nnatural, widely used operation. In particular, natural join is frequently used\nwhen joining two tables on a foreign key field. In Query Q4, for exalnple, the\njoin equates the sid fields of Sailors and Reserves, and the sid field of Reserves\nis a foreign key that refers to the sid field of Sailors.\n(Q5) Find the narnes of sailors who have reserved a Ted OT a gTeen boat.\np(Tempboats, (acoloT='rcd' Boats) U (acolor='green' Boats))\nJrsna·rne(Tempboats [XJ ReseTves [XJ Sailors)\n\n114\nCHAPTER $4\nvVe identify the set of all boats that are either red or green (Tempboats, which\ncontains boats \\vith the bids 102, 103, and 104 on instances E1, R2, and S3).\nThen we join with Reserves to identify sid.., of sailors who have reserved OIle of\nthese boats; this gives us sids 22, 31, 64, and 74 over our example instances.\nFinally, we join (an intermediate relation containing this set of sids) with Sailors\nto find the names of Sailors with these sids. This gives us the names Dustin,\nHoratio, and Lubber on the instances E1, R2, and S3.\nAnother equivalent\ndefinition is the following:\np(Tempboats, (acolor='red'Vcolor='green' Boats))\n7fsname(Tempboats [><] Reserves [><] Sailors)\nLet us now consider a very similar query.\n(Q6) Find the names of sailors who have reserved a red and a green boat. It\nis tempting to try to do this by simply replacing U by n in the definition of\nTempboats:\np(Tempboats2, (acolor='red,Eoats) n (O\"color='green,Boats))\n'7fsname(Tempboats2 [><] Reserves [><] Sailors)\nHowever, this solution is incorrect-it instead tries to compute sailors who have\nreserved a boat that is both red and green. (Since bid is a key for Boats, a boat\ncan be only one color; this query will always return an empty answer set.) The\ncorrect approach is to find sailors who have reserved a red boat, then sailors\nwho have reserved a green boat, and then take the intersection of these two\nsets:\np(Tempred, '7fsid((acolor='red' Eoats) [><] Reserves))\np(Tempgreen, '7fsid((O\"color='green,Boats) [><] Reserves))\n'7f,marne((Ternpred n Tempgreen)\n[><] Sailors)\nThe two temporary relations compute the sids of sailors, and their intersection\nidentifies sailors who have reserved both red and green boats.\nOn instances\nBI, R2, and 53, the sids of sailors who have reserved a red boat are 22, 31,\nand 64. The s'icLs of sailors who have reserved a green boat are 22, 31, and 74.\nThus, sailors 22 and 31 have reserved both a red boat and a green boat; their\nnames are Dustin and Lubber.\nThis formulation of Query Q6 can easily be adapted to find sailors \\vho have\nreserved red or green boats (Query Q5); just replace n by U:\np(TempTed, '7fsid( (O\"color=lrcd' Boats) [)<] Reserves))\np(Tempgreen, '7fsid((O\"color='green' Boats) [)<] Reserves))\n'7fsTwme((Tempred U Tempgreen) [)<] Sailors)\n\nRelatiorwl Algebra and Galcurus\n115\nIn the formulations of Queries Q5 and Q6, the fact that sid (the field over\nwhich we compute union or intersection) is a key for Sailors is very important.\nConsider the following attempt to answer Query Q6:\np(Tempred, Jrsname((CJcolor='red,Boats) [><] Reserves [><] Sailors))\np(Tempgreen,Jrsname((CJcoloT='gTeenlBoats) [><] Reserves [><] Sailors))\nTempred n Tempgreen\nThis attempt is incorrect for a rather subtle reason. Two distinct sailors with\nthe same name, such as Horatio in our example instances, may have reserved\nred and green boats, respectively. In this case, the name Horatio (incorrectly)\nis included in the answer even though no one individual called Horatio has\nreserved a red boat and a green boat. The cause of this error is that sname\nis used to identify sailors (while doing the intersection) in this version of the\nquery, but sname is not a key.\n(Q7) Find the names of sailors who have reser-ved at least two boats.\np(Reser-vations, Jrsid,sname,bid (Sailors [><] Reserves))\np(Reservationpairs(l\n---'? sid1, 2 ---'? sname1, 3 ---'? bid1, 4 ---'? sid2,\n5 ---'? sname2, 6 ---'? bid2), Reservations x Reservations)\nJrsname1 CJ(sidl=sid2)I\\(bidl=1-bid2) Reservationpair-s\nFirst, we compute tuples of the form (sid,sname, bid) , where sailor sid has made\na reservation for boat bid; this set of tuples is the temporary relation Reserva-\ntions. Next we find all pairs of Reservations tuples where the same sailor has\nmade both reservations and the boats involved are distinct. Here is the central\nidea: To show that a sailor has reserved two boats, we must find two Reserva-\ntions tuples involving the same sailor but distinct boats. Over instances El,\nR2, and S3, each of the sailors with sids 22, 31, and 64 have reserved at least\ntwo boats. Finally, we project the names of such sailors to obtain the answer,\ncontaining the names Dustin, Horatio, and Lubber.\nNotice that we included sid in Reservations because it is the key field identifying\nsailors, and we need it to check that two Reservations tuples involve the same\nsailor. As noted in the previous example, we cannot use sname for this purpose.\n(Q8) Find the sids of sailors w'ith age over 20 who have not TeseTved a Ted boat.\nJrsid(CJage>20Sa'ilors) -\n7rsid((CJco[oT='red,Boats) [><] Reserves [><] Sa'ilors)\nThis query illustrates the use of the set-difference operator. Again, we use the\nfact that sid is the key for Sailors. vVe first identify sailors aged over 20 (over\n\n116\ninstances B1, R2, and S3, .'!'ids 22, 29, 31, 32, 58, 64, 74, 85, and 95) and then\ndiscard those who have reserved a red boat (sid.c; 22, 31, and 64), to obtain the\nanswer (sids 29, 32, 58, 74, 85, and 95). If we want to compute the names of\nsuch sailors, \\ve must first compute their sids (as shown earlier) and then join\nwith Sailors and project the sname values.\n(Q9) Find the names of sailors 'Who have rese'rved all boats.\nThe use of the word all (or every) is a good indication that the division operation\nmight be applicable:\np(Tempsids, (7l\"sid,bidReserves) / (7l\"bidBoats))\n7l\"sname(Tempsids N Sailors)\nThe intermediate relation Tempsids is defined using division and computes the\nset of sids of sailors who have reserved every boat (over instances Bl, R2, and\nS3, this is just sid 22). Note how we define the two relations that the division\noperator (/) is applied to·-·--the first relation has the schema (sid,bid) and the\nsecond has the schema (b'id). Division then returns all sids such that there is a\ntuple (sid,bid) in the first relation for each bid in the second. Joining Tempsids\nwith Sailors is necessary to associate names with the selected sids; for sailor\n22, the name is Dustin.\n(Q10) Find the names of sailors 'Who have reserved all boats called Interlake.\np(Tempsids, (7l\".5'id,bidReserves) / (7l\"bid((Jbname='Interlake' Boats)))\n7l\"sname(Tempsids [Xl Sailors)\nThe only difference with respect to the previous query is that now we apply a\nselection to Boats, to ensure that we compute bids only of boats named Interlake\nin defining the second argument to the division operator. Over instances El,\nR2, and S3, Tempsids evaluates to sids 22 and 64, and the answer contains\ntheir names, Dustin and Horatio.\n403\nRELATIONAL CALCULUS\nRelational calculus is an alternative to relational algebra. In contra.':;t to the\nalgebra, which is procedural, the calculus is nonprocedural, or declarative, in\nthat it allows us to describe the set of answers without being explicit about\nhow they should be computed. Relational calculus has had a big influence on\nthe design of commercial query languages such a,s SQL and, especially, Query-\nby-Example (QBE).\nThe variant of the calculus we present in detail is called the tuple relational\ncalculus (TRC). Variables in TRC take on tuples as values. In another vari-\n\nRelatiO'Tul,l Algebra and Calculus\nant, called the domain relational calculus (DRC), the variables range over\nfield values. TRC has had more of an influence on SQL, \\vhile DRC has strongly\ninfluenced QBE. vVe discuss DRC in Section 4.3.2.2\n4$3.1\nTuple Relational Calculus\nA tuple variable is a variable that takes on tuples of a particular relation\nschema as values. That is, every value assigned to a given tuple variable has\nthe same number and type of fields. A tuple relational calculus query has the\nform { T I p(T) }, where T is a tuple variable and p(T) denotes a formula that\ndescribes T; we will shortly define formulas and queries rigorously. The result\nof this query is the set of all tuples t for which the formula p(T) evaluates to\ntrue with T = t. The language for writing formulas p(T) is thus at the heart of\nTRC and essentially a simple subset of first-order logic. As a simple example,\nconsider the following query.\n(Q11) Find all sailors with a rating above 7.\n{S I S E Sailors 1\\ S.rating > 7}\nWhen this query is evaluated on an instance of the Sailors relation, the tuple\nvariable S is instantiated successively with each tuple, and the test S. rat'ing> 7\nis applied. The answer contains those instances of S that pass this test. On\ninstance S3 of Sailors, the answer contains Sailors tuples with sid 31, 32, 58,\n71, and 74.\nSyntax of TRC Queries\nWe now define these concepts formally, beginning with the notion of a formula.\nLet Rel be a relation name, Rand S be tuple variables, a be an attribute of\nR, and b be an attribute of S. Let op denote an operator in the set {<, >, =\n, :S;, 2:, =I-}. An atomic formula is one of the following:\nIII\nR E Ref\nlIII\nR.a op S.b\nIIlI\nR.a op constant, or constant op R.a\nA formula is recursively defined to be one of the following, where p and q\nare themselves formula.s and p(R) denotes a formula in which the variable R\nappears:\n.~-----------\n2The material on DRC is referred to in the (online) chapter OIl QBE; with the exception of this\nchapter, the material on DRC and TRe can be omitted without loss of continuity.\n\n118\nCHAPTER .,4\n•\nany atomic formula\n•\n-'p, P /\\ q, P V q, or p :::} q\n•\n3R(p(R)), where R is a tuple variable\n•\n'ifR(p(R)) , where R is a tuple variable\nIn the last two clauses, the quantifiers :3 and 'if are said to bind the variable R.\nA variable is said to be free in a formula or subformuia (a formula contained\nin a larger formula) if the (sub)formula does not contain an occurrence of a\nquantifier that binds it.3\nWe observe that every variable in a TRC formula appears in a subformula\nthat is atomic, and every relation schema specifies a domain for each field; this\nobservation ensures that each variable in a TRC formula has a well-defined\ndomain from which values for the variable are drawn. That is, each variable\nhas a well-defined type, in the programming language sense.\nInformally, an\natomic formula R E Rei gives R the type of tuples in ReI, and comparisons\nsuch as R.a op S.b and R.a op constant induce type restrictions on the field\nR.a. If a variable R does not appear in an atomic formula of the form R E Rei\n(Le., it appears only in atomic formulas that are comparisons), we follow the\nconvention that the type of R is a tuple whose fields include all (and only) fields\nof R that appear in the formula.\nWe do not define types of variables formally, but the type of a variable should\nbe clear in most cases, and the important point to note is that comparisons of\nvalues having different types should always fail.\n(In discussions of relational\ncalculus, the simplifying assumption is often made that there is a single domain\nof constants and this is the domain associated with each field of each relation.)\nA TRC query is defined to be expression of the form {T I p(T)}, where T is\nthe only free variable in the formula p.\nSemantics of TRC Queries\nWhat does a TRC query mean? More precisely, what is the set of answer tuples\nfor a given TRC query? The answer to a TRC query {T I p(T)}, as noted\nearlier, is the set of all tuples t for which the formula peT) evaluates to true\nwith variable T &'3signed the tuple value t:. To complete this definition, we must\nstate which assignments of tuple values to the free variables in a formula make\nthe formula evaluate to true.\n3vVe make the assumption that each variable in a formula is either free or bound by exactly one\noccurrence of a quantifier, to avoid worrying about details such a.'l nested occurrences of quantifiers\nthat bind some, but not all, occurrences of variables.\n\nRelational Algebra and Calcuhl8\n119\nA query is evaluated on a given instance of the database. Let each free variable\nin a formula F be bound to a tuple value. For the given assignment of tuples\nto variables, with respect to the given database instance, F evaluates to (or\nsimply 'is') true if one of the following holds:\n•\nF is an atomic formula R E Rel, and R is assigned a tuple in the instance\nof relation Rel.\n•\nF is a comparison R.a op S.b, R.a op constant, or constant op R.a, and\nthe tuples assigned to Rand S have field values R.a and S.b that make the\ncomparison true.\n•\nF is of the form ---,p and p is not true, or of the form p 1\\ q, and both p and\nq are true, or of the form p V q and one of them is true, or of the form\np =} q and q is true whenever4 p is true.\n•\nF is of the form 3R(p(R)), and there is some assignment of tuples to the\nfree variables in p(R), including the variable R,5 that makes the formula\np(R) true.\n•\nF is of the form VR(p(R)), and there is some assignment of tuples to the\nfree variables in p(R) that makes the formula p(R) true no matter what\ntuple is assigned to R.\nExamples of TRC Queries\nWe now illustrate the calculus through several examples, using the instances\nB1 of Boats, R2 of Reserves, and S3 of Sailors shown in Figures 4.15, 4.16,\nand 4.17. We use parentheses as needed to make our formulas unambiguous.\nOften, a formula p(R) includes a condition R E Rel, and the meaning of the\nphrases some tuple R and for all tuples R is intuitive.\nWe use the notation\n3R E Rel(p(R)) for 3R(R E Rel 1\\ p(R)).\nSimilarly, we use the notation\nVR E Rel(p(R)) for VR(R E Rel =} p(R)).\n(Q12) Find the names and ages of sailors with a rating above 7.\n{P I 3S E Sailors(S.rating > 7 1\\ Pname = S.sname 1\\ Page = S.age)}\nThis query illustrates a useful convention: P is considered to be a tuple variable\nwith exactly two fields, which are called name and age, because these are the\nonly fields of P mentioned and P does not range over any of the relations in\nthe query; that is, there is no subformula of the form P\nE Relname.\nThe\nresult of this query is a relation with two fields, name and age.\nThe atomic\n4 WheneveT should be read more precisely as 'for all assignments of tuples to the free variables.'\n5Note that some of the free variables in p(R) (e.g., the variable R itself) IIlay be bound in P.\n\n120\nCHAPTER J4\nformulas P.name = S.sname and Page = S.age give values to the fields of an\nanswer tuple P. On instances E1, R2, and S3, the answer is the set of tuples\n(Lubber,55.5), (Andy, 25.5), (Rusty, ~~5.0), (Zorba, 16.0), ::lnd (Horatio, 35.0).\n(Q1S) Find the so;ilor name, boat'id, and reseT1}Q.tion date for each reservation.\n{P I 3R E ReseT\"ues 3S E Sailors\n(R.sid = 8.sid!\\ P.bid = R.bid!\\ P.day = R.day !\\ P.sname = S.sname)}\nFor each Reserves tuple, we look for a tuple in Sailors with the same sid. Given\na pair of such tuples, we construct an answer tuple P with fields sname, bid,\nand day by copying the corresponding fields from these two tuples. This query\nillustrates how we can combine values from different relations in each answer\ntuple. The answer to this query on instances E1, R2, and 83 is shown in Figure\n4.20.\nIsname ~..... day\nDustin\n101\n10/10/98\nDustin\n102\n10/10/98\nDustin\n103\n10/8/98\nDustin\n104\n10/7/98\nLubber\n102\n11/10/98\nLubber\n103\n11/6/98\nLubber\n104\n11/12/98\nHoratio\n101\n9/5/98\nHoratio\n102\n9/8/98\nHoratio\n103\n9/8/98\nFigure 4.20\nAnswer to Query Q13\n(Q1) Find the names of sailors who have reserved boat lOS.\n{P I 35 E Sailors 3R E Reserves(R.s'id = S.sid!\\ R.b'id = 103\n!\\Psname = 8.snarne)}\nThis query can be read as follows:\n\"Retrieve all sailor tuples for which there\nexists a tuple ,in Reserves having the same value in the s,id field and with\nb'id = 103.\" That is, for each sailor tuple, we look for a tuple in Reserves that\nshows that this sailor ha\" reserved boat\n10~~. The answer tuple P contains just\none field, sname.\n((22) Find the narnes of sailors who have reserved a n:.d boat.\n{P I :38 E Sailors\n:3R E Reserves(R.sid = 5.sid !\\ P.sname = S.8name\n\nRelational Algebra (nul Calculus\n121\n)\n1\\3B E Boats(B.llid = R.md 1\\ B.color ='red'))}\nThis query can be read as follows:\n\"Retrieve all sailor tuples S for which\nthere exist tuples R in Reserves and B in Boats such that S.sid =\nR.sid,\nR.bid = B.b'id, and B.coior ='red'.\"\nAnother way to write this query, which\ncorresponds more closely to this reading, is as follows:\n{P I 3S E SailoTs 3R E Reserves 3B E Boats\n(Rsid = S.sid 1\\ B.bid = R.bid 1\\ B.color ='red' 1\\ Psname = S.sname)}\n(Q7) Find the names of sailors who have reserved at least two boats.\n{P I 3S E Sailors 3Rl E Reserves 3R2 E Reserves\n(S.sid = R1.sid 1\\ R1.sid = R2.sid 1\\ R1.bid =I- R2.bid\nI\\Psname = S.sname)}\nContrast this query with the algebra version and see how much simpler the\ncalculus version is. In part, this difference is due to the cumbersome renaming\nof fields in the algebra version, but the calculus version really is simpler.\n(Q9) Find the narnes of sailors who have reserved all boats.\n{P I 3S E Sailors VB E Boats\n(3R E Reserves(S.sid = R.sid 1\\ R.bid = B.bid 1\\ Psname = S.sname))}\nThis query was expressed using the division operator in relational algebra. Note\nhow easily it is expressed in the calculus. The calculus query directly reflects\nhow we might express the query in English: \"Find sailors S such that for all\nboats B there is a Reserves tuple showing that sailor S has reserved boat B.\"\n(Q14) Find sailors who have reserved all red boats.\n{S I S E Sailor's 1\\ VB E Boats\n(B.color ='red' :::} (3R E Reserves(S.sid = R.sid 1\\ R.bid = B.bid)))}\nThis query can be read as follows: For each candidate (sailor), if a boat is red,\nthe sailor must have reserved it. That is, for a candidate sailor, a boat being\nred must imply that the sailor has reserved it. Observe that since we can return\nan entire sailor tuple as the ans\\ver instead of just the sailor's name, we avoided\nintroducing a new free variable (e.g., the variable P in the previous example)\nto hold the answer values. On instances Bl. R2, and S3, the answer contains\nthe Sailors tuples with sids 22 and 31.\nWe can write this query without using implication, by observing that an ex-\npression of the form p :::} q is logically equivalent to -'p V q:\n{S ! S E Sailors 1\\ VB E Boats\n\n122\nCHAPTER\n.~\n(B.coioT i-'Ted' V (3R E ReSeTVeS(S.sid = R..':tid/\\ R.b'id = B.lJid)))}\nThis query should be read a.s follows: \"Find sailors S such that, for all boats B,\neither the boat is not red or a Reserves tuple shows that sailor S has reserved\nboat B.\"\n4.3.2\nDomain Relational Calculus\nA domain variable is a variable that ranges over the values in the domain\nof some attribute (e.g., the variable can be assigned an integer if it appears\nin an attribute whose domain is the set of integers).\nA DRC query has the\nform {(XI,X2, ... ,Xn ) I P((XI,X2, ... ,Xn ))}, where each Xi is either a domain\nvariable or a constant and p((Xl, X2, ... ,xn )) denotes a DRC formula whose\nonly free variables are the variables among the Xi, 1 Sis n. The result of this\nquery is the set of all tuples (Xl, X2, ... , x n ) for which the formula evaluates to\ntrue.\nA DRC formula is defined in a manner very similar to the definition of a TRC\nformula. The main difference is that the variables are now domain variables.\nLet op denote an operator in the set {<, >, =, S,~, i-} and let X and Y be\ndomain variables. An atomic formula in DRC is one of the following:\nII\n(Xl, X2, ... , Xn )\nE Rel, where Rei is a relation with n attributes; each\nXi, 1 SiS n is either a variable or a constant\nII\nX op Y\nII\nX op constant, or constant op X\nA formula is recursively defined to be one of the following, where P and q\nare themselves formulas and p(X) denotes a formula in which the variable X\nappears:\nII\nany atomic formula\nII\n--.p, P /\\ q, P V q, or p =} q\nII\n3X(p(X)), where X is a domain variable\nII\n\\/X(p(X)), where X is a domain variable\nThe reader is invited to compare this definition with the definition of TRC\nforrnulch'3 and see how closely these two definitions correspond.\n\\Ve will not\ndefine the semantics of DRC formula.s formally; this is left as an exercise for\nthe reader.\n\nRelat'ional Algebra and Calculus\nExamples of DRC Queries\nvVe now illustrate DRC through several examples.\nThe reader is invited to\ncompare these with the TRC versions.\n(Q11) Find all sa'ilors with a rating above 7.\n{(1, N, T, A) I (I, N, T, A) E Sa'ilors /\\ T > 7}\nThis differs from the TRC version in giving each attribute a (variable) name.\nThe condition (1, N, T, A) E Sailors ensures that the domain variables I, N,\nT, and A are restricted to be fields of the same tuple. In comparison with the\nTRC query, we can say T > 7 instead of S.rating > 7, but we must specify the\ntuple (I, N, T, A) in the result, rather than just S.\n(Q1) Find the names of sailors who have reserved boat 103.\n{(N) I 31, T, A( (1, N, T, A) E Sa'ilors\n/\\311', Br, D( (11', Br, D) E Reserves /\\ 11' = I /\\ Br = 103))}\nNote that only the sname field is retained in the answer and that only N\nis a free variable.\nWe use the notation 3Ir,Br,D(... ) as a shorthand for\n3Ir(3Br(?JD(.. .))).\nVery often, all the quantified variables appear in a sin-\ngle relation, as in this example. An even more compact notation in this case\nis 3(11', Br, D) E Reserves. With this notation, which we use henceforth, the\nquery would be as follows:\n{(N) I 31, T, A((I, N, T, A) E Sailors\n/\\3(11', Br, D) E Reserves(Ir = I /\\ Br = 103))}\nThe comparison with the corresponding TRC formula should now be straight-\nforward.\nThis query can also be written as follows; note the repetition of\nvariable I and the use of the constant 103:\n{(N) I 31, T, A((1, N, T, A) E Sailors\n/\\3D( (1,103, D) E Reserves))}\n(Q2) Find the names of sailors who have Teserved a red boat.\n{(N) I 31, T, A((1, N, T, A) E Sailors\n/\\3(1, Br, D) E ReseTves /\\ 3(Br, BN,'Ted') E Boats)}\n(Q7) Find the names of sailoT.'! who have TeseTved at least two boat.s.\n{(N) I 31, T, A((1, N, T, A) E Sailors /\\\n?JBrl, BT2, Dl, D2( (1, Brl, DI) E Reserves\n/\\(1, Br2, D2) E Reserves /\\ Brl # Br2))}\n\n124\nCHAPTER\n..fl\nNote how the repeated use of variable I ensures that the same sailor has reserved\nboth the boats in question.\n(Q9) Find the names of sailors who have Teserved all boat8.\n{(N) I ~I, T, A( (I, N, T, A) E Sailors!\\\nVB, BN,C(-,((B, BN,C) E Boats) V\n(::J(Ir, Br, D) E Reserves(I = IT!\\ BT = B))))}\nThis query can be read as follows: \"Find all values of N such that some tuple\n(I, N, T, A) in Sailors satisfies the following condition: For every (B, BN, C),\neither this is not a tuple in Boats or there is some tuple (IT, BT, D) in Reserves\nthat proves that Sailor I has reserved boat B.\"\nThe V quantifier allows the\ndomain variables B, BN, and C to range over all values in their respective\nattribute domains, and the pattern '-,( (B, BN, C) E Boats)V' is necessary to\nrestrict attention to those values that appear in tuples of Boats. This pattern\nis common in DRC formulas, and the notation V(B, BN, C) E Boats can be\nused as a shortcut instead. This is similar to the notation introduced earlier\nfor 3. With this notation, the query would be written as follows:\n{(N)\nI 31, T, A((I, N, T, A) E Sa'iloTs !\\ V(B, BN, C) E Boats\n(3(1'1', BT, D) E ReseTves(I = IT!\\ BT = B)))}\n(Q14) Find sailoTs who have TeseTved all Ted boats.\n{(I, N, T, A)\nI (I, N, T, A) E SailoTs!\\ V(B, BN, C) E Boats\n(C ='red' =? ?J(Ir, BT, D) E Reserves(I = IT!\\ Br = B))}\nHere, we find all sailors such that, for every red boat, there is a tuple in Reserves\nthat shows the sailor has reserved it.\n4.4\nEXPRESSIVE POWER OF ALGEBRA AND\nCALCULUS\n\\Ve presented two formal query languages for the relational model. Are they\nequivalent in power?\nCan every query that can be expressed in relational\nalgebra also be expressed in relational calculus?\nThe answer is yes, it can.\nCan every query that can be expressed in relational calculus also be expressed\nin relational algebra?\nBefore we answer this question, we consider a major\nproblem with the calculus as we presented it.\nConsider the query {S I -,(S E Sailors)}. This query is syntactically correct.\nHowever, it asks for all tuples S such that S is not in (the given instance of)\n\nRelational Algebra an,d Calculu8\n125\nSailors. The set of such S tuples is obviously infinite, in the context of infinite\ndomains such as the set of all integers.\nThis simple example illustrates an\nunsafe query. It is desirable to restrict relational calculus to disallow unsafe\nqueries.\nvVe now sketch how calculus queries are restricted to be safe. Consider a set I\nof relation instances, with one instance per relation that appears in the query\nQ.\nLet Dom(Q, 1) be the set of all constants that appear in these relation\ninstances I or in the formulation of the query Q itself.\nSince we allow only\nfinite instances I, Dom(Q, 1) is also finite.\nFor a calculus formula Q to be considered safe, at a minimum we want to\nensure that, for any given I, the set of answers for Q contains only values in\nDom(Q, 1). While this restriction is obviously required, it is not enough. Not\nonly do we want the set of answers to be composed of constants in Dom(Q, 1),\nwe wish to compnte the set of answers by examining only tuples that contain\nconstants in Dom(Q, 1)! This wish leads to a subtle point associated with the\nuse of quantifiers V and :::J: Given a TRC formula of the form :::JR(p(R)), we want\nto find all values for variable R that make this formula true by checking only\ntuples that contain constants in Dom(Q, 1). Similarly, given a TRC formula of\nthe form VR(p(R)), we want to find any values for variable R that make this\nformula false by checking only tuples that contain constants in Dom(Q, 1).\nWe therefore define a safe TRC formula Q to be a formula such that:\n1. For any given I, the set of answers for Q contains only values that are in\nDom(Q, 1).\n2. For each subexpression of the form :::JR(p(R)) in Q, if a tuple r (assigned\nto variable R) makes the formula true, then r contains only constants in\nDorn(Q,I).\n3. For each subexpression of the form VR(p(R)) in Q, if a tuple r (assigned\nto variable R) contains a constant that is not in Dom(Q, 1), then r must\nmake the formula true.\nNote that this definition is not constructive, that is, it does not tell us hmv to\ncheck if a query is safe.\nThe query Q\n=\n{S I -.(S E Sailors)} is unsafe by this definition. Dom(Q,1)\nis the set of all values that appear in (an instance I of) Sailors. Consider the\ninstance Sl shown in Figure 4.1. The answer to this query obviously includes\nvalues that do not appear in Dorn(Q,81).\n\n126\nCHAPTERJ4\nReturning to the question of expressiveness, we can show that every query that\ncan be expressed using a safe relational calculus query can also be expressed as\na relational algebra query. The expressive power of relational algebra is often\nused as a metric of how powerful a relational database query language is. If\na query language can express all the queries that we can express in relational\nalgebra, it is said to be relationally complete. A practical query language is\nexpected to be relationally complete; in addition, commercial query languages\ntypically support features that allow us to express some queries that cannot be\nexpressed in relational algebra.\n4.5\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhat is the input to a relational query? What is the result of evaluating\na query? (Section 4.1)\n•\nDatabase systems use some variant of relational algebra to represent query\nevaluation plans. Explain why algebra is suitable for this purpose. (Sec-\ntion 4.2)\n•\nDescribe the selection operator. What can you say about the cardinality\nof the input and output tables for this operator? (That is, if the input has\nk tuples, what can you say about the output?)\nDescribe the projection\noperator. What can you say about the cardinality of the input and output\ntables for this operator? (Section 4.2.1)\n•\nDescribe the set operations of relational algebra, including union (U), in-\ntersection (n), set-difference (-), and cross-product (x). For each, what\ncan you say about the cardinality of their input and output tables? (Sec-\ntion 4.2.2)\n•\nExplain how the renaming operator is used. Is it required? That is, if this\noperator is not allowed, is there any query that can no longer be expressed\nin algebra? (Section 4.2.3)\n•\nDefine all the variations of the join operation. vVhy is the join operation\ngiven special attention? Cannot we express every join operation in terms\nof cross-product, selection, and projection? (Section 4.2.4)\n•\nDefine the division operation in terms of the ba--sic relational algebra op-\nerations. Describe a typical query that calls for division. Unlike join, the\ndivision operator is not given special treatment in database systems. Ex-\nplain why. (Section 4.2.5)\n\nRelational Algebnl and Calculus\n1~7\n•\nRelational calculus is said to be a declarati've language, in contrast to alge-\nbra, which is a procedural language. Explain the distinction. (Section 4.3)\n•\nHow does a relational calculus query 'describe' result tuples? Discuss the\nsubset of first-order predicate logic used in tuple relational calculus, with\nparticular attention to universal and existential quantifiers, bound and free\nvariables, and restrictions on the query formula. (Section 4.3.1).\n•\nvVhat is the difference between tuple relational calculus and domain rela-\ntional calculus? (Section 4.3.2).\n•\nWhat is an unsafe calculus query?\nWhy is it important to avoid such\nqueries? (Section 4.4)\n•\nRelational algebra and relational calculus are said to be equivalent in ex-\npressive power.\nExplain what this means, and how it is related to the\nnotion of relational completeness. (Section 4.4)\nEXERCISES\nExercise 4.1 Explain the statement that relational algebra operators can be composed. Why\nis the ability to compose operators important?\nExercise 4.2 Given two relations R1 and R2, where R1 contains N1 tuples, R2 contains N2\ntuples, and N2 > N1 > 0, give the minimum and maximum possible sizes (in tuples) for the\nresulting relation produced by each of the following relational algebra expressions. In each\ncase, state any assumptions about the schemas for R1 and R2 needed to make the expression\nmeaningful:\n(1) R1 U R2, (2) R1 n R2, (3) R1 ~ R2, (4) R1 x R2, (5) (Ta=5(R1), (6) 7Ta(R1), and\n(7) R1/R2\nExercise 4.3 Consider the following schema:\nSuppliers(sid: integer, sname: string, address: string)\nParts(pid: integer, pname: string, color: string)\nCatalog(sid: integer, pid: integer, cost: real)\nThe key fields are underlined, and the domain of each field is listed after the field name.\nTherefore sid is the key for Suppliers, pid is the key for Parts, and sid and pid together form\nthe key for Catalog. The Catalog relation lists the prices charged for parts by Suppliers. Write\nthe following queries in relational algebra, tuple relational calculus, and domain relational\ncalculus:\n1. Find the narnes of suppliers who supply some red part.\n2. Find the sids of suppliers who supply some red or green part.\n:3. Find the sids of suppliers who supply some red part or are at 221 Packer Ave.\n4. Find the sids of suppliers who supply some rcd part and some green part.\n\n128\nCHAPTER 4:\n5. Find the sids of suppliers who supply every part.\n6. Find the sids of suppliers who supply every red part.\n7. Find the sids of suppliers who supply every red or green part.\n8. Find the sids of suppliers who supply every red part or supply every green part.\n9. Find pairs of sids such that the supplier with the first sid charges more for some part\nthan the supplier with the second sid.\n10. Find the pids of parts supplied by at least two different suppliers.\n11. Find the pids of the most expensive parts supplied by suppliers named Yosemite Sham.\n12. Find the pids of parts supplied by every supplier at less than $200. (If any supplier either\ndoes not supply the part or charges more than $200 for it, the part is not selected.)\nExercise 4.4 Consider the Supplier-Parts-Catalog schema from the previous question. State\nwhat the following queries compute:\n1. 1fsname('rrsid(CTcolor='red' Parts) !Xl (O'cost<lOoCatalog) !Xl Suppliers)\n2.\n1fsname (1f S id ((0'color='red' Parts) !Xl (O'cost< looCatalog) !Xl Suppliers))\n3. (1fsname ((O'color'='red' Parts) [X] (crcost<looCatalog) !Xl Suppl'iers)) n\n4. (1fsid((crcolor='red,Parts) !Xl (crcost<10oCatalog) [X] Suppliers)) n\n(1fsid((CTcolor='green' Parts) [X] (crcost<lOoCatalog) !Xl Suppliers))\n(1f.sid,sname ((CTCO!07'='green' Parts) !Xl (CTcost< lOoCatalog) [X] Suppliers)))\nExercise 4.5 Consider the following relations containing airline flight information:\nFlights(fino: integer, from: string, to: string,\nd·istance: integer, depaTts: time, arrives: time)\nAircraft( aid: integer, aname: string, cTuisingrange: integer)\nCertified( eid: integer, aid: integer)\nEmployees( eid: integer, ename: string, salary: integer)\nNote that the Employees relation describes pilots and other kinds of employees cLS well; every\npilot is certified for some aircraft (otherwise, he or she would not qualify as a pilot), and only\npilots are certified to fly.\nWrite the following queries in relational algebra, tuple relational calculus, and domain rela-\ntional calculus. Note that some of these queries may not be expressible in relational algebra\n(and, therefore, also not expressible in tuple and domain relational calculus)! For such queries,\ninformally explain why they cannot be expressed. (See the exercises at the end of Chapter 5\nfor additional queries over the airline schenla.)\n1. Finel the eids of pilots certified for some Boeing aircraft.\n2. Find the names of pilots certified for some Boeing aircraft.\n~). Find the aids of all aircraft that. can be used on non-stop flights from Bonn to Madras.\n\nRelational Algebra and CalcuIu,s\n129\n4. Identi(y the flights that can be piloted by every pilot whose salary is more than $100,000.\n5. Find the names of pilots who can operate planes with a range greater than 3,000 miles\nbut are not certified on any Boeing aircraft.\n6. Find the eids of employees who make the highest salary.\n7. Find the eids of employees who make the second highest salary.\n8. Find the eids of employees who are certified for the largest number of aircraft.\n9. Find the eids of employees who are certified for exactly three aircraft.\n10. Find the total amount paid to employees as salaries.\n11. Is there a sequence of flights from Madison to Timbuktu? Each flight in the sequence is\nrequired to depart from the city that is the destination of the previous flight; the first\nflight must leave Madison, the last flight must reach Timbuktu, and there is no restriction\non the number of intermediate flights. Your query must determine whether a sequence\nof flights from Madison to Timbuktu exists for any input Flights relation instance.\nExercise 4.6 What is relational completeness? If a query language is relationally complete,\ncan you write any desired query in that language?\nExercise 4.7 What is an unsafe query? Give an example and explain why it is important\nto disallow such queries.\nBIBLIOGRAPHIC NOTES\nRelational algebra was proposed by Codd in [187], and he showed the equivalence of relational\nalgebra and TRC in [189]. Earlier, Kuhns [454] considered the use of logic to pose queries.\nLaCroix and Pirotte discussed DRC in [459]. Klug generalized the algebra and calculus to\ninclude aggregate operations in [439].\nExtensions of the algebra and calculus to deal with\naggregate functions are also discussed in [578].\nMerrett proposed an extended relational\nalgebra with quantifiers such as the number of that go beyond just universal and existential\nquantification [530]. Such generalized quantifiers are discussed at length in [52].\n\n5\nSQL: QUERIES,\nCONSTRNNTS, TRIGGERS\n..\nWhat is included in the SQL language? What is SQL:1999?\n..\nHow are queries expressed in SQL? How is the meaning of a query\nspecified in the SQL standard?\n,..-\nHow does SQL build on and extend relational algebra and calculus?\nl\"-\n\\Vhat is grouping? How is it used with aggregate operations?\n...\nWhat are nested queries?\n..\nWhat are null values?\n...\nHow can we use queries in writing complex integrity constraints?\n...\nWhat are triggers, and why are they useful? How are they related to\nintegrity constraints?\nItt\nKey concepts:\nSQL queries, connection to relational algebra and\ncalculus; features beyond algebra, DISTINCT clause and multiset se-\nmantics, grouping and aggregation; nested queries, correlation; set-\ncomparison operators; null values, outer joins; integrity constraints\nspecified using queries; triggers and active databases, event-condition-\naction rules.\n-------_.__._---_._------------_..__._---------------_\n_\n_----_.._ .._---\n\\Vhat men or gods are these? \\\\1hat Inaiclens loth?\n\\Vhat mad pursuit? \\1\\7hat struggle to escape?\n\\Vhat pipes and tilubrels? \\Vhat wild ecstasy?\n.... John Keats, Odc on\n(L Gr'ccian Urn\nStructured Query Language (SQL) is the most widely used conunercial rela-\ntional database language. It wa.\", originally developed at IBlVI in the SEQUEL-\n130\n\n131\nSQL Standards Conformance: SQL:1999 ha.,;; a collection of features\ncalled Core SQL that a vendor must implement to claim conformance with\nthe SQL:1999 standard. It is estimated that all the major vendors can\ncomply with Core SQL with little effort. l\\IIany of the remaining features\nare organized into packages.\nFor example, packages address each of the following (with relevant chapters\nin parentheses): enhanced date and time, enhanced integrity management\nI\nand active databases (this chapter), external language 'interfaces (Chapter\nl\n:6), OLAP (Chapter 25), and object features (Chapter 23). The SQL/Ml\\JI\nstandard complements SQL:1999 by defining additional packages that sup-\nport data mining (Chapter 26), spatial data (Chapter 28) and text docu-\nments (Chapter 27). Support for XML data and queries is forthcoming.\nXRM and System-R projects (1974-1977). Almost immediately, other vendors\nintroduced DBMS products based on SQL, and it is now a de facto standard.\nSQL continues to evolve in response to changing needs in the database area.\nThe current ANSI/ISO standard for SQL is called SQL:1999.\nWhile not all\nDBMS products support the full SQL:1999 standard yet, vendors are working\ntoward this goal and most products already support the core features.\nThe\nSQL:1999 standard is very close to the previous standard, SQL-92, with re-\nspect to the features discussed in this chapter. Our presentation is consistent\nwith both SQL-92 and SQL:1999, and we explicitly note any aspects that differ\nin the two versions of the standard.\n5.1\nOVERVIEW\nThe SQL language has several aspects to it.\n..\nThe Data Manipulation Language (DML): This subset of SQL allows\nusers to pose queries and to insert, delete, and modify rows. Queries are\nthe main focus of this chapter.\nWe covered DML commands to insert,\ndelete, and modify rows in Chapter 3.\n..\nThe Data Definition Language (DDL): This subset of SQL supports\nthe creation, deletion, and modification of definitions for tables and views.\nIntegrity constraints can be defined on tables, either when the table is\ncreated or later. \\Ve cocvered the DDL features of SQL in Chapter 3. Al-\nthough the standard does not discuss indexes, commercial implementations\nalso provide commands for creating and deleting indexes.\n..\nTriggers and Advanced Integrity Constraints: The new SQL:1999\nstandard includes support for triggers, which are actions executed by the\n\n132\nCHAPTER 5\nDBMS whenever changes to the databa..'3e meet conditions specified in the\ntrigger. vVe cover triggers in this chapter. SQL allows the use of queries\nto specify complex integrity constraint specifications. vVe also discuss such\nconstraints in this chapter.\n•\nEmbedded and Dynamic SQL: Embedded SQL features allow SQL\ncode to be called from a host language such as C or COBOL. Dynamic\nSQL features allow a query to be constructed (and executed) at run-time.\n\\Ve cover these features in Chapter 6.\n•\nClient-Server Execution and Remote Database Access: These com-\nmands control how a client application program can connect to an SQL\ndatabase server, or access data from a database over a network. We cover\nthese commands in Chapter 7.\n•\nTransaction Management: Various commands allow a user to explicitly\ncontrol aspects of how a tnmsaction is to be executed.\nWe cover these\ncommands in Chapter 21.\n•\nSecurity: SQL provides mechanisms to control users' access to data ob-\njects such as tables and views. We cover these in Chapter 2l.\n•\nAdvanced features:\nThe SQL:1999 standard includes object-oriented\nfeatures (Chapter 23), recursive queries (Chapter 24), decision support\nqueries (Chapter 25), and also addresses emerging areas such as data min-\ning (Chapter 26), spatial data (Chapter 28), and text and XML data man-\nagement (Chapter 27).\n5.1.1\nChapter Organization\nThe rest of this chapter is organized as follows. We present basic SQL queries\nin Section 5.2 and introduce SQL's set operators in Section 5.3.\nWe discuss\nnested queries, in which a relation referred to in the query is itself defined\nwithin the query, in Section 5.4. vVe cover aggregate operators, which allow us\nto write SQL queries that are not expressible in relational algebra, in Section\n5.5. \\Ve discuss null values, which are special values used to indicate unknown\nor nonexistent field values, in Section 5.6. We discuss complex integrity con-\nstraints that can be specified using the SQL DDL in Section 5.7, extending the\nSQL DDL discussion from Chapter 3; the new constraint specifications allow\nus to fully utilize the query language capabilities of SQL.\nFinally, we discuss the concept of an active databa8e in Sections 5.8 and 5.9.\nAn active database h&'3 a collection of triggers, which are specified by the\nDBA. A trigger describes actions to be taken when certain situations arise. The\nDBMS lllonitors the database, detects these situations, and invokes the trigger.\n\nSqL: QueT'ies. ConstTairLts, Triggf::T\"s\nThe SQL:1999 standard requires support for triggers, and several relational\nDB.rvIS products already support some form of triggers.\nAbout the Examples\n~Te will present a number of sample queries using the following table definitions:\nSailors(sid: integer, sname: string, rating: integer, age: real)\nBoats( bid: integer, bname: string, color: string)\nReserves(sid: integer, bid: integer, day: date)\nWe give each query a unique number, continuing with the numbering scheme\nused in Chapter 4. The first new query in this chapter has number Q15. Queries\nQ1 through Q14 were introduced in Chapter 4.1 We illustrate queries using the\ninstances 83 of Sailors, R2 of Reserves, and B1 of Boats introduced in Chapter\n4, which we reproduce in Figures 5.1, 5.2, and 5.3, respectively.\nAll the example tables and queries that appear in this chapter are available\nonline on the book's webpage at\nhttp://www.cs.wisc.edu/-dbbook\nThe online material includes instructions on how to set up Orade, IBM DB2,\nMicrosoft SQL Server, and MySQL, and scripts for creating the example tables\nand queries.\n5.2\nTHE FORM OF A BASIC SQL QUERY\nThis section presents the syntax of a simple SQL query and explains its meaning\nthrough a conceptual Evaluation strategy. A conceptual evaluation strategy is\na way to evaluate the query that is intended to be easy to understand rather\nthan efficient. A DBMS would typically execute a query in a different and more\nefficient way.\nThe basic form of an SQL query is &'3 follows:\nSELECT [DISTINCT] select-list\nFROM\nfrom-list\nWHERE\nqualification\n1All references to a query can be found in the subject index for the book.\n\n134\nI sid I sname·1 rating I age I\nCHAPTER 9\n22\nDustin\n7\n45.0\n29\nBrutus\n1\n33.0\n31\nLubber\n8\n55.5\n32\nAndy\n8\n25.5\n58\nRusty\n10\n35.0\n64\nHoratio\n7\n35.0\n71\nZorba\n10\n16.0\n74\nHoratio\n9\n35.0\n85\nArt\n3\n25.5\n95\nBob\n3\n63.5\nFigure 5.1\nAn Instance 53 of Sailors\n22\n101\n10/10/98\n22\n102\n10/10/98\n22\n103\n10/8/98\n22\n104\n10/7/98\n31\n102\n11/10/98\n31\n103\n11/6/98\n31\n104\n11/12/98\n64\n101\n9/5/98\n64\n102\n9/8/98\n74\n103\n9/8/98\nFigure 5.2\nAn Instance R2 of Reserves\n~ bname\nI color ··1\n101\nInterlake\nblue\n102\nInterlake\nred\n103\nClipper\ngreen\n104\nMarine\nred\nFigure 5.3\nAn Instance Bl of Boats\nEvery query must have a SELECT clause, which specifies columns to be retained\nin the result, and a FROM clause, which specifies a cross-product of tables. The\noptional WHERE clause specifies selection conditions on the tables mentioned in\nthe FROM clause.\nSuch a query intuitively corresponds to a relational algebra expression involving\nselections, projections, and cross-products. The close relationship between SQL\nand relational algebra is the basis for query optimization in a relational DBMS,\nas we will see in Chapters 12 and 15. Indeed, execution plans for SQL queries\nare represented using a variation of relational algebra expressions (Section 15.1).\nLet us consider a simple example.\n(Q15) Find the' names and ages of all sailors.\nSELECT DISTINCT S.sname, S.age\nFROM\nSailors S\nThe answer is a set of rows, each of which is a pair (sname, age). If two or\nmore sailors have the same name and age, the answer still contains just one pair\n\nSQL:\nQ1Le7~ies. Con8tnrint8, TriggeT8\n135\n~\nwith that name and age. This query is equivalent to applying the projection\noperator of relational algebra.\nIf we omit the keyword DISTINCT, we would get a copy of the row (s,a) for\neach sailor with name s and age a; the answer would be a rnultiset of rows. A\nmultiset is similar to a set in that it is an unordered collection of elements,\nbut there could be several copies of each element, and the number of copies is\nsignificant-two multisets could have the same elements and yet be different\nbecause the number of copies is different for some elements. For example, {a,\nb, b} and {b, a, b} denote the same multiset, and differ from the multiset {a,\na, b}.\nThe answer to this query with and without the keyword DISTINCT on instance\n53 of Sailors is shown in Figures 5.4 and 5.5. The only difference is that the\ntuple for Horatio appears twice if DISTINCT is omitted; this is because there\nare two sailors called Horatio and age 35.\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nHoratio\n35.0\nArt\n25.5\nBob\n63.5\nI sname\nI age I\n'-----\"--\nDustin\n45.0\nBrutus\n33.0\nLubber\n55.5\nAndy\n25.5\nRusty\n35.0\nHoratio\n35.0\nZorba\n16.0\nArt\n25.5\nBob\n63.5\nI.snarne\nI age I\nFigure 5.4\nAnswer to Q15\nFigure 5.5\nAnswer to Q15 without DISTINCT\nOur next query is equivalent to an application of the selection operator of\nrelational algebra.\n(Q11) Find all sailors with a rating above 7.\nSELECT S.sid, S.sname, S.rating, S.age\nFROM\nSailors AS S\nWHERE\nS.rating > 7\nThis query uses the optional keyword AS to introduce a range variable. Inci-\ndentally, when we want to retrieve all columns, as in this query, SQL provides a\n\n136\nCHAPTER fj\nconvenient shorthand: \\eVe can simply write SELECT *. This notation is useful\nfor interactive querying, but it is poor style for queries that are intended to be\nreused and maintained because the schema of the result is not clear from the\nquery itself; we have to refer to the schema of the underlying Sailors ta.ble.\nAs these two examples illustrate, the SELECT clause is actually used to do pm-\njection, whereas selections in the relational algebra sense are expressed using\nthe WHERE clause! This mismatch between the naming of the selection and pro-\njection operators in relational algebra and the syntax of SQL is an unfortunate\nhistorical accident.\nWe now consider the syntax of a basic SQL query in more detail.\n•\nThe from-list in the FROM clause is a list of table names. A table name\ncan be followed by a range variable; a range variable is particularly useful\nwhen the same table name appears more than once in the from-list.\n•\nThe select-list is a list of (expressions involving) column names of tables\nnamed in the from-list. Column names can be prefixed by a range variable.\n•\nThe qualification in the WHERE clause is a boolean combination (i.e., an\nexpression using the logical connectives AND, OR, and NOT) of conditions\nof the form expression op expression, where op is one of the comparison\noperators {<, <=, =, <>, >=, >}.2\nAn expression is a column name, a\nconstant, or an (arithmetic or string) expression.\n•\nThe DISTINCT keyword is optional. It indicates that the table computed\nas an answer to this query should not contain duplicates, that is, two copies\nof the same row. The default is that duplicates are not eliminated.\nAlthough the preceding rules describe (informally) the syntax of a basic SQL\nquery, they do not tell us the meaning of a query. The answer to a query is\nitself a relation\nwhich is a rnultisef of rows in SQL!--whose contents can be\nunderstood by considering the following conceptual evaluation strategy:\n1. Cmnpute the cross-product of the tables in the from-list.\n2. Delete rows in the cross-product that fail the qualification conditions.\n3. Delete all columns that do not appear in the select-list.\n4. If DISTINCT is specified, eliminate duplicate rows.\n2ExpressiollS with NOT can always be replaced by equivalent expressions without NOT given the set\nof comparison operators just listed.\n\nSCJL: Queries, ConstTaints, TriggeTs\n137\n~\nThis straightforward conceptual evaluation strategy makes explicit the rows\nthat must be present in the answer to the query.\nHowever, it is likely to be\nquite inefficient. We will consider how a DB:MS actually evaluates queries in\nlater chapters; for now, our purpose is simply to explain the meaning of a query.\n\\Ve illustrate the conceptual evaluation strategy using the following query':\n(Q1) Find the names of sailors 'Who have reseTved boat number 103.\nIt can be expressed in SQL as follows.\nSELECT S.sname\nFROM\nSailors S, Reserves R\nWHERE\nS.sid = R.sid AND R.bid=103\nLet us compute the answer to this query on the instances R3 of Reserves and\n84 of Sailors shown in Figures 5.6 and 5.7, since the computation on our usual\nexample instances (R2 and 83) would be unnecessarily tedious.\n~day\nI 22 I 101\n10/10/96\nI 58\nI 103\n11/12/96\nFigure 5.6\nInstance R3 of Reserves\n~ sname I Tating I age I\n22\ndustin\n7\n45.0\n31\nlubber\n8\n55.5\n58\nrusty\n10\n35.0\nFigure 5.7\nInstance 54 of Sailors\nThe first step is to construct the cross-product 84 x R3, which is shown in\nFigure 5.8.\n~ sname·j Tating·I···age~day\n22\ndustin\n7\n45.0\n22\n101\n10/10/96\n22\ndustin\n7\n45.0\n58\n103\n11/12/96\n31\nlubber\n8\n55.5\n22\n101\n10/10/96\n31\nlubber\n8\n---\n55.5\n58\n103\n11/12/96\n58\nrusty\n10\n3.5.0\n22\n101\n10/10/96\n58\nrusty\n10\n35.0\n58\n103\n11/12/96\nFigure 5.8\n84 x RS\nThe second step is to apply the qualification S./rid = R.sid AND R.bid=103.\n(Note that the first part of this qualification requires a join operation.) This\nstep eliminates all but the last row from the instance shown in Figure 5.8. The\nthird step is to eliminate unwanted columns; only sname appears in the SELECT\nclause. This step leaves us with the result shown in Figure .5.9, which is a table\nwith a single column and, a.c; it happens, just one row.\n\n138\n! sna'me!\n[I1lStL]\nFigure 5.9\nAnswer to Query Ql 011 R:l and 84\n5.2.1\nExamples of Basic SQL Queries\nCHAPTER 15\nvVe now present several example queries, many of which were expressed earlier\nin relational algebra and calculus (Chapter 4).\nOur first example illustrates\nthat the use of range variables is optional, unless they are needed to resolve an\nambiguity. Query Ql, which we discussed in the previous section, can also be\nexpressed as follows:\nSELECT sname\nFROM\nSailors 5, Reserves R\nWHERE\nS.sid = R.sid AND bid=103\nOnly the occurrences of sid have to be qualified, since this column appears in\nboth the Sailors and Reserves tables. An equivalent way to write this query is:\nSELECT SHame\nFROM\nSailors, Reserves\nWHERE\nSailors.sid = Reserves.sid AND bid=103\nThis query shows that table names can be used implicitly as row variables.\nRange variables need to be introduced explicitly only when the FROM clause\ncontains more than one occurrence of a relation. 3\nHowever, we recommend\nthe explicit use of range variables and full qualification of all occurrences of\ncolumns with a range variable to improve the readability of your queries. We\nwill follow this convention in all our examples.\n(Q16) Find the sids of sa'iloTs who have TeseTved a Ted boat.\nSELECT\nFROM\nWHERE\nR.sid\nBoats B, Reserves R\nB.bid = R.bid AND 8.color = 'red'\nThis query contains a join of two tables, followed by a selection on the color\nof boats. vVe can think of 13 and R\n&<; rows in the corresponding tables that\n:~The table name cannot be used aii an implicit. range variable once a range variable is introduced\nfor t.he relation.\n\nSQL: QUEeries, Constraints, Triggers\n:prove' that a sailor with sid R.sid reserved a reel boat B.bid. On our example\ninstances R2 and 83 (Figures 5.1 and 5.2), the answer consists of the Bids 22,\n31, and 64. If we want the names of sailors in the result, we must also consider\nthe Sailors relation, since Reserves does not contain this information, as the\nnext example illustrates.\n(Q2) Find the names of sailors 'Who have TeseTved a Ted boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats 13\nS.sid = R.sid AND R.bid = 13.bid AND B.color = 'red'\nThis query contains a join of three tables followed by a selection on the color\nof boats. The join with Sailors allows us to find the name of the sailor who,\naccording to Reserves tuple R, has reserved a red boat described by tuple 13.\n(QS) Find the coloTS of boats reseTved by LubbeT.\nSELECT 13.color\nFROM\nSailors S, Reserves R, Boats 13\nWHERE\nS.sid = R.sid AND R.bid = B.bid AND S.sname = 'Lubber'\nThis query is very similar to the previous one. Note that in general there may\nbe more than one sailor called Lubber (since sname is not a key for Sailors);\nthis query is still correct in that it will return the colors of boats reserved by\nsome Lubber, if there are several sailors called Lubber.\n(Q4) Find the names of sa'iloTs who have Teserved at least one boat.\nSELECT S.sname\nFROM\nSailors S, Reserves R\nWHERE\nS.sid = R.sid\nThe join of Sailors and Reserves ensures that for each selected sname, the\nsailor has made some reservation. (If a sailor has not made a reservation, the\nsecond step in the conceptual evaluation strategy would eliminate all rows in\nthe cross-product that involve this sailor.)\n5.2.2\nExpressions and Strings in the SELECT Command\nSQL supports a more general version of the select-list than just a list of\ncolu1nn8.\nEach item in a select-list can be of the form e:l:pTcssion AS col-\n'wnrLno:rne, where c:rprcs.sion is any arithmetic or string expression over column\n\n140\nCHAPTERf)\nnames (possibly prefixed by range variables) and constants, and colurnnswrne\nis a ne\"v name for this column in the output of the query. It can also contain\naggregates such as smn and count, which we will discuss in Section 5.5. The\nSQL standard also includes expressions over date and time values, which we will\nnot discuss. Although not part of the SQL standard, many implementations\nalso support the use of built-in functions such as sqrt, sin, and rnod.\n(Q17) Compute increments for the mtings of peTsons who have sailed two dif-\nferent boats on the same day.\nSELECT\nFROM\nWHERE\nS.sname, S.rating+1 AS rating\nSailors S, Reserves R1, Reserves R2\nS.sid = R1.sid AND S.sid = R2.sid\nAND R1.day = R2.day AND R1.bid <> R2.bid\nAlso, each item in a qualification can be as general as expTession1 = expression2.\nSELECT S1.sname AS name1, S2.sname AS name2\nFROM\nSailors Sl, Sailors S2\nWHERE\n2*S1.rating = S2.rating-1\nFor string comparisons, we can use the comparison operators (=, <, >, etc.)\nwith the ordering of strings determined alphabetically as usual.\nIf we need\nto sort strings by an order other than alphabetical (e.g., sort strings denoting\nmonth names in the calendar order January, February, March, etc.), SQL sup-\nports a general concept of a collation, or sort order, for a character set.\nA\ncollation allows the user to specify which characters are 'less than' which others\nand provides great flexibility in string manipulation.\nIn addition, SQL provides support for pattern matching through the LIKE op-\nerator, along with the use of the wild-card symbols % (which stands for zero\nor more arbitrary characters) and\n~ (which stands for exactly one, arbitrary,\ncharacter).\nThus, '_AB%' denotes a pattern matching every string that con-\ntains at lea.'3t three characters, with the second and third characters being A\nand B respectively.\nNote that unlike the other comparison operators, blanks\ncan be significant for the LIKE operator (depending on the collation for the\nunderlying character set). Thus, 'Jeff' = 'Jeff' is true while 'Jeff'LIKE 'Jeff\n, is false. An example of the use of LIKE in a query is given below.\n(Q18) Find the ages of sailors wh08e name begins and ends with B and has at\nleast three chamcters.\nSELECT S.age\n\nSQL: Q'Il,e'rie8, Constraints, TTiggeTs\n141\n$\nr---'-~~-~:~;- Expre~~~'~-:~'-'~:' '~Q'~'~\"\"~,~flecting the incr~~~~~~~mpo~~:l~ceof I\nI\ntext data, SQL:1999 includes a more powerful version of theLIKE operator\ni\ni\ncalled SIMILAR. This operator allows a rich set of regular expressions to be\nI\nI\nused as patterns while searching text. The regular expressions are similar t~\nI\nthose sUPPo.rted by the Unix operating systenifor string searches, although'\nthe syntax is a little different.\n-- -\n.\n••••._m.\n.-.-.-.-.-....•--..-----........\n.-••••••••••••.•.•••••••....-- ..-\n----.--\n-----..-- ..-\n------------ ---------- -.••..•.---•••••.••.•.•••••••••..•.•.••••••.••- ••••.....•- ••••.'.-\"\"-'-.\nJ '\n.\nRelational Algebra and SQL: The set operations of SQL are available in\nrelational algebra. The main difference, of course, is that they are multiset\noperations in SQL, since tables are multisets of tuples.\nFROM\nWHERE\nSailors S\nS.sname LIKE 'B.%B'\nThe only such sailor is Bob, and his age is 63.5.\n5.3\nUNION, INTERSECT, AND EXCEPT\nSQL provides three set-manipulation constructs that extend the basic query\nform presented earlier. Since the answer to a query is a multiset of rows, it is\nnatural to consider the use of operations such as union, intersection, and differ-\nence. SQL supports these operations under the names UNION, INTERSECT, and\nEXCEPT. 4 SQL also provides other set operations: IN (to check if an element\nis in a given set), op ANY, op ALL (to compare a value with the elements in\na given set, using comparison operator op), and EXISTS (to check if a set is\nempty). IN and EXISTS can be prefixed by NOT, with the obvious modification\nto their meaning.\nWe cover UNION,\nINTERSECT, and EXCEPT in this section,\nand the other operations in Section 5.4.\nConsider the following query:\n(Q5) Find the names of sailors who have reserved a red 01' a green boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sid = R.sid AND R.bid = B.bid\nAND (B.color = 'red' OR B.color = 'green')\n.. _----\n4Note that although the SQL standard includes these operations, many systems currently support\nonly UNION. Also. many systems recognize the keyword MINUS for EXCEPT.\n\n142\nCHAPTERD\nThis query is easily expressed using the OR connective in the WHERE clause.\nHovvever, the following query, which is identical except for the use of 'and'\nrather than 'or' in the English version, turns out to be much more difficult:\n(Q6) Find the names of sailor's who have rescr'ved both a red and a green boat.\nIf we were to just replace the use of OR in the previous query by AND, in analogy\nto the English statements of the two queries, we would retrieve the names of\nsailors who have reserved a boat that is both red and green.\nThe integrity\nconstraint that bid is a key for Boats tells us that the same boat cannot have\ntwo colors, and so the variant of the previous query with AND in place of OR will\nalways return an empty answer set.\nA correct statement of Query Q6 using\nAND is the following:\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves RI, Boats BI, Reserves R2, Boats B2\nS.sid = Rl.sid AND R1.bid = Bl.bid\nAND S.sid = R2.sid AND R2.bid = B2.bid\nAND B1.color='red' AND B2.color = 'green'\nWe can think of RI and BI as rows that prove that sailor S.sid has reserved a\nred boat. R2 and B2 similarly prove that the same sailor has reserved a green\nboat. S.sname is not included in the result unless five such rows S, RI, BI, R2,\nand B2 are found.\nThe previous query is difficult to understand (and also quite inefficient to ex-\necute, as it turns out). In particular, the similarity to the previous OR query\n(Query Q5) is completely lost. A better solution for these two queries is to use\nUNION and INTERSECT.\nThe OR query (Query Q5) can be rewritten as follows:\nSELECT\nFROM\nWHERE\nUNION\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sicl = R.sid AND R.bid = B.bid AND B.color = 'red'\nS2.sname\nSailors S2, Boats B2, Reserves H2\nS2.sid = H2.sid AND R2.bid = B2.bicl AND B2.color = 'green'\nThis query sa,)'s that we want the union of the set of sailors who have reserved\nred boats and the set of sailors who have reserved green boats.\nIn complete\nsymmetry, the AND query (Query Q6) can be rewritten a.s follovvs:\nSELECT S.snarne\n\nSqL: Q'lleries 7 Constraints, Triggers\nFROM\nSailors S, Reserves R, Boats B\nWHERE\nS.sid = R.sid AND R.bid = B.bid AND B.color = 'red'\nINTERSECT\nSELECT S2.sname\nFROM\nSailors S2, Boats B2, Reserves R2\nWHERE\nS2.sid = R2.sid AND R2.bid = B2.bid AND B2.color = 'green'\n143\nThis query actually contains a subtle bug-if there are two sailors such as\nHoratio in our example instances B1, R2, and 83, one of whom has reserved a\nred boat and the other has reserved a green boat, the name Horatio is returned\neven though no one individual called Horatio has reserved both a red and a\ngreen boat. Thus, the query actually computes sailor names such that some\nsailor with this name has reserved a red boat and some sailor with the same\nname (perhaps a different sailor) has reserved a green boat.\nAs we observed in Chapter 4, the problem arises because we are using sname\nto identify sailors, and sname is not a key for Sailors! If we select sid instead of\nsname in the previous query, we would compute the set of sids of sailors who\nhave reserved both red and green boats. (To compute the names of such sailors\nrequires a nested query; we will return to this example in Section 5.4.4.)\nOur next query illustrates the set-difference operation in SQL.\n(Q19) Find the sids of all sailor's who have reserved red boats but not green\nboats.\nSELECT\nFROM\nWHERE\nEXCEPT\nSELECT\nFROM\nWHERE\nS.sid\nSailors S, Reserves R, Boats B\nS.sid = R.sid AND R.bid = B.bid AND B.color = 'red'\nS2.sid\nSailors S2, Reserves R2, Boats B2\nS2.sid = R2.sid AND R2.bid = B2.bid AND B2.color = 'green'\nSailors 22, 64, and 31 have reserved red boats.\nSailors 22, 74, and 31 have\nreserved green boats. Hence, the answer contains just the sid 64.\nIndeed, since the Reserves relation contains sid information, there is no need\nto look at the Sailors relation, and we can use the following simpler query:\nSELECT\nFROM\nWHERE\nEXCEPT\nH..sid\nBoats B, Reserves R\nR.bicl = B.bid AND B.color = 'red'\n\n144\nSELECT R2.sid\nFROM\nBoats B2, Reserves R2\nWHERE\nR2.bicl = B2.bid AND B2.color = :green'\nCHAPTER~5\nObserve that this query relies on referential integrity; that is, there are no\nreservations for nonexisting sailors. Note that UNION, INTERSECT, and EXCEPT\ncan be used on any two tables that are union-compatible, that is, have the same\nnumber of columns and the columns, taken in order, have the same types. For\nexample, we can write the following query:\n(Q20) Find all sids of sailors who have a rating of 10 or reserved boat 104.\nSELECT\nFROM\nWHERE\nUNION\nSELECT\nFROM\nWHERE\nS.sid\nSailors S\nS.rating = 10\nR.sid\nReserves R\nR.bid = 104\nThe first part of the union returns the sids 58 and 71. The second part returns\n22 and 31.\nThe answer is, therefore, the set of sids 22, 31, 58, and 71.\nA\nfinal point to note about UNION, INTERSECT, and EXCEPT follows. In contrast\nto the default that duplicates are not eliminated unless DISTINCT is specified\nin the basic query form, the default for UNION queries is that duplicates are\neliminated! To retain duplicates, UNION ALL must be used; if so, the number\nof copies of a row in the result is always m + n, where m and n are the num-\nbers of times that the row appears in the two parts of the union.\nSimilarly,\nINTERSECT ALL retains cluplicates--the number of copies of a row in the result\nis min(m, n )-~ancl EXCEPT ALL also retains duplicates~thenumber of copies\nof a row in the result is m - n, where 'm corresponds to the first relation.\n5.4\nNESTED QUERIES\nOne of the most powerful features of SQL is nested queries. A nested query\nis a query that has another query embedded within it; the embedded query\nis called a suhquery. The embedded query can of course be a nested query\nitself; thus queries that have very deeply nested structures are possible. When\nwriting a query, we sornetimes need to express a condition that refers to a table\nthat must itself be computed. The query used to compute this subsidiary table\nis a subquery and appears as part of the main query.\nA subquery typically\nappears within the WHERE clause of a query. Subqueries can sometimes appear\nin the FROM clause or the HAVING clause (which we present in Section 5.5).\n\nSQL:\nQueT~ie8] Constraints] Triggers\n145\n$\nr-~'-'-'~\n-.-------\n-~---\n,.,.._,---..--\n, ,-, ,-_\n,\n. ---------\nI Relational Algebra and SQL: Nesting of queries is a feature that is not\nI\navailable in relational algebra, but nested queries can be translated into\ni\nalgebra, as we will see in Chapter 15. Nesting in SQL is inspired more by\n,\nrelational calculus than algebra. In conjunction with some of SQL's other\nfeatures, such as (multi)set operators and aggregation, nesting is a very\nexpressive construct.\nThis section discusses only subqueries that appear in the WHERE clause. The\ntreatment of subqueries appearing elsewhere is quite similar. Some examples of\nsubqueries that appear in the FROM clause are discussed later in Section 5.5.1.\n5.4.1\nIntroduction to Nested Queries\nAs an example, let us rewrite the following query, which we discussed earlier,\nusing a nested subquery:\n(Ql) Find the names of sailors who have reserved boat 103.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.sid IN ( SELECT\nFROM\nWHERE\nR.sid\nReserves R\nR.bid = 103 )\nThe nested subquery computes the (multi)set of sids for sailors who have re-\nserved boat 103 (the set contains 22,31, and 74 on instances R2 and 83), and\nthe top-level query retrieves the names of sailors whose sid is in this set. The\nIN operator allows us to test whether a value is in a given set of elements; an\nSQL query is used to generate the set to be tested. Note that it is very easy to\nmodify this query to find all sailors who have not reserved boat 103-we can\njust replace IN by NOT IN!\nThe best way to understand a nested query is to think of it in terms of a con-\nceptual evaluation strategy. In our example, the strategy consists of examining\nrows in Sailors and, for each such row, evaluating the subquery over Reserves.\nIn general, the conceptual evaluation strategy that we presented for defining\nthe semantics of a query can be extended to cover nested queries as follows:\nConstruct the cross-product of the tables in the FROM clause of the top-level\nquery as hefore. For each row in the cross-product, while testing the qllalifica-\n\n146\ntion in the WHERE clause, (re)compute the subquery.5 Of course, the subquery\nmight itself contain another nested subquery, in which case we apply the same\nidea one more time, leading to an evaluation strategy \\vith several levels of\nnested loops.\nAs an example of a multiply nested query, let us rewrite the following query.\n(Q2) Find the names of sailors 'who ha'ue reserved a red boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.sid IN ( SELECT R.sid\nFROM\nReserves R\nWHERE\nR.bid IN (SELECT B.bid\nFROM\nBoats B\nWHERE\nB.color = 'red'\nThe innermost subquery finds the set of bids of red boats (102 and 104 on\ninstance E1). The subquery one level above finds the set of sids of sailors who\nhave reserved one of these boats. On instances E1, R2, and 83, this set of sids\ncontains 22, 31, and 64. The top-level query finds the names of sailors whose\nsid is in this set of sids; we get Dustin, Lubber, and Horatio.\nTo find the names of sailors who have not reserved a red boat, wc replace the\noutermost occurrence of IN by NOT IN, as illustrated in the next query.\n(Q21) Find the names of sailors who have not reserved a red boat.\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.sid NOT IN ( SELECT\nFROM\nWHERE\nR.sid\nReserves R\nR.bid IN ( SELECT B.bid\nFROM\nBoats B\nWHERE\nB.color = 'red' )\nThis qucry computes the names of sailors whose sid is not in the set 22, 31,\nand 64.\nIn contrast to Query Q21, we can modify the previous query (the nested version\nof Q2) by replacing the inner occurrence (rather than the outer occurence) of\n5Since the inner subquery in our example does not depend on the 'current' row from the outer\nquery ill any way, you rnight wonder why we have to recompute the subquery for each outer row. For\nan answer, see Section 5.4.2.\n\nSQL: Queries, Con.,trf1'inis, Triggers\nIN with NOT IN. This modified query would compute the naU1eS of sailors who\nhave reserved a boat that is not red, that is, if they have a reservation, it is not\nfor a red boat. Let us consider how. In the inner query, we check that R.bid\nis not either 102 or 104 (the bids of red boats). The outer query then finds the\nsids in Reserves tuples \\vhere the bid is not 102 or 104. On instances E1, R2,\nand 53, the outer query computes the set of sids 22, 31, 64, and 74. Finally,\nwe find the names of sailors whose sid is in this set.\n\\Ve can also modify the nested query Q2 by replacing both occurrences of IN\nwith NOT IN. This variant finds the names of sailors who have not reserved a\nboat that is not red, that is, who have reserved only red boats (if they've re-\nserved any boats at all). Proceeding as in the previous paragraph, on instances\nE1, R2, and 53, the outer query computes the set of sids (in Sailors) other\nthan 22, 31, 64, and 74. This is the set 29, 32, 58, 71, 85, and 95. We then find\nthe names of sailors whose sid is in this set.\n5.4.2\nCorrelated Nested Queries\nIn the nested queries seen thus far, the inner subquery has been completely\nindependent of the outer query. In general, the inner subquery could depend on\nthe row currently being examined in the outer query (in terms of our conceptual\nevaluation strategy). Let us rewrite the following query once more.\n(Q1) Pind the names of sailors who have reserved boat nv,mber 103.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nEXISTS ( SELECT *\nFROM\nReserves R\nWHERE\nR.bid = 103\nAND R.sid = S.sid )\nThe EXISTS operator is another set comparison operator, such as IN. It allows\nus to test whether a set is nonempty, an implicit comparison with the empty\nset.\nThus, for each Sailor row 5, we test whether the set of Reserves rows\nR such that R.bid = 103 AND S.sid = R.sid is nonempty. If so, sailor 5 has\nreserved boat t03, and we retrieve the name.\n'1'he subquery clearly depends\non the current row Sand IIlUSt be re-evaluated for each row in Sailors. The\noccurrence of S in the subquery (in the form of the literal S.sid) is called a\ncOTTelation, and such queries are called con-elated queries.\nThis query also illustrates the use of the special symbol * in situations where\nall we want to do is to check that a qualifying row exists, and do Hot really\n\n148\nCHAPTER,.5\nwant to retrieve any columns from the row. This is one of the two uses of * in\nthe SELECT clause that is good programming style; the other is &':1 an argument\nof the COUNT aggregate operation, which we describe shortly.\nAs a further example, by using NOT EXISTS instead of EXISTS, we can compute\nthe names of sailors who have not reserved a red boat.\nClosely related to\nEXISTS is the UNIQUE predicate. \\Vhen we apply UNIQUE to a subquery, the\nresulting condition returns true if no row appears twice in the answer to the\nsubquery, that is, there are no duplicates; in particular, it returns true if the\nanswer is empty. (And there is also a NOT UNI QUE version.)\n5.4.3\nSet-Comparison Operators\nWe have already seen the set-comparison operators EXISTS, IN, and UNIQUE,\nalong with their negated versions. SQL also supports op ANY and op ALL, where\nop is one of the arithmetic comparison operators {<, <=, =, <>, >=, >}. (SOME\nis also available, but it is just a synonym for ANY.)\n(Q22) Find sailors whose rating is better than some sailor called Horatio.\nSELECT S.sid\nFROM\nSailors S\nWHERE\nS.rating > ANY ( SELECT\nFROM\nWHERE\nS2.rating\nSailors S2\nS2.sname = 'Horatio' )\nIf there are several sailors called Horatio, this query finds all sailors whose rating\nis better than that of some sailor called Horatio. On instance 83, this computes\nthe sids 31, 32, 58, 71, and 74. \\\\That if there were no sailor called Horatio? In\nthis case the comparison S.rating > ANY ... is defined to return false, and the\nquery returns an elnpty answer set. To understand comparisons involving ANY,\nit is useful to think of the comparison being carried out repeatedly.\nIn this\nexample, S. rating is successively compared with each rating value that is an\nanswer to the nested query. Intuitively, the subquery must return a row that\nmakes the comparison true, in order for S. rat'ing > ANY ... to return true.\n(Q23) Find sailors whose rating is better than every sailor' called Horat·to.\nvVe can obtain all such queries with a simple modification to Query Q22: Just\nreplace ANY with ALL in the WHERE clause of the outer query. On instance 8~~,\nwe would get the sid\", 58 and 71. If there were no sailor called Horatio, the\ncomparison S.rating > ALL ... is defined to return true! The query would then\nreturn the names of all sailors. Again, it is useful to think of the comparison\n\nSQL: C2uerie,s, ConstTain,ts, Triggers\n149\nbeing carried out repeatedly. Intuitively, the comparison must be true for every\nreturned row for S.rating> ALL ... to return true.\nAs another illustration of ALL, consider the following query.\n(Q24J Find the 8ailor's with the highest rating.\nSELECT S.sid\nFROM\nSailors S\nWHERE\nS.rating >= ALL ( SELECT S2.rating\nFROM\nSailors S2 )\nThe subquery computes the set of all rating values in Sailors. The outer WHERE\ncondition is satisfied only when S.rating is greater than or equal to each of\nthese rating values, that is, when it is the largest rating value. In the instance\n53, the condition is satisfied only for rating 10, and the answer includes the\nsid.\" of sailors with this rating, Le., 58 and 71.\nNote that IN and NOT IN are equivalent to = ANY and <> ALL, respectively.\n5.4.4\nMore Examples of Nested Queries\nLet us revisit a query that we considered earlier using the INTERSECT operator.\n(Q6) Find the names of sailors who have reserved both a red and a green boat.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S, Reserves R, Boats B\nS.sid = R.sid AND R.bid = B.bid AND B.color = 'red'\nAND S.sid IN ( SELECT S2.sid\nFROM\nSailors S2, Boats B2, Reserves R2\nWHERE\nS2.sid = R2.sid AND R2.bid = B2.bid\nAND B2.color = 'green' )\nThis query can be understood as follows: \"Find all sailors who have reserved\na red boat and, further, have sids that are included in the set of sids of sailors\nwho have reserved a green boat.\"\nThis formulation of the query illustrates\nhow queries involving INTERSECT can be rewritten using IN, which is useful to\nknow if your system does not support INTERSECT. Queries using EXCEPT can\nbe similarly rewritten by using NOT IN. To find the side:, of sailors who have\nreserved red boats but not green boats, we can simply replace the keyword IN\nin the previous query by NOT IN.\n\n150\nCHAPTER\"S\nAs it turns out, writing this query (Q6) using INTERSECT is more complicated\nbecause we have to use sids to identify sailors (while intersecting) and have to\nreturn sailor names:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.sid IN (( SELECT R.sid\nFROM\nBoats B, Reserves R\nWHERE\nR.bid = B.bid AND B.color = 'red' )\nINTERSECT\n(SELECT R2.sid\nFROM\nBoats B2, Reserves R2\nWHERE\nR2.bid = B2.bid AND B2.color = 'green' ))\nOur next example illustrates how the division operation in relational algebra\ncan be expressed in SQL.\n(Q9) Find the names of sailors who have TeseTved all boats.\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS (( SELECT B.bid\nFROM\nBoats B )\nEXCEPT\n(SELECT R.bid\nFROM\nReserves R\nWHERE\nR.sid = S.sid ))\nNote that this query is correlated--for each sailor S, we check to see that the\nset of boats reserved by S includes every boat. An alternative way to do this\nquery without using EXCEPT follows:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS ( SELECT\nFROM\nWHERE\nB.bid\nBoats B\nNOT EXISTS ( SELECT R.bid\nFROM\nReserves R\nWHERE\nR.bid = B.bid\nAND R.sid = S.sid ))\nIntuitively, for each sailor we check that there is no boat that has not been\nreserved by this sailor.\n\nSQL: Q'ueT'ics. Constraint8, Triggers\nlQJ\nSQL:1999 Aggregate Functions: The collection of aggregate functions\nis greatly expanded in the new standard, including several statistical\ntions such as standard deviation, covariance, and percentiles. However,\nnew aggregate functions are in the SQLjOLAP package and may not\nsupported by all vendors.\n5.5\nAGGREGATE OPERATORS\nIn addition to simply retrieving data, we often want to perform some compu-\ntation or summarization. As we noted earlier in this chapter, SQL allows the\nuse of arithmetic expressions. We now consider a powerful class of constructs\nfor computing aggregate values such as MIN and SUM. These features represent\na significant extension of relational algebra. SQL supports five aggregate oper-\nations, which can be applied on any column, say A, of a relation:\n1. COUNT ([DISTINCT] A): The number of (unique) values in the A column.\n2. SUM ([DISTINCT] A): The sum of all (unique) values in the A column.\n3. AVG ([DISTINCT] A): The average of all (unique) values in the A column.\n4. MAX (A): The maximum value in the A column.\n5. MIN (A): The minimum value in the A column.\nNote that it does not make sense to specify DISTINCT in conjunction with MIN\nor MAX (although SQL does not preclude this).\n(Q25) Find the average age of all sailors.\nSELECT AVG (S.age)\nFROM\nSailors S\nOn instance 53, the average age is 37.4. Of course, the WHERE clause can be\nused to restrict the sailors considered in computing the average age.\n(Q26) Find the average age of sailors with a rating of 10.\nSELECT AVG (S.age)\nFROM\nSailors S\nWHERE\nS.rating = 10\nThere are two such sailors, and their average age is 25.5. MIN (or MAX) can be\nused instead of AVG in the above queries to find the age of the youngest (oldest)\n\n1,..')\n0 ...\nsailor. However) finding both the name and the age of the oldest sailor is more\ntricky, as the next query illustrates.\n(Q,\"21) Find the name and age of the oldest sailor.\nConsider the following attempt to answer this query:\nSELECT S.sname, MAX (S.age)\nFROM\nSailors S\nThe intent is for this query to return not only the maximum age but also the\nname of the sailors having that age. However, this query is illegal in SQL-if\nthe SELECT clause uses an aggregate operation, then it must use only aggregate\noperations unless the query contains a GROUP BY clause! (The intuition behind\nthis restriction should become clear when we discuss the GROUP BY clause in\nSection 5.5.1.) Therefore, we cannot use MAX (S.age) as well as S.sname in the\nSELECT clause. We have to use a nested query to compute the desired answer\nto Q27:\nSELECT\nFROM\nWHERE\nS.sname, S.age\nSailors S\nS.age = ( SELECT MAX (S2.age)\nFROM Sailors S2 )\nObserve that we have used the result of an aggregate operation in the subquery\nas an argument to a comparison operation. Strictly speaking, we are comparing\nan age value with the result of the subquery, which is a relation.\nHowever,\nbecause of the use of the aggregate operation, the subquery is guaranteed to\nreturn a single tuple with a single field, and SQL Gonverts such a relation to a\nfield value for the sake of the comparison. The following equivalent query for\nQ27 is legal in the SQL standard but, unfortunately, is not supported in many\nsystems:\nSELECT\nFROM\nWHERE\nS.sname, S.age\nSailors S\n( SELECT MAX (S2.age)\nFROM Sailors S2 ) = S.age\n\\Vc can count the number of sailors using COUNT. This exarnple illustrates the\nuse of * as an argument to COUNT, which is useful when \\ve want to count all\nrows.\n(Q28) Count the n:umbCT of sa:iloTs.\nSELECT COUNT (*)\n\nFROM\nSailors S\nvVe can think of * as shorthand for all the columns (in the cross-product of the\nfrom-list in the FROM clause). Contrast this query with the following query,\nwhich computes the number of distinct sailor names. (Remember that ,'mame\nis not a key!)\n(Q29) Count the nmnber of d'i.fferent sailor names.\nSELECT COUNT ( DISTINCT S.sname )\nFROM\nSailors S\nOn instance 83, the answer to Q28 is 10, whereas the answer to Q29 is 9\n(because two sailors have the same name, Horatio). If DISTINCT is omitted,\nthe answer to Q29 is 10, because the name Horatio is counted twice. If COUNT\ndoes not include DISTINCT, then COUNT (*) gives the same answer as COUNT (x) ,\nwhere x is any set of attributes.\nIn our example, without DISTINCT Q29 is\nequivalent to Q28.\nHowever, the use of COUNT (*) is better querying style,\nsince it is immediately clear that all records contribute to the total count.\nAggregate operations offer an alternative to the ANY and ALL constructs. For\nexample, consider the following query:\n(Q30) Find the names of sailors who are older than the oldest sailor with a\nrating of 10.\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.age > ( SELECT MAX ( S2.age )\nFROM\nSailors S2\nWHERE\nS2.rating = 10 )\nOn instance 83, the oldest sailor with rating 10 is sailor 58, whose age is\n~j5.\nThe names of older sailors are Bob, Dustin, Horatio, and Lubber. Using ALL,\nthis query could alternatively be written as follows:\nSELECT S.sname\nFROM\nSailors S\nWHERE\nS.age > ALL ( SELECT\nFROM\nWHERE\nS2.age\nSailors S2\nS2.rating = 10 )\nHowever, the ALL query is more error proncone could easily (and incorrectly!)\nuse ANY instead of ALL, and retrieve sailors who are older than some sailor with\n\n154\nCHAPTEFt,~5\nRelationa~ Algebra and SQL: ~~~~:egation is a fUIl~~~:·mental operati(~:l-'-l\nthat canIlot be expressed in relational algebra. Similarly, SQL'8 grouping\nI\nconstruct cannot be expressed in algebra.\nI\nL..-\n._.\n.....__\n.\nI\na rating of 10. The use of ANY intuitively corresponds to the use of MIN, instead\nof MAX, in the previous query.\n5.5.1\nThe GROUP BY and HAVING Clauses\nThus far, we have applied aggregate operations to all (qualifying) rows in a\nrelation.\nOften we want to apply aggregate operations to each of a number\nof groups of rows in a relation, where the number of groups depends on the\nrelation instance (i.e., is not known in advance).\nFor example, consider the\nfollowing query.\n(Q31) Find the age of the youngest sailor for each rating level.\nIf we know that ratings are integers in the range 1 to la, we could write 10\nqueries of the form:\nSELECT MIN (S.age)\nFROM\nSailors S\nWHERE\nS.rating = i\nwhere i\n=\n1,2, ... ,10. vVriting 10 such queries is tedious. More important,\nwe may not know what rating levels exist in advance.\nTo write such queries, we need a major extension to the basic SQL query\nform, namely, the GROUP BY clause.\nIn fact, the extension also includes an\noptional HAVING clause that can be used to specify qualificatioIls over groups\n(for example, we may be interested only in rating levels> 6. The general form\nof an SQL query with these extensions is:\nSELECT\n[ DISTINCT] select-list\nFROM\nfrom-list\nWHERE\n'qualification\nGROUP BY grouping-list\nHAVING\ngroup-qualification\nUsing the GROUP BY clause, we can write Q:n a.s follows:\nSELECT\nS.rating, MIN (S.age)\n\nS(JL: queries. Constraints. Triggers\nFROM\nSailors S\nGROUP BY S.rating\nLet us consider some important points concerning the new clauses:\nII\nThe select-list in the SELECT clause consists of (1) a list of column names\nand (2) a list of terms having the form aggop ( column-name) AS new-\nname. vVe already saw AS used to rename output columns. Columns that\nare the result of aggregate operators do not already have a column name,\nand therefore giving the column a name with AS is especially useful.\nEvery column that appears in (1) must also appear in grouping-list. The\nreason is that each row in the result of the query corresponds to one gmup,\nwhich is a collection of rows that agree on the values of columns in grouping-\nlist. In general, if a column appears in list (1), but not in grouping-list,\nthere can be multiple rows within a group that have different values in this\ncolumn, and it is not clear what value should be assigned to this column\nin an answer row.\nWe can sometimes use primary key information to verify that a column\nhas a unique value in all rows within each group.\nFor example, if the\ngrouping-list contains the primary key of a table in the from-list, every\ncolumn of that table has a unique value within each group. In SQL:1999,\nsuch columns are also allowed to appear in part (1) of the select-list.\nII\nThe expressions appearing in the group-qualification in the HAVING clause\nmust have a single value per group. The intuition is that the HAVING clause\ndetermines whether an answer row is to be generated for a given group.\nTo satisfy this requirement in SQL-92, a column appearing in the group-\nqualification must appear a'3 the argument to an aggregation operator, or\nit must also appear in grouping-list. In SQL:1999, two new set functions\nhave been introduced that allow us to check whether every or any row in a\ngroup satisfies a condition; this allows us to use conditions similar to those\nin a WHERE clause.\nIII\nIf GROUP BY is omitted, the entire table is regarded as a single group.\nvVe explain the semantics of such a query through an example.\n(QS2) Find the age of the youngest sa'ilor who is eligible to vote (i.e., is at least\n18 years old) for each rating level with at least h.uo such sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, MIN (S.age) AS minage\nSailors S\nS.age >= 18\nS.rating\nCOUNT (*) > 1\n\n156\nCHAPTERl,5\nvVe will evaluate this query on instance 83 of Sailors, reproduced in Figure 5.10\nfor convenience. The instance of Sailors on which this query is to be evaluated is\nshown in Figure 5.10. Extending the conceptual evaluation strategy presented\nin Section 5.2, we proceed as follows. The first step is to construct the cross-\nproduct of tables in the from-list. Because the only relation in the from-list\nin Query Q32 is Sailors, the result is just the instance shown in Figure 5.10.\n22\nDustin\n7\n45.0\n29\nBrutus\n1\n33.0\n31\nLubber\n8\n55.5\n32\nAndy\n8\n25.5\n58\nRusty\n10\n35.0\n64\nHoratio\n7\n35.0\n71\nZorba\n10\n16.0\n74\nHoratio\n9\n35.0\n85\nArt\n3\n25.5\n95\nBob\n3\n63.5\n96\nFrodo\n3\n25.5\nFigure 5.10\nInstance 53 of Sailors\nThe second step is to apply the qualification in the WHERE clause, S. age >= 18.\nThis step eliminates the row (71, zorba, 10, 16). The third step is to eliminate\nunwanted columns. Only columns mentioned in the SELECT clause, the GROUP\nBY clause, or the HAVING clause are necessary, which means we can eliminate\nsid and sname in our example. The result is shown in Figure 5.11.\nObserve\nthat there are two identical rows with rating 3 and age 25.5-SQL does not\neliminate duplicates except when required to do so by use of the DISTINCT\nkeyword! The number of copies of a row in the intermediate table of Figure\n5.11 is determined by the number of rows in the original table that had these\nvalues in the projected columns.\nThe fourth step is to sort the table according to the GROUP BY clause to identify\nthe groups. The result of this step is shown in Figure 5.12.\nThe fifth step ,-is to apply the group-qualification in the HAVING clause, that\nis, the condition COUNT (*) > 1. This step eliminates the groups with rating\nequal to 1, 9, and 10. Observe that the order in which the WHERE and GROUP\nBY clauses are considered is significant: If the WHERE clause were not consid-\nered first, the group with rating=10 would have met the group-qualification\nin the HAVING clause. The sixth step is to generate one answer row for each\nremaining group. The answer row corresponding to a group consists of a subset\n\nSqL: queries, Constraints, Triggers\n3\n25.5\n3\n25.5\n3\n63.5\n55.5\n25.5\n35.0\n35.0\nI 10\n~~tl?l\n(J;fJ6;\n11\nI 33.0\n.f'(Lf'tTbf}\niigge·····\n..\n. ....\n7\n45.0\n1\n33.0\n8\n55.5\n8\n25.5\n10\n35.0\n7\n35.0\n9\n35.0\n3\n25.5\n3\n63.5\n3\n25.5\nFigure 5.11\nAfter Evaluation Step 3\nFigure 5.12\nAfter Evaluation Step 4\nof the grouping columns, plus one or more columns generated by applying an\naggregation operator. In our example, each answer row has a rating column\nand a minage column, which is computed by applying MIN to the values in the\nage column of the corresponding group.\nThe result of this step is shown in\nFigure 5.13.\nI rating I minage I\n3\n25.5\n7\n35.0\n8\n25.5\nFigure 5.13\nFinal Result in Sample Evaluation\nIf the query contains DISTINCT in the SELECT clause, duplicates are eliminated\nin an additional, and final, step.\nSQL:1999 ha.s introduced two new set functions, EVERY and ANY. To illustrate\nthese functions, we can replace the HAVING clause in our example by\nHAVING\nCOUNT (*) > 1 AND EVERY ( S.age <= 60 )\nThe fifth step of the conceptual evaluation is the one affected by the change\nin the HAVING clause. Consider the result of the fourth step, shown in Figure\n5.12. The EVERY keyword requires that every row in a group must satisfy the\nattached condition to meet the group-qualification. The group for rat'ing 3 does\nmeet this criterion and is dropped; the result is shown in Figure 5.14.\n\n158\nCHAPTER. 5\nSQL:1999 Extensions: Two new set functions, EVERY and ANY, have\nbeen added. vVhen they are used in the HAVING clause, the basic intuition\nthat the clause specifies a condition to be satisfied by each group, taken as\na whole, remains unchanged. However, the condition can now involve tests\non individual tuples in the group, whereas it previously relied exclusively\non aggregate functions over the group of tuples.\nIt is worth contrasting the preceding query with the following query, in which\nthe condition on age is in the WHERE clause instead of the HAVING clause:\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, MIN (S.age) AS minage\nSailors S\nS.age >= 18 AND S.age <= 60\nS.rating\nCOUNT (*) > 1\nNow, the result after the third step of conceptual evaluation no longer contains\nthe row with age 63.5. Nonetheless, the group for rating 3 satisfies the condition\nCOUNT (*) > 1, since it still has two rows, and meets the group-qualification\napplied in the fifth step. The final result for this query is shown in Figure 5.15.\n3\n25.5\n7\n45.0\n8\n55.5\nI rating I minage I\n55.5\nrating I minage\n~\n1\n45\n.\n0\nFigure 5.14\nFinal Result of EVERY Query\nFigure 5.15\nResult of Alternative Query\n5.5.2\nMore Examples of Aggregate Queries\n(Q33) FOT each red boat; find the number of reservations for this boat.\nSELECT\nB.bid, COUNT (*) AS reservationcount\nFROM\nBoats B, Reserves R\nWHERE\nR.bid = B.bid AND B.color = 'red'\nGROUP BY B.bid\nOn instances B1 and R2, the answer to this query contains the two tuples (102,\n3) and (104, 2).\nObserve that this version of the preceding query is illegal:\n\nSCdL: (J'ueries, Constraints. IhggeT8\nSELECT\nB.bicl, COUNT (*) AS reservationcount\nFROM\nBoats B, Reserves R\nWHERE\nR.bid = B.bid\nGROUP BY B.bid\nHAVING\nB.color = 'red'\nlijj9\nEven though the gToup-qualification B.coloT = 'Ted'is single-valued per group,\nsince the grouping attribute bid is a key for Boats (and therefore determines\ncoloT) , SQL disallows this query.6 Only columns that appear in the GROUP BY\nclause can appear in the HAVING clause, unless they appear as arguments to\nan aggregate operator in the HAVING clause.\n(Q34) Find the avemge age of sailoTs fOT each mting level that has at least two\nsailoTs.\nSELECT\nFROM\nGROUP BY\nHAVING\nS.rating, AVG (S.age) AS avgage\nSailors S\nS.rating\nCOUNT (*) > 1\nAfter identifying groups based on mting, we retain only groups with at least\ntwo sailors. The answer to this query on instance 83 is shown in Figure 5.16.\nI mting I avgage I\n3\n44.5\n7\n40.0\n8\n40.5\n10\n25.5\nFigure 5.16\nQ34 Answer\nI mting I avgage I\n3\n45.5\n7\n40.0\n8\n40.5\n10\n35.0\nFigure 5.17\nQ35 Answer\nI··rating I av.qage]\n3\n45.5\n7\n40.0\n8\n40.5\nFigure 5.18\nQ:36 Answer\nThe following alternative formulation of Query Q34 illustrates that the HAVING\nclause can have a nested subquery, just like the WHERE clause. Note that we\ncan use S. mtiTLg inside the nested subquery in the HAVING clause because it\nhas a single value for the current group of sailors:\nSELECT\nFROM\nGROUP BY\nHAVING\nS.rating, AVG ( S.age ) AS avgage\nSailors S\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.Hl,ting )\n6This query can be ea..'iily rewritten to be legal in SQL: 1999 using EVERY in the HAVING clause.\n\n160\nCHAPTER .;5\n(Q35) Find the average age of sailors 'Who aTe of voting age\n(i.e.~ at least 18\nyear8 old) for each 'rating level that has at least two sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, AVG ( S.age ) AS avgage\nSailors S\nS. age >= 18\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.rating )\nIn this variant of Query Q34, we first remove tuples with age <= 18 and group\nthe remaining tuples by rating.\nFor each group, the subquery in the HAVING\nclause computes the number of tuples in Sailors (without applying the selection\nage <= 18) with the same rating value as the current group. If a group has\nless than two sailors, it is discarded.\nFor each remaining group, we output\nthe average age. The answer to this query on instance 53 is shown in Figure\n5.17. Note that the answer is very similar to the answer for Q34, with the only\ndifference being that for the group with rating 10, we now ignore the sailor\nwith age 16 while computing the average.\n(Q36) Find the average age oj sailors who aTe of voting age (i.e., at least 18\nyeaTs old) JOT each rating level that has at least two such sailors.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, AVG ( S.age ) AS avgage\nSailors S\nS. age> 18\nS.rating\n1 < ( SELECT COUNT (*)\nFROM\nSailors S2\nWHERE S.rating = S2.rating AND S2.age >= 18 )\nThis formulation of the query reflects its similarity to Q35. The answer to Q36\non instance 53 is shown in Figure 5.18. It differs from the answer to Q35 in\nthat there is no tuple for rating 10, since there is only one tuple with rating 10\nand age 2 18.\nQuery Q36 is actually very similar to Q32, as the following simpler formulation\nshows:\nSELECT\nFROM\nWHERE\nGROUP BY\nS.rating, AVG\nSailors S\nS. age> 18\nS.rating\n( S.age ) AS avgage\n\nSQL: QueTies, Constraints, Triggers\nHAVING\nCOUNT (*) > 1\nThis formulation of Q36 takes advantage of the fact that the WHERE clause is\napplied before grouping is done; thus, only sailors with age> 18 are left when\ngrouping is done. It is instructive to consider yet another way of writing this\nquery:\nSELECT\nFROM\nWHERE\nTemp.rating, Temp.avgage\n( SELECT\nS.rating, AVG ( S.age ) AS\nCOUNT (*) AS ratingcount\nFROM\nSailors S\nWHERE\nS. age> 18\nGROUP BY\nS.rating) AS Temp\nTemp.ratingcount > 1\navgage,\nThis alternative brings out several interesting points.\nFirst, the FROM clause\ncan also contain a nested subquery according to the SQL standard. 7 Second,\nthe HAVING clause is not needed at all. Any query with a HAVING clause can\nbe rewritten without one, but many queries are simpler to express with the\nHAVING clause.\nFinally, when a subquery appears in the FROM clause, using\nthe AS keyword to give it a name is necessary (since otherwise we could not\nexpress, for instance, the condition Temp. ratingcount > 1).\n(Q37) Find those ratings fOT which the average age of sailoTS is the m'inirnum\nover all ratings.\nWe use this query to illustrate that aggregate operations cannot be nested. One\nmight consider writing it as follows:\nSELECT\nFROM\nWHERE\nS.rating\nSailors S\nAVG (S.age) = ( SELECT\nMIN (AVG (S2.age))\nFROM\nSailors S2\nGROUP BY S2.rating )\nA little thought shows that this query will not work even if the expression MIN\n(AVG (S2.age)), which is illegal, were allowed. In the nested query, Sailors is\npartitioned int,o groups by rating, and the average age is computed for each\nrating value. for each group, applying MIN to this average age value for the\ngroup will return the same value! A correct version of this query follows. It\nessentially computes a temporary table containing the average age for each\nrating value and then finds the rating(s) for which this average age is the\nminimum.\n7Not all commercial database systems currently support nested queries in the FROM clause.\n\n162\nGHAPTER r5\nr-_ m\n.\nI\nThe Relational Model and SQL: Null values arc not part of the bask\nI\nrelational model.\nLike SQL's treatment of tables as multisets of tuples,\n~liS is a del~.~~~~r~...~~~..1~._t_h_e_l_)ru_s_,i_c_l_l1_o_d_e_1.\n----'\nSELECT\nFROM\nWHERE\nTemp.rating, Temp.avgage\n( SELECT\nS.rating, AVG (S.age) AS avgage,\nFROM\nSailors S\nGROUP BY S.rating) AS Temp\nTemp.avgage = ( SELECT MIN (Temp.avgage) FROM Temp)\nThe answer to this query on instance 53 is (10, 25.5).\nAs an exercise, consider whether the following query computes the same answer.\nSELECT\nFROM\nGROUP BY\nTemp.rating, MIN (Temp.avgage )\n( SELECT\nS.rating, AVG (S.age) AS\nFROM\nSailors S\nGROUP BY\nS.rating) AS Temp\nTemp.rating\navgage,\n5.6\nNULL VALUES\nThus far, we have assumed that column values in a row are always known. In\npractice column values can be unknown. For example, when a sailor, say Dan,\njoins a yacht club, he may not yet have a rating assigned. Since the definition\nfor the Sailors table has a rating column, what row should we insert for Dan?\n\\\\That is needed here is a special value that denotes unknown. Suppose the Sailor\ntable definition was modified to include a rnaiden-name column. However, only\nmarried women who take their husband's last name have a maiden name. For\nwomen who do not take their husband's name and for men, the nw'idcn-nmnc\ncolumn is inapphcable. Again, what value do we include in this column for the\nrow representing Dan?\nSQL provides H special column value called null to use in such situations. \"Ve\nuse null when the column value is either 'lJ,nknown or inapplicable. Using our\nSailor table definition, we might enter the row (98. Dan, null, 39) to represent\nDan. The presence of null values complicates rnany issues, and we consider the\nimpact of null values on SQL in this section.\n\nSQL: Q'lteT'leS, ConstT'aJnt.\"\nTrigger's\n5.6.1\nComparisons Using Null Values\nConsider a comparison such as rat'in,g = 8. If this is applied to the row for Dan,\nis this condition true or false'? Since Dan's rating is unknown, it is reasonable\nto say that this comparison should evaluate to the value unknown. In fact, this\nis the C::lse for the comparisons rating> 8 and rating < 8 &'3 well. Perhaps less\nobviously, if we compare two null values using <, >, =, and so on, the result is\nalways unknown. For example, if we have null in two distinct rows of the sailor\nrelation, any comparison returns unknown.\nSQL also provides a special comparison operator IS NULL to test whether a\ncolumn value is null; for example, we can say rating IS NULL, which would\nevaluate to true on the row representing Dan. We can also say rat'ing IS NOT\nNULL, which would evaluate to false on the row for Dan.\n5.6.2\nLogical Connectives AND, OR, and NOT\nNow, what about boolean expressions such as mting = 8 OR age < 40 and\nmting = 8 AND age < 40?\nConsidering the row for Dan again, because age\n< 40, the first expression evaluates to true regardless of the value of rating, but\nwhat about the second? We can only say unknown.\nBut this example raises an important\npoint~once we have null values, we\nmust define the logical operators AND, OR, and NOT using a three-val1LCd logic in\nwhich expressions evaluate to true, false, or unknown. We extend the usu1'11\ninterpretations of AND, OR, and NOT to cover the case when one of the arguments\nis unknown &., follows. The expression NOT unknown is defined to be unknown.\nOR of two arguments evaluates to true if either argument evaluates to true,\nand to unknown if one argument evaluates to false and the other evaluates to\nunknown. (If both arguments are false, of course, OR evaluates to false.) AND\nof two arguments evaluates to false if either argument evaluates to false, and\nto unknown if one argument evaluates to unknown and the other evaluates to\ntrue or unknown. (If both arguments are true, AND evaluates to true.)\n5.6.3\nImpact on SQL Constructs\nBoolean expressions arise in many contexts in SQI\", and the impact of nv,ll\nvalues must be recognized. H)r example, the qualification in the WHERE clause\neliminates rows (in the cross-product of tables named in the FROM clause) for\nwhich the qualification does not evaluate to true. Therefore, in the presence\nof null values, any row that evaluates to false or unknown is eliminated. Elim-\ninating rows that evaluate to unknown h&') a subtle but signifieant impaet on\nqueries, especially nested queries involving EXISTS or UNIQUE.\n\n164\nCHAPTER~5\nAnother issue in the presence of 'null values is the definition of when two rows\nin a relation instance are regarded a.'3 duplicates. The SQL definition is that two\nrows are duplicates if corresponding columns are either equal, or both contain\nTrull. Contra..9t this definition with the fact that if we compare two null values\nusing =, the result is unknown! In the context of duplicates, this comparison is\nimplicitly treated as true, which is an anomaly.\nAs expected, the arithmetic operations +, -, *, and / all return Tmll if one of\ntheir arguments is null. However, nulls can cause some unexpected behavior\nwith aggregate operations. COUNT(*) handles 'null values just like other values;\nthat is, they get counted. All the other aggregate operations (COUNT, SUM, AVG,\nMIN, MAX, and variations using DISTINCT) simply discard null values--thus SUM\ncannot be understood as just the addition of all values in the (multi)set of\nvalues that it is applied to; a preliminary step of discarding all null values must\nalso be accounted for. As a special case, if one of these operators-other than\nCOUNT-is applied to only null values, the result is again null.\n5.6.4\nOuter Joins\nSome interesting variants of the join operation that rely on null values, called\nouter joins, are supported in SQL. Consider the join of two tables, say Sailors\nMe Reserves. Tuples of Sailors that do not match some row in Reserves accord-\ning to the join condition c do not appear in the result. In an outer join, on\nthe other hanel, Sailor rows without a matching Reserves row appear exactly\nonce in the result, with the result columns inherited from Reserves assigned\nnull values.\nIn fact, there are several variants of the outer join idea. In a left outer join,\nSailor rows without a matching Reserves row appear in the result, but not vice\nversa. In a right outer join, Reserves rows without a matching Sailors row\nappear in the result, but not vice versa.\nIn a full outer join, both Sailors\nand Reserves rows without a match appear in the result. (Of course, rows with\na match always appear in the result, for all these variants, just like the usual\njoins, sometimes called inner joins, presented in Chapter 4.)\nSQL allows the desired type of join to be specified in the FROM clause.\nFor\nexample, the following query lists (sid, b'id) pairs corresponding to sailors and\nboats they ha~e reserved:\nSELECT S.sid, R.bid\nFROM\nSailors S NATURAL LEFT OUTER JOIN Reserves R\nThe NATURAL keyword specifies that the join condition is equality on all common\nattributes (in this example, sid), and the WHERE clause is not required (unless\n\nHj5\nwe want to specify additional, non-join conditions). On the instances of Sailors\nand Reserves shown in Figure 5.6, this query computes the result shown in\nFigure 5.19.\nI sid I bid I\n22\n101\n31\nnull\n58\n103\nFigure 5.19\nLeft Outer Join of Sailo7\"1 and Rese1<Jesl\n5.6.5\nDisallowing Null Values\nWe can disallow null values by specifying NOT NULL as part of the field def-\ninition; for example, sname CHAR(20)\nNOT NULL. In addition, the fields in a\nprimary key are not allowed to take on null values. Thus, there is an implicit\nNOT NULL constraint for every field listed in a PRIMARY KEY constraint.\nOur coverage of null values is far from complete. The interested reader should\nconsult one of the many books devoted to SQL for a more detailed treatment\nof the topic.\n5.7\nCOMPLEX INTEGRITY CONSTRAINTS IN SQL\nIn this section we discuss the specification of complex integrity constraints that\nutilize the full power of SQL queries.\nThe features discussed in this section\ncomplement the integrity constraint features of SQL presented in Chapter 3.\n5.7.1\nConstraints over a Single Table\nWe can specify complex constraints over a single table using table constraints,\nwhich have the form CHECK conditional-expression. For example, to ensure that\nrating must be an integer in the range 1 to 10, we could use:\nCREATE TABLE Sailors ( sid\nINTEGER,\nsname CHAR(10),\nrating\nINTEGER,\nage\nREAL,\nPRIMARY KEY (sid),\nCHECK (rating >= 1 AND rating <= 10 ))\n\n166\nCHAPTER*5\nTo enforce the constraint that Interlake boats cannot be reserved, we could use:\nCREATE TABLE Reserves (sid\nINTEGER,\nbid\nINTEGER,\nday\nDATE,\nFOREIGN KEY (sid) REFERENCES Sailors\nFOREIGN KEY (bid) REFERENCES Boats\nCONSTRAINT noInterlakeRes\nCHECK ( 'Interlake' <>\n( SELECT B.bname\nFROM\nBoats B\nWHERE\nB.bid = Reserves.bid )))\nWhen a row is inserted into Reserves or an existing row is modified, the condi-\ntional expression in the CHECK constraint is evaluated. If it evaluates to false,\nthe command is rejected.\n5.7.2\nDomain Constraints and Distinct Types\nA user can define a new domain using the CREATE DOMAIN statement, which\nuses CHECK constraints.\nCREATE DOMAIN ratingval INTEGER DEFAULT 1\nCHECK ( VALUE >= 1 AND VALUE <= 10 )\nINTEGER is the underlying, or source, type for the domain ratingval, and\nevery ratingval value must be of this type. Values in ratingval are further\nrestricted by using a CHECK constraint; in defining this constraint, we use the\nkeyword VALUE to refer to a value in the domain.\nBy using this facility, we\ncan constrain the values that belong to a domain using the full power of SQL\nqueries.\nOnce a domain is defined, the name of the domain can be used to\nrestrict column values in a table; we can use the following line in a schema\ndeclaration, for example:\nrating\nratingval\nThe optional DEFAULT keyword is used to associate a default value with a do-\nmain.\nIf the domain ratingval is used for a column in some relation and\nno value is entered for this column in an inserted tuple, the default value 1\nassociated with ratingval is used.\nSQL's support for the concept of a domain is limited in an important respect.\nFor example, we can define two domains called SailorId and BoatId, each\n\niii?\nSQL:1999 Distinct Types: :Many systems, e.g., Informix UDS and IBM\nDB2, already support this feature. With its introduction, we expect that\nthe support for domains will be deprecated, and eventually eliminated, in\nfuture versions of the SqL standard. It is really just one part of a broad\nset of object-oriented features in SQL:1999, which we discuss in Chapter\n23.\nusing INTEGER as the underlying type. The intent is to force a comparison of a\nSailorId value with a BoatId value to always fail (since they are drawn from\ndifferent domains); however, since they both have the same base type, INTEGER,\nthe comparison will succeed in SqL. This problem is addressed through the\nintroduction of distinct types in SqL:1999:\nCREATE TYPE ratingtype AS INTEGER\nThis statement defines a new distinct type called ratingtype, with INTEGER\nas its source type.\nValues of type ratingtype can be compared with each\nother, but they cannot be compared with values of other types. In particular,\nratingtype values are treated as being distinct from values of the source type,\nINTEGER--····we cannot compare them to integers or combine them with integers\n(e.g., add an integer to a ratingtype value). If we want to define operations\non the new type, for example, an average function, we must do so explicitly;\nnone of the existing operations on the source type carryover. We discuss how\nsuch functions can be defined in Section 23.4.1.\n5.7.3\nAssertions: ICs over Several Tables\nTable constraints are associated with a single table, although the conditional\nexpression in the CHECK clause can refer to other tables.\nTable constraints\nare required to hold only if the a,ssociated table is nonempty.\nThus, when\na constraint involves two or more tables, the table constraint mechanism is\nsometimes cumbersome and not quite what is desired. To cover such situations,\nSqL supports the creation of assertions, which are constraints not associated\nwith anyone table.\nAs an example, suppose that we wish to enforce the constraint that the number\nof boats plus the number of sailors should be less than 100.\n(This condition\nIllight be required, say, to qualify as a 'smaIl' sailing club.) We could try the\nfollowing table constraint:\nCREATE TABLE Sailors ( sid\nINTEGER,\nsname CHAR ( 10) ,\n\n168\nCHAPTER$5\nrating\nINTEGER,\nage\nREAL,\nPRIMARY KEY (sid),\nCHECK ( rating >= 1 AND rating <= 10)\nCHECK ( ( SELECT COUNT (S.sid) FROM Sailors S )\n+ ( SELECT COUNT (B.bid) FROM Boats B )\n< 100 ))\nThis solution suffers from two drawbacks.\nIt is associated with Sailors, al-\nthough it involves Boats in a completely symmetric way.\nMore important,\nif the Sailors table is empty, this constraint is defined (as per the semantics\nof table constraints) to always hold, even if we have more than 100 rows in\nBoats!\nvVe could extend this constraint specification to check that Sailors is\nnonempty, but this approach becomes cumbersome.\nThe best solution is to\ncreate an assertion, as follows:\nCREATE ASSERTION smallClub\nCHECK (( SELECT COUNT (S.sid) FROM Sailors S )\n+ ( SELECT COUNT (B.bid) FROM Boats B)\n< 100 )\n5.8\nTRIGGERS AND ACTIVE DATABASES\nA trigger is a procedure that is automatically invoked by the DBMS in re-\nsponse to specified changes to the database, and is typically specified by the\nDBA. A database that has a set of associated triggers is called an active\ndatabase. A trigger description contains three parts:\n•\nEvent: A change to the database that activates the trigger.\n..\nCondition: A query or test that is run when the trigger is activated.\n..\nAction: A procedure that is executed when the trigger is activated and\nits condition is true.\nA trigger can be thought of as a 'daemon' that monitors a databa.se, and is exe-\ncuted when the database is modified in a way that matches the event specifica-\ntion. An insert, delete, or update statement could activate a trigger, regardless\nof which user or application invoked the activating statement; users may not\neven be aware that a trigger wa.'3 executed as a side effect of their program.\nA condition in a trigger can be a true/false statement (e.g., all employee salaries\nare less than $100,000) or a query. A query is interpreted as true if the answer\n\nSQL: Que'ries, Constrairds, TriggeTs\nIt>9\nset is nonempty and false if the query ha.') no answers. If the condition part\nevaluates to true, the action a.,sociated with the trigger is executed.\nA trigger action can examine the answers to\nth(~ query in the condition part\nof the trigger, refer to old and new values of tuples modified by the statement\nactivating the trigger, execute Hew queries, and make changes to the database.\nIn fact, an action can even execute a series of data-definition commands (e.g.,\ncreate new tables, change authorizations) and transaction-oriented commands\n(e.g., commit) or call host-language procedures.\nAn important issue is when the action part of a trigger executes in relation to\nthe statement that activated the trigger. For example, a statement that inserts\nrecords into the Students table may activate a trigger that is used to maintain\nstatistics on how many studen~s younger than 18 are inserted at a time by a\ntypical insert statement. Depending on exactly what the trigger does, we may\nwant its action to execute before changes are made to the Students table or\nafterwards: A trigger that initializes a variable used to count the nurnber of\nqualifying insertions should be executed before, and a trigger that executes once\nper qualifying inserted record and increments the variable should be executed\nafter each record is inserted (because we may want to examine the values in\nthe new record to determine the action).\n5.8.1\nExamples of Triggers in SQL\nThe examples shown in Figure 5.20, written using Oracle Server syntax for\ndefining triggers, illustrate the basic concepts behind triggers. (The SQL:1999\nsyntax for these triggers is similar; we will see an example using SQL:1999\nsyntax shortly.) The trigger called iniLcount initializes a counter variable be-\nfore every execution of an INSERT statement that adds tuples to the Students\nrelation. The trigger called incr_count increments the counter for each inserted\ntuple that satisfies the condition age < 18.\nOne of the example triggers in Figure 5.20 executes before the aetivating state-\nment, and the other example executes after it. A trigger can also be scheduled\nto execute instead of the activating statement; or in deferred fashion, at the\nend of the transaction containing the activating statement; or in asynchronous\nfashion, as part of a separate transaction.\nThe example in Figure 5.20 illustrates another point about trigger execution:\nA user must be able to specify whether a trigger is to be executed once per\nmodified record or once per activating statement. If the action depends on in-\ndividual changed records, for example, we have to examine the age field of the\ninserted Students record to decide whether to increment the count, the trigger-\n\n170\nCHAPTER 5\nCREATE TRIGGER iniLeount BEFORE INSERT ON Students\n1* Event *1\nDECLARE\ncount INTEGER:\nBEGIN\n1* Action *I\ncount := 0:\nEND\nCREATE TRIGGER incLcount AFTER INSERT ON Students\n1* Event *1\nWHEN (new.age < 18)\n1* Condition; 'new' is just-inserted tuple *1\nFOR EACH ROW\nBEGIN\n1* Action; a procedure in Oracle's PL/SQL syntax *1\ncount := count + 1;\nEND\nFigure 5.20\nExamples Illustrating Triggers\ning event should be defined to occur for each modified record; the FOR EACH\nROW clause is used to do this. Such a trigger is called a row-level trigger. On\nthe other hand, the iniLcount trigger is executed just once per INSERT state-\nment, regardless of the number of records inserted, because we have omitted\nthe FOR EACH ROW phrase. Such a trigger is called a statement-level trigger.\nIn Figure 5.20, the keyword new refers to the newly inserted tuple. If an existing\ntuple were modified, the keywords old and new could be used to refer to the\nvalues before and after the modification. SQL:1999 also allows the action part\nof a trigger to refer to the set of changed records, rather than just one changed\nrecord at a time. For example, it would be useful to be able to refer to the set\nof inserted Students records in a trigger that executes once after the INSERT\nstatement; we could count the number of inserted records with age < 18 through\nan SQL query over this set. Such a trigger is shown in Figure 5.21 and is an\naJternative to the triggers shown in Figure 5.20.\nThe definition in Figure 5.21 uses the syntax of SQL:1999, in order to illustrate\nthe similarities and differences with respect to the syntax used in a typical\ncurrent DBMS. The keyword clause NEW TABLE enables us to give a table name\n(InsertedTuples) to the set of newly inserted tuples. The FOR EACH STATEMENT\nclause specifies a statement-level trigger and can be omitted because it is the\ndefault. This definition does not have a WHEN clause; if such a clause is included,\nit follows the FOR EACH STATEMENT clause, just before the action specification.\nThe trigger is evaluated once for each SQL statement that inserts tuples into\nStudents, and inserts a single tuple into a table that contains statistics on mod-\n\nS(JL: (JneTie,s, Constraints, Triggers\n171\nifications to database tables. The first two fields of the tuple contain constants\n(identifying the modified table, Students, and the kind of modifying statement,\nan INSERT), and the third field is the number of inserted Students tuples with\nage < 18. (The trigger in Figure 5.20 only computes the count; an additional\ntrigger is required to insert the appropriate tuple into the statistics table.)\nCREATE TRIGGER seLcount AFTER INSERT ON Students\nj* Event *j\nREFERENCING NEW TABLE AS InsertedTuples\nFOR EACH STATEMENT\nINSERT\nj* Action *j\nINTO StatisticsTable(ModifiedTable, ModificationType, Count)\nSELECT 'Students', 'Insert', COUNT *\nFROM InsertedTuples I\nWHERE 1.age < 18\nFigure 5.21\nSet-Oriented Trigger\n5.9\nDESIGNING ACTIVE DATABASES\nTriggers offer a powerful mechanism for dealing with changes to a database,\nbut they must be used with caution. The effect of a collection of triggers can\nbe very complex, and maintaining an active database can become very difficult.\nOften, a judicious use of integrity constraints can replace the use of triggers.\n5.9.1\nWhy Triggers Can Be Hard to Understand\nIn an active database system, when the DBMS is about to execute a statement\nthat modifies the databa.se, it checks whether some trigger is activated by the\nstatement. If so, the DBMS processes the trigger by evaluating its condition\npart, and then (if the condition evaluates to true) executing its action part.\nIf a statement activates more than one trigger, the DBMS typically processes\nall of them, in senne arbitrary order. An important point is that the execution\nof the action part of a trigger could in turn activate another trigger. In par-\nticular, the execution of the action part of a trigger could a,gain activate the\nsarne trigger; such triggers \"u'e called recursive triggers. The potential for\nsuch chain activations and the unpredictable order in which a DBMS processes\nactivated triggers can make it difficult to understand the effect of a collection\nof triggers.\n\n172\nCHAPTER'5\n5.9.2\nConstraints versus Triggers\nA common use of triggers is to maintain databa..'3e consistency, and in such\ncases, we should always consider whether using an integrity constraint (e.g., a\nforeign key constraint) achieves the same goals. The meaning of a constraint is\nnot defined operationally, unlike the effect of a trigger. This property makes a\nconstraint easier to understand, and also gives the DBMS more opportunities\nto optimize execution. A constraint also prevents the data from being made\ninconsistent by any kind of statement, whereas a trigger is activated by a specific\nkind of statement (INSERT, DELETE, or UPDATE). Again, this restriction makes\na constraint easier to understand.\nOn the other hand, triggers allow us to maintain database integrity in more\nflexible ways, as the following examples illustrate.\n•\nSuppose that we have a table called Orders with fields iternid, quantity,\ncustornerid, and unitprice.\nWhen a customer places an order, the first\nthree field values are filled in by the user (in this example, a sales clerk).\nThe fourth field's value can be obtained from a table called Items, but it\nis important to include it in the Orders table to have a complete record of\nthe order, in case the price of the item is subsequently changed. We can\ndefine a trigger to look up this value and include it in the fourth field of\na newly inserted record. In addition to reducing the number of fields that\nthe clerk h&'3 to type in, this trigger eliminates the possibility of an entry\nerror leading to an inconsistent price in the Orders table.\n•\nContinuing with this example, we may want to perform some additional\nactions when an order is received.\nFor example, if the purchase is being\ncharged to a credit line issued by the company, we may want to check\nwhether the total cost of the purch&'3e is within the current credit limit.\nWe can use a trigger to do the check; indeed, we can even use a CHECK\nconstraint. Using a trigger, however, allows us to implement more sophis-\nticated policies for dealing with purchases that exceed a credit limit. For\ninstance, we may allow purchases that exceed the limit by no more than\n10% if the customer has dealt with the company for at least a year, and\nadd the customer to a table of candidates for credit limit increases.\n5.9.3\nOther Uses of Triggers\n.l\\'Iany potential uses of triggers go beyond integrity maintenance. Triggers can\nalert users to unusual events (&'3 reflected in updates to the databa..<;e).\nFor\nexample, we may want to check whether a customer placing an order h&s made\nenough purchases in the past month to qualify for an additional discount; if\nso, the sales clerk must be informed so that he (or she) can tell the customer\n\nSQL: Q'UeT'Ze,S, CO'l/stmints, Tr'iggers\nand possibly generate additional sales! \\Ve can rela;y this information by using\na trigger that checks recent purcha.ses and prints a message if the customer\nqualifies for the discount.\nTriggers can generate a log of events to support auditing and security checks.\nFor example, each time a customer places an order, we can create a record with\nthe customer's ID and current credit limit and insert this record in a customer\nhistory table.\nSubsequent analysis of this table might suggest candidates for\nan increased credit limit (e.g., customers who have never failed to pay a bill on\ntime and who have come within 10% of their credit limit at least three times\nin the last month).\nAs the examples in Section 5.8 illustrate, we can use triggers to gather statistics\non table accesses and modifications. Some database systems even use triggers\ninternally as the basis for managing replicas of relations (Section 22.11.1). Our\nlist of potential uses of triggers is not exhaustive; for example, triggers have\nalso been considered for workflow management and enforcing business rules.\n5.10\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhat are the parts of a basic SQL query? Are the input and result tables\nof an SQL query sets or multisets? How can you obtain a set of tuples as\nthe result of a query? (Section 5.2)\n•\nWhat are range variables in SQL? How can you give names to output\ncolumns in a query that are defined by arithmetic or string expressions?\nWhat support does SQL offer for string pattern matching? (Section 5.2)\n•\nWhat operations does SQL provide over (multi)sets of tuples, and how\nwould you use these in writing queries? (Section 5.3)\n•\nvVhat are nested queries?\nWhat is correlation in nested queries?\nHow\nwould you use the operators IN, EXISTS, UNIQUE, ANY, and ALL in writing\nnested queries? Why are they useful? Illustrate your answer by showing\nhow to write the division operator in SQL. (Section 5.4)\n•\n\\Vhat aggregate operators does SQL support? (Section 5.5)\n•\n\\i\\That is gmvping? Is there a counterpart in relational algebra? Explain\nthis feature, and discllss the interaction of the HAVING and WHERE clauses.\nMention any restrictions that mllst be satisfied by the fields that appear in\nthe GROUP BY clause. (Section 5.5.1)\n\n174\nCHAPTER?5\n•\n\\Vhat are null values?\nAre they supported in the relational model,\n&'3\ndescribed in Chapter 3'1 Hc)\\v do they affect the meaning of queries? Can\nprimary key fields of a table contain null values? (Section 5.6)\n•\nvVhat types of SQL constraints can be specified using the query language?\nCan you express primary key constraints using one of these new kinds\nof constraints? If so, why does SQL provide for a separate primary key\nconstraint syntax? (Section 5.7)\n•\nWhat is a trigger, and what are its three parts? vVhat are the differences\nbetween row-level and statement-level triggers? (Section 5.8)\n•\n\\Vhy can triggers be hard to understand? Explain the differences between\ntriggers and integrity constraints, and describe when you would use trig-\ngers over integrity constrains and vice versa. What are triggers used for?\n(Section 5.9)\nEXERCISES\nOnline material is available for all exercises in this chapter on the book's webpage at\nhttp://www.cs.wisc.edu/-dbbOok\nThis includes scripts to create tables for each exercise for use with Oracle, IBM DB2, Microsoft\nSQL Server, and MySQL.\nExercise 5.1 Consider the following relations:\nStudent(snum: integer, sname: string, major: string, level: string, age: integer)\nClass( name: string, meets_at: time, room: string, fid: integer)\nEnrolled(snum: integer, cname: string)\nFaculty (fid: integer, fnarne: string, deptid: integer)\nThe meaning of these relations is straightforward; for example, Enrolled has one record per\nstudent-class pair such that the student is enrolled in the class.\nWrite the following queries in SQL. No duplicates should be printed in any of the ans\\vers.\n1. Find the nari1es of all Juniors (level = JR) who are enrolled in a class taught by 1. Teach.\n2. Find the age of the oldest student who is either a History major or enrolled in a course\ntaught by I. Teach.\n:3. Find the names of all classes that either meet in room R128 or have five or more students\nenrolled.\n4. Find the Ilames of all students who are enrolled in two classes that meet at the same\ntime.\n\nSCJL: Queries, ConstrainLsl Triggers\n175t\n5. Find the names of faculty members \\vho teach in every room in which some class is\ntaught.\n6. Find the names of faculty members for \\vhorn the combined enrollment of the courses\nthat they teach is less than five.\n7. Print the level and the average age of students for that level, for each level.\n8. Print the level and the average age of students for that level, for all levels except JR.\n9. For each faculty member that has taught classes only in room R128, print the faculty\nmember's name and the total number of classes she or he has taught.\n10. Find the names of students enrolled in the maximum number of classes.\n11. Find the names of students not enrolled in any class.\n12. For each age value that appears in Students, find the level value that appears most often.\nFor example, if there are more FR level students aged 18 than SR, JR, or SO students\naged 18, you should print the pair (18, FR).\nExercise 5.2 Consider the following schema:\nSuppliers( sid: integer, sname: string, address: string)\nParts(pid: integer, pname: string, color: string)\nCatalog( sid: integer, pid: integer, cost: real)\nThe Catalog relation lists the prices charged for parts by Suppliers.\nWrite the following\nqueries in SQL:\n1. Find the pnames of parts for which there is some supplier.\n2. Find the snames of suppliers who supply every part.\n3. Find the snames of suppliers who supply every red part.\n4. Find the pnamcs of parts supplied by Acme Widget Suppliers and no one else.\n5. Find the sids of suppliers who charge more for some part than the average cost of that\npart (averaged over all the suppliers who supply that part).\n6. For each part, find the sname of the supplier who charges the most for that part.\n7. Find the sids of suppliers who supply only red parts.\n8. Find the sids of suppliers who supply a red part anel a green part.\n9. Find the sirl'i of suppliers who supply a red part or a green part.\n10. For every supplier that only supplies green parts, print the name of the supplier and the\ntotal number of parts that she supplies.\n11. For every supplier that supplies a green part and a reel part, print the name and price\nof the most expensive part that she supplies.\nExercise 5.3 The following relations keep track of airline flight information:\nFlights(.flno: integer, from: string, to: string, di8tance: integer,\nrlepa7'i:s: time,\na'T'l~ivcs: time,\nTn~ice: integer)\nAircraft( aid: integer, aname: string, cTllisingT'ange: integer)\nCertified( eid: integer, aid: integer)\nEmployees( eid: integer I ename: string, salary: integer)\n\n176\nCHAPTE&5\nNote that the Employees relation describes pilots and other kinds of employees as well; every\npilot is certified for some aircraft, and only pilots are certified to fly.\nWrite each of the\nfollO\\ving queries in SQL. (Additional queries using the same schema are listed in the exereises\nfoT' Chapter 4·)\n1. Find the names of aircraft such that all pilots certified to operate them earn more than\n$80,000.\n2. For each pilot who is certified for more than three aircraft, find the eid and the maximum\ncruisingmnge of the aircraft for which she or he is certified.\n3. Find the names of pilots whose salary is less than the price of the cheapest route from\nLos Angeles to Honolulu.\n4. For all aircraft with cmisingmnge over 1000 miles, find the name of the aircraft and the\naverage salary of all pilots certified for this aircraft.\n5. Find the names of pilots certified for some Boeing aircraft.\n6. Find the aids of all aircraft that can be used on routes from Los Angeles to Chicago.\n7. Identify the routes that can be piloted by every pilot who makes more than $100,000.\n8. Print the enames of pilots who can operate planes with cruisingmnge greater than 3000\nmiles but are not certified on any Boeing aircraft.\n9. A customer wants to travel from Madison to New York with no more than two changes\nof flight. List the choice of departure times from Madison if the customer wants to arrive\nin New York by 6 p.m.\n10. Compute the difference between the average salary of a pilot and the average salary of\nall employees (including pilots).\n11. Print the name and salary of every nonpilot whose salary is more than the average salary\nfor pilots.\n12. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles.\n13. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles, but on at least two such aircrafts.\n14. Print the names of employees who are certified only on aircrafts with cruising range\nlonger than 1000 miles and who are certified on some Boeing aircraft.\nExercise 5.4 Consider the following relational schema. An employee can work in more than\none department; the pcLtime field of the Works relation shows the percentage of time that a\ngiven employee works in a given department.\nEmp(eid: integer, ename: string, age: integer, salary: real)\nWorks(eid: integer, did: integer, pet_time: integer)\nDept(did.· integer, budget: real, managerid: integer)\nWrite the following queries in SQL:\n1. Print the names and ages of each employee who works in both the Hardware department\nand the Software department.\n2. For each department with more than 20 full-time-equivalent employees (i.e., where the\npart~time and full-time employees add up to at least that many full-time employees),\nprint the did together with the number of employees that work in that department.\n\nSQL: quehes, ConstTaint.s, Triggers\n117\n,\n,\n18\njones\n3\n30.0\n~-\n- jonah\n6\n56.0\n41\n22\nahab\n7\n44.0\n63\nmoby\n'mdl\n15.0\nI s'id I sname I mting , age I\nFigure 5.22\nAn Instance of Sailors\n3. Print the name of each employee whose salary exceeds the budget of all of the depart-\nments that he or she works in.\n4. Find the managerids of managers who manage only departments with budgets greater\nthan $1 million.\n5. Find the enames of managers who manage the departments with the largest budgets.\n6. If a manager manages more than one department, he or she controls the sum of all the\nbudgets for those departments. Find the managerids of managers who control more than\n$5 million.\n7. Find the managerids of managers who control the largest amounts.\n8. Find the enames of managers who manage only departments with budgets larger than\n$1 million, but at least one department with budget less than $5 million.\nExercise 5.5 Consider the instance of the Sailors relation shown in Figure 5.22.\n1. Write SQL queries to compute the average rating, using AVGj the sum of the ratings,\nusing SUM; and the number of ratings, using COUNT.\n2. If you divide the sum just computed by the count, would the result be the same as the\naverage? How would your answer change if these steps were carried out with respect to\nthe age field instead of mting?\n~3. Consider the following query:\nFind the names of sailors with a higher rating than all\nsailors with age < 21.\nThe following two SQL queries attempt to obtain the answer\nto this question.\nDo they both compute the result? If not, explain why. Under what\nconditions would they compute the same result?\nS2.rating\nSailors S2\nS2.age < 21\nSELECT *\nFROM\nSailors S\nWHERE\nS.rating > ANY (SELECT\nFROM\n\\-/HERE\nSELECT S.sname\nFROM\nSailors S\nWHERE\nNOT EXISTS ( SELECT *\nFROM\nSailors S2\nWHERE\nS2.age < 21\nAND S.rating <= S2.rating )\n4. Consider the instance of Sailors shown in Figure 5.22. Let us define instance Sl of Sailors\nto consist of the first two tuples, instance S2 to be the last two tuples, and S to be the\ngiven instance.\n\n178\nCHAPTER'5\nShow the left outer join of S with itself, with the join condition being 8'id=sid.\n(b) Show the right outer join of S ,vith itself, with the join condition being s'id=sid.\n(c) Show the full outer join of S with itself, with the join condition being S'id=sid.\n(d) Show the left outer join of Sl with S2, with the join condition being sid=sid.\n(e) Show the right outer join of Sl with S2, with the join condition being sid=sid.\n(f) Show the full outer join of 81 with S2, with the join condition being sid=sid.\nExercise 5.6 Answer the following questions:\n1. Explain the term 'impedance mismatch in the context of embedding SQL commands in a\nhost language such as C.\n2. How can the value of a host language variable be passed to an embedded SQL command?\n3. Explain the WHENEVER command's use in error and exception handling.\n4. Explain the need for cursors.\n5. Give an example of a situation that calls for the use of embedded SQL; that is, interactive\nuse of SQL commands is not enough, and some host lang;uage capabilities are needed.\n6. Write a C program with embedded SQL commands to address your example in the\nprevious answer.\n7. Write a C program with embedded SQL commands to find the standard deviation of\nsailors' ages.\n8. Extend the previous program to find all sailors whose age is within one standard deviation\nof the average age of all sailors.\n9. Explain how you would write a C program to compute the transitive closure of a graph,\nrepresented as an 8QL relation Edges(jrom, to), using embedded SQL commands. (You\nneed not write the program, just explain the main points to be dealt with.)\n10. Explain the following terms with respect to cursors: 'tlpdatability, sens,itivity, and scml-\nlability.\n11. Define a cursor on the Sailors relation that is updatable, scrollable, and returns answers\nsorted by age. Which fields of Sailors can such a cursor not update? Why?\n12. Give an example of a situation that calls for dynamic 8QL; that is, even embedded SQL\nis not sufficient.\nExercise 5.7 Consider the following relational schema and briefly answer the questions that\nfollow:\nEmp( eid: integer, cname: string, age: integer, salary: real)\n\\Vorks( eid: integer, did: integer, pet-time: integer)\nDept( did.' integer, budget:\nre~l, managerid: integer)\n1. Define a table constraint on Emp that will ensure that ever)' employee makes at leELst\n$10,000.\n2. Define a table constraint on Dept that will ensure that all managers have age> ;'W.\n:3. Define an assertion on Dept that will ensure that all managers have age> 30. Compare\nthis assertion with the equivalent table constraint. Explain which is better.\n\nSQL: (JwTies, Const7nint.s, Triggers\n119\n4. vVrite SQL statements to delete all information about employees whose salaries exceed\nthat of the manager of one or more departments that they work in. Be sure to ensure\nthat all the relevant integrity constraints are satisfied after your updates.\nExercise 5.8 Consider the following relations:\nStudent (sn'llrn: integer, sname: string, rnajor: string,\nlevel: string, age: integer)\nClass(narne: string, rneets_at: time, roorn: string, fid: integer)\nEnrolled(snurn: integer, cnarne: string)\nFaculty(fid: integer, fnarne: string, deptid: integer)\nThe meaning of these relations is straightforward; for example, Enrolled has one record per\nstudent-class pair such that the student is enrolled in the class.\n1. Write the SQL statements required to create these relations, including appropriate ver-\nsions of all primary and foreign key integrity constraints.\n2. Express each of the following integrity constraints in SQL unless it is implied by the\nprimary and foreign key constraint; if so, explain how it is implied. If the constraint\ncannot be expressed in SQL, say so. For each constraint, state what operations (inserts,\ndeletes, and updates on specific relations) must be monitored to enforce the constraint.\n(a) Every class has a minimum enrollment of 5 students and a maximum enrollment\nof 30 students.\n(b) At least one dass meets in each room.\n(c) Every faculty member must teach at least two courses.\n(d) Only faculty in the department with deptid=33 teach more than three courses.\n(e) Every student must be enrolled in the course called lVlathlOl.\n(f) The room in which the earliest scheduled class (i.e., the class with the smallest\nnucets_at value) meets should not be the same as the room in which the latest\nscheduled class meets.\n(g) Two classes cannot meet in the same room at the same time.\n(h) The department with the most faculty members must have fewer than twice the\nnumber of faculty members in the department with the fewest faculty members.\n(i) No department can have more than 10 faculty members.\n(j) A student cannot add more than two courses at a time (i.e., in a single update).\n(k) The number of CS majors must be more than the number of Math majors.\n(I) The number of distinct courses in which CS majors are enrolled is greater than the\nnumber of distinct courses in which Math majors are enrolled.\n(rn) The total enrollment in courses taught by faculty in the department with deptid=SS\nis greater than the number of ivlath majors.\n(n) There lIlUst be at least one CS major if there are any students whatsoever.\n(0) Faculty members from different departments cannot teach in the same room.\nExercise 5.9 Discuss the strengths and weaknesses of the trigger mechanism.\nContrast\ntriggers with other integrity constraints supported by SQL.\n\n180\nExercise 5.10 Consider the following relational schema.\nAn employee can work in more\nthan one department; the pel-time field of the \\Vorks relation shows the percentage of time\nthat a given employee works in a given department.\nEmp( eid: integer, ename: string, age: integer, salary: real)\n\\Vorks( eid: integer, did: integer, pcLtime: integer)\nDept(did: integer, budget: real, mana,gerid: integer)\n\\Vrite SQL-92 integrity constraints (domain, key, foreign key, or CHECK constraints; or asser··\nbons) or SQL:1999 triggers to ensure each of the following requirements, considered indepen-\ndently.\n1. Employees must make a minimum salary of $1000.\n2. Every manager must be also be an employee.\n3. The total percentage of aU appointments for an employee must be under 100%.\n4. A manager must always have a higher salary than any employee that he or she manages.\n5. Whenever an employee is given a raise, the manager's salary must be increased to be at\nleast as much.\n6. Whenever an employee is given a raise, the manager's salary must be increased to be\nat least as much.\nFurther, whenever an employee is given a raise, the department's\nbudget must be increased to be greater than the sum of salaries of aU employees in the\ndepartment.\nPROJECT-BASED EXERCISE\nExercise 5.11 Identify the subset of SQL queries that are supported in Minibase.\nBIBLIOGRAPHIC NOTES\nThe original version of SQL was developed as the query language for IBM's System R project,\nand its early development can be traced in [107, 151].\nSQL has since become the most\nwidely used relational query language, and its development is now subject to an international\nstandardization process.\nA very readable and comprehensive treatment of SQL-92 is presented by Melton and Simon\nin [524], and the central features of SQL:1999 are covered in [525]. We refer readers to these\ntwo books for an authoritative treatment of SQL. A short survey of the SQL:1999 standard\nis presented in [237].\nDate offers an insightful critique of SQL in [202].\nAlthough some of\nthe problems have been addressed in SQL-92 and later revisions, others remain.\nA formal\nsemantics for a large subset ofSQL queries is presented in [560]. SQL:1999 is the current Inter-\nnational Organization for Standardization (ISO) and American National Standards Institute\n(ANSI) standard. Melton is the editor of the ANSI and ISO SQL:1999 standard, document\nANSI/ISO/IEe 9075-:1999.\nThe corresponding ISO document is ISO/lEe 9075-:1999.\nA\nsuccessor, planned for 2003, builds on SQL:1999 SQL:200:3 is close to ratification (a.s of June\n20(2). Drafts of the SQL:200:3 deliberations are available at the following URL:\nftp://sqlstandards.org/SC32/\n\nSqL: queries, COll./:!tTo/inLs, Triggers\nlSI\n[774] contains a collection of papers that cover the active database field.\n[794J includes a\ngood in-depth introduction to active rules, covering smnantics, applications and design issues.\n[251] discusses SQL extensions for specifying integrity constraint checks through triggers.\n[123] also discusses a procedural mechanism, called an alerter, for monitoring a database.\n[185] is a recent paper that suggests how triggers might be incorporated into SQL extensions.\nInfluential active database prototypes include Ariel [366], HiPAC [516J, ODE [18], Postgres\n[722], RDL [690], and Sentinel [36]. [147] compares various architectures for active database\nsystems.\n[32] considers conditions under which a collection of active rules has the same behavior,\nindependent of evaluation order. Semantics of active databases is also studied in [285] and\n[792]. Designing and managing complex rule systems is discussed in [60, 225]. [142] discusses\nrule management using Chimera, a data model and language for active database systems.\n\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\nPART II\nAPPLICATION DEVELOPMENT\n\n6\nDATABASE APPLICATION\nDEVELOPMENT\n..\nHow do application programs connect to a DBMS?\n..\nHow can applications manipulate data retrieved from a DBMS?\n..\nHow can applications modify data in a DBMS?\n..\nWhat are cursors?\n..\nWhat is JDBC and how is it used?\n..\nWhat is SQLJ and how is it used?\n..\nWhat are stored procedures?\n..\nKey concepts:\nEmbedded SQL, Dynamic SQL, cursors; JDBC,\nconnections, drivers, ResultSets, java.sql, SQLJ; stored procedures,\nSQL/PSM\nHf~ profits most who serves best.\n------Ivlotto for Rotary International\nIn Chapter 5, we looked at a wide range of SQL query constructs, treating SQL\nas an independent language in its own right. A relational DBMS supports an\ninteuLctive SqL interface, and users can directly enter SQL commands. This\nsimple approach is fine as long as the task at hand can be accomplished entirely\nwith SQL cormnands. In practice, we often encounter situations in which we\nneed the greater flexibility of a general-purpose programming language in addi-\ntion to the data manipulation facilities provided by SQL. For example, we rnay\nwant to integrate a database application with a nice graphical user interface,\nor we may want to integrate with other existing applications.\n185\n\n186\nCHAPTEFt 6\nJ\nApplications that rely on the DBMS to manage data run as separate processes\nthat connect to the DBlvIS to interact with it. Once a connection is established,\nSQL commands can be used to insert, delete, and modify data. SQL queries can\nbe used to retrieve desired data. but we need to bridge an important difference\nin how a database system sees data and how an application program in a\nlanguage like Java or C sees data: The result of a database query is a set (or\nmultiset) or records, hut Java has no set or multiset data type. This mismatch\nis resolved through additional SQL constructs that allow applications to obtain\na handle on a collection and iterate over the records one at a time.\nvVe introduce Embedded SQL, Dynamic SQL, and cursors in Section 6.1. Em-\nbedded SQL allows us to access data using static SQL queries in application\ncode (Section 6.1.1); with Dynamic SQL, we can create the queries at run-time\n(Section 6.1.3). Cursors bridge the gap between set-valued query answers and\nprogramming languages that do not support set-values (Section 6.1.2).\nThe emergence of Java as a popular application development language, espe-\ncially for Internet applications, has made accessing a DBMS from Java code a\nparticularly important topic. Section 6.2 covers JDBC, a prograruming inter-\nface that allows us to execute SQL queries from a Java program and use the\nresults in the Java program. JDBC provides greater portability than Embed-\nded SQL or Dynamic SQL, and offers the ability to connect to several DBMSs\nwithout recompiling the code. Section 6.4 covers SQLJ, which does the same\nfor static SQL queries, but is easier to program in than Java, with JDBC.\nOften, it is useful to execute application code at the database server, rather than\njust retrieve data and execute application logic in a separate process. Section\n6.5 covers stored procedures, which enable application logic to be stored and\nexecuted at the databa\"se server.\nWe conclude the chapter by discussing our\nB&N case study in Section 6.6.\n'Vhile writing database applications, we must also keep in mind that typically\nmany application programs run concurrently. The transaction concept, intro-\nduced in Chapter 1, is used to encapsulate the effects of an application on\nthe datahase. An application can select certain transaction properties through\nSQL cormnands to control the degree to which it is exposed to the changes of\nother concurrently running applications. \\Ve touch on the transaction concept\nat many points i,n this chapter, and, in particular, cover transaction-related\n~hS­\npects of JDBC. A full discussion of transaction properties and SQL's support\nfor transactions is deferred until Chapter 16.\nExamples that appear in this chapter are available online at\nhttp://www.cs.wisc.edu/-dbbook\n\nDatabase Application DeveloplTu:nt\n6.1\nACCESSING DATABASES FROlVl APPLICATIONS\n187\nIn this section, we cover how SQL commands can be executed from within a\nprogram in a host language such as C or Java. The use of SQL commands\nwithin a host language program is called Embedded SQL. Details of Embed~\nded SQL also depend on the host language. Although similar capabilities are\nsupported for a variety of host languages, the syntax sometimes varies.\nvVe first cover the basics of Embedded SQL with static SQL queries in Section\n6.1.1. We then introduce cursors in Section 6.1.2. vVe discuss Dynamic SQL,\nwhich allows us to construct SQL queries at runtime (and execute them) in\nSection 6.1.:3.\n6.1.1\nEmbedded SQL\nConceptually, embedding SQL commands in a host language program is straight-\nforward. SQL statements (i.e., not declarations) can be used wherever a state-\nment in the host language is allowed (with a few restrictions). SQL statements\nmust be clearly marked so that a preprocessor can deal with them before in-\nvoking the compiler for the host language. Also, any host language variables\nused to pass arguments into an SQL command must be declared in SQL. In\nparticular, some special host language variables must be declared in SQL (so\nthat, for example, any error conditions arising during SQL execution can be\ncommunicated back to the main application program in the host language).\nThere are, however, two complications to bear in mind. First, the data types\nrecognized by SQL may not be recognized by the host language and vice versa.\nThis mismatch is typically addressed by casting data values appropriately be-\nfore passing them to or frorn SQL commands. (SQL, like other programming\nlanguages, provides an operator to cast values of aIle type into values of an-\nother type.) The second complication h~s to do with SQL being set-oriented,\nand is addressed using cursors (see Section 6.1.2. Commands operate on and\nproduce tables, which are sets\nIn our discussion of Embedded SQL,\nw(~ assmne thi'Lt the host language is C\nfor\nconcretenc~ss. because minor differcnces exist in how SQL statements are\nembedded in differcnt host languages.\nDeclaring Variables and Exceptions\nSQL statements can refer to variables defined in the host program. Such host-\nlanguage variables must be prefixed by a colon (:) in SQL statements and be\ndeclared between the commands EXEC SQL BEGIN DECLARE SECTION and EXEC\n\n188\nCHAPTER 6\n~\nSQL END DECLARE SECTION. The declarations are similar to how they would\nlook in a C program and, as usual in C. are separated by semicolons.\nFor\nexample. we can declare variables c-sname, c_sid, c_mt'ing, and cage (with the\ninitial c used as a naming convention to emphasize that these are host language\nvariables) as follows:\nEXEC SQL BEGIN DECLARE SECTION\nchar c_sname[20];\nlong csid;\nshort crating;\nfloat cage;\nEXEC SQL END DECLARE SECTION\nThe first question that arises is which SQL types correspond to the various\nC types, since we have just declared a collection of C variables whose val-\nues are intended to be read (and possibly set) in an SQL run-time environ-\nment when an SQL statement that refers to them is executed.\nThe SQL-92\nstandard defines such a correspondence between the host language types and\nSQL types for a number of host languages. In our example, c_snamc has the\ntype CHARACTER(20) when referred to in an SQL statement, csid has the type\nINTEGER, crating has the type SMALLINT, and cage has the type REAL.\n\\Ve also need some way for SQL to report what went wrong if an error condition\narises when executing an SQL statement.\nThe SQL-92 standard recognizes\ntwo special variables for reporting errors, SQLCODE and SQLSTATE. SQLCODE is\nthe older of the two and is defined to return some negative value when an\nerror condition arises, without specifying further just what error a particular\nnegative integer denotes. SQLSTATE, introduced in the SQL-92 standard for the\nfirst time, &':lsociates predefined values with several common error conditions,\nthereby introducing some uniformity to how errors are reported. One of these\ntwo variables must be declared. The appropriate C type for SQLCODE is long\nand the appropriate C type for SQLSTATE is char [6J , that is, a character string\nfive characters long. (Recall the null-terminator in C strings.) In this chapter,\nwe assume that SQLSTATE is declared.\nEmbedding SQL Statements\nAll SQL staternents embedded within a host program must be clearly marked,\nwith the details dependent on the host language; in C, SQL statements must be\nprefixed by EXEC SQL. An SQL statement can essentially appear in any place\nin the host language program where a host language statement can appear.\n\nDatabase Application DC?lelopment\n189\nAs a simple example, the following Embedded' SQL statement inserts a row,\nwhose column values me based on the values of the host language variables\ncontained in it, into the Sailors relation:\nEXEC SQL\nINSERT INTO Sailors VALUES (:c_sname, :csid, :crating, :cage);\nObserve that a semicolon terminates the command, as per the convention for\nterminating statements in C.\nThe SQLSTATE variable should be checked for errors and exceptions after each\nEmbedded SQL statement. SQL provides the WHENEVER command to simplify\nthis tedious task:\nEXEC SQL WHENEVER [SQLERROR I NOT FOUND] [ CONTINUE I GOTO st'mt ]\nThe intent is that the value of SQLSTATE should be checked after each Embedded\nSQL statement is executed. If SQLERROR is specified and the value of SQLSTATE\nindicates an exception, control is transferred to stmt, which is presumably re-\nsponsible for error and exception handling. Control is also transferred to stmt\nif NOT FOUND is specified and the value of SQLSTATE is 02000, which denotes NO\nDATA.\n6.1.2\nCursors\nA major problem in embedding SQL statements in a host language like C is\nthat an impedance mismatch occurs because SQL operates on set\" of records,\nwhereas languages like C do not cleanly support a set-of-records abstraction.\nThe solution is to essentially provide a mechanism that allows us to retrieve\nrows one at a time from a relation.\nThis mechanism is called a cursor. vVe can declare a cursor on any relation\nor on any SQL query (because every query returns a set of rows).\nOnce a\ncurwr is declared, we can open it (which positions the cursor just before the\nfirst row); fetch the next row; move the cursor (to the next row, to the row\nafter the next n, to the first row, or to the previous row, etc., by specifying\nadditional parameters for the FETCH command); or close the cursor. Thus, a\ncursor essentially allows us to retrieve the rows in a table by positioning the\ncursor at a particular row and reading its contents.\nBasic Cursor Definition and Usage\nr'11rsors enable us to examine, in the host language program, a collection of\nJWS computed by an Embedded SQL statement:\n\n190\nCHAPTEl} 6\n..\n\\Ve usually need to open a cursor if the embedded statement is a SELECT\n(i.e.) a query).\nHowever, we can avoid opening a cursor if the answer\ncontains a single row, as we see shortly.\n..\nINSERT, DELETE, and UPDATE staternents typically require no cursor, al-\nthough some variants of DELETE and UPDATE use a cursor.\nAs an example, we can find the name and age of a sailor, specified by assigning\na value to the host variable\nc~sir1, declared earlier, as follows:\nEXEC SQL SELECT\nINTO\nFROM\nWHERE\nS.sname, S.age\n:c_sname, :c_age\nSailors S\nS.sid = :c_sid;\nThe INTO clause allows us to assign the columns of the single answer row to\nthe host variables csname and c_age. Therefore, we do not need a cursor to\nembed this query in a host language program. But what about the following\nquery, which computes the names and ages of all sailors with a rating greater\nthan the current value of the host variable cminmting?\nSELECT S.sname, S.age\nFROM\nSailors S\nWHERE\nS.rating > :c_minrating\nThis query returns a collection of rows, not just one row.\n'When executed\ninteractively, the answers are printed on the screen. If we embed this query in\na C program by prefixing the cOlnmand with EXEC SQL, how can the answers\nbe bound to host language variables? The INTO clause is inadequate because\nwe must deal with several rows. The solution is to use a cursor:\nDECLARE sinfo CURSOR FOR\nSELECT S.sname, S.age\nFROM\nSailors S\nWHERE\nS.rating > :c_minrating;\nThis code can be included in a C program, and once it is executed, the cursor\n8ir~lo is defined. Subsequently, we can open the cursor:\nOPEN sinfo:\nThe value of cminmting in the SQL query associated with the cursor is the\nvalue of this variable when we open the cursor.\n(The cursor declaration is\nprocessed at compile-time, and the OPEN command is executed at run-time.)\n\nDatabase Applicat'ion Development\n191,\nA cursor can be thought of as 'pointing' to a row in the collection of answers\nto the query associated with it. vVhen a cursor is opened, it is positioned just\nbefore the first row. \\Ve can use the FETCH command to read the first row of\ncursor sinfo into host language variables:\nFETCH sinfo INTO :csname, :cage;\nWhen the FETCH statement is executed, the cursor is positioned to point at\nthe next row (which is the first row in the table when FETCH is executed for\nthe first time after opening the cursor) and the column values in the row are\ncopied into the corresponding host variables.\nBy repeatedly executing this\nFETCH statement (say, in a while-loop in the C program), we can read all the\nrows computed by the query, one row at a time. Additional parameters to the\nFETCH command allow us to position a cursor in very flexible ways, but we do\nnot discuss them.\nHow do we know when we have looked at all the rows associated with the\ncursor? By looking at the special variables SQLCODE or SQLSTATE, of course.\nSQLSTATE, for example, is set to the value 02000, which denotes NO DATA, to\nindicate that there are no more rows if the FETCH statement positions the cursor\nafter the last row.\nWhen we are done with a cursor, we can close it:\nCLOSE sinfo;\nIt can be opened again if needed, and the value of : cminrating in the\nSQL query associated with the cursor would be the value of the host variable\ncminrating at that time.\nProperties of Cursors\nThe general form of a cursor declaration is:\nDECLARE cursomame [INSENSITIVE] [SCROLL] CURSOR\n[WITH HOLD]\nFOR some query\n[ ORDER BY order-item-list ]\n[ FOR READ ONLY I FOR UPDATE ]\nA cursor can be declared to be a read-only cursor (FOR READ ONLY) or, if\nit is a cursor on a base relation or an updatable view, to be an updatable\ncursor (FOR UPDATE). If it is IIpdatable, simple variants of the UPDATE and\n\n192\nCHAPTER 6\nil'\nDELETE commands allow us to update or delete the row on which the cursor\nis positioned. For example, if sinfa is an updatable cursor and open, we can\nexecute the following statement:\nUPDATE Sailors S\nSET\nS.rating = S.rating ~ 1\nWHERE\nCURRENT of sinfo;\nThis Embedded SQL statement modifies the rating value of the row currently\npointed to by cursor sinfa; similarly, we can delete this row by executing the\nnext statement:\nDELETE Sailors S\nWHERE\nCURRENT of sinfo;\nA cursor is updatable by default unless it is a scrollable or insensitive cursor\n(see below), in which case it is read-only by default.\nIf the keyword SCROLL is specified, the cursor is scrollable, which means that\nvariants of the FETCH command can be used to position the cursor in very\nflexible ways; otherwise, only the basic FETCH command, which retrieves the\nnext row, is allowed.\nIf the keyword INSENSITIVE is specified, the cursor behaves as if it is ranging\nover a private copy of the collection of answer rows. Otherwise, and by default,\nother actions of some transaction could modify these rows, creating unpre-\ndictable behavior.\nFor example, while we are fetching rows using the sinfa\ncursor, we might modify rating values in Sailor rows by concurrently executing\nthe command:\nUPDATE Sailors S\nSET\nS.rating = S.rating -\nConsider a Sailor row such that (1) it has not yet been fetched, and (2) its\noriginal rating value would have met the condition in the WHERE clause of the\nquery associated with sinfa, but the new rating value does not.\nDo we fetch\nsuch a Sailor row'? If INSENSITIVE is specified, the behavior is as if all answers\nwere computed,and stored when sinfo was opened; thus, the update command\nhas no effect on the rows fetched by sinfa if it is executed after sinfo is opened.\nIf INSENSITIVE is not specified, the behavior is implementation dependent in\nthis situation.\nA holdable cursor is specified using the WITH HOLD clause, and is not closed\nwhen the transaction is conunitted. The motivation for this cornes from long\n\nDatabase Apphcation Development\n193\ntransactions in which we access (and possibly change) a large number of rows of\na table. If the transaction is aborted for any reason, the system potentially has\nto redo a lot of work when the transaction is restarted. Even if the transaction\nis not aborted, its locks are held for a long time and reduce the concurrency\nof the system. The alternative is to break the transaction into several smaller\ntransactions, but remembering our position in the table between transactions\n(and other similar details) is complicated and error-prone.\nAllowing the ap-\nplication program to commit the transaction it initiated, while retaining its\nhandle on the active table (i.e., the cursor) solves this problem: The applica-\ntion can commit its transaction and start a new transaction and thereby save\nthe changes it has made thus far.\nFinally, in what order do FETCH commands retrieve rows? In general this order\nis unspecified, but the optional ORDER BY clause can be used to specify a sort\norder. Note that columns mentioned in the ORDER BY clause cannot be updated\nthrough the cursor!\nThe order-item-list is a list of order-items; an order-item is a column name,\noptionally followed by one of the keywords ASC or DESC. Every column men-\ntioned in the ORDER BY clause must also appear in the select-list of the query\nassociated with the cursor; otherwise it is not clear what columns we should\nsort on. The keywords ASC or DESC that follow a column control whether the\nresult should be sorted-with respect to that column-in ascending or descend-\ning order; the default is ASC. This clause is applied as the last step in evaluating\nthe query.\nConsider the query discussed in Section 5.5.1, and the answer shown in Figure\n5.13. Suppose that a cursor is opened on this query, with the clause:\nORDER BY minage ASC, rating DESC\nThe answer is sorted first in ascending order by minage, and if several rows\nhave the same minage value, these rows are sorted further in descending order\nby rating. The cursor would fetch the rows in the order shown in Figure 6.1.\nI rating I minage I\n8\n25.5\n3\n25.5\n7\n35.0\nFigure 6.1\nOrder in which 'fuples Are Fetched\n\n194\nCHAPTER c6\n6.1.3\nDynamic SQL\nConsider an application such as a spreadsheet or a graphical front-end that\nneeds to access data from a DBMS. Such an application must accept commands\nfrom a user and, based on what the user needs, generate appropriate SQL\nstatements to retrieve the necessary data. In such situations, we may not be\nable to predict in advance just what SQL statements need to be executed, even\nthough there is (presumably) some algorithm by which the application can\nconstruct the necessary SQL statements once a user's command is issued.\nSQL provides some facilities to deal with such situations; these are referred\nto as Dynamic SQL. We illustrate the two main commands, PREPARE and\nEXECUTE, through a simple example:\nchar c_sqlstring[] = {\"DELETE FROM Sailors WHERE rating>5\"};\nEXEC SQL PREPARE readytogo FROM :csqlstring;\nEXEC SQL EXECUTE readytogo;\nThe first statement declares the C variable c_sqlstring and initializes its value to\nthe string representation of an SQL command. The second statement results in\nthis string being parsed and compiled as an SQL command, with the resulting\nexecutable bound to the SQL variable readytogo.\n(Since readytogo is an SQL\nvariable, just like a cursor name, it is not prefixed by a colon.)\nThe third\nstatement executes the command.\nMany situations require the use of Dynamic SQL. However, note that the\npreparation of a Dynamic SQL command occurs at run-time and is run-time\noverhead.\nInteractive and Embedded SQL commands can be prepared once\nat compile-time and then re-executecl as often as desired.\nConsequently you\nshould limit the use of Dynamic SQL to situations in which it is essential.\nThere are many more things to know about Dynamic SQL~~~how we can pa'3S\nparameters from the host language program to the SQL statement being pre-\nparcel, for example--but we do not discuss it further.\n6.2\nAN INTRODUCTION TO JDBC\nEmbedded SQL enables the integration of SQL with a general-purpose pro-\ngramming language. As described in Section 6.1.1, a DBMS-specific preproces-\nsor transforms the Embedded SQL statements into function calls in the host\nlanguage.\nThe details of this translation vary across DBMSs, and therefore\neven though the source code can be cOlnpiled to work with different DBMSs,\nthe final executable works only with one specific DBMS.\n\nDatabase Application Develop'Tnent\n195,\nODBC and JDBC, short for Open DataBase Connectivity and Java DataBase\nConnectivity, also enable the integration of SQL with a general-purpose pro-\ngramming language. Both ODBC and JDBC expose database capabilities in\na standardized way to the application programmer through an application\nprogramming interface (API). In contrast to Embedded SQL, ODBC and\nJDBC allow a single executable to access different DBMSs 'Without recompi-\nlation. Thus, while Embedded SQL is DBMS-independent only at the source\ncode level, applications using ODBC or JDBC are DBMS-independent at the\nsource code level and at the level of the executable. In addition, using ODBC\nor JDBC, an application can access not just one DBMS but several different\nones simultaneously.\nODBC and JDBC achieve portability at the level of the executable by introduc-\ning an extra level of indirection. All direct interaction with a specific DBMS\nhappens through a DBMS-specific driver.\nA driver is a software program\nthat translates the ODBC or JDBC calls into DBMS-specific calls.\nDrivers\nare loaded dynamically on demand since the DBMSs the application is going\nto access are known only at run-time. Available drivers are registered with a\ndriver manager.\nOne interesting point to note is that a driver does not necessarily need to\ninteract with a DBMS that understands SQL. It is sufficient that the driver\ntranslates the SQL commands from the application into equivalent commands\nthat the DBMS understands. Therefore, in the remainder of this section, we\nrefer to a data storage subsystem with which a driver interacts as a data\nsource.\nAn application that interacts with a data source through ODBC or JDBC se-\nlects a data source, dynamically loads the corresponding driver, and establishes\na connection with the data source. There is no limit on the number of open\nconnections, and an application can have several open connections to different\ndata sources. Each connection has transaction semantics; that is, changes from\none connection are visible to other connections only after the connection has\ncommitted its changes. While a connection is opcn, transactions are executed\nby submitting SQL statements, retrieving results, processing errors, and finally\ncommitting or rolling back. The application disconnects from the data source\nto terminate the interaction.\nIn the remainder of this chapter, we concentrate on JDBC.\n\n196\nCHAPTEij. 6\nr--\nI\nJDBC Drivers: The most up-to-date source of .IDBC drivers is the Sun\nJDBC Driver page at\nhttp://industry.java.sun.com/products/jdbc/drivers\nJDBC drivers are available for all major database sytems.\n6.2.1\nArchitecture\nThe architecture of JDBC has four main components:\nthe application, the\ndriver manager, several data source specific dr-iveTs, and the corresponding\ndata SOUTces.\nThe application initiates and terminates the connection with a data source.\nIt sets transaction boundaries, submits SQL statements, and retrieves the\nresults-----all through a well-defined interface as specified by the JDBC API. The\nprimary goal of the dr-iver manager is to load JDBC drivers and pass JDBC\nfunction calls from the application to the correct driver. The driver manager\nalso handles JDBC initialization and information calls from the applications\nand can log all function calls. In addition, the driver manager performs· some\nrudimentary error checking.\nThe dr-iver establishes the connection with the\ndata source. In addition to submitting requests and returning request results,\nthe driver translates data, error formats, and error codes from a form that is\nspecific to the data source into the JDBC standard. The data source processes\ncommands from the driver and returns the results.\nDepending on the relative location of the data source and the application,\nseveral architectural scenarios are possible. Drivers in JDBC are cla.ssified into\nfour types depending on the architectural relationship between the application\nand the data source:\nIII\nType I\nBridges:\nThis type of driver translates JDBC function calls\ninto function calls of another API that is not native to the DBMS. An\nexample is a JDBC-ODBC bridge; an application can use JDBC calls to\naccess an ODBC compliant data source. The application loads only one\ndriver, the bridge.\nBridges have the advantage that it is easy to piggy-\nback the applica.tion onto an existing installation, and no new drivers have\nto be installed. But using bridges hl:l.-'3 several drawbacks. The increased\nnumber of layers between data source and application affects performance.\nIn addition, the user is limited to the functionality that the ODBC driver\nsupports.\niii\nType II\nDirect Thanslation to the Native API via N on-Java\nDriver: This type of driver translates JDBC function calls directly into\nmethod invocations of the API of one specific data source. The driver is\n\nDatabase Application Develop'll1,ent\n197\n}\nusually ,vritten using a combination of C++ and Java; it is dynamically\nlinked and specific to the data source. This architecture performs signif-\nicantly better than a JDBC-ODBC bridge. One disadvantage is that the\ndatabase driver that implements the API needs to be installed on each\ncomputer that runs the application.\nII\nType\nIII~~Network Bridges:\nThe driver talks over a network to a\nmiddleware server that translates the JDBC requests into DBMS-specific\nmethod invocations.\nIn this case, the driver on the client site (Le., the\nnetwork bridge) is not DBMS-specific. The JDBC driver loaded by the ap~\nplication can be quite small, as the only functionality it needs to implement\nis sending of SQL statements to the middleware server. The middleware\nserver can then use a Type II JDBC driver to connect to the data source.\nII\nType IV-Direct Translation to the Native API via Java Driver:\nInstead of calling the DBMS API directly, the driver communicates with\nthe DBMS through Java sockets. In this case, the driver on the client side is\nwritten in Java, but it is DBMS-specific. It translates JDBC calls into the\nnative API of the database system. This solution does not require an in-\ntermediate layer, and since the implementation is all Java, its performance\nis usually quite good.\n6.3\nJDBC CLASSES AND INTERFACES\nJDBC is a collection of Java classes and interfaces that enables database access\nfrom prograrl1s written in the Java language.\nIt contains methods for con-\nnecting to a remote data source, executing SQL statements, examining sets\nof results from SQL statements, transaction management, and exception han-\ndling. The cla.sses and interfaces are part of the java. sql package. Thus, all\ncode fragments in the remainder of this section should include the statement\nimport java. sql .* at the beginning of the code; we omit this statement in\nthe remainder of this section.\nJDBC 2.0 also includes the j avax. sql pack-\nage, the JDBC Optional Package.\nThe package j avax. sql adds, among\nother things, the capability of connection pooling and the Row-Set interface.\n\\\\Te discuss connection pooling in Section 6.3.2, and the ResultSet interface in\nSection 6.3.4.\n\\\\Te now illustrate the individual steps that are required to submit a databa.se\nquery to a data source and to retrieve the results.\n6.3.1\nJDBC Driver Management\nIn .lDBe, data source drivers are managed by the Drivermanager class, which\nmaintains a list of all currently loaded drivers. The Drivermanager cla.ss has\n\n198\nCHAPTEa 6\nmethods registerDriver, deregisterDriver, and getDrivers to enable dy-\nnamic addition and deletion of drivers.\nThe first step in connecting to a data source is to load the corresponding JDBC\ndriver.\nThis is accomplished by using the Java mechanism for dynamically\nloading classes. The static method forName in the Class class returns the Java\nclass as specified in the argument string and executes its static constructor.\nThe static constructor of the dynamically loaded class loads an instance of the\nDriver class, and this Driver object registers itself with the DriverManager\nclass.\nThe following Java example code explicitly loads a JDBC driver:\nClass.forName(\"oracle/jdbc.driver.OracleDriver\");\nThere are two other ways ofregistering a driver. We can include the driver with\n-Djdbc. drivers=oracle/jdbc. driver at the command line when we start the\nJava application. Alternatively, we can explicitly instantiate a driver, but this\nmethod is used only rarely, as the name of the driver has to be specified in the\napplication code, and thus the application becomes sensitive to changes at the\ndriver level.\nAfter registering the driver, we connect to the data source.\n6.3.2\nConnections\nA session with a data source is started through creation of a Connection object;\nA connection identifies a logical session with a data source; multiple connections\nwithin the same Java program can refer to different data sources or the same\ndata source. Connections are specified through a JDBC URL, a URL that\nuses the jdbc protocol. Such a URL has the form\njdbc:<subprotocol>:<otherParameters>\nThe code example shown in Figure 6.2 establishes a connection to an Oracle\ndatabase assuming that the strings userld and password are set to valid values.\nIn JDBC, connections can have different properties. For example, a connection\ncan specify the granularity of transactions.\nIf autocommit is set for a con-\nnection, then each SQL statement is considered to be its own transaction. If\nautocommit is off, then a series of statements that compose a transaction can\nbe committed using the commit 0 method of the Connection cla..<;s, or aborted\nusing the rollbackO method. The Connection cla.'ss has methods to set the\n\nDatabase Appl'ication Development\nString uri =\n..jdbc:oracle:www.bookstore.com:3083..\nConnection connection;\ntry {\nConnection connection =\nDriverManager.getConnection(urI,userId,password);\n}\ncatch(SQLException excpt) {\nSystem.out.println(excpt.getMessageO);\nreturn;\n}\nFigure 6.2\nEstablishing a Connection with JDBC\n- ----~--._--_._---~,._-----~---_._-----,\n199\nI\nJDBC Connections:\nRemember to close connections to data sources\nand return shared connections to the connection pool. Database systems\nhave a limited number of resources available for connections, and orphan\nconnections can often only be detected through time-outs-and while the\ndatabase system is waiting for the connection to time-out, the resources\nused by the orphan connection are wasted.\nautocommit mode (Connection. setAutoCommit) and to retrieve the current\nautocommit mode (getAutoCommit).\nThe following methods are part of the\nConnection interface and permit setting and getting other properties:\n•\npublic int getTransactionIsolation() throws SQLExceptionand\npublic void setTransactionlsolation(int 1) throws SQLException.\nThese two functions get and set the current level of isolation for transac-\ntions handled in the current connection.\nAll five SQL levels of isolation\n(see Section 16.6 for a full discussion) are possible, and argument 1can be\nset as follows:\n-\nTRANSACTIONJNONE\n-\nTRANSACTIONJREAD.UNCOMMITTED\n-\nTRANSACTIONJREAD.COMMITTED\n-\nTRANSACTIONJREPEATABLEJREAD\n-\nTRANSACTION.BERIALIZABLE\n•\npublic boolean getReadOnlyO throws SQLException and\npublic void setReadOnly(boolean readOnly) throws SQLException.\nThese two functions allow the user to specify whether the transactions\nexecutecl through this connection are rcad only.\n\n200\nCHAPTER ()\n..\npublic boolean isClosed() throws SQLException.\nChecks whether the current connection has already been closed.\n..\nsetAutoCommit and get AutoCommit.\nvVe already discussed these two functions.\nEstablishing a connection to a data source is a costly operation since it in-\nvolves several steps, such as establishing a network connection to the data\nsource, authentication, and allocation of resources such as memory. In case an\napplication establishes many different connections from different parties (such\nas a Web server), connections are often pooled to avoid this overhead. A con-\nnection pool is a set of established connections to a data source. Whenever a\nnew connection is needed, one of the connections from the pool is used, instead\nof creating a new connection to the data source.\nConnection pooling can be handled either by specialized code in the application,\nor the optional j avax. sql package, which provides functionality for connection\npooling and allows us to set different parameters, such as the capacity of the\npool, and shrinkage and growth rates.\nMost application servers (see Section\n7.7.2) implement the j avax .sql package or a proprietary variant.\n6.3.3\nExecuting SQL Statements\nWe now discuss how to create and execute SQL statements using JDBC. In the\nJDBC code examples in this section, we assume that we have a Connection\nobject named con. JDBC supports three different ways of executing statements:\nStatement, PreparedStatement, and CallableStatement.\nThe Statement\nclass is the base class for the other two statment classes. It allows us to query\nthe data source with any static or dynamically generated SQL query. We cover\nthe PreparedStatement class here and the CallableStatement class in Section\n6.5, when we discuss stored procedures.\nThe PreparedStatement cla,Cis dynamicaJly generates precompiled SQL state-\nments that can be used several times; these SQL statements can have param-\neters, but their structure is fixed when the PreparedStatement object (repre-\nsenting the SQL statement) is created.\nConsider the sample code using a PreparedStatment object shown in Figure\n6.3.\nThe SQL query specifies the query string, but uses ''1'\nfor the values\nof the parameters, which are set later using methods setString, setFloat,\nand setlnt. The ''1' placeholders can be used anywhere in SQL statements\nwhere they can be replaced with a value. Examples of places where they can\nappear include the WHERE clause (e.g., 'WHERE author=?'), or in SQL UPDATE\nand INSERT staternents, as in Figure 6.3. The method setString is one way\n\nDatabase Application Develop'ment\n/ / initial quantity is always zero\nString sql = \"INSERT INTO Books VALUES('?, 7, '?, ?, 0, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\n/ / now instantiate the parameters with values\n/ / a,ssume that isbn, title, etc. are Java variables that\n/ / contain the values to be inserted\npstmt.clearParameters();\npstmt.setString(l, isbn);\npstmt.setString(2, title);\npstmt.setString(3, author);\npstmt.setFloat(5, price);\npstmt.setInt(6, year);\nint numRows = pstmt.executeUpdate();\nFigure 6.3\nSQL Update Using a PreparedStatement Object\n201\nto set a parameter value; analogous methods are available for int, float,\nand date. It is good style to always use clearParameters 0 before setting\nparameter values in order to remove any old data.\nThere are different ways of submitting the query string to the data source. In\nthe example, we used the executeUpdate command, which is used if we know\nthat the SQL statement does not return any records (SQL UPDATE, INSERT,\nALTER, and DELETE statements). The executeUpdate method returns an inte-\nger indicating the number of rows the SQL statement modified; it returns 0 for\nsuccessful execution without modifying any rows.\nThe executeQuery method is used if the SQL statement returns data, such &\"l\nin a regular SELECT query. JDBC has its own cursor mechanism in the form\nof a ResultSet object, which we discuss next. The execute method is more\ngeneral than executeQuery and executeUpdate; the references at the end of\nthe chapter provide pointers with more details.\n6.3.4\nResul,tSets\nAs discussed in the previous section, the statement executeQuery returns a,\nResultSet object, which is similar to a cursor. ResultSet cursors in JDBC\n2.0 are very powerful; they allow forward and reverse scrolling and in-place\nediting and insertions.\n\n202\nCHAPTER\n6\nIn its most basic form, the ResultSet object allows us to read one row of the\noutput of the query at a time.\nInitially, the ResultSet is positioned before\nthe first row, and we have to retrieve the first row with an explicit call to the\nnext 0 method. The next method returns false if there are no more rows in\nthe query answer, and true other\\vise. The code fragment shown in Figure 6.4\nillustrates the basic usage of a ResultSet object.\nResultSet rs=stmt.executeQuery(sqlQuery);\n/ / rs is now a cursor\n/ / first call to rs.nextO moves to the first record\n/ / rs.nextO moves to the next row\nString sqlQuery;\nResultSet rs = stmt.executeQuery(sqlQuery)\nwhile (rs.next()) {\n/ / process the data\n}\nFigure 6.4\nUsing a ResultSet Object\nWhile next () allows us to retrieve the logically next row in the query answer,\nwe can move about in the query answer in other ways too:\n•\nprevious 0 moves back one row.\n•\nabsolute (int num) moves to the row with the specified number.\n•\nrelative (int num) moves forward or backward (if num is negative) rela-\ntive to the current position. relative (-1) has the same effect as previous.\n•\nfirst 0 moves to the first row, and last 0\nmoves to the last row.\nMatching Java and SQL Data Types\nIn considering the interaction of an application with a data source, the issues\nwe encountered in the context of Embedded SQL (e.g., passing information\nbetween the application and the data source through shared variables) arise\nagain. To deal with such issues, JDBC provides special data types and speci-\nfies their relationship to corresponding SQL data types. Figure 6.5 shows the\naccessor methods in a ResultSet object for the most common SQL datatypes.\nWith these accessor methods, we can retrieve values from the current row of\nthe query result referenced by the ResultSet object. There are two forms for\neach accessor method: One method retrieves values by column index, starting\nat one, and the other retrieves values by column name. The following exam-\nple shows how to access fields of the current ResultSet row using accesssor\nmethods.\n\nDatabase Application Development\n2Q3\nI SQL Type I\nJava cla.c;;s\nI ResultSet get method I\nBIT\nBoolean\ngetBooleanO\nCHAR\nString\ngetStringO\nVARCHAR\nString\ngetStringO\nDOUBLE\nDouble\ngetDoubleO\nFLOAT\nDouble\ngetDoubleO\nINTEGER\nInteger\ngetIntO\nREAL\nDouble\ngetFloatO\nDATE\njava.sql.Date\ngetDateO\nTIME\njava.sql.Time\ngetTimeO\nTIMESTAMP\njava.sql.TimeStamp\ngetTimestamp ()\nFigure 6.5\nReading SQL Datatypes from a ResultSet Object\nResultSet rs=stmt.executeQuery(sqIQuery);\nString sqlQuerYi\nResultSet rs = stmt.executeQuery(sqIQuery)\nwhile (rs.nextO) {\nisbn = rs.getString(l);\ntitle = rs.getString(\" TITLE\");\n/ / process isbn and title\n}\n6.3.5\nExceptions and Warnings\nSimilar to the SQLSTATE variable, most of the methods in java. sql can throw\nan exception of the type SQLException if an error occurs.\nThe information\nincludes SQLState, a string that describes the error (e.g., whether the statement\ncontained an SQL syntax error).\nIn addition to the standard getMessage 0\nmethod inherited from Throwable, SQLException has two additional methods\nthat provide further information, and a method to get (or chain) additional\nexceptions:\nIII\npublic String getSQLState 0 returns an SQLState identifier based on\nthe SQL:1999 specification, as discussed in Section 6.1.1.\n..\npublic i:p.t getErrorCode () retrieves a vendor-specific error code.\nIII\npublic SQLException getNextExceptionO gets the next exception in a\nchain of exceptions associated with the current SQLException object.\nAn SQL\\¥arning is a subclass of SQLException. Warnings are not H•.'3 severe as\nerrors and the program can usually proceed without special handling of warn-\nings. \\Varnings are not thrown like other exceptions, and they are not caught a.,\n\n204\nCHAPTER\npart of the try\"-catch block around a java. sql statement. VVe Heed to specif-\nically test whether warnings exist.\nConnection, Statement, and ResultSet\nobjects all have a getWarnings 0\nmethod with which we can retrieve SQL\nwarnings if they exist. Duplicate retrieval of warnings can be avoided through\nclearWarnings O. Statement objects clear warnings automatically on execu-\ntion of the next statement; ResultSet objects clear warnings every time a new\ntuple is accessed.\nTypical code for obtaining SQLWarnings looks similar to the code shown in\nFigure 6.6.\ntry {\nstmt = con.createStatement();\nwarning = con.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\nwarning = warning.getNextWarningO;\n/ /get next warning\n}\ncon.clear\\Varnings() ;\nstmt.executeUpdate( queryString );\nwarning = stmt.getWarnings();\nwhile( warning != null) {\n/ / handleSQLWarnings\n/ / code to process warning\nwarning = warning.getNextWarningO;\n/ /get next warning\n}\n} / / end try\ncatch ( SQLException SQLe) {\n/ / code to handle exception\n} / / end catch\nFigure 6.6\nProcessing JDBC Warnings and Exceptions\n6.3.6\nExamining Database Metadata\n\\Ve can use tlw DatabaseMetaData object to obtain information about the\ndatabase system itself, as well as information frorn the database catalog. For\nexample, the following code fragment shows how to obtain the name and driver\nversion of the JDBC driver:\nDataba..seMetaData md = con.getMetaD<Lta():\nSystem.out.println(\"Driver Information:\");\n\nDatabase Appl'imtion Developrnent\nSystem.out.println(\"Name:\" + md.getDriverNameO\n+ \"; version:\" + mcl.getDriverVersion());\n205\n~\nThe DatabaseMetaData object has many more methods (in JDBC 2.0, exactly\n134); we list some methods here:\n•\npublic ResultSet getCatalogs 0\nthrows SqLException. This function\nreturns a ResultSet that can be used to iterate over all the catalog relations.\nThe functions getIndexInfo 0 and getTables 0 work analogously.\n•\npUblic int getMaxConnections 0\nthrows SqLException. This function\nreturns the ma.ximum number of connections possible.\nWe will conclude our discussion of JDBC with an example code fragment that\nexamines all database metadata shown in Figure 6.7.\nDatabaseMetaData dmd = con.getMetaDataO;\nResultSet tablesRS = dmd.getTables(null,null,null,null);\nstring tableName;\nwhile(tablesRS.next()) {\ntableNarne = tablesRS.getString(\"TABLE_NAME\");\n/ / print out the attributes of this table\nSystem.out.println(\"The attributes of table\"\n+ tableName + \" are:\");\nResultSet columnsRS = dmd.getColums(null,null,tableName, null);\nwhile (columnsRS.next()) {\nSystem.out.print(colummsRS.getString(\" COLUMN_NAME\")\n+\" \");\n}\n/ / print out the primary keys of this table\nSystem.out.println(\"The keys of table\" + tableName + \" are:\");\nResultSet keysRS = dmd.getPrimaryKeys(null,null,tableName);\nwhile (keysRS. next()) {\n'System.out.print(keysRS.getStringC'COLUMN_NAME\") +\" \");\n}\n}\nFigure 6.7\nObtaining Infon-nation about it Data Source\n\n206\nCHAPTER.:6\n6.4\nSQLJ\nSQLJ (short for 'SQL-Java') was developed by the SQLJ Group, a group of\ndatabase vendors and Sun. SQLJ was developed to complement the dynamic\nway of creating queries in JDBC with a static model. It is therefore very close\nto Embedded SQL. Unlike JDBC, having semi-static SQL queries allows the\ncompiler to perform SQL syntax checks, strong type checks of the compatibil-\nity of the host variables with the respective SQL attributes, and consistency\nof the query with the database schema-tables, attributes, views, and stored\nprocedures--all at compilation time. For example, in both SQLJ and Embed-\nded SQL, variables in the host language always are bound statically to the\nsame arguments, whereas in JDBC, we need separate statements to bind each\nvariable to an argument and to retrieve the result. For example, the following\nSQLJ statement binds host language variables title, price, and author to the\nreturn values of the cursor books.\n#sql books = {\nSELECT title, price INTO :title, :price\nFROM Books WHERE author = :author\n};\nIn JDBC, we can dynamically decide which host language variables will hold\nthe query result. In the following example, we read the title of the book into\nvariable ftitle if the book was written by Feynman, and into variable otitle\notherwise:\n/ / assume we have a ResultSet cursor rs\nauthor = rs.getString(3);\nif (author==\"Feynman\") {\nftitle = rs.getString(2):\n}\nelse {\notitle = rs.getString(2);\n}\nvVhen writing SQLJ applications, we just write regular Java code and embed\nSQL statements according to a set of rules. SQLJ applications are pre-processed\nthrough an SQLJ translation program that replaces the embedded SQLJ code\nwith calls to an SQLJ Java library. The modified program code can then be\ncompiled by any Java compiler. Usually the SQLJ Java library makes calls to\na JDBC driver, which handles the connection to the datab&'3e system.\n\nDatabase Application Development\n2Q7\nAn important philosophical difference exists between Embedded SQL and SQLJ\nand JDBC. Since vendors provide their own proprietary versions of SQL, it is\nadvisable to write SQL queries according to the SQL-92 or SQL:1999 standard.\nHowever, when using Embedded SQL, it is tempting to use vendor-specific SQL\nconstructs that offer functionality beyond the SQL-92 or SQL:1999 standards.\nSQLJ and JDBC force adherence to the standards, and the resulting code is\nmuch more portable across different database systems.\nIn the remainder of this section, we give a short introduction to SQLJ.\n6.4.1\nWriting SQLJ Code\nWe will introduce SQLJ by means of examples. Let us start with an SQLJ code\nfragment that selects records from the Books table that match a given author.\nString title; Float price; String atithor;\n#sql iterator Books (String title, Float price);\nBooks books;\n/ / the application sets the author\n/ / execute the query and open the cursor\n#sql books =\n{\nSELECT title, price INTO :titIe, :price\nFROM Books WHERE author = :author\n};\n/ / retrieve results\nwhile (books.next()) {\nSystem.out.println(books.titleO + \", \" + books.price());\n}\nbooks.close() ;\nThe corresponding JDBC code fragment looks as follows (assuming we also\ndeclared price, name, and author:\nPrcparcdStatcment stmt = connection.prepareStatement(\n\" SELECT title, price FROM Books WHERE author = ?\");\n/ / set the parameter in the query ancl execute it\nstmt.setString(1, author);\nResultSet 1'8 = stmt.executeQuery();\n/ / retrieve the results\nwhile (rs.next()) {\n\n208\nCHAPTER\nSystem.out.println(rs.getString(l) + \", \" + rs.getFloat(2));\n}\n6\nComparing the JDBC and SQLJ code, we see that the SQLJ code is much\neasier to read than the JDBC code. Thus, SQLJ reduces software development\nand maintenance costs.\nLet us consider the individual components of the SQLJ code in more detail.\nAll SQLJ statements have the special prefix #sql. In SQLJ, we retrieve the\nresults of SQL queries with iterator objects, which are basically cursors. An\niterator is an instance of an iterator class. Usage of an iterator in SQLJ goes\nthrough five steps:\n•\nDeclare the Iterator Class: In the preceding code, this happened through\nthe statement\n#sql iterator Books (String title, Float price);\nThis statement creates a new Java class that we can use to instantiate\nobjects.\n•\nInstantiate an Iterator Object from the New Iterator Class: We\ninstantiated our iterator in the statement Books books;.\n•\nInitialize the Iterator Using a SQL Statement: In our example, this\nhappens through the statement #sql books\n;;;;;; ....\n•\nIteratively, Read the Rows From the Iterator Object: This step is\nvery similar to reading rows through a ResultSet object in JDBC.\n•\nClose the Iterator Object.\nThere are two types of iterator classes: named iterators and positional iterators.\nFor named iterators, we specify both the variable type and the name of each\ncolumn of the iterator. This allows us to retrieve individual columns by name as\nin our previous example where we could retrieve the title colunm from the Books\ntable using the expression books. titIe (). For positional iterators, we need\nto specifY only the variable type for each column of the iterator.\nTo access\nthe individual columns of the iterator, we use a FETCH ...\nINTO eonstruct,\nsimilar to Embedded SQL. Both iterator types have the same performance;\nwhich iterator to use depends on the programmer's taste.\nLet us revisit our example.\n\\Ve can make the iterator a positional iterator\nthrough the following statement:\n#sql iterator Books (String, Float);\nvVe then retrieve the individual rows from the iterator 3,.'3 follows:\n\nDatabase Application Development\nwhile (true) {\n#sql { FETCH :books INTO :title, :price, };\nif (books.endFetch()) {\nbreak:\n}\n/ / process the book\n}\n6.5\nSTORED PROCEDURES\n200\nIt is often important to execute some parts of the application logic directly in\nthe process space of the database system. Running application logic directly\nat the databa.se has the advantage that the amount of data that is transferred\nbetween the database server and the client issuing the SQL statement can be\nminimized, while at the same time utilizing the full power of the databa.se\nserver.\nWhen SQL statements are issued from a remote application, the records in the\nresult of the query need to be transferred from the database system back to\nthe application. If we use a cursor to remotely access the results of an SQL\nstatement, the DBMS has resources such as locks and memory tied up while the\napplication is processing the records retrieved through the cursor. In contrast,\na stored procedure is a program that is executed through a single SQL\nstatement that can be locally executed and completed within the process space\nof the database server.\nThe results can be packaged into one big result and\nreturned to the application, or the application logic can be performed directly\nat the server, without having to transmit the results to the client at alL\nStored procedures are also beneficial for software engineering rea,sons.\nOnce\na stored procedure is registered with the database server, different users can\nre-use the stored procedure, eliminating duplication of efforts in writing SQL\nqueries or application logic, and making code maintenance ea.\"lY. In addition,\napplication programmers do not need to know the the databa.se schema if we\nencapsulate all databa.'3e access into stored procedures.\nAlthough they,are called stored procedur'es, they do not have to be procedures\nin a programming language sense; they can be functions.\n6.5.1\nCreating a Simple Stored Procedure\nLet us look at the example stored procedure written in SQL shown in Figure\n(i.S.\nvVe see that stored procedures must have a name; this stored procedure\n\n210\nCHAPTER' 6\nhas the name 'ShowNumberOfOrders.'\nOtherwise, it just contains an SQL\nstatement that is precompiled and stored at the server.\nCREATE PROCEDURE ShowNumberOfOrders\nSELECT C.cid, C.cname, COUNT(*)\nFROM\nCustomers C, Orders a\nWHERE\nC.cid = O.cid\nGROUP BY C.cid, C.cname\nFigure 6.8\nA Stored Procedure in SQL\nStored procedures can also have parameters.\nThese parameters have to be\nvalid SQL types, and have one of three different modes: IN,\nOUT, or INOUT.\nIN parameters are arguments to' the stored procedure.\nOUT parameters are\nreturned from the stored procedure; it assigns values to all OUT parameters\nthat the user can process. INOUT parameters combine the properties of IN and\nOUT parameters: They contain values to be passed to the stored procedures, and\nthe stored procedure can set their values as return values. Stored procedures\nenforce strict type conformance: If a parameter is of type INTEGER, it cannot\nbe called with an argument of type VARCHAR.\nLet us look at an example of a stored procedure with arguments. The stored\nprocedure shown in Figure 6.9 has two arguments: book_isbn and addedQty.\nIt updates the available number of copies of a book with the quantity from a\nnew shipment.\nCREATE PROCEDURE Addlnventory (\nIN book_isbn CHAR(lO),\nIN addedQty INTEGER)\nUPDATE Books\nSET\nWHERE\nqty_in_stock = qtyjn_stock + addedQty\nbookjsbn = isbn\nFigure 6.9\nA Stored Procedure with Arguments\nStored procedures do not have to be written in SQL; they can be written in any\nhost language. As an example, the stored procedure shown in Figure 0.10 is a\nJava function that is dynamically executed by the databa..<;e server whenever it\nis called by the dient:\n6.5.2\nCalling Stored Procedures\nStored procedures can be called in interactive SQL with the CALL statement:\n\nDatabase Application Development\nCREATE PROCEDURE RallkCustomers(IN number INTEGER)\nLANGUAGE Java\nEXTERNAL NAME 'file:// /c:/storedProcedures/rank.jar'\nFigure 6.10\nA Stored Procedure in Java\n211\nCALL storedProcedureName(argumentl, argument2, ... , argumentN);\nIn Embedded SQL, the arguments to a stored procedure are usually variables\nin the host language. For example, the stored procedure AddInventory would\nbe called as follows:\nEXEC SQL BEGIN DECLARE SECTION\nchar isbn[lO];\nlong qty;\nEXEC SQL END DECLARE SECTION\n/ / set isbn and qty to some values\nEXEC SQL CALL AddInventory(:isbn,:qty);\nCalling Stored Procedures from JDBC\nWe can call stored procedures from JDBC using the CallableStatment class.\nCallableStatement is a subclass of PreparedStatement and provides the same\nfunctionality. A stored procedure could contain multiple SQL staternents or a\nseries of SQL statements-thus, the result could be many different ResultSet\nobjects.\nWe illustrate the case when the stored procedure result is a single\nResultSet.\nCallableStatement cstmt=\nCOIl.prepareCall(\" {call ShowNumberOfOrders}\");\nResultSet rs = cstmt.executeQueryO\nwhile (rs.next())\nCalling Stored Procedures from SQLJ\nThe stored procedure 'ShowNumberOfOrders' is called as follows using SQLJ:\n/ / create the cursor class\n#sql !terator CustomerInfo(int cid, String cname, int count);\n/ / create the cursor\n\n212\nCustomerInfo customerinfo;\n/ / call the stored procedure\n#sql customerinfo = {CALL ShowNumberOfOrders};\nwhile (customerinfo.nextO) {\nSystem.out.println(customerinfo.cid() + \",\" +\ncustomerinfo.count()) ;\n}\n6.5.3\nSQLIPSM\nCHAPTER (5\nAll major databa...<;e systems provide ways for users to write stored procedures in\na simple, general purpose language closely aligned with SQL. In this section, we\nbriefly discuss the SQL/PSM standard, which is representative of most vendor-\nspecific languages. In PSM, we define modules, which are collections of stored\nprocedures, temporary relations, and other declarations.\nIn SQL/PSM, we declare a stored procedure as follows:\nCREATE PROCEDURE name (parameter1,... , parameterN)\nlocal variable declarations\nprocedure code;\nWe can declare a function similarly as follows:\nCREATE FUNCTION name (parameterl,... , parameterN)\nRETURNS sqIDataType\nlocal variable declarations\nfunction code;\nEach parameter is a triple consisting of the mode (IN, OUT, or INOUT as\ndiscussed in the previous section), the parameter name, and the SQL datatype\nof the parameter. We can seen very simple SQL/PSM procedures in Section\n6.5.1. In this case, the local variable declarations were empty, and the procedure\ncode consisted of an SQL query.\nWe start out with an example of a SQL/PSM function that illustrates the\nmain SQL/PSM constructs. The function takes as input a customer identified\nby her cid and a year. The function returns the rating of the customer, which\nis defined a...'3 follows: Customers who have bought more than ten books during\nthe year are rated 'two'; customer who have purcha...<;ed between 5 and 10 books\nare rated 'one', otherwise the customer is rated 'zero'. The following SQL/PSM\ncode computes the rating for a given customer and year.\nCREATE PROCEDURE RateCustomer\n\nDatabase Appl'ication Development\n(IN custId INTEGER, IN year INTEGER)\nRETURNS INTEGER\nDECLARE rating INTEGER;\nDECLARE numOrders INTEGER;\nSET numOrders =\n(SELECT COUNT(*) FROM Orders 0 WHERE O.tid = custId);\nIF (numOrders>10) THEN rating=2;\nELSEIF (numOrders>5) THEN rating=1;\nELSE rating=O;\nEND IF;\nRETURN rating;\nLet us use this example to give a short overview of some SQL/PSM constructs:\n•\nWe can declare local variables using the DECLARE statement. In our exam-\nple, we declare two local variables: 'rating', and 'numOrders'.\n•\nPSM/SQL functions return values via the RETURN statement. In our ex-\nample, we return the value of the local variable 'rating'.\n•\nvVe can assign values to variables with the SET statement. In our example,\nwe assigned the return value of a query to the variable 'numOrders'.\n•\nSQL/PSM h&<; branches and loops. Branches have the following form:\nIF (condition) THEN statements;\nELSEIF statements;\nELSEIF statements;\nELSE statements; END IF\nLoops are of the form\nLOOP\nstaternents:\nEND LOOP\n•\nQueries can be used as part of expressions in branches; queries that return\na single ;ralue can be assigned to variables as in our example above.\n•\n'We can use the same cursor statements &s in Embedded SQL (OPEN, FETCH,\nCLOSE), but we do not need the EXEC SQL constructs, and variables do not\nhave to be prefixed by a colon ':'.\nWe only gave a very short overview of SQL/PSM; the references at the end of\nthe chapter provide more information.\n\n214\nCHAPTER €i\n6.6\nCASE STUDY: THE INTERNET BOOK SHOP\nDBDudes finished logical database design, as discussed in Section 3.8, and now\nconsider the queries that they have to support. They expect that the applica-\ntion logic will be implemented in Java, and so they consider JDBC and SQLJ as\npossible candidates for interfacing the database system with application code.\nRecall that DBDudes settled on the following schema:\nBooks(isbn: CHAR(10), title: CHAR(8), author: CHAR(80),\nqty_in_stock: INTEGER, price: REAL, year_published: INTEGER)\nCustomers( cid: INTEGER, cname: CHAR(80), address: CHAR(200))\nOrders(ordernum: INTEGER, isbn: CHAR(lO), cid: INTEGER,\ncardnum: CHAR(l6), qty: INTEGER, order_date: DATE, ship_date: DATE)\nNow, DBDudes considers the types of queries and updates that will arise. They\nfirst create a list of tasks that will be performed in the application.\nTasks\nperformed by customers include the following.\nII\nCustomers search books by author name, title, or ISBN.\n..\nCustomers register with the website.\nRegistered customers might want\nto change their contact information.\nDBDudes realize that they have to\naugment the Customers table with additional information to capture login\nand password information for each customer; we do not discuss this aspect\nany further.\nIII\nCustomers check out a final shopping basket to complete a sale.\nIII\nCustomers add and delete books from a 'shopping basket' at the website.\n..\nCustomers check the status of existing orders and look at old orders.\nAdministrative ta.'3ks performed by employees of B&N are listed next.\nII\nEmployees look up customer contact information.\nIII\nEmployees add new books to the inventory.\n..\nEmployees fulfill orders, and need to update the shipping date of individual\nbooks.\n..\nEmployees analyze the data to find profitable customers and customers\nlikely to respond to special marketing campaigns.\nNext, DBDudes consider the types of queries that will a,rise out of these tasks.\nTo support searching for books by name, author, title, or ISBN, DBDudes\ndecide to write a stored procedure as follows:\n\nDatabase Application Development\nCREATE PROCEDURE SearchByISBN (IN book.isbn CHAR (10) )\nSELECT B.title, B.author,\nB.qty_in~'3tock,B.price, B.yeaLpublished\nFROM\nBooks B\nWHERE\nB.isbn = book.isbn\nPlacing an order involves inserting one or more records into the Orders table.\nSince DBDudes has not yet chosen the Java-based technology to program the\napplication logic, they assume for now that the individual books in the order\nare stored at the application layer in a Java array. To finalize the order, they\nwrite the following JDBC code shown in Figure 6.11, which inserts the elements\nfrom the array into the Orders table.\nNote that this code fragment assumes\nseveral Java variables have been set beforehand.\nString sql = \"INSERT INTO Orders VALUES(7, 7, 7, 7, 7, 7)\";\nPreparedStatement pstmt = con.prepareStatement(sql);\ncon.setAutoCommit(false);\ntry {\n/ / orderList is a vector of Order objects\n/ / ordernum is the current order number\n/ / dd is the ID of the customer, cardnum is the credit card number\nfor (int i=O; iiorderList.lengthO; i++)\n/ / now instantiate the parameters with values\nOrder currentOrder = orderList[i];\npstmt.clearParameters() ;\npstmt.setInt(l, ordernum);\npstmt.setString(2, Order.getlsbnO);\npstmt.setInt(3, dd);\npstmt.setString(4, creditCardNum);\npstmt.setlnt(5, Order.getQtyO);\npstmt.setDate(6, null);\npstmt.executeUpdate();\n}\ncon.commit();\ncatch (SqLException e){\ncon.rollbackO;\nSystem.out.println(e.getMessage());\n}\nFigure 6.11\nInserting a Completed Order into the Database\n\n216\nCHAPTER (}\nDBDudes writes other JDBC code and stored procedures for all of the remain-\ning tasks. They use code similar to some of the fragments that we have seen in\nthis chapter.\nII\nEstablishing a connection to a database, as shown in Figure 6.2.\nII\nAdding new books to the inventory, a'3 shown in Figure 6.3.\nII\nProcessing results from SQL queries a'3 shown in Figure 6.4-\nII\nFor each customer, showing how many orders he or she has placed.\nWe\nshowed a sample stored procedure for this query in Figure 6.8.\nII\nIncrea'3ing the available number of copies of a book by adding inventory,\nas shown in Figure 6.9.\nII\nRanking customers according to their purchases, as shown in Figure 6.10.\nDBDudcs takes care to make the application robust by processing exceptions\nand warnings, as shown in Figure 6.6.\nDBDudes also decide to write a trigger, which is shown in Figure 6.12. When-\never a new order is entered into the Orders table, it is inserted with ship~date\nset to NULL. The trigger processes each row in the order and calls the stored\nprocedure 'UpdateShipDate'. This stored procedure (whose code is not shown\nhere) updates the (anticipated) ship_date of the new order to 'tomorrow', in\ncase qtyjlLstock of the corresponding book in the Books table is greater than\nzero. Otherwise, the stored procedme sets the ship_date to two weeks.\nCREATE TRIGGER update_ShipDate\nAFTER INSERT ON Orders\nFOR EACH ROW\nBEGIN CALL UpdatcShipDate(new); END\n1* Event *j\n1* Action *j\nFigure 6.12\nTrigger to Update the Shipping Date of New Orders\n6.7\nREVIEW QUESTIONS\nAnswers to the i'eview questions can be found in the listed sections.\nlYl\nvVhy is it not straightforward to integrate SQL queries with a host pro-\ngramming language? (Section 6.1.1)\nIIii\nHow do we declare variables in Ernbcdded SQL? (Section 6.1.1)\n\nDatabase Applicat'ion Deuelop'Tnent\n217\n'*\n•\nHow do we use SQL statements within a host langl.lage? How do we check\nfor errors in statement execution? (Section 6.1.1)\n•\nExplain the impedance mismatch between host languages and SQL, and\ndescribe how cursors address this. (Section 6.1.2)\n•\n'\\That properties can cursors have? (Section 6.1.2)\n•\nWhat is Dynamic SQL and how is it different from Embedded SQL? (Sec-\ntion 6.1.3)\n•\nWhat is JDBC and what are its advantages? (Section 6.2)\n•\nWhat are the components of the JDBC architecture? Describe four differ-\nent architectural alternatives for JDBC drivers. (Section 6.2.1)\n•\nHow do we load JDBC drivers in Java code? (Section 6.3.1)\n•\nHow do we manage connections to data sources?\nWhat properties can\nconnections have? (Section 6.3.2)\n•\nWhat alternatives does JDBC provide for executing SQL DML and DDL\nstatements? (Section 6.3.3)\n•\nHow do we handle exceptions and warnings in JDBC? (Section 6.3.5)\n•\n'What functionality provides the DatabaseMetaDataclass? (Section 6.3.6)\n•\nWhat is SQLJ and how is it different from JDBC? (Section 6.4)\n•\nvVhy are stored procedures important? How do we declare stored proce-\ndures and how are they called from application code? (Section 6.5)\nEXERCISES\nExercise 6.1 Briefly answer the following questions.\n1. Explain the following terms: Cursor, Embedded SQL, JDBC, SQLJ, stored procedure.\n2. What are the differences between JDBC and SQLJ? \\Nhy do they both exist?\n3. Explain the term stored procedure, and give examples why stored procedures are useful.\nExercise 6.2 Explain how the following steps are performed in JDBC:\n1. Connect to a data source.\n2. Start, commit, and abort transactions.\n3. Call a stored procedure.\nHow are these steps performed in SQLJ?\n\n218\nCHAPTER (:)\nExercise 6.3 Compare exception handling and handling of warnings ill embedded SQL, dy-\nnamic SQL, .IDBC, and SQL.I.\nExercise 6.4 Answer the following questions.\n1. Why do we need a precompiler to translate embedded SQL and SQL.J? Why do we not\nneed a precompiler for .IDBC?\n2. SQL.J and embedded SQL use variables in the host language to pass parameters to SQL\nqueries, whereas .JDBC uses placeholders marked with a ''1'. Explain the difference, and\nwhy the different mechanisms are needed.\nExercise 6.5 A dynamic web site generates HTML pages from information stored in a\ndatabase.\nWhenever a page is requested, is it dynamically assembled from static data and\ndata in a database, resulting in a database access.\nConnecting to the database is usually\na\ntime~consuming process, since resources need to be allocated, and the user needs to be\nauthenticated.\nTherefore, connection pooling--setting up a pool of persistent database\nconnections and then reusing them for different requests can significantly improve the per-\nformance of database-backed websites.\nSince servlets can keep information beyond single\nrequests, we can create a connection pool, and allocate resources from it to new requests.\nWrite a connection pool class that provides the following methods:\nIII\nCreate the pool with a specified number of open connections to the database system.\n11II\nObtain an open connection from the pool.\nIII\nRelease a connection to the pool.\nIII\nDestroy the pool and close all connections.\nPROJECT-BASED EXERCISES\nIn the following exercises, you will create database-backed applications. In this chapter, you\nwill create the parts of the application that access the database.\nIn the next chapter, you\nwill extend this code to other &'3pects of the application. Detailed information about these\nexercises and material for more exercises can be found online at\nhttp://www.cs.wisc.edu/-dbbook\nExercise 6.6 Recall the Notown Records database that you worked with in Exercise 2.5 and\nExercise 3.15.\nYou have now been tasked with designing a website for Notown. It should\nprovide the following functionality:\nIII\nUsen; can sem'ch for records by name of the musician, title of the album, and Bame of\nthe song.\n11II\nUsers can register with the site, and registered users ca.n log on to the site. Once logged\non, users should not have to log on again unless they are inactive for a long time.\nIII\nUsers who have logged on to the site can add items to a shopping basket.\n11II\nUsers with items in their shopping basket can check out and ma.ke a purchase.\n\nDatabase Apphcation De'velopment\n219\nNOtOWIl wants to use JDBC to access the datab&<;e,\n\\¥rite .JDBC code that performs the\nnecessary data access and manipulation. You will integrate this code with application logic\nand presentation in the next chapter.\nIf Notown had chosen SQLJ instead of JDBC, how would your code change?\nExercise 6.7 Recall the database schema for Prescriptions-R-X that you created in\nExer~\ncise 2.7.\nThe Prescriptions-R-X chain of pharmacies has now engaged you to design their\nnew website. The website has two different classes of users: doctors and patients. Doctors\nshould be able to enter new prescriptions for their patients and modify existing prescriptions.\nPatients should be able to declare themselves as patients of a doctor; they should be able\nto check the status of their prescriptions online; and they should be able to purchase the\nprescriptions online so that the drugs can be shipped to their home address.\nFollow the analogous steps from Exercise 6.6 to write JDBC code that performs the nec-\nessary data access and manipulation. You will integrate this code with application logic and\npresentation in the next chapter.\nExercise 6.8 Recall the university database schema that you worked with in Exercise 5.l.\nThe university has decided to move enrollment to an online system.\nThe website has two\ndifferent classes of users: faculty and students. Faculty should be able to create new courses\nand delete existing courses, and students should be able to enroll in existing courses.\nFollow the analogous steps from Exercise 6.6 to write JDBC code that performs the nec-\nessary data access and manipulation. You will integrate this code with application logic and\npresentation in the next chapter.\nExercise 6.9 Recall the airline reservation schema that you worked on in Exercise 5.3. De-\nsign an online airline reservation system. The reservation system will have two types of users:\nairline employees, and airline passengers. Airline employees can schedule new flights and can-\ncel existing flights. Airline passengers can book existing flights from a given destination.\nFollow the analogous steps from Exercise 6.6 to write JDBC code that performs the nec-\nessary data access and manipulation. You will integrate this code with application logic and\npresentation in the next chapter.\nBIBLIOGRAPHIC NOTES\nInformation on ODBC can be found on Microsoft's web page (www.microsoft.com/data/odbc),\nand information on JDBC can be found on tlw Java web page (j ava. sun. com/products/jdbc).\nThere exist rnany books on ODBC, for example, Sanders' ODBC Developer's Guicle [652] and\nthe lvIicrosoft ODBC SDK [5:3;3].\nBooks on JDBC include works by Hamilton et al. [359],\nReese [621], and White et a!. [773].\n\n7\nINTERNET APPLICATIONS\nIt\nHow do we name resources on the Internet?\nIt\nHow do Web browsers and webservers communicate?\nIt\nHow do we present documents on the Internet? How do we differen-\ntiate between formatting and content?\nIt\nWhat is a three-tier application architecture? How do we write three-\ntiered applications?\nIt\nWhy do we have application servers?\n..\nKey concepts:\nUniform Resource Identifiers (URI), Uniform Re.-\nsource Locators (URL); Hypertext Transfer Protocol (HTTP), state-\nless protocol; Java; HTML; XML, XML DTD; three-tier architecture,\nclient-server architecture; HTML forms; JavaScript; cascading style\nsheets, XSL; application server; Common Gateway Interface (CGI);\nservlet; JavaServer Page (JSP); cookie\nWow! They've got the Internet on computers now!\n--Homer Simpson, The Simpsons\n7.1\nINTROpUCTION\nThe proliferation of computer networks, including the Internet and corporate\n'intranets,' has enabled users to access a large number of data sources. This\nincreased access to databases is likely to have a great practical impact; data\nand services can now be offered directly to customers in ways impossible until\n220\n\nIntc'T7wt Applications\n221\nrecently.\nExamples of such electronic commerce rtpplications include pur-\nchasing books through a \\Veb retailer such <1.'3 Amazon.com, engaging in online\nauctions at a site such as eBay, and exchanging bids and specifications for\nproducts between companies. The emergence of standards such as XrvIL for\ndescribing the content of documents is likely to further accelerate electronic\ncommerce and other online applications.\nWhile the first generation of Internet sites were collections of HTML files, most\nmajor sites today store a large part (if not all) of their data in database systems.\nThey rely on DBMSs to provide fast, reliable responses to user requests received\nover the Internet. This is especially true of sites for electronic commerce and\nother business applications.\nIn this chapter, we present an overview of concepts that are central to Internet\napplication development. We start out with a basic overview of how the Internet\nworks in Section 7.2. We introduce HTML and XML, two data formats that are\nused to present data on the Internet, in Sections 7.3 and 7.4. In Section 7.5, we\nintroduce three-tier architectures, a way of structuring Internet applications\ninto different layers that encapsulate different functionality.\nIn Sections 7.6\nand 7.7, we describe the presentation layer and the middle layer in detail; the\nDBMS is the third layer. We conclude the chapter by discussing our B&N case\nstudy in Section 7.8.\nExamples that appear in this chapter are available online at\nhttp://www.cs.wisc.edu/-dbbook\n7.2\nINTERNET CONCEPTS\nThe Internet has emerged as a universal connector between globally distributed\nsoftware systems. To understand how it works, we begin by discussing two ba\"lic\nissues: how sites on the Internet are identified, and how programs at one site\ncommunicate with other sites.\nvVe first introduce Uniform Resource Identifiers, a naming schema for locating\nresources on the Internet in Section 7.2.1. \\Ve then talk about the most popular\nprotocol for accessing resources over the Vv\"eh, the hypertext transfer protocol\n(HTTP) in Se(tion 7.2.2.\n7.2.1\nUniform Resource Identifiers\nUniform Resource Identifiers (URIs), are strings that uniquely identify\nresources\n011 the Internet.\nA resource is any kind of information that can\n\n222\nCHAPTER 7;\nj;istributed Applications and Service-Oriented Architectures:\nI\n~he advent of XML, due to its loosely-coupled nature, has made· infor-\nmation exchange between different applications feasible to an extent previ-\nously unseen. By using XML for information exchange, applications can be\nwritten in different programming languages, run on different operating sys-\ntems, and yet they can still share information with each other. There are\nalso standards for externally describing the intended content of an XML\nfile or message, most notably the recently adopted W3C XML Schemas\nstandard.\nA promising concept that has arisen out of the XML revolution is the notion\nof a Web service. A Web service is an application that provides a well-\ndefined service, packaged as a set of remotely callable procedures accessible\nthrough the Internet. Web services have the potential to enable powerful\nnew applications by composing existing Web services-all communicating\nseamlessly thanks to the use of standardizedXML-based information ex-\nchange. Several technologies have been developed or are currently under\ndevelopment that facilitate design and implementation of distributed ap-\nplications. SOAP is a W3C standard for XML-based invocation of remote\nservices (think XML RPC) that allows distributed applications to commu-\nnicate either synchronously or asynchronously via structured, typed XML\nmessages. SOAP calls can ride on a variety of underlying transport layers,\nincluding HTTP (part of what is making SOAP so successful) and vari-\nous reliable messaging layers. Related to the SOAP standard are W3C's\nWeb Services Description Language (WSDL) for describing Web\nservice interfaces, and Universal Description, Dis.;;overy, and Inte-\ngration (UDDI), a WSDL-based Web services registry standard (think\nyellow pages for Web services).\nSOAP-based Web services are the foundation for Microsoft's recently re-\nleased .NET framework, their application development infrastructure and\nassociated run-time system for developing distributed applications, as well\nas for the Web services offerings of major software vendors such as IBM,\nBEA, and others. Many large software application vendors (major compa-\nnies like PeopleSoft and SAP) have announced plans to provide Web service\ninterfaces to their products and the data that they manage, and many are\nhoping that XML and Web services will finally provide the answer to the\nlong-standing problem of enterprise application integration. Web services\nare also being looked to as a natural foundation for the next generation of\nbusiness process management (or workflow) systems.\n\nInternet Applications\n223\nbe identified by a URI, and examples include webpages, images, downloadable\nfiles, services that can be remotely invoked, mailboxes, and so on. The most\ncommon kind of resource is a static file (such as a HTML document), but a\nresource may also be a dynamically-generated HTML file, a movie, the output\nof a program, etc.\nA URI has three parts:\n•\nThe (name of the) protocol used to access the resource.\n•\nThe host computer where the resource is located.\n•\nThe path name of the resource itself on the host computer.\nConsider an example URI, such as http://www.bookstore.com/index .html.\nThis URI can be interpreted as follows. Use the HTTP protocol (explained in\nthe next section) to retrieve the document index. html located at the computer\nwww.bookstore.com.This example URI is an instance of a Universal Re-\nsource Locator (URL) , a subset of the more general URI naming scheme;\nthe distinction is not important for our purposes.\nAs another example, the\nfollowing HTML fragment shows a URI that is an email address:\n<a href=lImailto:webmaster@bookstore.com ll >Email the webmaster.</A>\n7.2.2\nThe Hypertext Transfer Protocol (HTTP)\nA communication protocol is a set of standards that defines the structure\nof messages between two communicating parties so that they can understand\neach other's messages. The Hypertext Transfer Protocol (HTTP) is the\nmost common communication protocol used over the Internet. It is a client-\nserver protocol in which a client (usually a Web browser) sends a request to an\nHTTP server, which sends a response back to the client. When a user requests\na webpage (e.g., clicks on a hyperlink), the browser sends HTTP request\nmessages for the objects in the page to the server. The server receives the\nrequests and responds with HTTP response messages, which include the\nobjects. It is important to recognize that HTTP is used to transmit all kinds\nof resources, not just files, but most resources on the Internet today are either\nstatic files or :(lIes output from server-side scripts.\nA variant of the HTTP protocol called the Secure Sockets Layer (SSL)\nprotocol uses encryption to exchange information securely between client and\nserver. We postpone a discussion of SSL to Section 21.5.2 and present the basic\nHTTP protocol in this chapter.\n\n224\nCHAPTER\nAs an example, consider what happens if a user clicks on the following link:\nhttp://www.bookstore.com/index .html. 'We first explain the structure of an\nHTTP request message and then the structure of an HTTP response message.\nHTTP Requests\nThe client (\\\\Teb browser) establishes a connection with the webserver that\nhosts the resource and sends a HTTP request message. The following example\nshows a sample HTTP request message:\nGET index.html HTTP/l.l\nUser-agent: Mozilla/4.0\nAccept: text/html, image/gif, image/jpeg\nThe general structure of an HTTP request consists of several lines of ASCII\ntext, with an empty line at the end. The first line, the request line, has three\nfields: the HTTP method field, the URI field, and the HTTP version\nfield. The method field can take on values GET and POST; in the exam-\nple the message requests the object index. html. (We discuss the differences\nbetween HTTP GET and HTTP POST in detail in Section 7.11.) The version\nfield indicates which version of HTTP is used by the client and can be used\nfor future extensions of the protocol. The user agent indicates the type of\nthe client (e.g., versions of Netscape or Internet Explorer); we do not discuss\nthis option further. The third line, starting with Accept, indicates what types\nof files the client is willing to accept.\nFor example, if the page index. html\ncontains a movie file with the extension .mpg, the server will not send this file\nto the client, as the client is not ready to accept it.\nHTTP Responses\nThe server responds with an HTTP response message. It retrieves tht: page\nindex. html, uses it to assemble the HTTP response message, and sends the\nmessage to the client. A sample HTTP response looks like this:\nHTTP/l.l 200 OK\nDate: Mon, 04 Mar 2002 12:00:00 GMT\nContent-Length: 1024\nContent-Type: text/html\nLast-Modified: Mall, 22 JUIl 1998 09:23:24 GMT\n<HTML>\n<HEAD>\n</HEAD>\n<BODY>\n\nInternet Applications\n<H1>Barns and Nobble Internet Bookstore</H1>\nOur inventory:\n<H3>Science</H3>\n<B>The Character of Physical Law</B>\n225\nThe HTTP response message has three parts:\na status line, several header\nlines, and the body of the message (which contains the actual object that the\nclient requested). The status line has three fields (analogous to the request\nline of the HTTP request message): the HTTP version (HTTP/1.1), a status\ncode (200), and an associated server message (OK). Common status codes and\nassociated messages are:\n•\n200 OK: The request succeeded and the object is contained in the body of\nthe response message\";\n•\n400 Bad Request: A generic error code indicating that the request could\nnot be fulfilled by the server.\n•\n404 Not Found: The requested object does not exist on the server.\n•\n505 HTTP Version Not Supported: The HTTP protocol version that the\nclient uses is not supported by the server. (Recall that the HTTP protocol\nversion sent in the client's request.)\nOur example has three header lines: The date header line indicates the time\nand date when the HTTP response was created (not that this is not the object\ncreation time). The Last-Modified header line indicates when the object was\ncreated. The Content-Length header line indicates the number of bytes in the\nobject being sent after the last header line.\nThe Content-Type header line\nindicates that the object in the entity body is HTML text.\nThe client (the Web browser) receives the response message, extracts the HTML\nfile, parses it, and displays it. In doing so, it might find additional URIs in the\nfile, and it then uses the HTTP protocol to retrieve each of these resources,\nestablishing a new connection each time.\nOne important issue is that the HTTP protocol is a stateless protocol. Every\nmessage----from, the client to the HTTP server and vice-versa-is self-contained,\nand the connection established with a request is maintained only until the\nresponse message is sent. The protocol provides no mechanism to automatically\n'remember' previous interactions between client and server.\nThe stateless nature of the HTTP protocol has a major impact on how Inter-\nnet applications are written. Consider a user who interacts with our exalIlple\n\n226\nCHAPTER ,7\nbookstore application.\nAssume that the bookstore permits users to log into\nthe site and then carry out several actions, such as ordering books or changing\ntheir address, without logging in again (until the login expires or the user logs\nout). How do we keep track of whether a user is logged in or not? Since HTTP\nis stateless, we cannot switch to a different state (say the 'logged in' state) at\nthe protocol level. Instead, for every request that the user (more precisely, his\nor her Web browser) sends to the server, we must encode any state information\nrequired by the application, such as the user's login status. Alternatively, the\nserver-side application code must maintain this state information and look it\nup on a per-request basis. This issue is explored further in Section 7.7.5.\nNote that the statelessness of HTTP is a tradeoff between ease of implementa-\ntion of the HTTP protocol and ease of application development. The designers\nof HTTP chose to keep the protocol itself simple, and deferred any functionality\nbeyond the request of objects to application layers above the HTTP protocol.\n7.3\nHTML DOCUMENTS\nIn this section and the next, we focus on introducing HTML and XML. In\nSection 7.6, we consider how applications can use HTML and XML to create\nforms that capture user input, communicate with an HTTP server, and convert\nthe results produced by the data management layer into one of these formats.\nHTML is a simple language used to describe a document. It is also called a\nmarkup language because HTML works by augmenting regular text with\n'marks' that hold special meaning for a Web browser. Commands in the lan-\nguage, called tags, consist (usually) of a start tag and an end tag of the\nform <TAG> and </TAG>, respectively. For example, consider the HTML frag-\nment shown in Figure 7.1. It describes a webpage that shows a list of books.\nThe document is enclosed by the tags <HTML> and </HTML>, marking it as an\nHTML document.\nThe remainder of the document-enclosed in <BODY> ...\n</BoDY>-contains information about three books. Data about each book is\nrepresented as an unordered list (UL) whose entries are marked with the LI\ntag. HTML defines the set of valid tags as well 8.'3 the meaning of the tags. :For\nexample, HTML specifies that the tag <TITLE> is a valid tag that denotes the\ntitle of the document.\nAs another example, the tag <UL> always denotes an\nunordered list.\nAudio, video, and even programs (written in Java, a highly portable language)\ncan be included in HTML documents. vVhen a user retrieves such a document\nusing a suitable browser, images in the document arc displayed, audio and video\nclips are played, and embedded programs are executed at the uset's machine;\nthe result is a rich multimedia presentation. The e8.\"ie with which HTML docu-\n\nInternet Applications\n<HTML>\n<HEAD>\n</HEAD>\n<BODY>\n<Hl>Barns and Nobble Internet Bookstore</Hl>\nOur inventory:\n<H3>Science</H3>\n<B>The Character of Physical Law</B>\n<UL>\n<LI>Author: Richard Feynman</LI>\n<LI>Published 1980</LI>\n<Ll>Hardcover</LI>\n</UL>\n<H3>Fiction</H3>\n<B>Waiting for the Mahatma</B>\n<UL>\n<LI>Author: R.K. Narayan</LI>\n<LI>Published 1981</Ll>\n</UL>\n<B>The English Teacher</B>\n<UL>\n<LI>Author: R.K. Narayan</LI>\n<LI>Published 1980</LI>\n<LI>Paperback</LI>\n</UL>\n</BODY>\n</HTML>\nFigure 7.1\nBook Listing in HTML\n227\n»\nments can be created--there are now visual editors that automatically generate\nHTML----and accessed using Internet browsers has fueled the explosive growth\nof the Web.\n7.4\nXML DOCUMENTS\nIn this section, we introduce XML a.'3 a document format, and consider how\napplications can utilize XML. Managing XML documents in a DBMS poses\nseveral new challenges; we discuss this a.'3pect of XML in Chapter 27.\n\n228\nCHAPTER l\nvVhile HTl\\.fL can be used to mark up documents for display purposes, it is\nnot adequate to describe the structure of the content for more general applica-\ntions. For example, we can send the HTML document shown in Figure 7.1 to\nanother application that displays it, but the second application cannot distin-\nguish the first names of authors from their last names. (The application can\ntry to recover such information by looking at the text inside the tags, but this\ndefeats the purpose of using tags to describe document structure.) Therefore,\nHTML is unsuitable for the exchange of complex documents containing product\nspecifications or bids, for example.\nExtensible Markup Language (XML) is a markup language developed to\nremedy the shortcomings of HTML. In contrast to a fixed set of tags whose\nmeaning is specified by the language (as in HTML), XML allows users to de-\nfine new collections of tags that can be used to structure any type of data or\ndocument the user wishes to transmit. XML is an important bridge between\nthe document-oriented view of data implicit in HTML and the schema-oriented\nview of data that is central to a DBMS. It has the potential to make database\nsystems more tightly integrated into Web applications than ever before.\nXML emerged from the confluence of two technologies, SGML and HTML. The\nStandard Generalized Markup Language (SGML) is a metalanguage\nthat allows the definition of data and document interchange languages such as\nHTML. The SGML standard was published in 1988, and many organizations\nthat rnanage a large number of complex documents have adopted it. Due to its\ngenerality, SGML is complex and requires sophisticated programs to harness\nits full potential.\nXML was developed to have much of the power of SGML\nwhile remaining relatively simple.\nNonetheless, XML, like SGML, allows the\ndefinition of new document markup languages.\nAlthough XML does not prevent a user from designing tags that encode the\ndisplay of the data in a Web browser, there is a style language for XML called\nExtensible Style Language (XSL). XSL is a standard way of describing\nhow an XML docmnent that adheres to a certain vocabulary of tags should be\ndisplayed.\n7.4.1\nIntroduction to XML\nVve use the smaJI XML docmnent shown in Figure 7.2 a,s an example.\n11II\nElements: Elements, also called tags, a.rc the primary building blocks of\nan XML docmnent. The start of the content of an element ELM is marked\nwith <ELM>, which is called the start tag, and the end of the content end\nis marked with </ELM>, called the end tag.\nIn our example document.\n\nInternet Applicai'ions\n229\nThe Design Goals ofXML: XML wa..\"l developed'startingin 1996 by a\nworking group under guidance of the ';Yorld Wide Web Consortium (W3C)\nXML Special Interest Group.\nThe design goals for XML included the\nfollowing:\n1. XML should be compatible with SGML.\n2. It should be easy to write programs that process XML documents.\n3. The design of XML should be formal and concise.\nthe element BOOKLIST encloses all information in the sample document.\nThe element BOOK demarcates all data associated with a single book.\nXML elements are case sensitive:\nthe element BOOK is different from\nBook.\nElements must be properly nested. Start tags that appear inside\nthe content of other tags must have a corresponding end tag. For example,\nconsider the following XML fragment:\n<BOOK>\n<AUTHOR>\n<FIRSTNAME>Richard</FIRSTNAME>\n<LASTNAME>Feynluan</LASTNAME>\n</AUTHOR>\n</BOOK>\nThe element AUTHOR is completely nested inside the element BOOK, and\nboth the elements LASTNAME and FIRSTNAME are nested inside the element\nAUTHOR.\n..\nAttributes: An element can have descriptive attributes that provide ad-\nditional information about the element. The values of attributes are set\ninside the start tag of an element. For example, let ELM denote an element\nwith the attribute att. We can set the value of att to value through the\nfollowing expression:\n<ELM att=\" value II >.\nAll attribute values must be\nenclosed in quotes.\nIn Figure 7.2, the element BOOK has two attributes.\nThe attribute GENRE indicates the genre of the book (science or fiction)\nand the attribute FORMAT indicates whether the book is a hardcover or a\npaperback.\nIII\nEntity References: Entities are shortcuts for portions of common text or\nthe content of external files, and we call the usage of an entity in the XML\ndocument an entity reference. Wherever an entity reference appears in\nthe document, it is textually replaced by its content.\nEntity references\nstart with a '&' and end with a '; '. Five predefined entities in XML are\nplaceholders for chara.cters with special meaning in XML. For example, the\n\n230\nCHAPTER~7\n<?xml version=11.0\" encoding=\"UTF-S Il standalone=llyes ll?>\n<BOOKLIST>\n<BOOK GENRE=\" Science\" FORMAT=\" Hardcover\" >\n<AUTHOR>\n<FIRSTNAME>Richard</FIRSTNAME>\n<LASTNAME>Feynman</LASTNAME>\n</AUTHOR>\n<TITLE>The Character of Physical Law</TITLE>\n<PUBLISHED>1980</PUBLISHED>\n</BOOK>\n<BOOK> GENRE=\" Fiction\" >\n<AUTHOR>\n<FIRSTNAME>R.K.</FIRSTNAME>\n<LASTNAME>Narayan</LASTNAME>\n</AUTHOR>\n<TITLE>Waiting for the Mahatma</TITLE>\n<PUBLISHED>1981</PUBLISHED>\n</BOOK>\n<BOOK GENRE=\" Fiction\" >\n<AUTHOR>\n<FIRSTNAME>R.K.</FIRSTNAME>\n<LASTNAME>Narayan</LASTNAME>\n</AUTHOR>\n<TITLE>The English Teacher</TITLE>\n<PUBLISHED>1980</PUBLISHED>\n</BOOK>\n</BOOKLIST>\nFigure 7.2\nBook Information in XML\n< character that marks the beginning of an XML command is reserved and\nhas to be represented by the entity It. The other four reserved characters\nare &, >, \", and '; they are represented by the entities amp, gt, quot,\nand apos.\nFor example, the text '1 < 5' has to be encoded in an XML\ndocument &'3 follows:\n&apos; 1&1t ;5&apos;. We can also use entities to\ninsert arbitrary Unicode characters into the text. Unicode is a standard\nfor character representations, similar to ASCII. For example, we can display\nthe Japanese Hiragana character a using the entity reference &#x3042.\n•\nComments: We can insert comments anywhere in an XML document.\nComments start with <! - and end with ->. Comments can contain arbi-\ntrary text except the string --.\n\nInternet Applications\n•\nDocument Type Declarations (DTDs): In XML, we can define our\nown markup language. A DTD is a set of rules that allows us to specify\nour own set of elements, attributes, and entities. Thus, a DTD is basically\na grammar that indicates what tags are allowed, in what order they can\nappear, and how they can be nested. We discuss DTDs in detail in the\nnext section.\nWe call an XML document well-formed if it has no associated DTD but\nfollows these structural guidelines:\n•\nThe document starts with an XML declaration. An example of an XML\ndeclaration is the first line of the XML document shown in Figure 7.2.\n•\nA root element contains all the other elements. In our example, the root\nelement is the element BOOKLIST.\n•\nAll elements must be properly nested. This requirement states that start\nand end tags of an element must appear within the same enclosing element.\n7.4.2\nXML DTDs\nA DTD is a set of rules that allows us to specify our own set of elements,\nattributes, and entities. A DTD specifies which elements we can use and con-\nstraints on these elements, for example, how elements can be nested and where\nelements can appear in the document. We call a document valid if a DTD is\nassociated with it and the document is structured according to the rules set by\nthe DTD. In the remainder of this section, we use the example DTD shown in\nFigure 7.3 to illustrate how to construct DTDs.\n<!DOCTYPE BOOKLIST [\n<! ELEMENT BOOKLIST (BOOK)*>\n<! ELEMENT BOOK (AUTHOR,TITLE,PUBLISHED?»\n<!ELEMENT AUTHOR (FIRSTNAME,LASTNAME»\n<! ELEMENT FIRSTNAME (#PCDATA»\n<! ELEMENT LASTNAME (#PCDATA»\n<! ELEMENT TITLE (#PCDATA»\n<! ELEMENT PUBLISHED (#PCDATA»\n<! ATTLIST BOOK GENRE (ScienceIFiction) #REQUIRED>\n<!ATTLIST BOOK FORMAT (PaperbackIHardcover) \"Paperback\">\n]>\nFigure 7.3\nBookstore XML DTD\n\n232\nCHAPTER .;{\nA DTD is enclosed in <! DOCTYPE name [DTDdeclarationJ >, where name is\nthe name of the outermost enclosing tag, and DTDdeclaration is the text of\nthe rules of the DTD. The DTD starts with the outermost element---the root\nelenwnt--which is BOOKLIST in our example. Consider the next rule:\n<!ELEMENT BOOKLIST (BOOK)*>\nThis rule tells us that the element BOOKLIST consists of zero or more BOOK\nelements.\nThe * after BOOK indicates how many BOOK elements can appear\ninside the BOOKLIST element. A * denotes zero or more occurrences, a + denotes\none or more occurrences, and a? denotes zero or one occurrence. For example,\nif we want to ensure that a BOOKLIST has at least one book, we could change\nthe rule as follows:\n<!ELEMENT BOOKLIST (BOOK)+>\nLet us look at the next rule:\n<!ELEMENT BOOK (AUTHOR,TITLE,PUBLISHED?»\nThis rule states that a BOOK element contains a AUTHOR element, a TITLE ele-\nment, and an optional PUBLISHED clement. Note the use of the? to indicate\nthat the information is optional by having zero or one occurrence of the element.\nLet us move ahead to the following rule:\n< !ELEMENT LASTNAME (#PCDATA»\nUntil now we considered only elements that contained other elements.\nThis\nrule states that LASTNAME is an element that does not contain other elements,\nbut contains actual text. Elements that only contain other elements are said\nto have element content, whereas elements that also contain #PCDATA are\n::laid to have mixed content. In general, an element type declaration has the\nfollowing structure:\n< !ELEMENT (contentType»\nFive possible content types are:\nIII\nOther elements.\nII\nThe special syrnbol #PCDATA, which indicates (parsed) character data.\nII\nThe special symbol EMPTY, which indicates that the element has no content.\nElements that have no content are not required to have an end tag.\n11II\nThe special symbol ANY, which indicates that any content is permitted.\nThis content should be avoided whenever possible ::lince it disables all check-\ning of the document structure inside the element.\n\nInternet Apphcat'ions\n2~3\n•\nA regular expression constructed from the preceding four choices.\nA\nregular expression is one of the following:\n- expL exp2, exp3: A list of regular expressions.\n- exp*: An optional expression (zero or more occurrences).\n- exp?: An optional expression (zero or one occurrences).\n- exp+: A mandatory expression (one or more occurrences).\n- expl\nI exp2: expl or exp2.\nAttributes of elements are declared outside the element. For example, consider\nthe following attribute declaration from Figure 7.3:\n<! ATTLIST BOOK GENRE (ScienceIFiction) #REQUIRED»\nThis XML DTD fragment specifies the attribute GENRE, which is an attribute\nof the element BOOK. The attribute can take two values:\nScience or Fiction.\nEach BOOK element must be described in its start tag by a GENRE attribute\nsince the attribute is required as indicated by #REQUIRED. Let us look at the\ngeneral structure of a DTD attribute declaration:\n<! ATTLIST elementName (attName attType default)+>\nThe keyword ATTLIST indicates the beginning of an attribute declaration. The\nstring elementName is the name of the element with which the following at-\ntribute dcfinition is associated. What follows is the declaration of one or more\nattributes. Each attribute has a name, as indicated by attName, and a type,\nas indicated by attType. XML defines several possible types for an attribute.\nWe discuss only string types and enumerated types here. An attribute of\ntype string can take any string as a value. We can declare such an attribute by\nsetting its type field to CDATA. F'or example, we can declare a third attribute of\ntype string of the elernent BOOK a.s follows:\n<!ATTLIST BOOK edition CDATA \"1\">\nIf an attribute has an enumerated type, we list all its possible values in the\nattribute declaration. In our example, the itttribute GENRE is an enumerated\nattribute type; its possible attribute values are 'Science' and 'Fiction'.\nThe last part 'Of an attribute declaration is called its default specification.\nThe DTD in Figure 7.:3 shows two different default specifications: #REQUIRED\nitnd the string 'Pitperback'. The default specification #REQUIRED indicates that\nthe attribute is required and whenever its associated element itppears some-\nwhere in the XML document\n~t value for the attribute must be specified. The\ndebult specification indicated by the string 'Paperback' indicates that the at-\ntribute is not required; whenever its a.')sociated element itppears without setting\n\n234\n<?xml version=11.0\" encoding=IUTF-8\" standalone=\"no\"?>\n<! DOCTYPE BOOKLIST SYSTEM\" books.dtd\" >\n<BOOKLIST>\n<BOOK GENRE=\" Science\" FORMAT=\" Hardcover\" >\n<AUTHOR>\nFigure 7.4\nBook Information in XML\nXML Schema: The DTD mechanism has several limitations, in spite of\nits widespread use.\nFor example, elements and attributes cannot be as-\nsigned types in a flexible way, and elements are always ordered, even if the\napplication does not require this. XML Schema is a new W3C proposal\nthat provides a more powerful way to describe document structure than\nDTDs; it is a superset of DTDs, allowing legacy data to be handled eas-\nily.\nAn interesting aspect is that it supports uniqueness and foreign key\nconstraints.\na value for the attribute, the attribute automatically takes the value 'Paper-\nback'. For example, we can make the attribute value 'Science' the default value\nfor the GENRE attribute as follows:\n<! ATTLIST BOOK GENRE (ScienceIFiction) \"Science\" >\nIn our bookstore example, the XML document with a reference to the DTD is\nshown in Figure 7.4.\n7.4.3\nDomain-Specific DTDs\nRecently, DTDs have been developed for several specialized domains-including\na wide range of commercial, engineering, financial, industrial, and scientific\ndomains----and a lot of the excitement about XML h3...<; its origins in the belief\nthat more and more standardized DTDs will be developed. Standardized DTDs\nwould enable seamless data exchange among heterogeneous sources, a problem\nsolved today either by implementing specialized protocols such as Electronic\nData Interchange (EDI) or by implementing ad hoc solutions.\nEven in an environment where all XML data is valid, it is not possible to\nstraightforwardly integrate several XML documents by matching elements in\ntheir DTDs, because even when two elements have identical names in two\ndifferent DTDs, the meaning of the elements could be completely different.\nIf both documents use a single, standard DTD, we avoid this problem. The\n\nInternet Applications\n235\ndevelopment of standardized DTDs is more a social process than a research\nproblem, since the major players in a given domain or industry segment have\nto collaborate.\nFor example, the mathematical markup language (MathML) has been\ndeveloped for encoding mathematical material on the Web.\nThere are two\ntypes of MathML elements. The 28 presentation elements describe the lay-\nout structure of a document; examples are the mrow element, which indicates a\nhorizontal row of characters, and the msup element, which indicates a base and a\nsubscript. The 75 content elements describe mathematical concepts. An ex-\nample is the plus element, which denotes the addition operator. (A third type\nof element, the math element, is used to pass parameters to the MathML pro-\ncessor.) MathML allows us to encode mathematical objects in both notations\nsince the requirements of the user of the objects might be different. Content\nelements encode the precise mathematical meaning of an object without ambi-\nguity, and the description can be used by applications such as computer algebra\nsystems. On the other hand, good notation can suggest the logical structure to\na human and emphasize key aspects of an object; presentation elements allow\nus to describe mathematical objects at this level.\nFor example, consider the following simple equation:\nx 2 - 4x - 32 = 0\nUsing presentation elements, the equation is represented as follows:\n<mrow>\n<mrow> <msup><mi>x</mi><mn>2</mn></msup>\n<mo>-</mo>\n<mrow><mn>4</mn>\n<mo>&invisibletimes;</mo>\n<mi>x</mi>\n</mrow>\n<mo>-</ mo><mn>32</ mn>\n</mrow><mo>=</mo><mn>O</nm>\n</mrow>\nUsing content elements, the equation is described as follows:\n<reln><eq/>\n<apply>\n<minus/>\n<apply> <power/> <ci>x</ci> <cn>2</cn> </apply>\n<apply> <times/> <cn>4</cn> <ci>x</ci> </apply>\n<cn>32</cn>\n\n236\n</apply> <cn>O</cn>\n</reln>\nCHAPTER J7\nNote the additional power that we gain from using MathML instead of en-\ncoding the formula in HTML. The common way of displaying mathematical\nobjects inside an HTML object is to include images that display the objects,\nfor example, as in the following code fragment:\n<IMG SRC=lIimages/equation.gifll ALI=II x**2 - 4x - 32 = 10 II >\nThe equation is encoded inside an IMG tag with an alternative display format\nspecified in the ALI tag. Using this encoding of a mathematical object leads\nto the following presentation problems.\nFirst, the image is usually sized to\nmatch a certain font size, and on systems with other font sizes the image is\neither too small or too large. Second, on systems with a different background\ncolor, the picture does not blend into the background and the resolution of the\nimage is usually inferior when printing the document.\nApart from problems\nwith changing presentations, we cannot easily search for a formula or formula\nfragments on a page, since there is no specific markup tag.\n7.5\nTHE THREE-TIER APPLICATION ARCHITECTURE\nIn this section, we discuss the overall architecture of data-intensive Internet\napplications. Data-intensive Internet applications can be understood in terms\nof three different functional components: data management, application logic,\nand pTesentation. The component that handles data mallgement usually utilizes\na DBMS for data storage, but application logic and presentation involve much\nmore than just the DBMS itself.\nWe start with a short overview of the history of database-backed application\narchitectures, and introduce single-tier and client-server architectures in Section\n7.5.1. \\Ve explain the three-tier architecture in detail in Section 7.5.2, and show\nits advantages in Section 7.5.3.\n7.5.1\nSingle-Tier and Client-Server Architectures\nIn this section, we provide some perspective on the three-tier architecture by\ndiscussing single-tier and client-server architectures, the predecessors of the\nthree-tier architecture. Initially, data-intensive applications were combined into\na single tier, including the DBMS, application logic, and user interface, a\"\nillustrated in Figure 7.5. The application typically ran on a mainframe, and\nusers accessed it through dumb teT'minals that could perform only data input\nand display.\nThis approach ha..s the benefit of being easily maintained by a\ncentral administrator.\n\nInteTnet Applications\nClient\nApplication Logic\nDBMS\nj\nFigure 7.5\nA Single-Tier Architecture\nFigure 7.6\nA Two-Server Architecture: Thin Clients\nSingle-tier architectures have a,n important drawback: Users expect graphical\ninterfaces that require much more computational power than simple dumb ter-\nminals.\nCentralized computation of the graphical displays of such interfaces\nrequires much more computational power than a single server hclS available,\nand thus single-tier architectures do not scale to thousands of users. The com-\nmoditization of the PC and the availability of cheap client computers led to\nthe developlnent of the two-tier architecture.\nTwo-tier architectures, often also referred to a<; client-server architec-\ntures, consist of a client computer and a server computer, which interact\nthrough a well-defined protocol. What part of the functionality the client im-\nplements, and what part is left to the server, can vary. In the traditional client-\nserver architecture, the client implements just the graphical user interface,\nand the server. implements both the business logic and the data management;\nsuch clients are often called thin clients, and this architecture is illustra,ted in\nFigure 7.6.\nOther divisions are possible, such as more powerful clients that hnplement both\nuser interface and business logic, or clients that implement user interface and\npart of the business logic, with the remaining part being implemented at the\n\n238\n//~\\\nI--\"-\"-'---~-~I\n1\\/\nChent\n.. j\nI\n\\ I\nApplication Logic I\ni\ni\n/\n~.\n__-1\nClient\nApplication Logic\nFigure 7.7\nA Two-Tier Architecture: Thick Clients\nCHAPTERt7\nserver level; such clients are often called thick clients, and this architecture is\nillustrated in Figure 7.7.\nCompared to the single-tier architecture, two-tier architectures physically sep-\narate the user interface from the data management layer. To implement two-\ntier architectures, we can no longer have dumb terminals on the client side;\nwe require computers that run sophisticated presentation code (and possibly,\napplication logic).\nOver the last ten years, a large number of client-server development tools such\nMicrosoft Visual Basic and Sybase Powerbuilder have been developed. These\ntools permit rapid development of client-server software, contributing to the\nsuccess of the client-server model, especially the thin-client version.\nThe thick-client model has several disadvantages when compared to the thin-\nclient model. First, there is no central place to update and maintain the busi-\nness logic, since the application code runs at many client sites. Second, a large\namount of trust is required between the server and the clients. As an exam--\npIe, the DBMS of a bank has to trust the (application executing at an) ATM\nmachine to leave the database in a consistent state. (One way to address this\nproblem is through stored procedures, trusted application code that is registered\nwith the DBMS and can be called from SQL statelnents. 'Ve discuss stored\nprocedures in detail in Section 6.5.)\nA third disadvantage of the thick-client architecture is that it does not scale\nwith the number of clients; it typically cannot handle more than a few hundred\nclients.\nThe application logic at the client issues SQL queries to the server\nand the server returns the query result to the client, where further processing\ntakes place. Large query results might be transferred between client and server.\n\nInter-net Applications\n2:19\nApplication\nLogic\nClient\n• • •\nClient\nFigure 7.8\nA Standard Three-Tier Architecture\n(Stored procedures can mitigate this bottleneck.) Fourth, thick-client systems\ndo not scale as the application accesses more and more database systems. As-\nsume there are x different database systems that are accessed by y clients, then\nthere are x . y different connections open at any time, clearly not a scalable\nsolution.\nThese disadvantages of thick-client systems and the widespread adoption of\nstandard, very thin clients~notably,Web browsers~haveled to the widespread\nuse thin-client architectures.\n7.5.2\nThree~Tier Architectures\nThe thin-client two-tier architecture essentially separates presentation issues\nfrom the rest of the application.\nThe three-tier architecture goes one step\nfurther, and also separates application logic from data management:\nIII\nPresentation Tier: Users require a natural interface to make requests,\nprovide input, and to see results. The widespread use of the Internet has\nmade Web-based interfaces increasingly popular.\nIII\nMiddle Tier: The application logic executes here.\nAn enterprise-class\napplication reflects complex business processes, and is coded in a general\npurpose language such as C++ or Java.\nIII\nData Management Tier: Data-intensive Web applications involve DBMSs,\nwhich are the subject of this book.\nFigure 7.8 shows a basic three-tier architecture.\nDifferent technologies have\nbeen developed to enable distribution of the three tiers of an application across\nmultiple hardware platforms and different physical sites. Figure 7.9 shows the\ntechnologies relevant to each tier.\n\n240\n~i~-----\",\".~._,----~~---.~u·\"r-----··'\"\n,\nClient Program\nI\nJavaScript\n(Web Br_~=:~)__~_~_~~~~~__________\nJ\nHTIP\n---:;;\"\"io:~~I--\"\nservle~~~----l\nJSP\nI\n~PPlication Server) ~__.__.__ .....XSLT\n~\nJDBe. SQLJ\nData Storage---I----- XML -\n(Database system)__\n___ Stored p:cedure~~__\nFigure 7.9\nTechnologies for the Three Tiers\nOverview of the Presentation Tier\nCHAPTERi' 7\nAt the presentation layer, we need to provide forms through which the user\ncan issue requests, and display responses that the middle tier generates. The\nhypertext markup language (HTML) discussed in Section 7.3 is the basic data\npresentation language.\nIt is important that this layer of code be easy to adapt to different display\ndevices and formats; for example, regular desktops versus handheld devices\nversus cell phones. This adaptivity can be achieved either at the middle tier\nthrough generation of different pages for different types of client, or directly at\nthe client through style sheets that specify how the data should be presented.\nIn the latter case, the middle tier is responsible for producing the appropriate\ndata in response to user requests, whereas the presentation layer decides how\nto display that information.\n\\Ve cover presentation tier technologies, including style sheets, in Section 7.6.\nOverview of the Middle Tier\nThe middle layer runs code that implements the business logic of the applica-\ntion: It controls what data needs to be input before an action can be executed,\ndetermines the control flow between multi-action steps, controls access to the\ndatabase layer, and often assembles dynamically generated HTML pages from\ndataba\"se query results.\n\nInternet Applications\n24;1\nThe middle tier code is responsible for supporting all the different roles involved\nin the application. For example, in an Internet shopping site implementation,\nwe would like customers to be able to browse the catalog and make purchases,\nadministrators to be able to inspect current inventory, and possibly data ana-\nlysts to ask summary queries about purchase histories. Each of these roles can\nrequire support for several complex actions.\nFor example, consider the a customer who wants to buy an item (after browsing\nor searching the site to find it). Before a sale can happen, the customer has\nto go through a series of steps: She has to add items to her shopping ba.sket,\nshe has to provide her shipping address and credit card number (unless she has\nan account at the site), and she has to finally confirm the sale with tax and\nshipping costs added. Controlling the flow among these steps and remembering\nalready executed steps is done at the middle tier of the application. The data\ncarried along during this series of steps might involve database accesses, but\nusually it is not yet permanent (for example, a shopping basket is not stored\nin the database until the sale is confirmed).\nWe cover the middle tier in detail in Section 7.7.\n7.5.3\nAdvantages of the Three-Tier Architecture\nThe three-tier architecture has the following advantages:\n1/\nHeterogeneous Systems: Applications can utilize the strengths of dif-\nferent platforms and different software components at the different tiers.\nIt is easy to modify or replace the code at any tier without affecting the\nother tiers.\nII\nThin Clients: Clients only need enough computation power for the pre-\nsentation layer. Typically, clients are Web browsers.\nII\nIntegrated Data Access: In many applications, the data must be ac-\ncessed from several sources.\nThis can be handled transparently at the\nmiddle tier, where we can centrally manage connections to all database\nsystems involved.\nII\nScalabilit,y to Many Clients: Each client is lightweight and all access to\nthe system is through the middle tier. The middle tier can share database\nconnections across clients, and if the middle tier becomes the bottle-neck,\nwe can deploy several servers executing the middle tier code; clients can\nconnect to anyone of these servers, if the logic is designed appropriately.\nThis is illustrated in Figure 7.10, which also shows how the middle tier\naccesses multiple data sources. Of course, we rely upon the DBMS for each\n\n242\nCHAPTER\nFigure 7.10\nMiddle~Tier Replication and Access to Multiple Data Sources\n7\ndata source to be scalable (and this might involve additional parallelization\nor replication, as discussed in Chapter 22).\n•\nSoftware Development Benefits: By dividing the application cleanly\ninto parts that address presentation, data access, and business logic, we\ngain many advantages. The business logic is centralized, and is therefore\neasy to maintain, debug, and change.\nInteraction between tiers occurs\nthrough well-defined, standardized APls. Therefore, each application tier\ncan be built out of reusable components that can be individually developed,\ndebugged, and tested.\n7.6\nTHE PRESENTATION LAYER\nIn this section, we describe technologies for the client side of the three-tier ar-\nchitecture. vVe discuss HTML forms as a special means of pa.ssing arguments\nfrom the client to the middle tier (i.e., from the presentation tier to the middle\ntier) in Section 7.6.1. In Section 7.6.2, we introduce JavaScript, a Java-based\nscripting language that can be used for light-weight computation in the client\ntier (e.g., for simple animations). We conclude our discussion of client-side tech-\nnologies by presenting style sheets in Section 7.6.3. Style sheets are languages\nthat allow us to present the same webpage with different formatting for clients\nwith different presentation capabilities; for example, Web browsers versus cell\nphones, or even a Netscape browser versus Microsoft's Internet Explorer.\n7.6.1\nHTML Forms\nHTML forms are a common way of communicating data from the client tier to\nthe middle tier. The general format of a form is the following:\n<FORM ACTION=\"page.jsp\" METHOD=\"GET\" NAME=\"LoginForm\">\n\nInternet Applications\n</FORM>\n246\nA single HTML document can contain more than one form. Inside an HTML\nform, we can have any HTML tags except another FORM element.\nThe FORM tag has three important attributes:\n•\nACTION: Specifies the URI of the page to which the form contents are\nsubmitted; if the ACTION attribute is absent, then the URI of the current\npage is used. In the sample above, the form input would be submited to\nthe page named page. j sp, which should provide logic for processing the\ninput from the form.\n(We will explain methods for reading form data at\nthe middle tier in Section 7.7.)\n•\nMETHOD: The HTTP/1.0 method used to submit the user input from the\nfilled-out form to the webserver. There are two choices, GET and POST; we\npostpone their discussion to the next section.\n•\nNAME: This attribute gives the form a name.\nAlthough not necessary,\nnaming forms is good style.\nIn Section 7.6.2, we discuss how to write\nclient-side programs in JavaScript that refer to forms by name and perform\nchecks on form fields.\nInside HTML forms, the INPUT, SELECT, and TEXTAREA tags are used to specify\nuser input elements; a form can have many elements of each type. The simplest\nuser input element is an INPUT field, a standalone tag with no terminating tag.\nAn example of an INPUT tag is the following:\n<INPUT TYPE=ltext\" NAME=\"title\">\nThe INPUT tag has several attributes. The three most important ones are TYPE,\nNAME, and VALUE. The TYPE attribute determines the type of the input field. If\nthe TYPE attribute h&'3 value text, then the field is a text input field. If the\nTYPE attribute has value password, then the input field is a text field where the\nentered characters are displayed as stars on the screen. If the TYPE attribute\nhas value reset, it is a simple button that resets all input fields within the\nform to their default values. If the TYPE attribute has value submit, then it is\na button that sends the values of the different input fields in the form to the\nserver. Note that reset and submit input fields affect the entire form.\nThe NAME attribute of the INPUT tag specifies the symbolic name for this field\nand is used to identify the value of this input fi.eld when it is sent to the server.\nNAME has to be set for INPUT tags of all types except submit and reset. In the\npreceding example, we specified title as the NAME of the input field.\n\n244\nCHAPTER' 7\nThe VALUE attribute of an input tag can be used for text or password fields to\nspecify the default contents of the field.\nFor submit or reset buttons, VALUE\ndetermines the label of the button.\nThe form in Figure 7.11 shows two text fields, one regular text input field and\none password field. It also contains two buttons, a reset button labeled 'Reset\nValues' and a submit button labeled 'Log on.' Note that the two input fields\nare named, whereas the reset and submit button have no NAME attributes.\n<FORM ACTION=\"page.jsp\" METHoD=\"GET\" NAME=\"LoginForm\">\n<INPUT TYPE=\"text\" NAME=\"username\" VALUE=\" Joe\"><P>\n<INPUT TYPE=\"password\" NAME=\"p&ssword\"><P>\n<INPUT TYPE=\"reset\" VALUE=\"Reset Values\"><P>\n<INPUT TYPE=\"submit\" VALUE=\"Log on\">\n</FoRM>\nFigure 7.11\nHTl'vlL Form with Two Text Fields and Two Buttons\nHTML forms have other ways of specifying user input, such as the aforemen-\ntioned TEXTAREA and SELECT tags; we do not discuss them.\nPassing Arguments to Server~Side Scripts\nAs mentioned at the beginning of Section 7.6.1, there are two different ways to\nsubmit HTML Form data to the webserver. If the method GET is used, then\nthe contents of the form are assembled into a query URI (as discussed next)\nand sent to the server. If the method POST is used, then the contents of the\nform are encoded as in the GET method, but the contents are sent in a separate\ndata block instead of appending them directly to the URI. Thus, in the GET\nmethod the form contents are directly visible to the user as the constructed\nURI, whereas in the POST method, the form contents are sent inside the HTTP\nrequest message body and are not visible to the user.\nUsing the GET method gives users the opportunity to bookmark the page with\nthe constructed URI and thus directly jump to it in subsequent sessions; this\nis not possible with the POST method. The choice of GET versus POST should\nbe determined' by the application and its requirements.\nLet us look at the encoding of the URI when the GET method is used.\nThe\nencoded URI has the following form:\naction?name1=vallle1&name2=value2&name;J=value3\n\nInternet Applicat'icJns\n245\nThe action is the URI specified in the ACTION attribute to the FORM tag, or the\ncurrent document URI if no ACTION attribute was specified. The 'name=value'\npairs are the user inputs from the INPUT fields in the form.\nFor form INPUT\nfields where the user did not input anything, the name is stil present with an\nempty value (name=). As a concrete example, consider the PCl,.'3sword submission\nform at the end of the previous section.\nAssume that the user inputs 'John\nDoe' as username, and 'secret' as password. Then the request URI is:\npage.jsp?username=J01111+Doe&password=secret\nThe user input from forms can contain general ASCII characters, such as the\nspace character, but URIs have to be single, consecutive strings with no spaces.\nTherefore, special characters such as spaces, '=', and other unprintable charac-\nters are encoded in a special way. To create a URI that has form fields encoded,\nwe perform the following three steps:\n1. Convert all special characters in the names and values to '%xyz,' where\n'xyz' is the ASCII value of the character in hexadecimal. Special characters\ninclude =, &, %, +, and other unprintable characters. Note that we could\nencode all characters by their ASCII value.\n2. Convert all space characters to the '+' character.\n3. Glue corresponding names and values from an individual HTML INPUT tag\ntogether with '=' and then paste name-value pairs from different HTML\nINPUT tags together using'&' to create a request URI of the form:\naction?namel=value1&name2=value2&name3=value3\nNote that in order to process the input elements from the HTML form at\nthe middle tier, we need the ACTION attribute of the FORM tag to point to a\npage, script, or program that will process the values of the form fields the user\nentered. We discuss ways of receiving values from form fields in Sections 7.7.1\nand 7.7.3.\n7.6.2\nJavaScript\nJavaScript is a scripting language at the client tier with which we can add\nprograms to webpages that run directly at the client (Le., at the machine run-\nning the Web !)rowser).\nJavaScript is often used for the following types of\ncomputation at the client:\nIII\nBrowser Detection: J avaScript can be used to detect the browser type\nand load a browser-specific page.\nIII\nForm Validation: JavaScript is used to perform simple consistency checks\non form fields. For example, a JavaScript program might check whether a\n\n246\nCHAPTER~ 7\nform input that asks for an email address contains the character '@,' or if\nall required fields have been input by the user.\n•\nBrowser Control: This includes opening pages in customized windows;\nexamples include the annoying pop-up advertisements that you see at many\nwebsites, which are programmed using JavaScript.\nJ avaScript is usually embedded into an HTML document with a special tag,\nthe SCRIPT tag. The SCRIPT tag has the attribute LANGUAGE, which indicates\nthe language in which the script is written.\nFor JavaScript, we set the lan-\nguage attribute to JavaScript.\nAnother attribute of the SCRIPT tag is the\nSRC attribute, which specifies an external file with JavaScript code that is au-\ntomatically embedded into the HTML document.\nUsually JavaScript source\ncode files use a '.js' extension. The following fragment shows a JavaScript file\nincluded in an HTML document:\n<SCRIPT LANGUAGE=\" JavaScript\" SRC=\"validateForm.js\"> </SCRIPT>\nThe SCRIPT tag can be placed inside HTML comments so that the JavaScript\ncode is not displayed verbatim in Web browsers that do not recognize the\nSCRIPT tag.\nHere is another JavaScipt code example that creates a pop-up\nbox with a welcoming message. We enclose the JavaScipt code inside HTML\ncomments for the reasons just mentioned.\n<SCRIPT LANGUAGE=\" JavaScript\" >\n<I--\nalert (\" Welcome to our bookstore\");\n//-->\n</SCRIPT>\nJavaScript provides two different commenting styles: single-line comments that\nstart with the '//' character, and multi-line comments starting with '/*' and\nending with ,*/' characters.l\nJavaScript has variables that can be numbers, boolean values (true or false),\nstrings, and some other data types that we do not discuss. Global variables have\nto be declared in advance of their usage with the keyword var, and they can\nbe used anywhere inside the HTML documents. Variables local to a JavaScript\nfunction (explained next) need not be declared. Variables do not have a fixed\ntype, but implicitly have the type of the data to which they have been assigned.\n1Actually, '<! --' also marks the start of a single-line comment, which is why we did not have\nto mark the HTML starting cormnent '<! --' in the preceding example using J avaScript comment\nnotation. In contrast, the HTML closing comment \"-->\" has to be commented out in JavaScript as\nit is interpreted otherwise.\n\nInternet Applications\n247,\nJavaScript has the usual assignment operators (=, + =, etc.), the usual arith-\nmetic operators (+, -, *, /, %), the usual comparison operators (==, ! =,\n>=, etc.), and the usual boolean operators (&& for logical AND,\n11 for logical\nOR, and! for negation).\nStrings can be concatenated using the '+' charac-\nter. The type of an object determines the behavior of operators; for example\n1+1 is 2, since we are adding numbers, whereas \"1\"+\"1\" is \"11,\" since we\nare concatenating strings. JavaScript contains the usual types of statements,\nsuch as assignments, conditional statements (if Ccondition) {statements;}\nelse {statements; }), and loops (for-loop, do-while, and while-loop).\nJavaScript allows us to create functions using the function keyword: function\nf Cargl, arg2) {statements;}. We can call functions from JavaScript code,\nand functions can return values using the keyword return.\nWe conclude this introduction to JavaScript with a larger example of a JavaScript\nfunction that tests whether the login and password fields of a HTML form are\nnot empty.\nFigure 7.12 shows the JavaScript function and the HTML form.\nThe JavaScript code is a function called testLoginEmptyO that tests whether\neither of the two input fields in the form named LoginForm is empty. In the\nfunction testLoginEmpty, we first use variable loginForm to refer to the form\nLoginForm using the implicitly defined variable document, which refers to the\ncurrent HTML page. (JavaScript has a library of objects that are implicitly de-\nfined.) We then check whether either of the strings loginForm. userif. value\nor loginForm. password. value is empty.\nThe function testLoginEmpty is checked within a form event handler.\nAn\nevent handler is a function that is called if an event happens on an object in\na webpage. The event handler we use is onSubmit, which is called if the submit\nbutton is pressed (or if the user presses return in a text field in the form). If\nthe event handler returns true, then the form contents are submitted to the\nserver, otherwise the form contents are not submitted to the server.\nJ avaScript has functionality that goes beyond the basics that we explained in\nthis section; the interested reader is referred to the bibliographic notes at the\nend of this chapter.\n7.6.3\nStyle Sheets\nDifferent clients have different displays, and we need correspondingly different\nways of displaying the same information.\nFor example, in the simplest ca.se,\nwe might need to use different font sizes or colors that provide high-contra.st\non a black-and-white screen. As a more sophisticated example, we might need\nto re-arrange objects on the page to accommodate small screens in personal\n\n248\nCHAPTER 7\n<SCRIPT LANGUAGE==\" JavaScript\">\n<!--\nfunction testLoginEmpty()\n{\n10ginForm = document.LoginForm\nif ((loginForm.userid.value == \"\") II\n(loginFonn.password.value == I. II )) {\nalert(,Please enter values for userid and password.');\nreturn false;\n}\nelse\nreturn true;\n}\n//-->\n</SCRIPT>\n<Hi ALIGN = \"CENTER\" >Barns and Nobble Internet Bookstore</Hi>\n<H3 ALIGN = \"CENTER\">Plec1Se enter your userid and password:</H3>\n<FORM NAME = \"LoginForm ll METHOD=\"POST\"\nACTI ON= IITableOfContents.jsp\"\nonSubmit=\" return testLoginEmptyO\" >\nUserid: <INPUT TYPE=\"TEXT\" NAME=lI userid\"><P>\nPassword: <INPUT TYPE=\"PASSWORD\" NAME=\"password\"><P>\n<INPUT TYPE=\"SUBMIT\" VALUE=\"Login\" NAME=\"SUBMIT\">\n<INPUT TYPE=\"RESET\" VALUE=IIClear Input\" NAME=\"RESET\">\n</FORM>\nFigure 7.12\nForm Validation with JavaScript\ndigital assistants (PDAs).\nAs another example, we might highlight different\ninfonnation to focus on some important part of the page. A style sheet is a\nmethod to adapt the same document contents to different presentation formats.\nA style sheet contains instructions that tell a 'Veb browser (or whatever the\nclient uses to display the webpage) how to translate the data of a document\ninto a presentation that is suitable for the client's display.\nStyle sheets separate the transformative aspect of the page from the ren-\ndering aspects of the page. During transformation, the objects in the XML\ndocument are rearranged to form a different structure, to omit parts of the\nXML document, or to merge two different XML documents into a single docu-\nment. During rendering, we take the existing hierarchical structure of the XML\ndocument and format the document according to the user's display device.\n\nInte17u'.t Apphcations\nBODY {BACKGROUND-COLOR: yellow}\nHi {FONT-SIZE: 36pt}\nH3 {COLOR: blue}\nP {MARGIN-LEFT: 50px; COLOR: red}\nFigure 7.13\nAn Example Style sheet\n249\nThe use of style sheets has many advantages. First, we can reuse the same doc-\nument many times and display it differently depending on the context. Second,\nwe can tailor the display to the reader's preference such as font size, color style,\nand even level of detail. Third, we can deal with different output formats, such\nas different output devices (laptops versus cell phones), different display sizes\n(letter versus legal paper), and different display media (paper versus digital\ndisplay). Fourth, we can standardize the display format within a corporation\nand thus apply style sheet conventions to documents at any time.\nFurther,\nchanges and improvements to these display conventions can be managed at a\ncentral place.\nThere are two style sheet languages: XSL and ess. ess was created for HTML\nwith the goal of separating the display characteristics of different formatting\ntags from the tags themselves. XSL is an extension of ess to arbitrary XML\ndocurnents; besides allowing us to define ways of formatting objects, XSL con-\ntains a transformation language that enables us to rearrange objects.\nThe\ntarget files for ess are HTML files, whereas the target files for XSL are XML\nfiles.\nCascading Style Sheets\nA Cascading Style Sheet (CSS) defines how to display HTML elements.\n(In Section 7.13, we introduce a more general style sheet language designed for\nXML documents.)\nStyles are normally stored in style sheets, which are files\nthat contain style definitions.\nMany different HTML documents, such as all\ndocuments in a website, can refer to the same ess. Thus, we can change the\nformat of a website by changing a single file.\nThis is a very convenient way\nof changing the layout of many webpages at the seune time, and a first step\ntoward the separation of content from presentation.\nAn example style sheet is shown in Figure 7.13. It is included into an HTML\nfile with the following line:\n<LINK REL=\"style sheet\" TYPE=\"text/css\"\nHREF=\"books.css\" />\n\n250\nCHAPTER t 7\nEach line in a CSS sheet consists of three parts; a selector, a property, and a\nvalue. They are syntactically arranged in the following way:\nselector {property: value}\nThe selector is the element or tag whose format we are defining. The property\nindicates the tag's attribute whose value we want to set in the style sheet, and\nthe property is the actual value of the attribute. As an example, consider the\nfirst line of the example style sheet shown in Figure 7.13:\nBODY {BACKGROUND-COLOR: yellow}\nThis line has the same effect as changing the HTML code to the following:\n<BODY BACKGROUND-COLOR=\"yellow\" >.\nThe value should always be quoted, as it could consist of several words. More\nthan one property for the same selector can be separated by semicolons as\nshown in the last line of the example in Figure 7.13:\nP {MARGIN-LEFT: 50px; COLOR: red}\nCascading style sheets have an extensive syntax; the bibliographic notes at the\nend of the chapter point to books and online resources on CSSs.\nXSL\nXSL is a language for expressing style sheets. An XSL style sheet is, like CSS,\na file that describes how to display an XML document of a given type. XSL\nshares the functionality of CSS and is compatible with it (although it uses a\ndifferent syntax).\nThe capabilities of XSL vastly exceed the functionality of CSS. XSL contains\nthe XSL Transformation language, or XSLT, a language that allows 11S to\ntransform the input XML document into a XML document with another struc-\nture. For example, with XSLT we can change the order of elements that we are\ndisplaying (e.g.; by sorting them), process elements more than once, suppress\nelements in one place and present them in another, and add generated text to\nthe presentation.\nXSL also contains the XML Path Language (XPath), a language that\nallows us to refer to parts of an XML document. We discuss XPath in Section\n\nInte1~net Applications\n251\n27. XSL also contains XSL Formatting Object, a way of formatting the output\nof an XSL transformation.\n7.7\nTHE MIDDLE TIER\nIn this section, we discuss technologies for the middle tier.\nThe first gen-\neration of middle-tier applications were stand-alone programs written in a\ngeneral-purpose programming language such as C, C++, and Perl. Program-\nmers quickly realized that interaction with a stand-alone application was quite\ncostly; the overheads include starting the application every time it is invoked\nand switching processes between the webserver and the application. Therefore,\nsuch interactions do not scale to large numbers of concurrent users. This led\nto the development of the application server, which provides the run-time\nenvironment for several technologies that can be used to program middle-tier\napplication components. Most of today's large-scale websites use an application\nserver to run application code at the middle tier.\nOur coverage of technologies for the middle tier mirrors this evolution.\nWe\nstart in Section 7.7.1 with the Common Gateway Interface, a protocol that is\nused to transmit arguments from HTML forms to application programs run-\nning at the middle tier. We introduce application servers in Section 7.7.2. We\nthen describe technologies for writing application logic at the middle tier: Java\nservlets (Section 7.7.3) and Java Server Pages (Section 7.7.4). Another impor-\ntant functionality is the maintenance of state in the middle tier component of\nthe application as the client component goes through a series of steps to com-\nplete a transaction (for example, the purchase of a market basket of items or\nthe reservation of a flight). In Section 7.7.5, we discuss Cookies, one approach\nto maintaining state.\n7.7.1\nCGI: The Common Gateway Interface\nThe Common Gateway Interface connects HTML forms with application pro-\ngrams. It is a protocol that defines how arguments from forms are passed to\nprograms at the server side. We do not go into the details of the actual CGI\nprotocol since libraries enable application programs to get arguments from the\nHTML fonn; we shortly see an example in a CGI program.\nPrograms that\ncommunicate with the webserver via CGI are often called CGI scripts, since\nmany such application programs were written in a scripting language such Ike.;\nPerl.\nAs an example of a program that interfaces with an HTML form via CGI,\nconsider the sample page shown in Figure 7.14. This webpage contains a form\nwhere a user can fill in the name of an author. If the user presses the 'Send\n\n252\nCHAPTER, 7\n<HTML><HEAD><TITLE>The Database Bookstore</TITLE></HEAD>\n<BODY>\n<FORM ACTION=\"find_books.cgi II METHOD=POST>\nType an author name:\n<INPUT TYPE=\"text II NAME=lauthorName\"\nSIZE=30 MAXLENGTH=50>\n<INPUT TYPE=\"submitil value=\"Send it\">\n<INPUT TYPE=lreset\" VALUE=\"Clear form II >\n</FORM>\n</BODY></HTML>\nFigure 7.14\nA Sample 'Neb Page Where Form Input Is Sent to a CGI Script\nit' button, the Perl script 'findBooks.cgi' shown in Figure 7.14 is executed as\na separate process. The CGl protocol defines how the communication between\nthe form and the script is performed.\nFigure 7.15 illustrates the processes\ncreated when using the CGl protocol.\nFigure 7.16 shows the example CGl script, written in Perl.\nWe omit error-\nchecking code for simplicity. Perl is·an interpreted language that is often used\nfor CGl scripting and many Perl libraries, called modules, provide high-level\ninterfaces to the CGl protocol.\n\\Ve use one such library, called the DBI li-\nbrary, in our example. The CGI module is a convenient collection of functions\nfor creating CGl scripts. In part 1 of the sample script, we extract the argument\nof the HTML form that is passed along from the client as follows:\n$authorName = $dataln- >paramCauthorName');\nNote that the parameter name authorName wa.s used in the form in Figure\n7.14 to name the first input field. Conveniently, the CGl protocol abstracts the\nactual implementation of how the webpage is returned to the Web browser; the\nwebpage consists simply of the output of our program, and we start assembling\nthe output HTML page in part 2.\nEverything the script writes in print-\nstatements is part of the dynamically constructed webpage returned to the\nbrowser.\n\\Ve finish in part 3 by appending the closing format tags to the\nresulting page.\n7.7.2\nApplication Servers\nApplication logic can be enforced through server-side programs that are in-\nvoked using the CGl protocol. However, since each page request results in the\ncreation of a new process, this solution does not scale well to a large number\nof simultaneous requests. This performance problem led to the development of\n\n254\nWeb Browser\nDOD\nDO ••• :\nI\nPool of servlets\nI\n--~-----~-~_\n.....\nJDBC\nJDBC/ODBC\nCHAPTER\" 7\nc++\nApplication\nJavaBeans\nApplication\nDBMS I\nDBMS 2\nFigure 7.17\nProcess Structure in the Application Server Architecture\nalso help to ensure secure database access by supporting a general user-id mech-\nanism. (For more on security, see Chapter 21.)\nA possible architecture for a website with an application server is shown in Fig-\nure 7.17. The client (a Web browser) interacts with the webserver through the\nHTTP protocol. The webserver delivers static HTML or XML pages directly\nto the client. To assemble dynamic pages, the webserver sends a request to the\napplication server. The application server contacts one or more data sources to\nretrieve necessary data or sends update requests to the data sources. After the\ninteraction with the data sources is completed, the application server assembles\nthe webpage and reports the result to the webserver, which retrieves the page\nand delivers it to the client.\nThe execution of business logic at the webserver's site, server-side process-\ning, has become a standard model for implementing more complicated business\nprocesses on the Internet. There are many different technologies for server-side\nprocessing and we only mention a few in this section; the interested reader is\nreferred to the bibliographic notes at the end of the chapter.\n7.7.3\nServlets\nJava servlets are pieces of Java code that run on the middle tier, in either\nwebservers or application servers.\nThere are special conventions on how to\nread the input from the user request and how to write output generated by the\nservlet. Servlets are truly platform-independent, and so they have become very\npopular with Web developers.\nSince servlets are Java programs, they are very versatile. For example, servlets\ncan build webpages, access databases, and maintain state. Servlets have access\n\nInteT1Iet AIJplications\nimport java.io.*;\nimport javCLx.servlet.*;\nimport javax.servlet.http.*;\npUblic class ServletTemplate extends HttpServlet {\npublic void doGet(HttpServletRequest request,\nHttpServletResponse response)\nthrows ServletException, IOException {\nPrintWriter out = response.getWriter();\n/ / Use 'out' to send content to browser\nout.println(\"Hello World\");\n}\n}\nFigure 7.18\nServlet Template\n255\n@\nto all Java APls, including JDBC. All servlets must implement the Servlet\ninterface.\nIn most cases, servlets extend the specific HttpServlet class for\nservers that communicate with clients via HTTP. The HttpServlet class pro-\nvides methods such as doGet and doPost to receive arguments from HTML\nforms, and it sends its output back to the elient via HTTP. Servlets that\ncommunicate through other protocols (such as ftp) need to extend the class\nGenericServlet.\nServlets are compiled Java classes executed and maintained by a servlet con-\ntainer. The servlet container manages the lifespan of individual servlets by\ncreating and destroying them. Although servlets can respond to any type of re-\nquest, they are commonly used to extend the applications hosted by webservers.\nFor such applications, there is a useful library of HTTP-specific servlet classes.\nServlets usually handle requests from HTML forms and maintain state between\nthe client and the server. We discuss how to maintain state in Section 7.7.5.\nA template of a generic servlet structure is shown in Figure 7.18. This simple\nservlet just outputs the two words \"Hello World,\" but it shows the general\nstructure of a full-fledged servlet. The request object is used to read HTML\nform data. The response object is used to specify the HTTP response status\ncode and headers of the HTTP response. The object out is used to compose\nthe content that is returned to the client.\nRecall that HTTP sends back the status line, a header, a blank line, and then\nthe context. Right now our servlet just returns plain text. We can extend our\nservlet by setting the content type to HTML, generating HTML a,s follows:\n\n256\nCHAPTER .7\nPrinfWriter out = response.get\\Vriter();\nString docType =\n\"<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 \" +\n\"Transitional//EN\"> \\n\";\nout.println(docType +\n\"<HTML>\\n\" +\n\"<HEAD><TITLE>Hello 'vVWW</TITLE></HEAD>\\n\" +\n\"<BODY>\\n\" +\n\"<Hl>Hello WWW</Hl>\\n\" +\n\"</BODY></HTML>\");\nWhat happens during the life of a servlet?\nSeveral methods are called at\ndifferent stages in the development of a servlet.\nWhen a requested page is\na servlet, the webserver forwards the request to the servlet container, which\ncreates an instance of the servlet if necessary.\nAt servlet creation time, the\nservlet container calls the init () method, and before deallocating the servlet,\nthe servlet container calls the servlet's destroyO method.\nWhen a servlet container calls a servlet because of a requested page, it starts\nwith the service () method, whose default behavior is to call one of the follow-\ning methods based on the HTTP transfer method: service () calls doGet 0\nfor a HTTP GET request, and it calls doPost () for a HTTP POST request.\nThis automatic dispatching allows the servlet to perform different tasks on the\nrequest data depending on the HTTP transfer method. Usually, we do not over-\nride the service () method, unless we want to program a servlet that handles\nboth HTTP POST and HTTP GET requests identically.\nWe conclude our discussion of servlets with an example, shown in Figure 7.19,\nthat illustrates how to pass arguments from an HTML form to a servlet.\n7.7.4\nJavaServer Pages\nIn the previous section, we saw how to use Java programs in the middle tier\nto encode application logic and dynamically generate webpages. If we needed\nto generate HTML output, we wrote it to the out object. Thus, we can think\nabout servlets as Java code embodying application logic, with embedded HTML\nfor output.\nJavaServer pages (.JSPs) interchange the roles of output amI application logic.\nJavaServer pages are written in HTML with servlet-like code embedded in\nspecial HT1VIL tags.\nThus, in comparison to servlets, JavaServer pages are\nbetter suited to quickly building interfaces that have some logic inside, wherea..':i\nservlets are better suited for complex application logic.\n\nInternet Applications\nimport java.io.*;\nimport javax.servlet.*;\nimport javax.servlet.http.*;\nimport java.util.*;\npublic class ReadUserName extends HttpServlet {\npublic void doGet(HttpServletRequest request,\nHttpServletResponse response)\nthrows ServletException, IOException {\nresponse.setContentType('j textjhtml'j);\nPrintWriter out = response.getWriter();\nout.println(\"<BODY>\\n\" +\n\"<Hi ALIGN=CENTER> Username: </Hi>\\n\" +\n\"<UL>\\n\"\n+\n\" <LI>title: \"\n+ request.getParameter(\"userid\") + \"\\n\" +\n+ request.getParameter(\"password'j) + \"\\nj' +\n1</UL>\\n\" +\n1</BODY></HTML>\")j\n}\npublic void doPost(HttpServletRequest request,\nHttpServletResponse response)\nthrows ServletException, IOException {\ndoGet(request, response);\n}\n}\nFigure 7.19\nExtracting the User Name and Password From a Form\n257\nJ\n\n258\nCHAPTER,7\n~While there is a big difference for the programmer, the middle tier handles\nJavaServer pages in a very simple way: They are usually compiled into a servlet,\nwhich is then handled by a servlet container analogous to other servlets.\nThe code fragment in Figure 7.20 shows a simple JSP example. In the middle\nof the HTML code, we access information that was passed from a form.\n<!DOCTYPE HTML PUBLIC 11_//W3C//DTD HTML 4.0\nTransitional//EN lI >\n<HTML>\n<HEAD><TITLE>Welcome to Barnes and Nobble</TITLE></HEAD>\n<BODY>\n<Hl>Welcome back!</Hl>\n<% String name=\"NewUserll ;\nif (request.getParameter(lIusernamell) != null) {\nname=request.getParameter(\" username\" );\n}\n%>\nYou are logged on as user <%=name%>\n<P>\nRegular HTML for all the rest of the on-line store's webpage.\n</BODY>\n</HTML>\nFigure 7.20\nReading Form Parameters in JSP\n7.7.5\nMaintaining State\nAs discussed in previous sections, there is a need to maintain a user's state\nacross different pages. As an example, consider a user who wants to make a\npurchase at the Barnes and Nobble website.\nThe user must first add items\ninto her shopping basket, which persists while she navigates through the site.\nThus, we use the notion of state mainly to remember information as the user\nnavigates through the site.\nThe HTTP protocol is stateless. We call an interaction with a webserver state-\nless if no inforination is retained from one request to the next request. We call\nan interaction with a webserver stateful, or we say that state is maintained,\nif some memory is stored between requests to the server, and different actions\nare taken depending on the contents stored.\n\nInternet Applico,tiol/s\n259\n!\nIn our example of Barnes and Nobble, we need to maintain the shopping basket\nof a user. Since state is not encapsulated in the HTTP protocol, it has to be\nmaintained either at the server or at the client.\nSince the HTTP protocol\nis stateless by design, let us review the advantages and disadvantages of this\ndesign decision.\nFirst, a stateless protocol is easy to program and use, and\nit is great for applications that require just retrieval of static information. In\naddition, no extra memory is used to maintain state, and thus the protocol\nitself is very efficient. On the other hand, without some additional mechanism\nat the presentation tier and the middle tier, we have no record of previous\nrequests, and we cannot program shopping baskets or user logins.\nSince we cannot maintain state in the HTTP protocol, where should we mtain-\ntain state?\nThere are basically two choices.\nWe can maintain state in the\nmiddle tier, by storing information in the local main memory of the applica-\ntion logic, or even in a database system. Alternatively, we can maintain state\non the client side by storing data in the form of a cookie. We discuss these two\nways of maintaining state in the next two sections.\nMaintaining State at the Middle Tier\nAt the middle tier, we have several choices as to where we maintain state.\nFirst, we could store the state at the bottom tier, in the database server. The\nstate survives crashes of the system, but a database access is required to query\nor update the state, a potential performance bottleneck. An alternative is to\nstore state in main memory at the middle tier. The drawbacks are that this\ninformation is volatile and that it might take up a lot of main memory. We\ncan also store state in local files at the middle tier, &s a compromise between\nthe first two approaches.\nA rule of thumb is to use state maintenance at the middle tier or database tier\nonly for data that needs to persist over many different user sessions. Examples\nof such data are past customer orders, click-stream data recording a user's\nmovement through the website, or other permanent choices that a user makes,\nsuch as decisions about personalized site layout, types of messages the user is\nwilling to receive, and so on. As these examples illustrate, state information is\noften centered around users who interact with the website.\nMaintaining State at the Presentation Tier: Cookies\nAnother possibility is to store state at the presentation tier and pass it to the\nmiddle tier with every HTTP request.\nWe essentially work around around\nthe statelessness of the HTTP protocol by sending additional information with\nevery request. Such information is called a cookie.\n\n260\nCHAPTE~ 7\n/ / no 88L required\n/ / one month lifetime\nA cookie is a collection of (name,\nval'Ue)~~pairs that can be manipulated at\nthe presentation and middle tiers.\nCookies are ea..''!Y to use in Java servlets\nand Java8erver Pages and provide a simple way to make non-essential data\npersistent at the client. They survive several client sessions because they persist\nin the browser cache even after the browser is closed.\nOne disadvantage of cookies is that they are often perceived as as being invasive,\nand many users disable cookies in their Web browser; browsers allow users to\nprevent cookies from being saved on their machines. Another disadvantage is\nthat the data in a cookie is currently limited to 4KB, but for most applications\nthis is not a bad limit.\nWe can use cookies to store information such as the user's shopping basket, login\ninformation, and other non-permanent choices made in the current session.\nNext, we discuss how cookies can be manipulated from servlets at the middle\ntier.\nThe Servlet Cookie API\nA cookie is stored. in a small text file at the client and. contains (name, val'l1e/-\npairs, where both name and value are strings. We create a new cookie through\nthe Java Cookie class in the middle tier application code:\nCookie cookie = new Cookie( II username\" ,\"guest\" );\ncookie.setDomain(\"www.bookstore.com .. );\ncookie.set8ecure(false);\ncookie.setMaxAge(60*60*24*7*31);\nresponse.addCookie(cookie);\nLet us look at each part of this code. First, we create a new Cookie object with\nthe specified (name,\nval'l1e)~~·pair. Then we set attributes of the cookie; we list\nsome of the most common attributes below:\nIII\nsetDomain and getDomain:\nThe domain specifies the website that will\nreceive the cookie. The default value for this attribute is the domain that\ncreated the cookie.\nII\nsetSecure and getSecure: If this flag is true, then the cookie is sent only\nif we are llsing a secure version of the HTTP protocol, such <t,<; 88L.\nIII\nsetMaxAge and getMaxAge: The MaxAge attribute determines the lifetime\nof the cookie in seconds. If the value of MaxAge is less than or equal to\nzero, the cookie is deleted when the browser is closed.\n\nInte7~net Applications\n26~\n•\nsetName and getName: We did not use these functions in our code fragment;\nthey allow us to Ilame the cookie.\n•\nsetValue and getValue: These functions allow us to set and read the\nvalue of the cookie.\nThe cookie is added to the request object within the Java servlet to be sent\nto the client. Once a cookie is received from a site (www.bookstore.comin this\nexample), the client's Web browser appends it to all HTTP requests it sends\nto this site, until the cookie expires.\nWe can access the contents of a cookie in the middle-tier code through the\nrequest object getCookies 0 method, which returns an array of Cookie ob-\njects.\nThe following code fragment reads the array and looks for the cookie\nwith name 'username.'\nCookieD cookies = request.getCookiesO;\nString theUser;\nfor(int i=O; i < cookies.length; i++) {\nCookie cookie = cookies[i];\nif (cookie.getNameO.equals(\"username\"))\ntheUser = cookie.getValueO;\n}\nA simple test can be used to check whether the user has turned oft' cookies:\nSend a cookie to the user, and then check whether the request object that\nis returned still contains the cookie. Note that a cookie should never contain\nan unencrypted password or other private, unencrypted data, as the user can\neasily inspect, modify, and erase any cookie at any time, including in the middle\nof a session. The application logic needs to have sufficient consistency checks\nto ensure that the data in the cookie is valid.\n7.8\nCASE STUDY: THE INTERNET BOOK SHOP\nDBDudes now moves on to the implementation of the application layer and\nconsiders alternatives for connecting the DBMS to the World Wide Web.\nDBDudes begifls by considering session management. For example, users who\nlog in to the site, browse the catalog, and select books to buy do not want\nto re-enter their cllstomer identification numbers. Session management has to\nextend to the whole process of selecting books, adding them to a shopping cart,\npossibly removing books from the cart, and checking out and paying for the\nbooks.\n\n262\nCHAPTERi 7\nDBDudes then considers whether webpages for books should be static or dy-\nnamic.\nIf there is a static webpage for each book, then we need an extra\ndatabase field in the Books relation that points to the location of the file.\nEven though this enables special page designs for different books, it is a very\nlabor-intensive solution.\nDBDudes convinces B&N to dynamically assemble\nthe webpage for a book from a standard template instantiated with informa-\ntion about the book in the Books relation. Thus, DBDudes do not use static\nHTML pages, such as the one shown in Figure 7.1, to display the inventory.\nDBDudes considers the use of XML a'S a data exchange format between the\ndatabase server and the middle tier, or the middle tier and the client tier.\nRepresentation of the data in XML at the middle tier as shown in Figures 7.2\nand 7.3 would allow easier integration of other data sources in the future, but\nB&N decides that they do not anticipate a need for such integration, and so\nDBDudes decide not to use XML data exchange at this time.\nDBDudes designs the application logic as follows. They think that there will\nbe four different webpages:\n•\nindex. j sp: The home page of Barns and Nobble. This is the main entry\npoint for the shop. This page has search text fields and buttons that allow\nthe user to search by author name, ISBN, or title of the book. There is\nalso a link to the page that shows the shopping cart, cart. j sp.\n•\nlogin. j sp:\nAllows registered users to log in.\nHere DBDudes use an\nHTML form similar to the one displayed in Figure 7.11.\nAt the middle\ntier, they use a code fragment similar to the piece shown in Figure 7.19\nand JavaServerPages as shown in Figure 7.20.\n•\nsearch. j sp: Lists all books in the database that match the search condi-\ntion specified by the user. The user can add listed items to the shopping\nbasket; each book ha'3 a button next to it that adds it.\n(If the item is\nalready in the shopping basket, it increments the quantity by one.) There\nis also a counter that shows the total number of items currently in the\nshopping basket. (DBDucles makes a note that that a quantity of five for a\nsingle item in the shopping basket should indicate a total purcha'3c quantity\nof five as well.) The search. j sp page also contains a button that directs\nthe user to cart. j sp.\nIII\ncart. j sp: Lists all the books currently in the shopping basket. The list-\ning should include all items in the shopping basket with the product name,\nprice, a text box for the quantity (which the user can use to change quanti-\nties of items), and a button to remove the item from the shopping basket.\nThis page has three other buttons: one button to continue shopping (which\nreturns the user to page index. j sp), a second button to update the shop-\n\nInter'net Applications\nping basket with the altered quantities from the text boxes, and a third\nbutton to place the order, which directs the user to the page confirm.jsp.\nII\nconi irm. j sp: Lists the complete order so far and allows the user to enter\nhis or her contact information or customer ID. There are two buttons on\nthis page: one button to cancel the order and a second button to submit\nthe final order. The cancel button ernpties the shopping ba.'3ket and returns\nthe user to the home page. The submit button updates the database with\nthe new order, empties the shopping basket, and returns the user to the\nhome page.\nDBDudes also considers the use of JavaScript at the presentation tier to check\nuser input before it is sent to the middle tier.\nFor example, in the page\nlogin. j sp, DBDudes is likely to write JavaScript code similar to that shown\nin Figure 7.12.\nThis leaves DBDudes with one final decision: how to connect applications to\nthe DBMS. They consider the two main alternatives presented in Section 7.7:\nCGI scripts versus using an application server infrastructure. If they use CGI\nscripts, they would have to encode session management logic-not an easy task.\nIf they use an application server, they can make use of all the functionality\nthat the application server provides.\nTherefore, they recommend that B&N\nimplement server-side processing using an application server.\nB&N accepts the decision to use an application server, but decides that no\ncode should be specific to any particular application server, since B&N does\nnot want to lock itself into one vendor. DBDudes agrees proceeds to build the\nfollowing pieces:\nIII\nDBDudes designs top level pages that allow customers to navigate the\nwebsite as well as various search forms and result presentations.\nII\nAssuming that DBDudes selects a Java-ba..sed application server, they have\nto write Java servlets to process form-generated requests. Potentially, they\ncould reuse existing (possibly commercially available) JavaBeans.\nThey\ncan use JDBC a.\" a databa.':ie interface; exarnples of JDBC code can be\nfound in Section 6.2. Instead of prograrnming servlets, they could resort\nto Java Server Pages and annotate pages with special .JSP markup tags.\nII\nDBDudes select an application server that uses proprietary markup tags,\nbut due to their arrangement with B&N, they are not allowed to use such\ntags in their code.\nFor completeness, we remark that if DBDudes and B&N had agreed to use CGr\nscripts, DBDucles would have had the following ta.sks:\n\n264\nCHAPTER~ 7\nII\nCreate the top level HTML pages that allow users to navigate the site and\nvaTious forms that allow users to search the catalog by ISBN, author name,\nor title.\nAn example page containing a search form is shown in Figure\n7.1. In addition to the input forms, DBDudes must develop appropriate\npresentations for the results.\nII\nDevelop the logic to track a customer session. Relevant information must be\nstored either at the server side or in the customer's browser using cookies.\nII\nWrite the scripts that process user requests. For example, a customer can\nuse a form called 'Search books by title' to type in a title and search for\nbooks with that title. The CGI interface communicates with a script that\nprocesses the request. An example of such a script written in Perl using\nthe DBI library for data access is shown in Figure 7.16.\nOur discussion thus far covers only the customer interface, the part of the\nwebsite that is exposed to B&N's customers.\nDBDudes also needs to add\napplications that allow the employees and the shop owner to query and access\nthe database and to generate summary reports of business activities.\nComplete files for the case study can be found on the webpage for this book.\n7.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nII\nWhat are URIs and URLs? (Section 7.2.1)\nII\nHow does the HTTP protocol work? What is a stateless protocol? (Sec-\ntion 7.2.2)\nII\nExplain the main concepts of HTML. Why is it used only for data presen-\ntation and not data exchange? (Section 7.3)\nII\nWhat are some shortc.ornings of HTML, and how does XML address them?\n(Section 7.4)\nII\nWhat are the main components of an XML document? (Section 7.4.1)\nII\nWhy do we have XML DTDs? What is a well-formed XML document?\nWhat is a valid XML document? Give an example of an XML document\nthat is valid but not well-formed, and vice versa. (Section 7.4.2)\nII\n'What is the role of domain-specific DTDs? (Section 7.4.3)\nII\n\\Vhat is a three-tier architecture? 'What advantages does it offer over single-\ntier and two-tier architectures? Give a short overview of the functionality\nat each of the three tiers. (Section 7.5)\n\nInternet Apphcat-ions\n2&5\n•\nExplain hmv three-tier architectures address each of the following issues\nof databa.<;e-backed Internet applications: heterogeneity, thin clients, data\nintegration, scalability, software development. (Section 7.5.3)\n•\nWrite an HTML form.\nDescribe all the components of an HTML form.\n(Section 7.6.1)\n•\nWhat is the difference between the HTML GET and POST methods? How\ndoes URI encoding of an HT~IL form work? (Section 7.11)\n•\nWhat is JavaScript used for?\nWrite a JavaScipt function that checks\nwhether an HTML form element contains a syntactically valid email ad-\ndress. (Section 7.6.2)\n•\nWhat problem do style sheets address? What are the advantages of using\nstyle sheets? (Section 7.6.3)\n•\nWhat are Ca.5cading Style Sheets? Explain the components of Ca.<;cading\nStyle Sheets. What is XSL and how it is different from CSS? (Sections\n7.6.3 and 7.13)\n•\nWhat is CGl and what problem does it address? (Section 7.7.1)\n•\nWhat are application servers and how are they different from webservers?\n(Section 7.7.2)\n•\nWhat are servlets? How do servlets handle data from HTML forms? Ex-\nplain what happens during the lifetime of a servlet. (Section 7.7.3)\n•\nWhat is the difference between servlets and JSP? When should we use\nservlets and when should we use JSP? (Section 7.7.4)\n•\nWhy do we need to maintain state at the middle tier? What are cookies?\nHow does a browser handle cookies? How can we access the data in cookies\nfrom servlets? (Section 7.7.5)\nEXERCISES\nExercise 7.1 Briefly answer the following questions:\n1. Explain the following terms and describe what they are used for: HTML, URL, XML,\nJava, JSP, XSL, XSLT, servlet, cookie, HTTP, ess, DTD.\n2. What is eGl? Why was eGI introduced? What are the disadvantages of an architecture\nusing eel scripts?\n3. \\Vhat is the difference between a webserver and an application server? What fUl1cionality\ndo typical application servers provide?\n4. When is an XML document well-formed? When is an XML document valid?\nExercise 7.2 Briefly answer the following questions about the HTTP protocol:\n\n266\nCHAPTER$ 7\n1. \\Nhat is a communication protocol?\n2. \"What is the structure of an HTTP request message? What is the structure of an HTTP\nresponse message? \\Vhy do HTTP messages carry a version field?\n3. vVhat is a stateless protocol? \"Why was HTTP designed to be stateless?\n4. Show the HTTP request message generated when you request the home page of this\nbook (http://TNWW . cs. wisc. edur dbbook). Show the HTTP response message that the\nserver generates for that page.\nExercise 7.3 In this exercise, you are asked to write the functionality of a generic shopping\nbasket; you will use this in several subsequent project exercises. Write a set of JSP pages that\ndisplays a shopping basket of items and allows users to add, remove, and change the quantity\nof items. To do this, use a cookie storage scheme that stores the following information:\n•\nThe UserId of the user who owns the shopping basket.\n•\nThe number of products stored in the shopping basket.\nI!\nA product id and a quantity for each product.\nWhen manipulating cookies, remember to set the Expires property such that the cookie can\npersist for a session or indefinitely. Experiment with cookies using JSP and make sure you\nknow how to retrieve, set values, and delete the cookie.\nYou need to create five JSP pages to make your prototype complete:\n..\nIndex Page (index. j sp): This is the main entry point. It has a link that directs the\nuser to the Products page so they can start shopping.\nI!\nProducts Page (products. j sp): Shows a listing of all products in the database with\ntheir descriptions and prices. This is the main page where the user fills out the shopping\nbasket. Each listed product should have a button next to it, which adds it to the shopping\nbasket.\n(If the item is already in the shopping basket, it increments the quantity by\none.) There should also be a counter to show the total number of items currently in the\nshopping basket. Note that if a user has a quantity of five of a single item in the shopping\nbasket, the counter should indicate a total quantity of five.\nThe page also contains a\nbutton that directs the user to the Cart page.\nI!\nCart Page (cart. jsp): Shows a listing of all items in the shopping basket cookie. The\nlisting for each item should include the product name, price, a text box for the quantity\n(the user can changc the quantity of items here), and a button to remove the item from\nthe shopping basket. This page has three other buttons: one button to continue shopping\n(which returns the user to the Products page), a second button to update the cookie\nwith the altered quantities from the text boxes, and a third button to place or confirm\nthe order, which directs the user to the Confirm page.\nI!\nConfirm Pl;tge (confirm. j sp) :\nList.s the final order.\nThere are two but.tons on this\npage.\nOne button cancels t.he order and the other submits the completed order. The\ncancel button just deletes the cookie and returns the lIser to the Index page. The submit\nbutton updates the database with the new order, delet.es the cookie, and returns the lIser\nto the Index page.\nExercise 7.4 In the previous exercise, replace the page products. jsp with the follmving\nsearch page search. j sp.\n'T'his page allows users to search products by name or descrip-\ntion.\nThere should be both a text box for the search text and radio buttons to allow the\n\nInternet Applications\n2@7\nuser to choose between search-by-name and search-by-description (as \\vell as a submit but-\nton to retrieve the results),\nThe page that handles search results should be modeled after\nproducts.jsp (as described in the previous exercise) and be called products.jsp. It should\nretrieve all records where the search text is a substring of the name or description (as chosen\nby the user).\nTo integrate this with the previous exercise, simply replace all the links to\nproducts. j sp with search. j sp.\nExercise 7.5 'Write a simple authentication mechanism (without using encrypted transfer of\npasswords, for simplicity). We say a user is authenticated if she has provided a valid username-\npassword combination to the system; otherwise, we say the user is not authenticated. Assume\nfor simplicity that you have a database schema that stores only a customer id and a password:\nPasswords(cid: integer, username: string, password: string)\n1. How and where are you going to track when a user is 'logged on' to the system?\n2. Design a page that allows a registered user to log on to the system.\n3. Design a page header that checks whether the user visiting this page is logged in.\nExercise 7.6 (Due to Jeff Derstadt) TechnoBooks.com is in the process of reorganizing its\nwebsite.\nA major issue is how to efficiently handle a large number of search results.\nIn a\nhuman interaction study, it found that modem users typically like to view 20 search results at\na time, and it would like to program this logic into the system. Queries that return batches of\nsorted results are called top N queries. (See Section 25.5 for a discussion of database support\nfor top N queries.)\nFor example, results 1-20 are returned, then results\n21~40, then 41-60,\nand so OIl. Different techniques are used for performing top N queries and TechnoBooks.com\nwould like you to implement two of them.\nInfrastructure:\nCreate a database with a table called Books and populate it with some\nbooks, using the format that follows. This gives you III books in your database with a title\nof AAA, BBB, CCC, DDD, or EEE, but the keys are not sequential for books with the same\ntitle.\nBooks( bookid: INTEGER, title: CHAR(80), author: CHAR(80), price: REAL)\nFor i = 1 to 111 {\nInsert the tuple (i, \"AAA\", \"AAA Author\", 5.99)\ni=i+l\nInsert the tuple (i, \"BBB\", \"BBB Author\", 5.99)\ni = i + 1\nInsert the tuple (i, \"CCC\", \"CCC Author\", 5.99)\ni=i+1\nInsert the tuple (i, \"DDD\", \"DDD Author\", 5.99)\n1=i+l\nInsert the tuple (i, \"EEE\", \"EEE Author\", 5.99)\nPlaceholder Technique:\nThe simplest approach to top N queries is to store a placeholder\nfor the first and last result tuples, and then perform the same query. When the new query\nresults are returned, you can iterate to the placeholders and return the previous or next 20\nresults.\n\n268\nI Tuples Shown\nLower Placeholder\nPrevious Set\nUpper Placeholder\nNext Set I\n1-20\n1\nNone\n20\n\"-\n21-40\n21-40\n21\n1-20\n40\n41-60\n41-60\n41\n21-40\n60\n61-80\nWrite a webpage in JSP that displays the contents of the Books table, sorted by the Title and\nBookId, and showing the results 20 at a time. There should be a link (where appropriate) to\nget the previous 20 results or the next 20 results. To do this, you can encode the placeholders\nin the Previous or Next Links as follows. Assume that you are displaying records 21-40. Then\nthe previous link is display. j sp?lower=21 and the next link is display. j sp?upper=40.\nYou should not display a previous link when there are no previous results; nor should you\nshow a Next link if there are no more results. When your page is called again to get another\nbatch of results, you can perform the same query to get all the records, iterate through the\nresult set until you are at the proper starting point, then display 20 more results.\nWhat are the advantages and disadvantages of this technique?\nQuery Constraints Technique:\nA second technique for performing top N queries is to\npush boundary constraints into the query (in the WHERE clause) so that the query returns only\nresults that have not yet been displayed. Although this changes the query, fewer results are\nreturned and it saves the cost of iterating up to the boundary.\nFor example, consider the\nfollowing table, sorted by (title, primary key).\nI Batch I Result Number\nTitle\nI Primary Key\n1\n1\nAAA\n105\n1\n2\nBBB\n13\n1\n3\neee\n48\n1\n4\nDDD\n52\n1\n5\nDDD\n101\n2\n6\nDDD\n121\n2\n7\nEEE\n19\n2\n8\nEEE\n68\n2\n9\nFFF\n2\n2\n10\nFFF\n33\nFFF\n.\"~\n:~\n11\n58\n3\n12\nFFF\n59\n3\n13\nGGG\n93\n3\n14\nEHH\n132\n3\n15\nHHH\n135\nIn batch 1, rows 1 t.hrough 5 are displayed, in batch 2 rows 6 through 10 are displayed, and so\non. Using the placeholder technique, all 15 results would be returned for each batch. Using\nthe constraint technique, batch 1 displays results 1-5 but returns results 1-15, batch 2 will\ndisplay results 6-10 but returns only results 6-15, and batch\n:~ will display results 11-15 but\nreturn only results 11-15.\n\nInternet Applications\n299\nThe constraint can be pushed into the query because of the sorting of this table. Consider\nthe following query for batch 2 (displaying results 6-10):\nEXEC SQL SELECT B.Title\nFROM\nBooks B\nWHERE\n(B.Title = 'DDD' AND B.BookId > 101) OR (B.Title > 'DDD')\nORDER BY B.Title, B.Bookld\nThis query first selects all books with the title 'DDD,' but with a primary key that is greater\nthan that of record 5 (record 5 has a primary key of 101). This returns record 6. Also, any\nbook that has a title after 'DDD' alphabetically is returned. You can then display the first\nfive results.\nThe following information needs to be retained to have Previous and Next buttons that return\nmore results:\n•\nPrevious: The title of the first record in the previous set, and the primary key of the\nfirst record in the previous set.\n•\nNext: The title of the first record in the next set; the primary key of the first record in\nthe next set.\nThese four pieces of information can be encoded into the Previous and Next buttons as in the\nprevious part. Using your database table from the first part, write a JavaServer Page that\ndisplays the book information 20 records at a time. The page should include Previous and\nNext buttons to show the previous or next record set if there is one. Use the constraint query\nto get the Previous and Next record sets.\nPROJECT~BASEDEXERCISES\nIn this chapter, you continue the exercises from the previous chapter and create the parts of\nthe application that reside at the middle tier and at the presentation tier. More information\nabout these exercises and material for more exercises can be found online at\nhttp://~.cs.wisc.edu/-dbbook\nExercise 7.7 Recall the Notown Records website that you worked on in Exercise 6.6. Next,\nyou are asked to develop the actual pages for the Notown Records website. Design the part\nof the website that involves the presentation tier and the middle tier, and integrate the code\nthat you wrote in Exercise 6.6 to access the database.\nI. Describe in detail the set of webpages that users can access. Keep the following issues\nin mind:\n•\nAll users start at a common page.\n•\nFor each action, what input does the user provide? How will the user provide it -by\nclicking on a link or through an HTML form?\n•\nWhat sequence of steps does a user go through to purchase a record? Describe the\nhigh-level application flow by showing how each lIser action is handled.\n\n270\nCHAPTER .,7\n2. vVrite the webpages in HTML without dynamic content.\n3. vVrite a page that allows users to log on to the site. Use cookies to store the information\npermanently at the user's browser.\n4. Augment the log-on page with JavaScript code that checks that the username consists\nonly of the characters from a to z.\n5. Augment the pages that allow users to store items in a shopping basket with a condition\nthat checks whether the user has logged on to the site. If the user has not yet logged on,\nthere should be no way to add items to the shopping cart. Implement this functionality\nusing JSP by checking cookie information from the user.\n6. Create the remaining pages to finish the website.\nExercise 7.8 Recall the online pharmacy project that you worked on in Exercise 6.7 in\nChapter 6. Follow the analogous steps from Exercise 7.7 to design the application logic and\npresentation layer and finish the website.\nExercise 7.9 Recall the university database project that you worked on in Exercise 6.8 in\nChapter 6. Follow the analogous steps from Exercise 7.7 to design the application logic and\npresentation layer and finish the website.\nExercise 7.10 Recall the airline reservation project that you worked on in Exercise 6.9 in\nChapter 6. Follow the analogous steps from Exercise 7.7 to design the application logic and\npresentation layer and finish the website.\nBIBLIOGRAPHIC NOTES\nThe latest version of the standards mentioned in this chapter can be found at the website\nof the World Wide Web Consortium (www. w3. org).\nIt contains links to information about\nI-ITML, cascading style sheets, XIvIL, XSL, and much more.\nThe book by Hall is a gen-\neral introduction to Web progn1.111ming technologies [357]; a good starting point on the Web\nis www.Webdeve1oper.com.\nThere are many introductory books on CGI progranuning, for\nexample [210, 198].\nThe JavaSoft (java. sun. com) home page is a good starting point for\nServlets, .JSP, and all other Java-related technologies. The book by Hunter [394] is a good\nintroduction to Java Servlets. Microsoft supports Active Server Pages (ASP), a comparable\ntedmology to .lSI'. l'vIore information about ASP can be found on the Microsoft Developer's\nNetwork horne page (msdn. microsoft. com).\nThere are excellent websites devoted to the advancement of XML, for example 1.l1-iTW. xm1. com\nand www.ibm.com/xm1. that also contain a plethora of links with information about the other\nstandards. There are good introductory books on many diflerent aspects of XML, for exarnple\n[195, 158,597,474, :381, 320]. Information about UNICODE can be found on its home page\nhttp://www.unicode.org.\nInforrnation about .lavaServer Pages ane! servlets can be found on the JavaSoft home page at\njava. sun. com at java. sun. com/products/j sp and at java. sun. com/products/servlet.\n\nPART III\nSTORAGE AND INDEXING\n\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n8\nOVERVIEW'OF STORAGE\nAND INDEXING\n..\nHow does a DBMS store and access persistent data?\n..\nWhy is I/O cost so important for database operations?\n..\nHow does a DBMS organize files of data records on disk to minimize\nI/O costs?\n...\nWhat is an index, and why is it used?\n..\nWhat is the relationship between a file of data records and any indexes\non this file of records?\n..\nWhat are important properties of indexes?\n..\nHow does a hash-based index work, and when is it most effective?\n..\nHow does a tree-based index work, and when is it most effective?\n...\nHow can we use indexes to optimize performance for a given workload?\n..\nKey concepts: external storage, buffer manager, page I/O; file orga-\nnization, heap files, sorted files; indexes, data entries, search keys, clus-\ntered index, clustered file, primary index; index organization, hash-\nbased and tree-based indexes; cost comparison, file organizations and\ncommon operations; performance tuning, workload, composite search\nkeys, use of clustering,\n____________________J\nIf you don't find it in the index, look very carefully through the entire catalog.\n--Sears, Roebuck, and Co., Consumers' Guide, 1897\nThe ba.'3ic abstraction of data in a DBMS is a collection of records, or a file,\nand each file consists of one or more pages.\nThe files and access methods\n\n274\nCHAPTER 8\nsoftware layer organizes data carefully to support fast access to desired subsets\nof records.\nUnderstanding how records are organized is essential to using a\ndatabase system effectively, and it is the main topic of this chapter.\nA file organization is a method of arranging the records in a file when the\nfile is stored on disk. Each file organization makes certain operations efficient\nbut other operations expensive.\nConsider a file of employee records, each containing age, name, and sal fields,\nwhich we use as a running example in this chapter.\nIf we want to retrieve\nemployee records in order of increasing age, sorting the file by age is a good file\norganization, but the sort order is expensive to maintain if the file is frequently\nmodified. Further, we are often interested in supporting more than one oper-\nation on a given collection of records. In our example, we may also want to\nretrieve all employees who make more than $5000. We have to scan the entire\nfile to find such employee records.\nA technique called indexing can help when we have to access a collection of\nrecords in multiple ways, in addition to efficiently supporting various kinds of\nselection. Section 8.2 introduces indexing, an important aspect of file organi-\nzation in a DBMS. We present an overview of index data structures in Section\n8.3; a more detailed discussion is included in Chapters 10 and 11.\nWe illustrate the importance of choosing an appropriate file organization in\nSection 8.4 through a simplified analysis of several alternative file organizations.\nThe cost model used in this analysis, presented in Section 8.4.1, is used in\nlater chapters as welL In Section 8.5, we highlight some important choices to\nbe made in creating indexes.\nChoosing a good collection of indexes to build\nis arguably the single most powerful tool a database administrator has for\nimproving performance.\n8.1\nDATA ON EXTERNAL STORAGE\nA DBMS stores vast quantities of data, and the data must persist across pro-\ngram executions. Therefore, data is stored on external storage devices such as\ndisks and tapes, and fetched into main memory as needed for processing. The\nunit of information read from or written to disk is a page. The size of a page\nis a DBMS parameter, and typical values are 4KB or 8KB.\nThe cost of page I/O (input from disk to main Inemory and output from mem-\nory to disk) dominates the cost of typical database operations, and databa,'>e\nsystems are carefully optimized to rninimize this cost. While the details of how\n\nStorage and Indexing\n:175\nfiles of records are physically stored on disk and how main memory is utilized\nare covered in Chapter 9, the following points are important to keep in mind:\n•\nDisks are the most important external storage devices. They allow us to\nretrieve any page at a (more or less) fixed cost per page. However, if we\nread several pages in the order that they are stored physically, the cost can\nbe much less than the cost of reading the same pages in a random order.\n•\nTapes are sequential access devices and force us to read data one page after\nthe other. They are mostly used to archive data that is not needed on a\nregular basis.\n•\nEach record in a file has a unique identifier called a record id, or rid for\nshort. An rid ha.'3 the property that we can identify the disk address of the\npage containing the record by using the rid.\nData is read into memory for processing, and written to disk for persistent\nstorage, by a layer of software called the buffer manager. When the files and\naccess methods layer (which we often refer to as just the file layer) needs to\nprocess a page, it asks the buffer manager to fetch the page, specifying the\npage's rid. The buffer manager fetches the page from disk if it is not already\nin memory.\nSpace on disk is managed by the disk space m,anager, according to the DBMS\nsoftware architecture described in Section 1.8. When the files and access meth-\nods layer needs additional space to hold new records in a file, it asks the disk\nspace manager to allocate an additional disk page for the file; it also informs\nthe disk space manager when it no longer needs one of its disk pages. The disk\nspace manager keeps track of the pages in use by the file layer; if a page is freed\nby the file layer, the space rnanager tracks this, and reuses the space if the file\nlayer requests a new page later on.\nIn the rest of this chapter, we focus on the files and access methods layer.\n8.2\nFILE ORGANIZATIONS AND INDEXING\nThe file of records is an important abstraction in a DBMS, and is imple-\nmented by the files and access methods layer of the code. A file can be created,\ndestroyed, and have records inserted into and deleted from it. It also supports\nscallS; a scan operation allows us to step through all the records in the file one\nat a time. A relatioll is typically stored a.':l a file of records.\nThe file layer stores the records in a file in a collection of disk pages. It keeps\ntrack of pages allocated to each file, and as records are inserted into and deleted\nfrom the file, it also tracks availa.ble space within pages allocated to the file.\n\n276\nCHAPTER 8\nThe simplest file structure is an unordered file, or heap file.\nRecords in a\nheap file are stored in random order across the pages of the file.\nA heap file\norganization supports retrieval of all records, or retrieval of a particular record\nspecified by its rid; the file manager must keep track of the pages allocated for\nthe file. (\"Ve defer the details of how a heap file is implemented to Chapter 9.)\nAn index is a data structure that organizes data records on disk to optimize\ncertain kinds of retrieval operations. An index allows us to efficiently retrieve\nall records that satisfy search conditions on the search key fields of the index.\nWe can also create additional indexes on a given collection of data records,\neach with a different search key, to speed up search operations that are not\nefficiently supported by the file organization used to store the data records.\nConsider our example of employee records. We can store the records in a file\norganized as an index on employee age; this is an alternative to sorting the file\nby age. Additionally, we can create an auxiliary index file based on salary, to\nspeed up queries involving salary. The first file contains employee records, and\nthe second contains records that allow us to locate employee records satisfying\na query on salary.\n\"Ve use the term data entry to refer to the records stored in an index file. A\ndata entry with search key value k, denoted as k*, contains enough information\nto locate (one or more) data records with search key value k. We can efficiently\nsearch an index to find the desired data entries, and then use these to obtain\ndata records (if these are distinct from data entries).\nThere are three main alternatives for what to store as a data entry in an index:\n1. A data entry h\nis an actual data record (with search key value k).\n2. A data entry is a (k, rid) pair, where rid is the record id of a data record\nwith search key value k.\n3. A data entry is a (k. rid-list) pair, where rid-list is a list of record ids of\ndata records with search key value k.\nOf course, if the index is used to store actual data records, Alternative (1),\neach entry b\nis a data record with search key value k. We can think of such an\nindex &'3 a special file organization. Such an indexed file organization can\nbe used instead of, for exarnple, a sorted file or an unordered file of records.\nAlternatives (2) and (3), which contain data entries that point to data records,\nare independent of the file organization that is used for the indexed file (i.e.,\n\nStorage and Indexing\n277.\nthe file that contains the data records). Alternative (3) offers better space uti-\nlization than Alternative (2), but data entries are variable in length, depending\non the number of data records with a given search key value.\nIf we want to build more than one index on a collection of data records-for\nexample, we want to build indexes on both the age and the sal fields for a col-\nlection of employee records-~at most one of the indexes should use Alternative\n(1) because we should avoid storing data records multiple times.\n8.2.1\nClustered Indexes\nWhen a file is organized so that the ordering of data records is the same as\nor close to the ordering of data entries in some index, we say that the index\nis clustered; otherwise, it clustered is an unclustered index. An index that\nuses Alternative (1) is clustered, by definition. An index that uses Alternative\n(2) or (3) can be a clustered index only if the data records are sorted on the\nsearch key field. Otherwise, the order of the data records is random, defined\npurely by their physical order, and there is no reasonable way to arrange the\ndata entries in the index in the same order.\nIn practice, files are rarely kept sorted since this is too expensive to maintain\nwhen the data is updated~ So, in practice, a clustered index is an index that uses\nAlternative (1), and indexes that use Alternatives (2) or (3) are unclustered.\nWe sometimes refer to an index using Alternative (1) as a clustered file,\nbecause the data entries are actual data records, and the index is therefore a\nfile of data records. (As observed earlier, searches and scans on an index return\nonly its data entries, even if it contains additional information to organize the\ndata entries.)\nThe cost of using an index to answer a range search query can vary tremen-\ndously based on whether the index is clustered. If the index is clustered, i.e.,\nwe are using the search key of a clustered file, the rids in qualifying data entries\npoint to a contiguous collection of records, and we need to retrieve only a few\ndata pages. If the index is unclustered, each qualifying data entry could contain\na rid that points to a distinct data page, leading to as many data page l/Os\n8.'3 the number of data entries that match the range selection, as illustrated in\nFigure 8.1. This point is discussed further in Chapter 13.\n8.2.2\nPrimary and Secondary Indexes\nAn index on a set of fields that includes the primaTy key (see Chapter 3) is\ncalled a primary index; other indexes are called secondary indexes.\n(The\nterms jJrimaTy inde.T and secondaTy index are sometimes used with a different\n\n278\nIndex entries\n(Direct search for\ndata enrries)\nData entries\nData\nrecords\nFigure 8.1\nUuelllst.ered Index Using Alt.ernat.ive (2)\nCHAPTER\n.~\nIndex file\nData tile\nmeaning: An index that uses Alternative (1) is called a primary index, and\none that uses Alternatives (2) or (3) is called a secondary index. We will be\nconsistent with the definitions presented earlier, but the reader should be aware\nof this lack of standard terminology in the literature.)\nTwo data entries are said to be duplicates if they have the same value for the\nsearch key field associated with the index. A primary index is guaranteed not\nto contain duplicates, but an index on other (collections of) fields can contain\nduplicates.\nIn general, a secondary index contains duplicates.\nIf we know\ntha.t no duplicates exist, that is, we know that the search key contains some\ncandidate key, we call the index a unique index.\nAn important issue is how data entries in an index are organized to support\nefficient retrieval of data entries.vVe discuss this next.\n8.3\nINDEX DATA STRUCTURES\nOne way to organize data entries is to hash data entries on the sea.rch key.\nAnother way to organize data entries is to build a tree-like data structure that\ndirects a search for data entries. \"Ve introduce these two basic approaches ill\nthis section.\n\\iV~e study tree-based indexing in more detail in Chapter 10 and\nha\"sh-based indexing in Chapter 11.\nWe note that the choice of hash or tree indexing techniques can be combined\nwith any of the three alternatives for data entries.\n\nStoTage and Indexing\n8.3.1\nHash-Based Indexing\n2'49\nVie can organize records using a technique called hashing to quickly find records\nthat have a given search key value. For example, if the file of employee records\nis hashed on the name field, we can retrieve all records about Joe.\nIn this approach, the records in a file are grouped in buckets, where a bucket\nconsists of a primary page and, possibly, additional pages linked in a chain.\nThe bucket to which a record belongs can be determined by applying a special\nfunction, called a hash function, to the search key. Given a bucket number,\na hash-based index structure allows us to retrieve the primary page for the\nbucket in one or two disk l/Os.\nOn inserts, the record is inserted into the appropriate bucket, with 'overflow'\npages allocated as necessary.\nTo search for a record with a given search key\nvalue, we apply the hash function to identify the bucket to which such records\nbelong and look at all pages in that bucket. If we do not have the search key\nvalue for the record, for example, the index is based on sal and we want records\nwith a given age value, we have to scan all pages in the file.\nIn this chapter, we assume that applying the hash function to (the search key\nof) a record allows us to identify and retrieve the page containing the record\nwith one I/O. In practice, hash-based index structures that adjust gracefully\nto inserts and deletes and allow us to retrieve the page containing a record in\none to two l/Os (see Chapter 11) are known.\nHash indexing is illustrated in Figure 8.2, where the data is stored in a file that\nis hashed on age; the data entries in this first index file are the actual data\nrecords.\nApplying the hash function to the age field identifies the page that\nthe record belongs to. The hash function h for this example is quite simple;\nit converts the search key value to its binary representation and uses the two\nleast significant bits as the bucket identifier.\nFigure 8.2 also shows an index with search key sal that contains (sal, rid) pairs\nas data entries. The tid (short for record id) component of a data entry in this\nsecond index is a pointer to a record with search key value sal (and is shown\nin the figure as an arrow pointing to the data record).\nUsing the terminology introduced in Section 8.2, Figure 8.2 illustrates Alter-\nnativE\"S (1) and (2) for data entries. The file of employee records is hashed on\nage, and Alternative (1) is used for for data entries. The second index, on sal,\nalso uses hashing to locate data entries, which are now (sal, rid of employee\nrecoT'(~ pairs; that is, Alternative (2) is used for data entries.\n\n280\nCHAPTER 8\nAshby. 25, 3000\nBasu, 33, 4003\nBristow,29,2007\nh(age)=l0\nCass, 50, 5004\nDaniels, 22, 6003\nFile of <sal, rid> pairs\nEmployees file hashed on age\nhashed on sal\nFigure 8.2\nIndex-Organized File Hashed on age, with Auxiliary Index on sal\nNote that the search key for an index can be any sequence of one or more\nfields, and it need not uniquely identify records.\nFor example, in the salary\nindex, two data entries have the same search key value 6003.\n(There is an\nunfortunate overloading of the term key in the database literature. A primary\nkey or candidate key-fields that uniquely identify a record; see Chapter 3~is\nunrelated to the concept of a search key.)\n8.3.2\nTree-Based Indexing\nAn alternative to hash-based indexing is to organize records using a tree-\nlike data structure.\nThe data entries are arranged in sorted order by search\nkey value, and a hierarchical search data structure is maintained that directs\nsearches to the correct page of data entries.\nFigure 8.3 shows the employee records from Figure 8.2, this time organized in a\ntree-structured index with search keyage. Each node in this figure (e.g., nodes\nlabeled A, B, L1, L2) is a physical page, and retrieving a node involves a disk\nI/O.\nThe lowest level of the tree, called the leaf level, contains the data entries;\nin our example, these are employee records. To illustrate the ideas better, we\nhave drawn Figure 8.3 as if there were additional employee records, some with\nage less than 22 and some with age greater than EiO (the lowest and highest\nage values that appear in Figure 8.2). Additional records with age less than\n22 would appear in leaf pages to the left page L1, and records with age greater\nthan 50 would appear in leaf pages to the right of page\nL~~.\n\nStorage and Indel:ing\n281\n~\n...\nLEAF LEVEL\nL1\n/\nDaniels. 22. 6003\n/'\"\"\nAshby, 25, 3000\n/\nI--B-ris-to-w-,2-9,-2-00-7--Y\nL3\nSmith, 44, 3000\nTracy, 44, 5004\nCass, 50, 5004\n...\nFigure 8.3\nTree·Structured Index\nThis structure allows us to efficiently locate all data entries with search key\nvalues in a desired range. All searches begin at the topmost node, called the\nroot, and the contents of pages in non-leaf levels direct searches to the correct\nleaf page. Non-leaf pages contain node pointers separated by search key values.\nThe node pointer to the left of a key value k points to a subtree that contains\nonly data entries less than k. The node pointer to the right of a key value k\npoints to a subtree that contains only data entries greater than or equal to k.\nIn our example, suppose we want to find all data entries with 24 < age < 50.\nEach edge from the root node to a child node in Figure 8.2 has a label that\nexplains what the corresponding subtree contains. (Although the labels for the\nremaining edges in the figure are not shown, they should be easy to deduce.)\nIn our example search, we look for data entries with search key value > 24,\nand get directed to the middle child, node A. Again, examining the contents\nof this node, we are directed to node B. Examining the contents of node B, we\nare directed to leaf node Ll, which contains data entries we are looking for.\nObserve that leaf nodes L2 and L3 also contain data entries that satisfy our\nsearch criterion. To facilitate retrieval of such qualifying entries during search,\nall leaf pages are maintained in a doubly-linked list. Thus, we can fetch page\nL2 using the 'next' pointer on page Ll, and then fetch page L3 using the 'next'\npointer on L2.\nThus, the number of disk I/Os incurred during a search is equal to the length\nof a path from the root to a leaf, plus the number of leaf pages with qualifying\ndata entries. The B+ tree is an index structure that ensures that all paths\nfrom the root to a leaf in a given tree are of the same length, that is, the\nstructure is always balanced in height. Finding the correct leaf page is faster\n\n282\nCHAPTER 8\n-1,\nthan binary search of the pages in a sorted file because each non~leaf node can\naccommodate a very large number of node-pointers, and the height of the tree\nis rarely more than three or four in practice. The height of a balanced tree is\nthe length of a path from root to leaf; in Figure 8.3, the height is three. The\nnumber of l/Os to retrieve a desired leaf page is four, including the root and\nthe leaf page.\n(In practice, the root is typically in the buffer pool because it\nis frequently accessed, and we really incur just three I/Os for a tree of height\nthree.)\nThe average number of children for a non-leaf node is called the fan-out of\nthe tree. If every non-leaf node has n children, a tree of height h has nh leaf\npages. In practice, nodes do not have the same number of children, but using\nthe average value F for n, we still get a good approximation to the number of\nleaf pages, F h . In practice, F is at least 100, which means a tree of height four\ncontains 100 million leaf pages. Thus, we can search a file with 100 million leaf\npages and find the page we want using four l/Os; in contrast, binary search of\nthe same file would take log21OO,000,000 (over 25) l/Os.\n8.4\nCOMPARISON OF FILE ORGANIZATIONS\nWe now compare the costs of some simple operations for several basic file\norganizations on a collection of employee records. We assume that the files and\nindexes are organized according to the composite search key (age, sa~, and that\nall selection operations are specified on these fields. The organizations that we\nconsider are the following:\n•\nFile of randomly ordered employee records, or heap file.\n•\nFile of employee records sorted on (age, sal).\n•\nClustered B+ tree file with search key (age, sal).\n•\nHeap file with an unclustered B+ tree index on (age, sal).\n•\nHeap file with an unclustered hash index on (age, sal).\nOur goal is to emphasize the importance of the choice of an appropriate file\norganization, and the above list includes the main alternatives to consider in\npractice. Obviously, we can keep the records unsorted or sort them. We can\nalso choose to build an index on the data file. Note that even if the data file\nis sorted, an index whose search key differs from the sort order behaves like an\nindex on a heap file!\nThe operations we consider are these:\n\nStorage and Indexing\n283\n•\nScan: Fetch all records in the file. The pages in the file must be fetched\nfrom disk into the buffer pool. There is also a CPU overhead per record\nfor locating the record on the page (in the pool).\n•\nSearch with Equality Selection: Fetch all records that satisfy an equal-\nity selection; for example, \"Find the employee record for the employee with\nage 23 and sal 50.\" Pages that contain qualifying records must be fetched\nfrom disk, and qualifying records must be located within retrieved pages.\n•\nSearch with Range Selection: Fetch all records that satisfy a range\nselection; for example, \"Find all employee records with age greater than\n35.\"\n•\nInsert a Record: Insert a given record into the file. We must identify the\npage in the file into which the new record must be inserted, fetch that page\nfrom disk, modify it to include the new record, and then write back the\nmodified page. Depending on the file organization, we may have to fetch,\nmodify, and write back other pages as well.\n•\nDelete a Record: Delete a record that is specified using its rid. We must\nidentify the page that contains the record, fetch it from disk, modify it, and\nwrite it back. Depending on the file organization, we may have to fetch,\nmodify, and write back other pages as well.\n8.4.1\nCost Model\nIn our comparison of file organizations, and in later chapters, we use a simple\ncost model that allows us to estimate the cost (in terms of execution time) of\ndifferent database operations. We use B to denote the number of data pages\nwhen records are packed onto pages with no wasted space, and R to denote\nthe number of records per page.\nThe average time to read or write a disk\npage is D, and the average time to process a record (e.g., to compare a field\nvalue to a selection constant) is C. In the ha.'3hed file organization, we use a\nfunction, called a hash function, to map a record into a range of numbers; the\ntime required to apply the hash function to a record is H. For tree indexes, we\nwill use F to denote the fan-out, which typically is at lea.'3t 100 as mentioned\nin Section 8.3.2.\nTypical values today are D = 15 milliseconds, C and H = 100 nanoseconds; we\ntherefore expect the cost of I/O to dominate. I/O is often (even typically) the\ndominant component of the cost of database operations, and so considering I/O\ncosts gives us a good first approximation to the true costs. Further, CPU speeds\nare steadily rising, whereas disk speeds are not increasing at a similar pace. (On\nthe other hand, as main memory sizes increase, a much larger fraction of the\nneeded pages are likely to fit in memory, leading to fewer I/O requests!) \\Ve\n\n284\nCHAPTER,8\nhave chosen to concentrate on the I/O component of the cost model, and we\nassume the simple constant C for in-memory per-record processing cost. Bear\nthe follO\\ving observations in mind:\n..\nReal systems must consider other aspects of cost, such as CPU costs (and\nnetwork transmission costs in a distributed database).\n..\nEven with our decision to focus on I/O costs, an accurate model would be\ntoo complex for our purposes of conveying the essential ideas in a simple\nway. We therefore use a simplistic model in which we just count the number\nof pages read from or written to disk as a measure of I/O. \\lVe ignore the\nimportant issue of blocked access in our analysis-typically, disk systems\nallow us to read a block of contiguous pages in a single I/O request. The\ncost is equal to the time required to seek the first page in the block and\ntransfer all pages in the block. Such blocked access can be much cheaper\nthan issuing one I/O request per page in the block, especially if these\nrequests do not follow consecutively, because we would have an additional\nseek cost for each page in the block.\nWe discuss the implications of the cost model whenever our simplifying as-\nsumptions are likely to affect our conclusions in an important way.\n8.4.2\nHeap Files\nScan: The cost is B(D +RC) because we must retrieve each of B pages taking\ntime D per page, and for each page, process R records taking time C per record.\nSearch with Equality Selection: Suppose that we know in advance that\nexactly one record matches the desired equality selection, that is, the selection\nis specified on a candidate key. On average, we must scan half the file, assuming\nthat the record exists and the distribution of values in the search field is uniform.\nFor each retrieved data page, we must check all records on the page to see if\nit is the desired record. The cost is O.5B(D + RC). If no record satisfies the\nselection, however, we must scan the entire file to verify this.\nIf the selection is not on a candidate key field (e.g., \"Find employees aged 18\"),\nwe always have to scan the entire file because records with age = 18 could be\ndispersed all over the file, and we have no idea how many such records exist.\nSearch with Range Selection:\nThe entire file must be scanned because\nqualifying records could appear anywhere in the file, and we do not know how\nmany qualifying records exist. The cost is B(D + RC).\n\nStorage and Inde:r'ing\n285\nf\nInsert: \\Ve assume that records are always inserted at the end of the file. \\¥e\nmust fetch the last page in the file, add the record, and write the page back.\nThe cost is 2D + C.\nDelete: We must find the record, remove the record from the page, and write\nthe modified page back. vVe assume that no attempt is made to compact the\nfile to reclaim the free space created by deletions, for simplicity. 1 The cost is\nthe cost of searching plus C + D.\nWe assume that the record to be deleted is specified using the record id. Since\nthe page id can easily be obtained from the record id, we can directly read in\nthe page. The cost of searching is therefore D.\nIf the record to be deleted is specified using an equality or range condition\non some fields, the cost of searching is given in our discussion of equality and\nrange selections. The cost of deletion is also affected by the number of qualifying\nrecords, since all pages containing such records must be modified.\n8.4.3\nSorted Files\nScan: The cost is B(D +RC) because all pages must be examined. Note that\nthis case is no better or worse than the case of unordered files. However, the\norder in which records are retrieved corresponds to the sort order, that is, all\nrecords in age order, and for a given age, by sal order.\nSearch with Equality Selection:\nWe assume that the equality selection\nmatches the sort order (age, sal). In other words, we assume that a selection\ncondition is specified on at leclst the first field in the composite key (e.g., age =\n30). If not (e.g., selection sal =\nt50 or department = \"Toy\"), the sort order\ndoes not help us and the cost is identical to that for a heap file.\nWe can locate the first page containing the desired record or records, should\nany qualifying records exist, with a binary search in log2B steps. (This analysis\nassumes that the pages in the sorted file are stored sequentially, and we can\nretrieve the ith page on the file directly in one disk I/O.) Each step requires\na disk I/O and two cornparisons. Once the page is known, the first qualifying\nrecord can again be located by a binary search of the page at a cost of Clog2R.\nThe cost is Dlo92B+Clog2R, which is a significant improvement over searching\nheap files.\n]In practice, a directory or other data structure is used to keep track of free space, and records are\ninserted into the first available free slot, as discussed in Chapter 9. This increases the cost of insertion\nand deletion a little, but not enough to affect our comparison.\n\n286\nCHAPTER~8\nIf several records qualify (e.g., \"Find all employees aged 18\"), they are guar-\nanteed to be adjacent to each other due to the sorting on age, and so the\ncost of retrieving all such records is the cost of locating the first such record\n(Dlog2B+Clog2R) plus the cost ofreading all the qualifying records in sequen-\ntial order. Typically, all qualifying records fit on a single page. If no records\nqualify, this is established by the search for the first qualifying record, which\nfinds the page that would have contained a qualifying record, had one existed,\nand searches that page.\nSearch with Range Selection:\nAgain assuming that the range selection\nmatches the composite key, the first record that satisfies the selection is located\nas for search with equality. Subsequently, data pages are sequentially retrieved\nuntil a record is found that does not satisfy the range selection; this is similar\nto an equality search with many qualifying records.\nThe cost is the cost of search plus the cost of retrieving the set of records that\nsatisfy the search. The cost of the search includes the cost of fetching the first\npage containing qualifying, or matching, records.\nFor small range selections,\nall qualifying records appear on this page. For larger range selections, we have\nto fetch additional pages containing matching records.\nInsert: To insert a record while preserving the sort order, we must first find\nthe correct position in the file, add the record, and then fetch and rewrite all\nsubsequent pages (because all the old records are shifted by one slot, assuming\nthat the file has no empty slots). On average, we can &'3sume that the inserted\nrecord belongs in the middle of the file. Therefore, we must read the latter half\nof the file and then write it back after adding the new record. The cost is that\nof searching to find the position of the new record plus 2 . (O.5B(D + RC)),\nthat is, search cost plus B(D + RC).\nDelete: We must search for the record, remove the record from the page, and\nwrite the modified page back.\nWe must also read and write all subsequent\npages because all records that follow the deleted record must be moved up to\ncornpact the free space. 2 The cost is the same as for an insert, that is, search\ncost plus B(D + RC). Given the rid of the record to delete, we can fetch the\npage containing the record directly.\nIf records to be deleted are specified by an equality or range condition, the cost\nof deletion depends on the number of qualifying records. If the condition is\nspecified on the sort field, qualifying records are guaranteed to be contiguous,\nand the first qualifying record can be located using binary search.\n2Unlike a heap file. there is no inexpensive way to manage free space, so we account for the cost\nof compacting it file when il record is deleted.\n\nil\nStorage and Indexing\n8.4.4\nClustered Files\nIn a clustered file, extensive empirical study has shown that pages are usually\nat about 67 percent occupancy.\nThus, the Humber of physical data pages is\nabout 1.5B, and we use this observation in the following analysis.\nScan: The cost of a scan is 1.5B(D + RC) because all data pages must be\nexamined; this is similar to sorted files, with the obvious adjustment for the\nincreased number of data pages. Note that our cost metric does not capture\npotential differences in cost due to sequential I/O. We would expect sorted files\nto be superior in this regard, although a clustered file using ISAM (rather than\nB+ trees) would be close.\nSearch with Equality Selection:\nWe assume that the equality selection\nmatches the search key (age, sal).\nWe can locate the first page containing\nthe desired record or records, should any qualifying records exist, in logF1.5B\nsteps, that is, by fetching all pages from the root to the appropriate leaf. In\npractice, the root page is likely to be in the buffer pool and we save an I/O,\nbut we ignore this in our simplified analysis.\nEach step requires a disk I/O\nand two comparisons. Once the page is known, the first qualifying record can\nagain be located by a binary search of the page at a cost of Clog2R. The cost\nis DlogF1.5B +Clog2R, which is a significant improvement over searching even\nsorted files.\nIf several records qualify (e.g., \"Find all employees aged 18\"), they are guar-\nanteed to be adjacent to each other due to the sorting on age, and so the\ncost of retrieving all such records is the cost of locating the first such record\n(Dlogp1.5B + Clog2R) plus the cost of reading all the qualifying records in\nsequential order.\nSearch with Range Selection:\nAgain assuming that the range selection\nmatches the composite key, the first record that satisfies the selection is located\na..'3 it is for search with equality.\nSubsequently, data pages are sequentially\nretrieved (using the next and previous links at the leaf level) until a record is\nfound that does not satisfy the range selection; this is similar to an equality\nsearch with many qualifying records.\nInsert: To insert a record, we must first find the correct leaf page in the index,\nreading every page from root to leaf. Then, we must add the llew record. Most\nof the time, the leaf page has sufficient space for the new record, and all we\nneed to do is to write out the modified leaf page. Occasionally, the leaf is full\nand we need to retrieve and modify other pages, but this is sufficiently rare\n\n288\nCHAPTER :8\nthat we can ignore it in this simplified analysis. The cost is therefore the cost\nof search plus one write, DlogF L5B + Clog2R + D.\nDelete:\n\\;Ye must search for the record, remove the record from the page,\nand write the modified page back. The discussion and cost analysis for insert\napplies here as well.\n8.4.5\nHeap File with Unclustered Tree Index\nThe number of leaf pages in an index depends on the size of a data entry.\nWe assume that each data entry in the index is a tenth the size of an em-\nployee data record, which is typical. The number of leaf pages in the index is\no.1(L5B)\n=\nO.15B, if we take into account the 67 percent occupancy of index\npages.\nSimilarly, the number of data entries on a page 10(0.67R)\n=\n6.7R,\ntaking into account the relative size and occupancy.\nScan: Consider Figure 8.1, which illustrates an unclustered index. To do a full\nscan of the file of employee records, we can scan the leaf level of the index and\nfor each data entry, fetch the corresponding data record from the underlying\nfile, obtaining data records in the sort order (age, sal).\nWe can read all data entries at a cost of O.15B(D + 6.7RC) l/Os. Now comes\nthe expensive part: We have to fetch the employee record for each data entry\nin the index. The cost of fetching the employee records is one I/O per record,\nsince the index is unclustered and each data entry on a leaf page of the index\ncould point to a different page in the employee file.\nThe cost of this step is\nB R(D + C), which is prohibitively high.\nIf we want the employee records\nin sorted order, we would be better off ignoring the index and scanning the\nemployee file directly, and then sorting it. A simple rule of thumb is that a file\ncan be sorted by a two-Pl1SS algorithm in which each pass requires reading and\nwriting the entire file. Thus, the I/O cost of sorting a file with B pages is 4B,\nwhich is much less than the cost of using an unclustered index.\nSearch with Equality Selection:\n\\lVe assume that the equalit.y selection\nmatches the sort order (age, sal). \\Ve can locate the first page containing the\ndesired data entry or entries, should any qualifying entries exist, in lagrO.15B\nsteps, that is, by fetching all pages from the root to the appropriate leaf. Each\nstep requires a disk I/O and two comparisons.\nOnce the page is known, the\nfirst qua1ifying data entry can again be located by a binary search of the page\nat a cost of Clog2G. 7R. The first qualifying data record can be fetched fronl\nthe employee file with another I/O. The cost is DlogpO.15B + Clag26.7R + D,\nwhich is a significant improvement over searching sorted files.\n\nStorage and Inde:rzng\n289\n.~\nIf several records qualify (e.g., \"Find all employees aged ISn ), they are not\nguaranteed to be adjacent to each other. The cost of retrieving all such records\nis the cost oflocating the first qualifying data entry (Dlo9pO.15B +Clo926.7R)\nplus one I/O per qualifying record. The cost of using an unclustered index is\ntherefore very dependent on the number of qualifying records.\nSearch with Range Selection:\nAgain assuming that the range selection\nmatches the composite key, the first record that satisfies the selection is located\nas it is for search with equality.\nSubsequently, data entries are sequentially\nretrieved (using the next and previous links at the leaf level of the index)\nuntil a data entry is found that does not satisfy the range selection. For each\nqualifying data entry, we incur one I/O to fetch the corresponding employee\nrecords. The cost can quickly become prohibitive as the number of records that\nsatisfy the range selection increases. As a rule of thumb, if 10 percent of data\nrecords satisfy the selection condition, we are better off retrieving all employee\nrecords, sorting them, and then retaining those that satisfy the selection.\nInsert: \"Ve must first insert the record in the employee heap file, at a cost of\n2D + C. In addition, we must insert the corresponding data entry in the index.\nFinding the right leaf page costs Dl09pO.15B + Cl0926.7R, and writing it out\nafter adding the new data entry costs another D.\nDelete: We need to locate the data record in the employee file and the data\nentry in the index, and this search step costs Dl09FO.15B + Cl0926.7R + D.\nNow, we need to write out the modified pages in the index and the data file,\nat a cost of 2D.\n8.4.6\nHeap File With Unclustered Hash Index\nAs for unclustered tree indexes, we a.'3sume that each data entry is one tenth\nthe size of a data record. vVe consider only static hashing in our analysis, and\nfor simplicity we a.'3sume that there are no overflow chains.a\nIn a static ha.shed file, pages are kept at about SO percent occupancy (to leave\nspace for future insertions and minimize overflows as the file expands). This is\nachieved by adding a new page to a bucket when each existing page is SO percent\nfull, when records are initially loaded into a hashed file structure. The number\nof pages required to store data entries is therefore 1.2.5 times the number of\npages when the entries are densely packed, that is, 1.25(0.10B)\n=\nO.125B.\nThe number of data entries that fit on a page is 1O(O.80R) = 8R, taking into\naccount the relative size and occupancy.\n:JThe dynamic variants of hashing are less susceptible to the problem of overflow chains, and have\na slight.ly higher average cost per search, but are otherwise similar to the static version.\n\n290\nCHAPTER 8\nScan: As for an unclustered tree index, all data entries can be retrieved in-\nexpensively, at a cost of O.125B(D + 8RC) I/Os. However, for each entry, we\nincur the additional cost of one I/O to fetch the corresponding data record; the\ncost of this step is BR(D + C). This is prohibitively expensive, and further,\nresults are unordered. So no one ever scans a hash index.\nSearch with Equality Selection: This operation is supported very efficiently\nfor matching selections, that is, equality conditions are specified for each field\nin the composite search key (age, sal). The cost of identifying the page that\ncontains qualifying data entries is H.\nAssuming that this bucket consists of\njust one page (i.e., no overflow pages), retrieving it costs D. If we assume that\nwe find the data entry after scanning half the records on the page, the cost of\nscanning the page is O.5(8R)C =\n4RC. Finally, we have to fetch the data\nrecord from the employee file, which is another D. The total cost is therefore\nH + 2D + 4RC, which is even lower than the cost for a tree index.\nIf several records qualify, they are not guaranteed to be adjacent to each other.\nThe cost of retrieving all such records is the cost of locating the first qualifying\ndata entry (H+D+4RC) plus one I/O per qualifying record. The cost of using\nan unclustered index therefore depends heavily on the number of qualifying\nrecords.\nSearch with Range Selection: The hash structure offers no help, and the\nentire heap file of employee records must be scanned at a cost of B(D + RC).\nInsert: We must first insert the record in the employee heap file, at a cost\nof 2D + C. In addition, the appropriate page in the index must be located,\nmodified to insert a new data entry, and then written back.\nThe additional\ncost is H + 2D + C.\nDelete: We need to locate the data record in the employee file and the data\nentry in the index; this search step costs H + 2D + 4RC.\nNow, we need to\nwrite out the modified pages in the index and the data file, at a cost of 2D.\n8.4.7\nComparison of I/O Costs\nFigure 8.4 compares I/O costs for the various file organizations that we dis-\ncussed. A heap file has good storage efficiency and supports fast scanning and\ninsertion of records. However, it is slow for searches and deletions.\nA sorted file also offers good storage efficiency. but insertion and ddetion of\nrecords is slow. Searches are fa.ster than in heap files. It is worth noting that,\nin a real DBMS, a file is almost never kept fully sorted.\n\nStorage and Inde:r'lng\n29)\nSearch+\nD\nSorted\nBD\nDlog2B\nDlog2B +#\nSear-ch +\nSear-ch+\nmatching pages\nBD\nBD\nClustered\n1.5BD\nDlogF1.5B\nDlo9F1.5B+#\nSear-ch +\nSearch+\nmatching pages\nD\nD\nUnclustered\nBD(R +\nD(l\n+\nD(lo9FO.15B+#\nD(3\n+\nSear-ch+\ntree index\n0.15)\nlogFO.15B)\nmatching recor-ds)\nlogFO.15B)\n2D\nUnclustered\nBD(R +\n2D\nBD\n4D\nSearch+\nhash index\n0.125)\n2D\nFigure 8.4\nA Comparison of I/O Costs\nA clustered file offers all the advantages of a sorted file and supports inserts\nand deletes efficiently. (There is a space overhead for these benefits, relative to\na sorted file, but the trade-off is well worth it.) Searches are even faster than in\nsorted files, although a sorted file can be faster when a large number of records\nare retrieved sequentially, because of blocked I/O efficiencies.\nUnclustered tree and hash indexes offer fast searches, insertion, and deletion,\nbut scans and range searches with many matches are slow. Hash indexes are a\nlittle faster on equality searches, but they do not support range searches.\nIn summary, Figure 8.4 demonstrates that no one file organization is uniformly\nsuperior in all situations.\n8.5\nINDEXES AND PERFORMANCE TUNING\nIn this section, we present an overview of choices that arise when using indexes\nto improve performance in a database system.\nThe choice of indexes has a\ntremendous impact on system performance, and must be made in the context\nof the expected workload, or typical mix of queries and update operations.\nA full discussion of indexes and performance requires an understanding of\ndatabase query evaluation and concurrency control.\nWe therefore return to\nthis topic in Chapter 20, where we build on the discussion in this section. In\nparticular, we discuss examples involving multiple tables in Chapter 20 because\nthey require an understanding of join algorithms and query evaluation plans.\n\n292\nCHAPTER. 8\n8.5.1\nImpact of the Workload\nThe first thing to consider is the expected workload and the common opera-\ntions. Different file organizations and indexes, a:\"l we have seen, support different\noperations well.\nIn generaL an index supports efficient retrieval of data entries that satisfy a\ngiven selection condition. Recall from the previous section that there are two\nimportant kinds of selections:\nequality selection and range selection.\nHash-\nbased indexing techniques are optimized only for equality selections and fa.re\npoorly on range selections. where they are typically worse than scanning the\nentire file of records.\nTree-based indexing techniques support both kinds of\nselection conditions efficiently, explaining their widespread use.\nBoth tree and hash indexes can support inserts, deletes, and updates quite\nefficiently.\nTree-based indexes, in particular, offer a superior alternative to\nmaintaining fully sorted files of records. In contrast to simply maintaining the\ndata entries in a sorted file, our discussion of (B+ tree) tree-structured indexes\nin Section 8.3.2 highlights two important advantages over sorted files:\n1. vVo can handle inserts and deletes of data entries efficiently.\n2. Finding the correct leaf page when searching for a record by search key\nvalue is much faster than binary search of the pages in a sorted file.\nThe one relative disadvantage is that the pages in a sorted file can be allocated\nin physical order on disk, making it much faster to retrieve several pages in\nsequential order. Of course. inserts and deletes on a sorted file are extremely\nexpensive.\nA variant of B+ trees, called Indexed Sequential Access Method\n(ISAM), offers the benefit of sequential allocation of leaf pages, plus the benefit\nof fast searches. Inserts and deletes are not handled as well a'3 in B+ trees, but\nare rnuch better than in a sorted file. \\Ve will study tree-structured indexing\nin detail in Cha,pter 10.\n8.5.2\nClustered Index Organization\nAs we smv in Section 8.2.1. a clustered index is really a file organization for\nthe underlying data records. Data records can be la.rge, and we should avoid\nreplicating them; so there can be at most one clustered index on a given collec-\ntion of records. On the other hanel. we UU1 build several uncIustered indexes\non a data file. Suppose that employee records are sorted by age, or stored in a\nclustered file with search keyage. If. in addition. we have an index on the sal\nfield, the latter nlUst be an llnclllstered index. \\:Ve can also build an uncIustered\nindex on. say, depaThnent, if there is such a field.\n\nStomge and Inde:rin,g\n29,3\nClustered indexes, while less expensive to maintain than a fully sorted file, are\nnonetJleless expensive to maintain. When a new record h&'3 to be inserted into\na full leaf page, a new leaf page must be allocated and sorne existing records\nhave to be moved to the new page. If records are identified by a combination of\npage id and slot, &'5 is typically the case in current database systems, all places\nin the datab&\"ie that point to a moved record (typically, entries in other indexes\nfor the same collection of records) must also be updated to point to the new\nlocation.\nLocating all such places and making these additional updates can\ninvolve several disk I/Os.\nClustering must be used sparingly and only when\njustified by frequent queries that benefit from clustering. In particular, there\nis no good reason to build a clustered file using hashing, since range queries\ncannot be answered using h&c;h-indexcs.\nIn dealing with the limitation that at most one index can be clustered, it is\noften useful to consider whether the information in an index's search key is\nsufficient to answer the query. If so, modern database systems are intelligent\nenough to avoid fetching the actual data records.\nFor example, if we have\nan index on age, and we want to compute the average age of employees, the\nDBMS can do this by simply examining the data entries in the index. This is an\nexample of an index-only evaluation. In an index-only evaluation of a query\nwe need not access the data records in the files that contain the relations in the\nquery; we can evaluate the query completely through indexes on the files. An\nimportant benefit of index-only evaluation is that it works equally efficiently\nwith only unclustered indexes, as only the data entries of the index are used in\nthe queries. Thus, unclustered indexes can be used to speed up certain queries\nif we recognize that the DBMS will exploit index-only evaluation.\nDesign Examples Illustrating Clustered Indexes\nTo illustrate the use of a clustered index 011 a range query, consider the following\nexample:\nSELECT\nFROM\nWHERE\nE.dno\nEmployees E\nE.age > 40\nIf we have a H+ tree index on age, we can use it to retrieve only tuples that\nsatisfy the selection E. age> 40. \\iVhether such an index is worthwhile depends\nfirst of all on the selectivity of the condition. vVhat fraction of the employees are\nolder than 40'1 If virtually everyone is older than 40 1 we gain little by using an\nindex 011 age; a sequential scan of the relation would do almost as well. However,\nsuppose that only 10 percent of the employees are older than 40. Now, is an\nindex useful? The answer depends on whether the index is clustered. If the\n\n294\nCHAPTER~ 8\nindex is unclustered, we could have one page I/O per qualifying employee, and\nthis could be more expensive than a sequential scan, even if only 10 percent\nof the employees qualify!\nOn the other hand, a clustered B+ tree index on\nage requires only 10 percent of the l/Os for a sequential scan (ignoring the few\nl/Os needed to traverse from the root to the first retrieved leaf page and the\nl/Os for the relevant index leaf pages).\nAs another example, consider the following refinement of the previous query:\nSELECT\nFROM\nWHERE\nGROUP BY\nKdno, COUNT(*)\nEmployees E\nE.age> 10\nE.dno\nIf a B+ tree index is available on age, we could retrieve tuples using it, sort\nthe retrieved tuples on dna, and so answer the query. However, this may not\nbe a good plan if virtually all employees are more than 10 years old. This plan\nis especially bad if the index is not clustered.\nLet us consider whether an index on dna might suit our purposes better. We\ncould use the index to retrieve all tuples, grouped by dna, and for each dna\ncount the number of tuples with age> 10.\n(This strategy can be used with\nboth hash and B+ tree indexes; we only require the tuples to be grouped, not\nnecessarily sorted, by dna.) Again, the efficiency depends crucially on whether\nthe index is clustered. If it is, this plan is likely to be the best if the condition\non age is not very selective. (Even if we have a clustered index on age, if the\ncondition on age is not selective, the cost of sorting qualifying tuples on dna is\nlikely to be high.) If the index is not clustered, we could perform one page I/O\nper tuple in Employees, and this plan would be terrible. Indeed, if the index\nis not clustered, the optimizer will choose the straightforward plan based on\nsorting on dna. Therefore, this query suggests that we build a clustered index\non dna if the condition on age is not very selective. If the condition is very\nselective, we should consider building an index (not necessarily clustered) on\nage instead.\nClustering is also important for an index on a search key that does not include\na candidate key, that is, an index in which several data entries can have the\nsame key value. To illustrate this point, we present the following query:\nSELECT E.dno\nFROM\nEmployees E\nWHERE\nE.hobby='Stamps'\n\nStomge and Indexing\nIf many people collect stamps, retrieving tuples through an unclustered index\non hobby can be very inefficient. It may be cheaper to simply scan the relation\nto retrieve all tuples and to apply the selection on-the-fly to the retrieved tuples.\nTherefore, if such a query is important, we should consider making the index\non hobby a clustered index. On the other hand, if we assume that eid is a key\nfor Employees, and replace the condition E.hobby= 'Stamps' by E. eid=552, we\nknow that at most one Employees tuple will satisfy this selection condition. In\nthis case, there is no advantage to making the index clustered.\nThe next query shows how aggregate operations can influence the choice of\nindexes:\nSELECT\nE.dno, COUNT(*)\nFROM\nEmployees E\nGROUP BY E.dno\nA straightforward plan for this query is to sort Employees on dno to compute\nthe count of employees for each dno. However, if an index-hash or B+ tree---\non dno is available, we can answer this query by scanning only the index. For\neach dno value, we simply count the number of data entries in the index with\nthis value for the search key. Note that it does not matter whether the index\nis clustered because we never retrieve tuples of Employees.\n8.5.3\nComposite Search Keys\nThe search key for an index can contain several fields; such keys are called\ncomposite search keys or concatenated keys. As an example, consider a\ncollection of employee records, with fields name, age, and sal, stored in sorted\norder by name. Figure 8.5 illustrates the difference between a composite index\nwith key (age, sa0, a composite index with key (sal, age), an index with key\nage, and an index with key sal. All indexes shown in the figure use Alternative\n(2) for data entries.\nIf the search key is composite, an equality query is one in which each field in\nthe search key is bound to a constant. For example, we can ask to retrieve all\ndata entries with age = 20 and sal = 10. The hashed file organization supports\nonly equality queries, since a ha\"ih function identifies the bucket containing\ndesired records only if a value is specified for each field in the search key.\nWith respect to a composite key index, in a range query not all fields in the\nsearch key are bound to constants. For example, we can ask to retrieve all data\nentries with age\n:=0:: 20; this query implies that any value is acceptable for the\nsal field. As another example of a range query, we can ask to retrieve all data\nentries with age < 30 and sal> 40.\n\n296\nCHAPTEI48\n~I\n80\n75\nname\nage\nsal\n\"\n~-_.-\nbob\n12\n10\ncal\n11\n80\n-\njoe\n12\n20\n----\nsue\n13\n75\nData\nIndex\n75,13\n80,11\n10,12\n<age, sal>\n,~..._---!\nI 11,80 ,,! Index\nR;:IO ~ __:~i\n1 12,20 ~_.1\nU~:~,\n<sal, age>\n../\nFigure 8.5\nComposite Key Indexes\nNate that the index cannot help on the query sal > 40, because, intuitively,\nthe index organizes records by age first and then sal. If age is left unspeci-\nfied, qualifying records could be spread across the entire index. We say that\nan index matches a selection condition if the index can be used to retrieve\njust the tuples that satisf:y the condition. For selections of the form condition\n1\\ ... 1\\ condition, we can define when an index matches the selection as 1'01-\n10ws:4 For a hash index, a selection matches the index if it includes an equality\ncondition ('field = constant') on every field in the composite search key for the\nindex. For a tree index, a selection matches the index if it includes an equal-\nity or range condition on a prefi.T of the composite search key.\n(As examples,\n(age) and (age, sal, department) are prefixes of key (age, sal, depa7'tment) , but\n(age, department) and (sal, department) are not.)\nTrade-offs in Choosing Composite Keys\nA composite key index can support a broader range of queries because it\nmatches more selection conditions. Further, since data entries in a composite\nindex contain more information about the data record (i.e., more fields than\na single-attribute index), the opportunities for index-only evaluation strategies\nare increased.\n(Recall from Section 8.5.2 that an index-only evaluation does\nnot need to access data records, but finds all required field values in the data\nentries of indexes.)\nOn the negative side, a composite index must be updated in response to any\noperation (insert, delete, or update) that modifies any field in the search key.\nA composite index is Hlso likely to be larger than a singk'-attribute search key\n4 For a more general discussion, see Section 14.2.)\n\nStoTage and Inde.Ting\nindex because the size of entries is larger. For a composite B+ tree index, this\nalso means a potential increase in the number of levels, although key COlnpres-\nsion can be used to alleviate this problem (see Section 10.8.1).\nDesign Examples of Composite Keys\nConsider the following query, which returns all employees with 20 < age < 30\nand 3000 < sal < 5000:\nSELECT\nFROM\nWHERE\nE.eid\nEmployees E\nE.age BETWEEN 20 AND 30\nAND E.sal BETWEEN 3000 AND 5000\nA composite index on (age, sal) could help if the conditions in the WHERE clause\nare fairly selective. Obviously, a hash index will not help; a B+ tree (or ISAM)\nindex is required. It is also clear that a clustered index is likely to be superior\nto an unclustered index. For this query, in which the conditions on age and sal\nare equally selective, a composite, clustered B+ tree index on (age, sal) is as\neffective as a composite, clustered B+ tree index on (sal, age). However, the\norder of search key attributes can sometimes make a big difference, as the next\nquery illustrates:\nSELECT\nFROM\nWHERE\nE.eid\nEmployees E\nE.age = 25\nAND E.sal BETWEEN 3000 AND 5000\nIn this query a composite, clustered B+ tree index on (age, sal) will give good\nperformance because records are sorted by age first and then (if two records\nhave the same age value) by sal. Thus, all records with age = 25 are clustered\ntogether. On the other hand, a composite, clustered B+ tree index on (sal, age)\nwill not perform as well. In this case, records are sorted by sal first, and there-\nfore two records with the same age value (in particular, with age = 25) may be\nquite far apart. In effect, this index allows us to use the range selection on sal,\nbut not the equality selection on age, to retrieve tuples.\n(Good performance\non both variants of the query can be achieved using a single spatial index. \\:Ye\ndiscuss spatial indexes in Chapter 28.)\nComposite indexes are also useful in dealing with many aggregate queries. Con-\nsider:\nSELECT AVG (E.sal)\n\n298\nFROM\nWHERE\nEmployees E\nE.age = 25\nAND Ksal BETWEEN 3000 AND 5000\nCHAPTERt 8\nA composite B+ tree index on (age, sal) allows us to answer the query with\nan index-only scan.\nA composite B+ tree index on (sal, age) also allows us\nto answer the query with an index-only scan, although more index entries are\nretrieved in this case than with an index on (age, sal).\nHere is a variation of an earlier example:\nSELECT\nFROM\nWHERE\nGROUP BY\nKdno, COUNT(*)\nEmployees E\nE.sal=lO,OOO\nKdno\nAn index on dna alone does not allow us to evaluate this query with an index-\nonly scan, because we need to look at the sal field of each tuple to verify that\nsal = 10, 000. However, we can use an index-only plan if we have a composite\nB+ tree index on (sal, dna) or (dna, sal). In an index with key (sal, dno) , all\ndata entries with sal = 10,000 are arranged contiguously (whether or not the\nindex is clustered). Further, these entries are sorted by dna, making it easy to\nobtain a count for each dna group.\nNote that we need to retrieve only data\nentries with sal = 10, 000.\nIt is worth observing that this strategy does not work if the WHERE clause is\nmodified to use sal> 10, 000. Although it suffices to retrieve only index data\nentries-that is, an index-only strategy still applies-these entries must now\nbe sorted by dna to identify the groups (because, for example, two entries with\nthe same dna but different sal values may not be contiguous). An index with\nkey (dna, sal) is better for this query: Data entries with a given dna value are\nstored together, and each such group of entries is itself sorted by sal. For each\ndna group, we can eliminate the entries with sal not greater than 10,000 and\ncount the rest. (Using this index is less efficient than an index-only scan with\nkey (sal, dna) for the query with sal = 10, 000, because we must read all data\nentries. Thus, the choice between these indexes is influenced by which query is\nmore common.)\nAs another eXEunple, suppose we want to find the minimum sal for each dna:\nSELECT\nKdno, MIN(E.sal)\nFROM\nEmployees E\nGROUP BY E.dno\n\nStomge and Indexing\n2~9\nAn index on dna alone does not allow us to evaluate this query with an index-\nonly scan. However, we can use an index-only plan if we have a composite B+\ntree index on (dna, sal). Note that all data entries in the index with a given\ndna value are stored together (whether or not the index is clustered). :B\\lrther,\nthis group of entries is itself sorted by 8al. An index on (sal, dna) enables us\nto avoid retrieving data records, but the index data entries must be sorted on\ndno.\n8.5.4\nIndex Specification in SQL:1999\nA natural question to ask at this point is how we can create indexes using\nSQL. The SQL:1999 standard does not include any statement for creating or\ndropping index structures.\nIn fact, th.e standard does not even require SQL\nimplementations to support indexes! In practice, of course, every commercial\nrelational DBMS supports one or more kinds of indexes. The following com-\nmand to create a B+ tree index-we discuss B+ tree indexes in Chapter 10----·-is\nillustrative:\nCREATE INDEX IndAgeRating ON Students\nWITH\nSTRUCTURE = BTREE,\nKEY = (age, gpa)\nThis specifies that a B+ tree index is to be created on the Students table using\nthe concatenation of the age and gpa columns as the key. Thus, key values are\npairs of the form (age, gpa) , and there is a distinct entry for each such pair.\nOnce created, the index is automatically maintained by the DBMS adding or\nremoving data entries in response to inserts or deletes of records on the Students\nrelation.\n8.6\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\n'Where does a DBMS store persistent data? How does it bring data into\nmain memory for processing? What DBMS component reads and writes\ndata from main memory, and what is the unit of I/O? (Section 8.1)\n•\n'What is a file organization? vVhat is an index? What is the relationship\nbetween files and indexes?\nCan we have several indexes on a single file\nof records?\nCan an index itself store data records (i.e., act as a file)?\n(Section 8.2)\nIII\nWhat is the 8earch key for an index? What is a data entry in an index?\n(Section 8.2)\n\n300\nCHAPTER S\n•\nvVhat is a clustered index? vVhat is a prinwry index? How many clustered\nindexes can you build on a file? How many unclustered indexes can you\nbuild? (Section 8.2.1)\n•\nHmv is data organized in a hash-ba'lcd index?\n\\Vhen would you use a\nhash-based index? (Section 8.3.1)\n•\nHow is data organized in a tree-based index? vVhen would you use a tree-\nbased index? (Section 8.3.2)\n•\nConsider the following operations:\nscans, equality and 'range selections,\ninserts, and deletes, and the following file organizations: heap files, sorted\nfiles, clustered files, heap files with an unclustered tree index on the search\nkey, and heap files with an unclusteTed hash index. Which file organization\nis best suited for each operation? (Section 8.4)\n•\nWhat are the main contributors to the cost of database operations? Discuss\na simple cost model that reflects this. (Section 8.4.1)\n•\nHow does the expected workload influence physical database design deci-\nsiems such as what indexes to build? vVhy is the choice of indexes a central\naspect of physical database design? (Section 8.5)\n•\nWhat issues are considered in using clustered indexes? What is an indcl;-\nonly evaluation method? \\\\That is its primary advantage? (Section 8.5.2)\n•\nWhat is a composite 8earch key? What are the pros and cons of composite\nsearch keys? (Section 8.5.3)\n•\nWhat SQL commands support index creation? (Section 8.5.4)\nEXERCISES\nExercise 8.1 Answer the following questions about data on external storage in a DBMS:\n1. \\Vhy does a DBMS store data on external storage?\n2. Why are I/O costs important in a DBMS?\n3. \\Vhat is a record id?\nGiven a record's id, how many I/Os are needed to fetch it into\nmain memory?\n4. \\Vhat is the role of the buffer manager in a DBMS? What is the role of the disk space\nmanager? How do these layers interact with the file and access methods layer?\nExercise 8.2 Answer the following questions about files and indexes:\n1. What operations arc supported by the file of records abstraction?\n2. \\Vhat is an index on a file of records? \\Nhat is a search key for an index? Why do we\nneed indexes?\n\nStorage and Inde:ring\n301\nage I gpo, ]\nnarnc\n.-\n_.\n5:3831\nr\\ladayan\nrnadayan(Q:!music\nh\n1.8\n53832\nGulclu\nguldu@music\n12\n2.0\n53666\nJones\njones(Q;cs\n18\n3.4\n53688\nSmith\nsmith(@ee\n19\n3.2\n53650\nSmith\nsrnithtg]math\n19\n3.8\nFigure 8.6\nAn Instance of t.he St.udents Relation. Sorted by age\n3. What alternatives are available for the data entries in an index?\n4. What is the difference between a primary index and a secondary index?\n\\Vhat is a\nduplicate data entry in an index? Can a primary index contain duplicates?\n5. What is the difference between a clustered index and an unclustered index? If an index\ncontains data records as 'data entries,' can it be unclustered?\n6. How many clustered indexes can you create on a file? Woule! you always create at least\none clustered index for a file?\n7. Consider Alternatives (1), (2) and (3) for 'data entries' in an index, as discussed in\nSection 8.2 . Are all of them suitable for secondary indexes? Explain.\nExercise 8.3 Consider a relation stored as a randomly ordered file for which the only index\nis an unclustered index on a field called sal. If you want to retrieve all records with sal> 20,\nis using the index always the best alternative? Explain.\nExercise 8.4 Consider the instance of the Students relation shown in Figure 8.6, sorted by\nage: For the purposes of this question, assume that these tuples are stored in a sorted file in\nthe order shown; the first tuple is on page 1 the second tuple is also on page 1; and so on.\nEach page can store up to three data records; so the fourth tuple is on page 2.\nExplain what the data entries in each of the following indexes contain. If the order of entries\nis significant, say so and explain why. If such all index cannot be constructeel, say so and\nexplain why.\n1. An unclustereel index on age using Alternative (1).\n2. An unclusterecl index on age using Alternative (2).\n3. An unclustered index on age using Alternative (:3).\n4. A clustered index on age using Alternative (1).\n5. A clustered index on age using Alt.ernative (2).\n6. A clustered index on age using Alternative (:3).\n7. An unc:lustered index on gpo using Alternative (1).\n8. An unclustered index on gpa using Alternative (2).\n9. An unclustered index on gpa using Alternative (3).\n10. A clustered index on gpa using Alternative (1).\n11. A clustered index on gpa using Alternative (2).\n12. A clustered index on gpa using Alternative (:3).\n\n302\nCHAPTERf8\nSorted file\nClustered file\nUnclustered tree index\nUnclustered hash index\nFigure 8.7\nI/O Cost Comparison\nExercise 8.5 Explain the difference between Hash indexes and B+-tree indexes. In partic-\nular, discuss how equality and range searches work, using an example.\nExercise 8.6 Fill in the I/O costs in Figure 8.7.\nExercise 8.7 If you were about to create an index on a relation, what considerations would\nguide your choice? Discuss:\n1. The choice of primary index.\n2. Clustered versus unclustered indexes.\n3. Hash versus tree indexes.\n4. The use of a sorted file rather than a tree-based index.\n5, Choice of search key for the index. What is a composite search key, and what consid-\nerations are made in choosing composite search keys? What are index-only plans, and\nwhat is the influence of potential index-only evaluation plans on the choice of search key\nfor an index?\nExercise 8.8 Consider a delete specified using an equality condition.\nFor each of the five\nfile organizations, what is the cost if no record qualifies? What is the cost if the condition is\nnot on a key?\nExercise 8.9 What main conclusions can you draw from the discussion of the five basic file\norganizations discussed in Section 8.4? Which of the five organizations would you choose for\na file where the most frequent operations are a<; follows?\n1. Search for records based on a range of field values.\n2. Perform inserts and scans, where the order of records docs not matter.\n3. Search for a record based on a particular field value.\nExercise 8.10 Consider the following relation:\nEmp( eid: integer, sal: integer l age: real, did: integer)\nThere is a clustered index on cid and an llnclustered index on age.\n1. How would you use the indexes to enforce the constraint that eid is a key?\n2. Give an example of an update that is definitely speeded\n1lJi because of the available\nindexes. (English description is sufficient.)\n\nStorage and Inde.7:ing\n303\n~\n3. Give an example of an update that is definitely slowed down because of the indexes.\n(English description is sufficient.)\n4. Can you give an example of an update that is neither speeded up nor slowed down by\nthe indexes?\nExercise 8.11 Consider the following relations:\nEmp( eid: integer, ename: varchar, sal: integer, age: integer, did: integer)\nDept(did: integer, budget: integer, floor: integer, mgr_eid: integer)\nSalaries range from $10,000 to $100,000, ages vary from 20 to 80, each department has about\nfive employees on average, there are 10 floors, and budgets vary from $10,000 to $1 million.\nYou can assume uniform distributions of values.\nFor each of the following queries, which of the listed index choices would you choose to speed\nup the query? If your database system does not consider index-only plans (i.e., data records\nare always retrieved even if enough information is available in the index entry), how would\nyour answer change? Explain briefly.\n1. Query: Print ename, age, and sal for all employees.\n(a) Clustered hash index on (ename, age, sal) fields of Emp.\n(b) Unclustered hash index on (ename, age, sal) fields of Emp.\n(c) Clustered B+ tree index on (ename, age, sal) fields of Emp.\n(d) Unclustered hash index on (eid, did) fields of Emp.\n(e) No index.\n2. Query: Find the dids of departments that are on the 10th floor and have a budget of less\nthan $15,000.\n(a) Clustered hash index on the floor field of Dept.\n(b) Unclustered hash index on the floor' field of Dept.\n(c) Clustered B+ tree index on (floor, budget) fields of Dept.\n(d) Clustered B+ tree index on the budget: field of Dept.\n(e) No index.\nPROJECT-BASED EXERCISES\nExercise 8.12 Answer the following questions:\n1. What indexing techniques are supported in Minibase?\n2. \\;v'hat alternatives for data entries are supported'?\n:3. Are clustered indexes supported?\nBIBLIOGRAPHIC NOTES\nSeveral books discuss file organization in detail [29, :312, 442, 531, 648, 695, 775].\nBibliographic: notes for hash-indexes and B+-trees are included in Chapters 10 and 11.\n\n9\nSTORING DATA:\nDISKS AND FILES\n..\nWhat are the different kinds of memory in a computer system?\n..\nWhat are the physical characteristics of disks and tapes, and how do\nthey affect the design of database systems?\n...\nWhat are RAID storage systems, and what are their advantages?\n..\nHow does a DBMS keep track of space on disks? How does a DBMS\naccess and modify data on disks? What is the significance of pages as\na unit of storage and transfer?\n,..\nHow does a DBMS create and maintain files of records?\nHow are\nrecords arranged on pages, and how are pages organized within a file?\n..\nKey concepts: memory hierarchy, persistent storage, random versus\nsequential devices; physical disk architecture, disk characteristics, seek\ntime, rotational delay, transfer time; RAID, striping, mirroring, RAID\nlevels; disk space manager; buffer manager, buffer pool, replacement\npolicy, prefetching, forcing; file implementation, page organization,\nrecord organization\nA memory is what is left when :iomething happens and does not cornpletely\nunhappen.\n. Edward DeBono\nThis chapter initiates a study of the internals of an RDBivIS. In terms of the\nDBj\\JS architecture presented in Section 1.8, it covers the disk space manager,\n304\n\nBto'ring Data: Disks and Files\nthe buffer manager, and implementation-oriented aspects of the Jiles and access\nmethods layer.\nSection 9.1 introduces disks and tapes. Section 9.2 describes RAID disk sys-\ntems. Section 9.3 discusses how a DBMS manages disk space, and Section 9.4\nexplains how a DBMS fetches data from disk into main memory. Section 9.5\ndiscusses how a collection of pages is organized into a file and how auxiliary\ndata structures can be built to speed up retrieval of records from a file. Sec-\ntion 9.6 covers different ways to arrange a collection of records on a page, and\nSection 9.7 covers alternative formats for storing individual records.\n9.1\nTHE MEMORY HIERARCHY\nMemory in a computer system is arranged in a hierarchy, a'S shown in Fig-\nure 9.1. At the top, we have primary storage, which consists of cache and\nmain memory and provides very fast access to data. Then comes secondary\nstorage, which consists of slower devices, such as magnetic disks. Tertiary\nstorage is the slowest class of storage devices; for example, optical disks and\ntapes. Currently, the cost of a given amount of main memory is about 100 times\nRequest for data\n----- .....\nData satisfying request\nCPU\n\",/\n.,\nCACHE\n....\n,/\nPrimary storage\nMAIN MEMORY\nf.: ....\n,/\nMAGNETIC DISK\nSecondary storage\n....\n,/\nTAPE\nTertiary storage\nFigure 9.1\nThe Ivlemory Hierarchy\nthe cost of the same amount of disk space, and tapes are even less expensive\nthan disks. Slower storage devices such as tapes and disks play an important\nrole in database systems because the amount of data is typically very large.\nSince buying e110ugh main memory to store all data is prohibitively expensive,\nwe must store data on tapes and disks and build database systems that can\nretrieve data from lower levels of the memory hierarchy into main mernory as\nneeded for processing.\n\n306\nThere are reasons other than cost for storing data on secondary and tertiaJ:y\nstorage. On systems with 32-bit addressing, only 232 bytes can be directly ref-\nerenced in main memory; the number of data objects may exceed this number!\nFurther, data must be maintained across program executions.\nThis requires\nstorage devices that retain information when the computer is restarted (after\na shutdown or a crash); we call such storage nonvolatile. Primary storage is\nusually volatile (although it is possible to make it nonvolatile by adding a bat-\ntery backup feature), whereas secondary and tertiary storage are nonvolatile.\nTapes are relatively inexpensive and can store very large amounts of data. They\nare a good choice for archival storage, that is, when we need to maintain data\nfor a long period but do not expect to access it very often. A Quantum DLT\n4000 drive is a typical tape device; it stores 20 GB of data and can store about\ntwice as much by compressing the data. It records data on 128 tape tracks,\nwhich can be thought of as a linear sequence of adjacent bytes, and supports\na sustained transfer rate of 1.5 MB/sec with uncompressed data (typically 3.0\nMB/sec with compressed data). A single DLT 4000 tape drive can be used to\naccess up to seven tapes in a stacked configuration, for a maximum compressed\ndata capacity of about 280 GB.\nThe main drawback of tapes is that they are sequential access devices. We must\nessentially step through all the data in order and cannot directly access a given\nlocation on tape. For example, to access the last byte on a tape, we would have\nto wind through the entire tape first. This makes tapes unsuitable for storing\noperational data, or data that is frequently accessed. Tapes are mostly used to\nback up operational data periodically.\n9.1.1\nMagnetic Disks\nMagnetic disks support direct access to a desired location and are widely used\nfor database applications. A DBMS provides seamless access to data on disk;\napplications need not worry about whether data is in main memory or disk.\nTo understand how disks work, eonsider Figure 9.2, which shows the structure\nof a disk in simplified form.\nData is stored on disk in units called disk blocks. A disk block is a contiguous\nsequence of bytes and is the unit in which data is written to a disk and read\nfrom a disk. Bloc:ks are arranged in concentric rings called tracks, on one or\nmore platters. Tracks can be recorded on one or both surfaces of a platter;\nwe refer to platters as single-sided or double-sided, accordingly. The set of all\ntracks with the SaIne diameter is called a cylinder, because the space occupied\nby these tracks is shaped like a cylinder; a cylinder contains one track per\nplatter surface. Each track is divided into arcs, called sectors, whose size is a\n\nStoring Data: Disks and Files\nDisk ann\nArm movement\nFigure 9.2\nStructure of a Disk\n____\nBlock\nSectors\nCylinder\n- Tracks\n,.. Platter\n~07\ncharacteristic of the disk and cannot be changed. The size of a disk block can\nbe set when the disk is initialized as a multiple of the sector size.\nAn array of disk heads, one per recorded surface, is moved as a unit; when\none head is positioned over a block, the other heads are in identical positions\nwith respect to their platters. To read or write a block, a disk head must be\npositioned on top of the block.\nCurrent systems typically allow at most one disk head to read or write at any\none time. All the disk heads cannot read or write in\nparallel~-this technique\nwould increa.se data transfer rates by a factor equal to the number of disk\nheads and considerably speed up sequential scans. The rea.son they cannot is\nthat it is very difficult to ensure that all the heads are perfectly aligned on the\ncorresponding tracks. Current approaches are both expensive and more prone\nto faults than disks with a single active heacl. In practice, very few commercial\nproducts support this capability and then only in a limited way; for example,\ntwo disk heads may be able to operate in parallel.\nA disk controller interfaces a disk drive to the computer. It implements com-\nmands to read or write a sector by moving the arm assembly and transferring\ndata to and from the disk surfaces. A checksum is computed for when data\nis written to a sector and stored with the sector. The checksum is computed\nagain when the data on the sector is read back. If the sector is corrupted or the\n\n308\nCHAPTER 9\nAn Example of a Current Disk: The IBM Deskstar 14G~~~Th~l\nIBM Deskstar 14GPX is a 3.5 inch§.J4.4 GB hfl,rd disk with an average\nseek time of 9.1 miUisecoudsTmsec) and an average rotational delay of\n4.17 msec. However, the time to seek from one track to the nexUs just 2.2\nmsec, the maximum seek time is 15.5 :rnsec. The disk has five double-sided\nplatters that spin at 7200 rotations per minute. Ea,ch platter holds 3.35 GB\nof data, with a density of 2.6 gigabit per square inch. The data transfer\nrate is about 13 MB per secmld.\nTo put these numbers in perspective,\nobserve that a disk access takes about 10 msecs, whereas accessing a main\nmemory location typically takes less than 60 nanoseconds!\nread is faulty for some reason, it is very unlikely that the checksum computed\nwhen the sector is read matches the checksum computed when the sector was\nwritten. The controller computes checksums, and if it detects an error, it tries\nto read the sector again. (Of course, it signals a failure if the sector is corrupted\nand read fails repeatedly.)\n~While direct access to any desired location in main memory takes approxi-\nmately the same time, determining the time to access a location on disk is\nmore complicated.\nThe time to access a disk block has several components.\nSeek time is the time taken to move the disk heads to the track on which\na desired block is located. As the size of a platter decreases, seek times also\ndecrease, since we have to move a disk head a shorter distance. Typical platter\ndiameters are 3.5 inches and 5.25 inches.\nRotational delay is the waiting\ntime for the desired block to rotate under the disk head; it is the time required\nfor half a rotation all average and is usually less than seek time.\nTransfer\ntime is the time to actually read or write the data in the block once the head\nis positioned, that is, the time for the disk to rotate over the block.\n9.1.2\nPerformance Implications of Disk Structure\n1. Data must be in mernory for the DBMS to operate on it.\n2. The unit for data transfer between disk and main memory is a block; if a\nsingle item on a block is needed, the entire block is transferred. Reading\nor writing a disk block is called an I/O (for input/output) operation.\n3. The time to read or write a block varies, depending on the location of the\ndata:\naccess time = seek time + rotational delay + tmn8feT time\nThese observations imply that the time taken for database operations is affected\nsignificantly by how data is stored OIl disks.\nThe time for moving blocks to\n\nStoring Data: D'isks and Files\n309\nor from disk usually dOlninates the time taken for database operations.\nTo\nminimize this time, it is necessary to locate data records strategically on disk\nbecause of the geometry and mechanics of disks. In essence, if two records are\nfrequently used together, we should place them close together.\nThe 'closest'\nthat two records can be on a disk is to be on the same block. In decrea<;ing\norder of closeness, they could be on the same track, the same cylinder, or an\nadjacent cylinder.\nTwo records on the same block are obviously as close together as possible,\nbecause they are read or written as part of the same block.\nAs the platter\nspins, other blocks on the track being read or written rotate under the active\nhead. In current disk designs, all the data on a track can be read or written\nin one revolution. After a track is read or written, another disk head becomes\nactive, and another track in the same cylinder is read or written. This process\ncontinues until all tracks in the current cylinder are read or written, and then\nthe arm assembly moves (in or out) to an adjacent cylinder. Thus, we have a\nnatural notion of 'closeness' for blocks, which we can extend to a notion of next\nand previous blocks.\nExploiting this notion of next by arranging records so they are read or written\nsequentially is very important in reducing the time spent in disk l/Os. Sequen-\ntial access minimizes seek time and rotational delay and is much faster than\nrandom access. (This observation is reinforced and elaborated in Exercises 9.5\nand 9.6, and the reader is urged to work through them.)\n9.2\nREDUNDANT ARRAYS OF INDEPENDENT DISKS\nDisks are potential bottlenecks for system performance and storage system rfc'-\nliability. Even though disk performance ha,s been improving continuously, mi-\ncroprocessor performance ha.'s advanced much more rapidly. The performance\nof microprocessors has improved at about 50 percent or more per year, but\ndisk access times have improved at a rate of about 10 percent per year and\ndisk transfer rates at a rate of about 20 percent per year. In addition, since\ndisks contain mechanical elements, they have much higher failure rates than\nelectronic parts of a computer system. If a disk fails, all the data stored on it\nis lost.\nA disk array is an arrangement of several disks, organized to increase per-\nformance and improve reliability of the resulting storage system. Performance\nis increased through data striping. Data striping distributes data over several\ndisks to give the impression of having a single large, very fa'3t disk. Reliabil-\nity is improved through redundancy. Instead of having a single copy of the\ndata, redundant information is maintained. The redundant information is carc-\n\n310\nCHAPTER. Q\nfully organized so that, in C&'3e of a disk failure, it can be used to reconstruct\nthe contents of the failed disk. Disk arrays that implement a combination of\ndata striping and redundancy are called redundant arrays of independent\ndisks, or in short, RAID.! Several RAID organizations, referred to as RAID\nlevels, have been proposed. Each RAID level represents a different trade-off\nbetween reliability and performance.\nIn the remainder of this section, we first discuss data striping and redundancy\nand then introduce the RAID levels that have become industry standards.\n9.2.1\nData Striping\nA disk array gives the user the abstraction of having a single, very large disk.\nIf the user issues an I/O request, we first identify the set of physical disk blocks\nthat store the data requested. These disk blocks may reside on a single disk in\nthe array or may be distributed over several disks in the array. Then the set\nof blocks is retrieved from the disk(s) involved. Thus, how we distribute the\ndata over the disks in the array influences how many disks are involved when\nan I/O request is processed.\nIn data striping, the data is segmented into equal-size partitions distributed\nover multiple disks. The size of the partition is called the striping unit. The\npartitions are usually distributed using a round-robin algorithm:\nIf the disk\narray consists of D disks, then partition i is written onto disk i\nmod D.\nAs an example, consider a striping unit of one bit. Since any D successive data\nbits are spread over all D data disks in the array, all I/O requests involve an\ndisks in the array. Since the smallest unit of transfer from a disk is a block,\neach I/O request involves transfer of at least D blocks. Since we can read the D\nblocks from the D disks in parallel, the transfer rate of each request is D times\nthe transfer rate of a single disk; each request uses the aggregated bandwidth\nof all disks in the array. But the disk access time of the array is ba.'3ically the\naccess time of a single disk, since all disk heads have to move for\" all requests.\nTherefore, for a disk array with a striping unit of a single bit, the number of\nrequests per time unit that the array can process and the average response time\nfor each individual request are similar to that of a single disk.\nAs another exarhple, consider a striping unit of a disk block. In this case, I/O\nrequests of the size of a disk block are processed by one disk in the array. If\nrnany I/O requests of the size of a disk block are made, and the requested\n1Historically, the J in RAID stood for inexpensive, as a large number of small disks was much more\neconornical than a single very large disk. Today, such very large disks are not even manufactured .. ··a\nsign of the impact of RAID.\n\nStoring Data: Disks and Piles\nRedundancy Schemes:\nAlternatives to the parity scheme include\nschemes based on Hamming codes and Reed-Solomon codes. In ad-\ndition to recovery from single disk failures, Hamming codes can identify-\nwhich disk failed. Reed-Solomon codes can recover from up to two simul-\ntaneous disk failures.\nA detailed discussion of these schemes is beyond\nthe scope of our discussion here; the bibliography provides pointers for the\ninterested reader.\nblocks reside on different disks, we can process all requests in parallel and thus\nreduce the average response time of an I/O request. Since we distributed the\nstriping partitions round-robin, large requests of the size of many contiguous\nblocks involve all disks. We can process the request by all disks in parallel and\nthus increase the transfer rate to the aggregated bandwidth of all D disks.\n9.2.2\nRedundancy\nWhile having more disks increases storage system performance, it also low-\ners overall storage system reliability. Assume that the mean-time-to-failure\n(MTTF), of a single disk is 50, 000 hours (about 5.7 years). Then, the MTTF\nof an array of 100 disks is only 50, 000/100 = 500 hours or about 21 days,\nassuming that failures occur independently and the failure probability of a disk\ndoes not change over time.\n(Actually, disks have a higher failure probability\nearly and late in their lifetimes.\nEarly failures are often due to undetected\nmanufacturing defects; late failures occur since the disk wears out. Failures do\nnot occur independently either: consider a fire in the building, an earthquake,\nor purchase of a set of disks that come from a 'bad' manufacturing batch.)\nReliability of a disk array can be increased by storing redundant information.\nIf a disk fails, the redundant information is used to reconstruct the data on the\nfailed disk.\nRedundancy can immensely increase the MTTF of a disk array.\nWhen incorporating redundancy into a disk array design, we have to make two\nchoices. First, we have to decide where to store the redundant information. We\ncan either store the redundant information on a small number of check disks\nor distribute the redundant information uniformly over all disks.\nThe second choice we have to make is how to compute the redundant infor-\nmation. Most disk arrays store parity information: In the parity scheme, an\nextra check disk contains information that can be used to recover from failure\nof anyone disk in the array. Assume that we have a disk array with D disks\nand consider the first bit on each data disk. Suppose that i of the D data bits\nare 1. The first bit on the check disk is set to 1 if i is odd; otherwise, it is set to\n\n312\nCHAPTER 9\nO. This bit on the check disk is called the parity of the data bits. The check\ndisk contains parity information for each set of corresponding D data bits.\nTo recover the value of the first bit of a failed disk we first count the number\nof bits that are 1 on the D - 1 nonfailed disks; let this number be j. If j is odd\nand the parity bit is 1, or if j is even and the parity bit is 0, then the value\nof the bit on the failed disk must have been O. Otherwise, the value of the bit\non the failed disk must have been 1. Thus, with parity we can recover from\nfailure of anyone disk. Reconstruction of the lost information involves reading\nall data disks and the check disk.\nFor example, with an additional 10 disks with redundant information, the\nMTTF of our example storage system with 100 data disks can be increased\nto more than 250 years!\n\"What is more important, a large MTTF implies a\nsmall failure probability during the actual usage time of the storage system,\nwhich is usually much smaller than the reported lifetime or the MTTF. (Who\nactually uses lO-year-old disks?)\nIn a RAID system, the disk array is partitioned into reliability groups, where\na reliability group consists of a set of data disks and a set of check disks.\nA\ncommon 7'cdundancy scheme (see box) is applied to each group. The number\nof check disks depends on the RAID level chosen.\nIn the remainder of this\nsection, we assume for ease of explanation that there is only one reliability\ngroup.\nThe reader should keep in mind that actual RAID implementations\nconsist of several reliability groups, and the number of groups plays a role in\nthe overall reliability of the resulting storage system.\n9.2.3\nLevels of Redundancy\nThroughout the discussion of the different RAID levels, we consider sample\ndata that would just fit on four disks. That is, with no RAID technology our\nstorage system would consist of exactly four data disks.\nDepending on the\nRAID level chosen, the number of additional disb varies from zero to four.\nLevel 0: Nonredundant\nA RAID Level 0 system uses data striping to incre,clse the maximum bandwidth\navailable. No redundant information is maintained. \\\\ThUe being the solution\nwith the lowest cost, reliability is a problem, since the MTTF decreases linearly\nwith the number of disk drives in the array. RAID Level 0 has the best write\nperformance of all RAID levels, because absence of redundant information im-\nplies that no redundant information needs to be updated! Interestingly, RAID\nLevel 0 docs not have the best read perfonnancc of all RAID levels, since sys-\n\nStoTing Data: Disks (1'lul Piles\n313\n,\ntems with redundancy have a choice of scheduling disk accesses, as explained\nin the next section.\nIn our example, the RAID Level a solution consists of only four data disks.\nIndependent of the number of data disks, the effective space utilization for a\nRAID Level a system is always 100 percent.\nLevell: Mirrored\nA RAID Level 1 system is the most expensive solution.\nInstead of having\none copy of the data, two identical copies of the data on two different disks are\nlnaintained. This type of redundancy is often called mirroring. Every write of\na disk block involves a write on both disks. These writes may not be performed\nsimultaneously, since a global system failure (e.g., due to a power outage) could\noccur while writing the blocks and then leave both copies in an inconsistent\nstate. Therefore, we always write a block on one disk first and then write the\nother copy on the mirror disk. Since two copies of each block exist on different\ndisks, we can distribute reads between the two disks and allow parallel reads\nof different disk blocks that conceptually reside on the same disk. A read of a\nblock can be scheduled to the disk that h&'3 the smaller expected access time.\nRAID Level 1 does not stripe the data over different disks, so the transfer rate\nfor a single request is comparable to the transfer rate of a single disk.\nIn our example, we need four data and four check disks with mirrored data for\na RAID Levell implementation. The effective space utilization is 50 percent,\nindependent of the number of data disks.\nLevel 0+1: Striping and Mirroring\nRAID Level 0+l---sometimes also referred to\nH..S RA ID Level 10- -combines\nstriping and mirroring. As in RAID Level L read requests of the size of a disk\nblock can be scheduled both to a disk and its mirror image. In addition, read\nrequests of the size of several contiguous blocks benefit froIl1 the aggregated\nbandwidth of all disks. Thc cost for writes is analogous to RAID LevelL\nAs in RAID Levell, our exa.Inple with four data disks requires four check disks\nand the effective space utilization is always 50 percent.\nLevel 2: Error-Correcting Codes\nIn RAID Level 2, the striping unit is a single bit. The redundancy scheme used\nis Hamming code. In our example with four data disks, only three check disks\n\n314\nCHAPTER 9\nare needed. In general, the number of check disks grows logarithmically with\nthe number of data disks.\nStriping at the bit level has the implication that in a disk array with D data\ndisks, the smallest unit of transfer for a read is a set of D blocks. Therefore,\nLevel 2 is good for workloads with many large requests, since for each request,\nthe aggregated bandwidth of all data disks is used. But RAID Level 2 is bad\nfor small requests of the size of an individual block for the same reason. (See\nthe example in Section 9.2.1.)\nA write of a block involves reading D blocks\ninto main memory, modifying D + C blocks, and writing D + C blocks to\ndisk, where C is the number of check disks. This sequence of steps is called a\nread-modify-write cycle.\nFor a RAID Level 2 implementation with four data disks, three check disks\nare needed. In our example, the effective space utilization is about 57 percent.\nThe effective space utilization increases with the number of data disks.\nFor\nexample, in a setup with 10 data disks, four check disks are needed and the\neffective space utilization is 71 percent.\nIn a setup with 25 data disks, five\ncheck disks are required and the effective space utilization grows to 83 percent.\nLevel 3: Bit~Interieaved Parity\nWhile the redundancy schema used in RAID Level 2 improves in terms of cost\nover RAID Level 1, it keeps more redundant information than is necessary.\nHamming code, as used in RAID Level 2, has the advantage of being able to\nidentify which disk has failed.\nBut disk controllers can easily detect which\ndisk has failed. Therefore, the check disks do not need to contain information\nto identify the failed disk.\nInformation to recover the lost data is sufficient.\nInstead of using several disks to store Hamming code, RAID Level 3 has a\nsingle check disk with parity information.\nThus, the reliability overhead for\nRAID Level 3 is a single disk, the lowest overhead possible.\nThe performance characteristics of RAID Levels 2 and 3 are very similar. RAID\nLevel 3 can also process only one I/O at a time, the minimum transfer unit is\nD blocks, and a write requires a read-modify-write cycle.\nLevel 4: Block~Interleaved Parity\nRAID Level 4 hEk\"i a striping unit of a disk block, instead of a single bit as in\nRAID Level 3.\nBlock-level striping has the advantage that read requests of\nthe size of a disk block can be sen;ed entirely by the disk where the requested\nblock resides.\nLarge read requests of several disk blocks can still utilize the\naggregated bandwidth of the D disks.\n\nSto'ring Data: D'isks and Files\n315\nThe \\vrite of a single block still requires a read-modify-write cycle, but only\none data disk and the check disk are involved. The parity on the check disk\ncan be updated without reading all D disk blocks, because the new parity can\nbe obtained by noticing the differences between the old data block and the new\ndata block and then applying the difference to the parity block on the check\ndisk:\nNewParity = (OldData XOR NewData) XOR OldParity\nThe read-modify-write cycle involves reading of the old data block and the old\nparity block, modifying the two blocks, and writing them back to disk, resulting\nin four disk accesses per write. Since the check disk is involved in each write,\nit can easily become the bottleneck.\nRAID Level 3 and 4 configurations with four data disks require just a single\ncheck disk. In our example, the effective space utilization is 80 percent. The\neffective space utilization increases with the number of data disks, since always\nonly one check disk is necessary.\nLevel 5: Block-Interleaved Distributed Parity\nRAID Level 5 improves on Level 4 by distributing the parity blocks uniformly\nover all disks, instead of storing them on a single check disk. This distribution\nhas two advantages. First, several write requests could be processed in parallel,\nsince the bottleneck of a unique check disk has been eliminated. Second, read\nrequests have a higher level of parallelism. Since the data is distributed over\nall disks, read requests involve all disks, whereas in systems with a dedicated\ncheck disk the check disk never participates in reads.\nA RAID Level 5 system has the best performance of all RAID levels with\nredundancy for small and large read ancllarge write requests. Small writes still\nrequire a read-modify-write cycle and are thus less efficient than in RAID Level\n1.\nIn our example, the corresponding RAID Level 5 system has five disks overall\nand thus the effective spa,ce utilization is the same as in RAID Levels 3 and 4.\nLevel 6: P+Q Redundancy\nThe motivation for RAID Level 6 is the observation that recovery from failure\nof a single disk is not sufficient in very large disk arrays. First, in large disk\narrays, a second disk lllight fail before replacement of an already failed disk\n\n316\nCHAPTER~ 9\ncould take place. In addition, the probability of a disk failure during recovery\nof a failed disk is not negligible.\nA RAID Level 6 system uses Reed-Solomon codes to be able to recover from\nup to two simultaneous disk failures.\nRAID Level 6 requires (conceptually)\ntwo check disks, but it also uniformly distributes redundant information at the\nblock level as in RAID Level 5. Thus. the performance characteristics for small\nand large read requests and for large write requests are analogous to RAID\nLevel 5. For small writes, the read-modify-write procedure involves six instead\nof four disks as compared to RAID Level 5, since two blocks with redundant\ninformation need to be updated.\nFor a RAID Level 6 system with storage capacity equal to four data disks, six\ndisks are required. In our example, the effective space utilization is 66 percent.\n9.2.4\nChoice of RAID Levels\nIf data loss is not an issue, RAID Level 0 improves overall system performance\nat the lowest cost. RAID Level 0+1 is superior to RAID Level 1. The main\napplication areas for RAID Level 0+1 systems are small storage subsystems\nwhere the cost of mirroring is moderate. Sometimes, RAID Level 0+1 is used\nfor applications that have a high percentage of writes in their workload, since\nRAID Level 0+1 provides the best write performance.\nRAID Levels 2 and\n4 are always inferior to RAID Levels 3 and 5, respectively.\nRAID Level 3 is\nappropriate for workloads consisting mainly of large transfer requests of several\ncontiguous blocks.\nThe performance of a RAID Level 3 system is bad for\nworkloads with many small requests of a single disk block. RAID Level 5 is a\ngood general-purpose solution. It provides high performance for large as well\nas small requests. RAID Level 6 is appropriate if a higher level of reliability is\nrequired.\n9.3\nDISK SPACE MANAGEMENT\nI\nThe lowest level of software in the DB.lVIS architecture discussed in Section 1.8,\ncalled the disk space manager, manages space on disk. Abstractly, the disk\nspace manager supports the concept of a page as a unit of data and provides\ncOlmnands to allocate or deallocate a page and read or write a page. The size\nof a page is chosen to be the size of a disk block and pages are stored as disk\nblocks so that reading or writing a page can be done in one disk I/O.\nIt is often useful to allocate a sequence of pages (lS a contiguous sequence of\nblocks to hold data frequently accessed in sequential order.\nThis capability\nis essential for exploiting the advantages of sequentially accessing disk blocks,\n\nStoTing Data: Disks and Files\n317\nwhich we discussed earlier in this chapter. Such a capability, if desired, must\nbe provided by the disk space manager to highcr-levellayers of the DBMS.\nThe disk space manager hides details of the underlying hardware (and possibly\nthe operating system) and allows higher levels of the software to think of the\ndata cLS a collection of pages.\n9.3.1\nKeeping Track of Free Blocks\nA database grows and shrinks\n<1.<; records are inserted and deleted over time.\nThe disk space manager keeps track of which disk blocks are in usc, in addition\nto keeping track of which pages are on which disk blocks. Although it is likely\nthat blocks are initially allocated sequentially on disk, subsequent allocations\nand deallocations could in general create 'holes.'\nOne way to keep track of block usage is to maintain a. list of free blocks. As\nblocks are deallocated (by the higher-level software that requests and uses these\nblocks), we can add them to the free list for future use. A pointer to the first\nblock on the free block list is stored in a known location on disk.\nA second way is to maintain a bitmap with one bit for each disk block, which\nindicates whether a block is in use or not.\nA bitmap also allows very fast\nidentification and allocation of contiguous areas on disk.\nThis is difficult to\naccomplish with a linked list approach.\n9.3.2\nUsing OS File Systems to Manage Disk Space\nOperating systems also manage space on disk. Typically, an operating system\nsupports the abstraction of a file as a sequence of bytes.\nThe as manages\nspace on the disk and translates requests, such as \"Read byte i of file f,\" into\ncorresponding low-level instructions:\n\"Read block m of track t of cylinder c\nof disk d.\" A database disk space manager could he built using OS files.\nJ:;'or\nexample, the entire database could reside in one or more as files for which\na number of blocks are allocated (by the aS) and initialized. The disk space\nmanager is then responsible for managing the space in these OS files.\nMany database systems do not rely on the as file system and instead do their\nown disk management, either from scratch or by extending as facilities. The\nreasons are practical <1.<; well eLe; technical One practical reason is that a DB~1S\nvendor who \\vishes to support several as platfonns cannot assume features\nspecific to any OS, for porta.bilit,'rr, and would therefore try to make the DBMS\ncode as self-contained as possible. A technical reason is that on a :32-bit systern,\nthe la.rgest file size is 4 GD, whereas a DBMS may want to access a single file\n\n318\nCHAPTER 9\nlarger than that. A related problem is that typical as files cannot span disk\ndevices, which is often desirable or even necessary in a DBMS. Additional\ntechnical reasons why a DBMS does not rely on the as file system are outlined\nin Section 9.4.2.\n9.4\nBUFFER MANAGER\nTo understand the role of the buffer manager, consider a simple example. Sup-\npose that the database contains 1 million pages, but only 1000 pages of main\nmemory are available for holding data. Consider a query that requires a scan\nof the entire file. Because all the data cannot be brought into main memory at\none time, the DBMS must bring pages into main memory as they are needed\nand, in the process, decide what existing page in main memory to replace to\nmake space for the new page. The policy used to decide which page to replace\nis called the replacement policy.\nIn terms of the DBMS architecture presented in Section 1.8, the buffer man-\nager is the software layer responsible for bringing pages from disk to main\nmemory as needed. The buffer manager manages the available main memory\nby partitioning it into a collection of pages, which we collectively refer to as the\nbuffer pool. The main memory pages in the buffer pool are called frames;\nit is convenient to think of them as slots that can hold a page (which usually\nresides on disk or other secondary storage media).\nHigher levels of the DBMS code can be written without worrying about whether\ndata pages are in memory or not; they ask the buffer manager for the page,\nand it is brought into a frame in the buffer pool if it is not already there.\nOf course, the higher-level code that requests a page must also release the\npage when it is no longer needed, by informing the buffer manager, so that\nthe frame containing the page can be reused. The higher-level code must also\ninform the buffer manager if it modifies the requested page; the buffer manager\nthen makes sure that the change is propagated to the copy of the page on disk.\nBuffer management is illustrated in Figure 9.3.\nIn addition to the buffer pool itself, the buffer manager maintains some book-\nkeeping information and two variables for each frame in the pool:\npirLcount\nand dirty. The number of times that the page currently in a given frame has\nbeen requested but not released-the number of current users of the page--is\nrecorded in the pin_count variable for that frame. The Boolean variable dirty\nindicates whether the page ha.<; been modified since it was brought into the\nbuffer pool from disk.\n\nSio'ring Data: D'isks and Files\nPage requests from higher-level code\nBUFFER POOL\nIf a requested page is not in the\npool and the pool is full, the\nbuffer manager's replacement\npolicy controls which existing\npage is replaced.\nFigure 9.3\nThe Buffer Pool\nMAIN MEMORY\nDISK\n319,\nInitially, the pin_count for every frame is set to 0, and the dirty bits are turned\noff. When a page is requested the buffer manager does the following:\n1. Checks the buffer pool to see if some frame contains the requested page\nand, if so, increments the pin_count of that frame. If the page is not in the\npool, the buffer manager brings it in as follows:\n(a) Chooses a frame for replacement, using the replacement policy, and\nincrements its pin_count.\n(b) If the dirty bit for the replacement frame is on, writes the page it\ncontains to disk (that is, the disk copy of the page is overwritten with\nthe contents of the frame).\n(c) Reads the requested page into the replacement frame.\n2. Returns the (main memory) address of the frame containing the requested\npage to the requestor.\nIncrementing pirLco'llnt is often called pinning the requested page in its frame.\nWhen the code that calls the buffer manager and requests the page subsequently\ncalls the buffer manager and releases the page, the pin_count of the frame\ncontaining the requested page is decremented. This is called unpinning the\npage. If the requestor has modified the page, it also informs the buffer manager\nof this at the time that it unpins the page, and the dirty bit for the frame is set.\n\n320\nCHAPTER ,,9\nThe buffer manager will not read another page into a frame until its pi'll-count\nbecomes 0, that is, until all requestors of the page have unpilln~d it.\nIf a requested page is not in the buffer pool and a free frame is not available\nin the buffer pool, a frame with pirl-count 0 is chosen for replacement. If there\nare many such frames, a frame is chosen according to the buffer manager's\nreplacement policy. vVe discuss various replacement policies in Section 9.4.1.\n\\-\\Then a page is eventually chosen for replacement, if the dir'ty bit is not set,\nit means that the page h1-:1..<; not been modified since being brought into main\nmemory.\nHence, there is no need to write the page back to disk; the copy\non disk is identical to the copy in the frame, and the frame can simply be\noverwritten by the newly requested page. Otherwise, the modifications to the\npage must be propagated to the copy on disk.\n(The crash recovery protocol\nmay impose further restrictions, as we saw in Section 1.7. For example, in the\nWrite-Ahead Log (WAL) protocol, special log records are used to describe the\nchanges made to a page. The log records pertaining to the page to be replaced\nmay well be in the buffer; if so, the protocol requires that they be written to\ndisk before the page is written to disk.)\nIf no page in the buffer pool has pin_count 0 and a page that is not in the pool\nis requested, the buffer manager must wait until some page is released before\nresponding to the page request. In practice, the transaction requesting the page\nmay simply be aborted in this situation! So pages should be released-by the\ncode that calls the buffer manager to request the page- as soon as possible.\nA good question to ask at this point is, \"What if a page is requested by several\ndifferent transactions?\"\nThat is, what if the page is requested by programs\nexecuting independently on behalf of different users?\nSuch programs could\nmake conflicting changes to the page. The locking protocol (enforced by higher-\nlevel DBMS code, in particular the transaction manager) ensures that each\ntransaction obtains a shared or exclusive lock before requesting a page to read\nor rnodify.\nTwo different transactions cannot hold an exclusive lock on the\nsame page at the same time; this is how conflicting changes are prevented. The\nbuffer rnanager simply\n~1..'3surnes tha.t the appropriate lock has been obtained\nbefore a page is requested.\n9.4.1\nBuffer Replacement Policies\nThe policy used to choose an unpinned page for replacement can affect the time\ntaken for database operations considerably. Of the man,Y alternative policies,\neach is suitable in different situations.\n\nSto7~ing Data: Disks and Files\n321\nThe best-known replacement policy is least recently used (LRU). This can\nbe implemented in the buffer manager using a queue of pointers to frames with\npin_count O.\nA frame is added to the end of the queue when it becomes a\ncandidate for replacement (that is, when the p'irLco'unt goes to 0). The page\nchosen for replacement is the one in the frame at the head of the queue.\nA variant of LRU, called clock replacement, has similar behavior but less\noverhead. The idea is to choose a page for replacement using a current variable\nthat takes on values 1 through N, where N is the number of buffer frames, in\ncircular order. Vie can think of the frames being arranged in a circle, like a\nclock's face, and current as a clock hand moving across the face. To approximate\nLRU behavior, each frame also has an associated referenced bit, which is turned\non when the page p'in~count goes to O.\nThe current frame is considered for replacement. If the frame is not chosen for\nreplacement, current is incremented and the next frame is considered; this pro-\ncess is repeated until some frame is chosen. If the current frame has pin_count\ngreater than 0, then it is not a candidate for replacement and current is in-\ncremented. If the current frame has the referenced bit turned on, the clock\nalgorithm turns the referenced bit off and increments cm'rent-this way, a re-\ncently referenced page is less likely to be replaced. If the current frame has\np'irLcount 0 and its referenced bit is off, then the page in it is chosen for re-\nplacement. If all frames are pinned in some sweep of the clock hand (that is,\nthe value of current is incremented until it repeats), this means that no page\nin the buffer pool is a replacement candidate.\nThe LRU and clock policies are not always the best replacement strategies for a\ndatabase system, particularly if many user requests require sequential scans of\nthe data. Consider the following illustrative situation. Suppose the buffer pool\nh<4'3 10 frames, and the file to be scanned has 10 or fewer pages.\nAssuming,\nfor simplicity, that there are no competing requests for pages, only the first\nscan of the file does any I/O. Page requests in subsequent scans always find the\ndesired page in the buffer pool. On the other hand, suppose that the file to be\nscanned has 11 pages (which is one more than the number of available pages\nin the buffer pool).\nUsing LRU, every scan of the file will result in reading\nevery page of the file! In this situation, called sequential flooding, LRU is\nthe worst possible replacement strategy.\nOther replacement policies include first in first out (FIFO) and most re-\ncently used (MRU), which also entail overhead similar to LRU, and random,\narnong others. The details of these policies should be evident from their names\nand the preceding discussion of LRU and clock.\n\n322\n,----------------\n_ _.._._-_..----~~------------ -··-~~l\nBuffer Management in Practice: IBM DB2 and Sybase ASE allow\n!\nbuffers to be partitioned into named pools. Each database, table, or in-\nI\ndex can be bound to one of these pools. Each pool can be configured to\nI\nuse either LRU or clock replacement in ASE; DB2 uses a variant of clock\n!\nreplacement, with the initial clock value based on the nature of the page\n!\n(e.g., index non-leaves get a higher starting clock value, which delays their\nreplacement). Interestingly, a buffer pool client in DB2 can explicitly indi-\ncate that it hates a page, making the page the next choice for replacement.\nAs a special case, DB2 applies MRU for the pages fetched in some utility\noperations (e.g., RUNSTATS), and DB2 V6 also supports FIFO. Informix\nand Oracle 7 both maintain a single global buffer pool using LRU; Mi-\ncrosoft SQL Server has a single pool using clock replacement. In Oracle\n8, tables can be bound to one of two pools; one has high priority, and the\nsystem attempts to keep pages in this pool in memory.\nBeyond setting a maximum number of pins for a given transaction, there\nare typically no features for controlling buffer pool usage on a per-\ntransaction basis. Microsoft SQL Server, however, supports a reservation of\nbuffer pages by queries that require large amounts of memory (e.g., queries\ninvolving sorting or hashing).\n9.4.2\nBuffer Management in DBMS versus OS\nObvious similarities exist between virtual memory in operating systems and\nbuffer management in database management systems. In both cases, the goal\nis to provide access to more data than will fit in main memory, and the basic\nidea is to bring in pages from disk to main memory a.<.; needed, replacing pages\nno longer needed in main memory.\nWhy can't we build a DBMS using the\nvirtual memory capability of an OS? A DBMS can often predict the order\nin which pages will be accessed, or page reference patterns, much more\naccurately than is typical in an as environment, and it is desirable to utilize\nthis property. Further, a DBMS needs more control over when a page is written\nto disk than an as typically provides.\nA DBMS can often predict reference patterns because most page references\nare generated by higher-level operations (such as sequential scans or particular\nimplementations of various relational algebra opera.tors) with a. known pattern\nof page accesses. This ability to predict reference patterns allows for a better\nchoice of pages to replace and makes the idea of specialized buffer replacmnent\npolicies more attractive in the DBMS environment.\nEven more important, being able to predict reference patterns enables the usc\nof a simple and very effective strategy called prefetching of pages.\nThe\n\nStoTing Data: Disks and Files\n323\n1\n~\"\n~--~--\"------------------------~\nPrefetching:\nIBM DB2 supports both sequential alld list prefeteh\n(prefetching a list of pages). In general, the prefeteh size is 32 4KB· pages,\nbut this can be set by the user. £tor some sequential type datahaseutilities\n(e.g., COPY, RUNSTATS), DB2 prefetches up to 64 4KB pages,·!'cJr a\nsmaller buffer pool (i.e., less than 1000 buffers), the prefetch quantity is\nadjusted downward to 16 or 8 pages. The prefetch size can be configured by\nthe user; for certain environments, it may be best to prefetch 1000 pages at\na time! Sybase ASE supports asynchronous prefetching of up to 256 pages,\nand uses this capability to reduce latency during indexed access to a table\nin a range scan. Oracle 8 uses prefetching for sequential scan, retrieving\nlarge objects, and certain index scans.\nMicrosoft SQL Server supports\nprefetching for sequential scan and for scarlS along the leaf level ofa B+\ntree index, and the prefetch size can be adjusted a<; a scan progresses. SQL\nServer also uses asynchronous prefetching extensively. Informix supports\nprefetching with a user-defined prefetch size.\nbuffer manager can anticipate the next several page requests and fetch the\ncorresponding pages into memory before the pages are requested. This strategy\nhas two benefits. First, the pages are available in the buffer pool when they\nare requested. Second, reading in a contiguous block of pages is much faster\nthan reading the same pages at different times in response to distinct requests.\n(Review the discussion of disk geometry to appreciate why this is so.) If the\npages to be prcfetched are not contiguous, recognizing that several pages need\nto be fetched can nonetheless lead to faster I/O because an order of retrieval\ncan be chosen for these pages that minimizes seek times and rotational delays.\nIncidentally, note that the I/O can typically be done concurrently with CPU\ncomputation.\nOnce the prefetch request is issued to the disk, the disk is re-\nsponsible for reading the requested pages into memory pages and the CPU can\ncontinue to do other work.\nA DBMS also requires the ability to explicitly force a page to disk, that is, to\nensure that the copy of the page on disk is updated with the copy in memory.\nAs a related point, a DBMS must be able to ensure that certain pages in the\nbuffer pool are written to disk before certain other pages to implement the ';VAL\nprotocol for cra,<;h recovery, as we saw in Section 1.7. Virtual memory imple-\nmentations in operating systems cannot be relied on to provide such control\nover when pages are written to disk; the OS command to write a page to disk\nmay be implemented by essentially recording the write request and deferring\nthe actual modification of the disk copy. If the systern crashes in the interim,\nthe effects can be catastrophic for a DBMS. (Crash recovery is discllssed further\nin Chapter 18.)\n\n324\nCHAPTER~9\nIndexes as Files: In Chapter 8, we presented indexes as a way of 6rga11i~~--··w·l\ning data records for efficient search. From an implementation standpoint,\nI\nI\ni\nindexes are just another kind of file, containing records that dil'ect traffic\non requests for data records. For example, a tree index is a collection of\nrecords organized into one page per node in the tree. It is convenient to\nactually think of a tree index as two files, because it contains two kinds\nof records: (1) a file of index entries, which are records with fields for the\nindex's search key, and fields pointing to a child node, and (2) a file of data\nentries, whose structure depends on the choice of data entry alternative.\n9.5\nFILES OF RECORDS\nWe now turn our attention from the way pages are stored on disk and brought\ninto main memory to the way pages are used to store records and organized\ninto logical collections or files. Higher levels of the DBMS code treat a page as\neffectively being a collection of records, ignoring the representation and storage\ndetails.\nIn fact, the concept of a collection of records is not limited to the\ncontents of a single page; a file can span several pages.\nIn this section, we\nconsider how a collection of pages can be organized as a file. We discuss how\nthe space on a page can be organized to store a collection of records in Sections\n9.6 and 9.7.\n9.5.1\nImplementing Heap Files\nThe data in the pages of a heap file is not ordered in any way, and the only\nguarantee is that one can retrieve all records in the file by repeated requests\nfor the next record. Every record in the file has a unique rid, and every page\nin a file is of the same size.\nSupported operations on a heap file include CTeatc and destmy files, 'insert a\nrecord, delete a record with a given rid, get a record with a given rid, and scan\nall records in the file. To get or delete a record with a given rid, note that we\nmust be able to find the id of the page containing the record, given the id of\nthe record.\nvVe must keep track of the pages in each heap file to support scans, and we must\nkeep track of pages that contain free space to implement insertion efficiently.\n\\Ve discuss two alternative ways to rnaintain this information. In each of these\nalternatives, pages must hold two pointers (which are page ids) for file-level\nbookkeeping in addition to the data.\n\nStoring Data: Disks aTul File,s\n32~\nLinked List of Pages\nOne possibility is to maintain a heap file as a doubly linked list of pages. The\nDBMS can remember where the first page is located by maintaining a table\ncontaining pairs of (heap_file_name, page_Laddr) in a known location on disk.\nWe call the first page of the file the header page.\nAn important task is to maintain information about empty slots created by\ndeleting a record from the heap file. This task has two distinct parts: how to\nkeep track of free space within a page and how to keep track of pages that have\nsome free space. We consider the first part in Section 9.6. The second part can\nbe addressed by maintaining a doubly linked list of pages with free space and\na doubly linked list of full pages; together, these lists contain all pages in the\nheap file. This organization is illustrated in Figure 9.4; note that each pointer\nis really a page id.\nData\npage\nData\npage\nLinked list of\nfull pages\nLinked list of pages\nwith free space\npage\nData\nData\npage\nFigure 9.4\nHeap File Organization with a Linked List\nIf a new page is required, it is obtained by making a request to the disk space\nmanager and then added to the list of pages in the file (probably as a page\nwith free space, because it is unlikely that the new record will take up all the\nspace on the page). If a page is to be deleted from the heap file, it is removed\nfrom the list and the disk space Inanager is told to deallocate it. (Note that the\nscheme can easily be generalized to allocate or deallocate a sequence of several\npages and maintain a doubly linked list of these page sequences.)\nOne disadvantage of this scheIue is that virtually all pages in a file will be on\nthe free list if records are of variable length, because it is likely that every page\nha\",,, at least a few free bytes. To insert a typical record, we must retrieve and\nexaInine several pages on the free list before we find one with enough free space.\nThe directory-based heap file organization that we discuss next addresses this\nproblem.\n\n326\nDirectory of Pages\nCHAPTER,g\nAn alternative to a linked list of pages is to maintain a directory of pages.\nThe DBMS must remember where the first directory page of each heap file is\nlocated. The directory is itself a collection of pages and is shown as a linked\nlist in Figure 9.5. (Other organizations are possible for the directory itself, of\ncourse.)\nHeader page\nData\npage 2\nData\npage N\nDIRECTORY\nFigure 9.5\nHeap File Organization with a Directory\nEach directory entry identifies a page (or a sequence of pages) in the heap file.\nAs the heap file grows or shrinks, the number of entries in the directory-and\npossibly the number of pages in the directory itself--grows or shrinks corre-\nspondingly. Note that since each directory entry is quite small in comparison to\na typical page, the size of the directory is likely to be very small in comparison\nto the size of the heap file.\nFree space can be managed by maintaining a bit per entry, indicating whether\nthe corresponding page has any free space, or a count per entry, indicating the\namount of free space on the page. If the file contains variable-length records,\nwe can examine the free space count for an entry to determine if the record\nfits on the page pointed to by the entry. Since several entries fit on a directory\npage, we can efficiently search for a data page with enough space to hold a\nrecord to be inserted.\n9.6\nPAGE FORMATS\nThe page abstraction is appropriate when dealing with I/O issue-s, but higher\nlevels of the DBMS see data a..<; a collection of records.\nIn this section, we\n\nStoTing Data: D'i.5ks and Files\n327.\nRids in COInmercial Systems:\nIBM DB2 l Informix, Microsoft SQL\nServer l Oracle 8, and Sybase ASE all implement record ids as a page id\nand slot number. Syba..c;e ASE uses the following page organization, which\nis typical: Pages contain a header followed by the rows and a slot array.\nThe header contains the page identity, its allocation state, page free space\nstate, and a timestamp. The slot array is simply a mapping of slot number\nto page offset.\nOracle 8 and SQL Server use logical record ids rather than page id and slot\nnumber in one special case: If a table has a clustered index, then records in\nthe table are identified using the key value for the clustered index. This has\nthe advantage that secondary indexes need not be reorganized if records\nare moved ac~oss pages.\nconsider how a collection of records can be arranged on a page. We can think\nof a page as a collection of slots, each of which contains a record. A record is\nidentified by using the pair (page id, slot number); this is the record id (rid).\n(We remark that an alternative way to identify records is to assign each record\na unique integer as its rid and maintain a table that lists the page and slot of\nthe corresponding record for each rid. Due to the overhead of maintaining this\ntable, the approach of using (page id, slot number) as an rid is more common.)\nWe now consider some alternative approaches to managing slots on a page.\nThe main considerations are how these approaches support operations such as\nsearching, inserting, or deleting records on a page.\n9.6.1\nFixed-Length Records\nIf all records on the page are guaranteed to be of the same length, record slots\narc uniform and can be arranged consecutively within a page. At any instant,\nsome slots are occupied by records and others are unoccupied. When a record\nis inserted into the page, we must locate an empty slot and place the record\nthere. The main issues are how we keep track of empty slots and how we locate\nall records on a page. The alternatives hinge on how we handle the deletion of\na record.\nThe first alternative is to store records in the first N slots (where N is the\nnumber of records on the page); whenever a record is deleted, we move the last\nrecord on the page into the vacated slot. This format allows us to locate the\nith record on a page by a simple offset calculation, and all empty slots appear\ntogether at the end of the page. However, this approach docs not work if there\n\n328\nCHAPTER.9\nare external references to the record that is moved (because the rid contains\nthe slot number, which is now changed).\nThe second alternative is to handle deletions by using an array of bits, one per\nslot, to keep track of free slot information. Locating records on the page requires\nscanning the bit array to find slots whose bit is on; when a record is deleted,\nits bit is turned off. The two alternatives for storing fixed-length records are\nillustrated in Figure 9.6. Note that in addition to the information about records\non the page, a page usually contains additional file-level information (e.g., the\nid of the next page in the file).\nThe figure does not show this additional\ninformation.\nPacked\nUnpacked, Bitmap\nM\n3\n2\n1J\nNumber of slots\nSlot 1\nSlot 2\nSlot 3\no\n0\n0\n~\nSlot M\n11\nI I 1011\\ M\nFree ~\nSpace\nJ\n'-S\"Page ~\nJ\nHeader\nNumber of records\no\n0\n0\n...\nIN\nSlot N\nSlot 1\nSlot 2\nFigure 9.6\nAlternative Page Organizations for Fixed-Length Recorcls\nThe slotted page organization described for variable-length records in Section\n9.6.2 can also be used for fixed-length records. It becomes attractive if we need\nto move records around on a page for reasons other than keeping track of space\nfreed by deletions. A typical example is that we want to keep the records on a\npage sorted (according to the value in some field).\n9.6.2\nVariable-Length Records\nIf records are of variable length, then we cannot divide the page into a fixed\ncollection of slots. The problem is that, when a new record is to be inserted,\nwe have to find an empty slot of just the right length----if we use a slot that\nis too big, we waste space, ancl obviously we cannot use a slot that is smaller\nthan the record length. Therefore, when a record is inserted, we must allocate\njust the right amount of space for it, and when a record is deleted, we must\nmove records to fill the hole created by the deletion, to ensure that all the free\nspace on the page is contiguous. Therefore, the ability to move records on a\npage becomes very important.\n\nSt011ng Data: Disks (l'nd Files\n3#29\nThe most flexible organization for variable-length records is to maintain a di-\nrectory of slots for each page, with a (record offset, recOT'd length) pair per\nslot. The first component (record offset) is a 'pointer' to the record, as shown\nin Figure 9.7; it is the ofl'set in bytes from the start of the data area on the\npage to the start of the record, Deletion is readily accomplished by setting the\nrecord ofl'set to -1. Records can be moved around on the page because the rid,\nwhich is the page number and slot number (that is, position in the directory),\ndoes not change when the record is moved; only the record ofl'set stored in the\nslot changes.\nDATA AREA\nrid = (i,N)\nPAGE i\n/\noffset of record from\n/\nstart of data area\nI\n\\\n\\\nRecord with rid = (i,l)\n\\~__II\nlength =24\nFigure 9.7\nPage Organization for Variable-Length R.ecords\nThe space available for new records must be managed carefully because the page\nis not preformatted into slots. One way to manage free space is to maintain a\npointer (that is, ofl'set from the start of the data area on the page) that indicates\nthe start of the free space area. vVhen a new record is too large to fit into the\nremaining free space, we have to move records on the page to reclairn the space\nfreed by records deleted earlier. The idea is to ensure that, after reorganization,\nall records appear in contiguous order, followed by the available free space.\nA subtle point to be noted is that the slot for a deleted record cannot always\nbe removed from the slot directory, because slot numbers are used to identify\nrecords-by deleting a slot, we change (decrement) the slot number of subse-\nquent slots in the slot directory, and thereby change the rid of records pointed\nto by subsequent slots. The only way to remove slots from the slot directory is\nto remove the last slot if the record that it points to is deleted. However, when\n\n330\nCHAPTER .9\na record is inserted, the slot directory should be scanned for an element that\ncurrently does not point to any record, and this slot should be used for the new\nrecord. A new slot is added to the slot directory only if all existing slots point\nto records. If inserts are much more common than deletes (as is typically the\ncase), the number of entries in the slot directory is likely to be very close to\nthe actual number of records on the page.\nThis organization is also useful for fixed-length records if we need to move\nthem around frequently; for example, when we want to maintain them in some\nsorted order. Indeed, when all records are the same length, instead of storing\nthis common length information in the slot for each record, we can store it once\nin the system catalog.\nIn some special situations (e.g., the internal pages of a B+ tree, which we\ndiscuss in Chapter 10), we lIlay not care about changing the rid of a record. In\nthis case, the slot directory can be compacted after every record deletion; this\nstrategy guarantees that the number of entries in the slot directory is the same\nas the number of records on the page. If we do not care about modifying rids,\nwe can also sort records on a page in an efficient manner by simply moving slot\nentries rather than actual records, which are likely to be much larger than slot\nentries.\nA simple variation on the slotted organization is to maintain only record offsets\nin the slots.\nfor variable-length records, the length is then stored with the\nrecord (say, in the first bytes). This variation makes the slot directory structure\nfor pages with fixed-length records the salIle a..s for pages with variab1e~length\nrecords.\n9.7\nRECORD FORMATS\nIn this section, we discuss how to organize fields within a record. While choosing\na way to organize the fields of a record, we must take into account whether the\nfields of the record are of fixed or variable length and consider the cost of various\noperations on the record, including retrieval and modification of fields.\nBefore discussing record fonnats, we note that in addition to storing individual\nrecords,\ninforlI1(~tion conllnon to all records of a given record type (such a.'3 the\nnumber of fields and field types) is stored in the system catalog, which can\nbe thought of as a description of the contents of a database, maintained by the\nDBMS (Section 12.1).\nThis avoids repeated storage of the same information\nwith each record of a given type.\n\nStoTing Data: Disks and Files\n:331,\nRecord Formats in Commercial Aystems: In IBM DB2, fixed-length\nfields are at fixed offsets from the beginning of the record. Variable-length\nfields have ofIset and length in the fixed offset part of the record, and\nthe fields themselves follow the fixed-length part of the record. Informix,\nMicrosoft SQL Server, and Sybase ASE use the same organization with\nminor variations.\nIn Oracle 8, records are structured as if all fields are\npotentially of variable length; a record is a sequence of length-data pairs,\nwith a special length value used to denote a null value.\n9.7.1\nFixed-Length Records\nIn a fixed-length record, each field h&<; a fixed length (that is, the value in this\nfield is of the same length in all records), and the number of fields is also fixed.\nThe fields of such a record can be stored consecutively, and, given the address of\nthe record, the address of a particular field can be calculated using information\nabout the lengths of preceding fields, which is available in the system catalog.\nThis record organization is illustrated in Figure 9.8.\nBase address (B)\nAddress =B+L1+L2\nFi = Field i\nLi = Length of\nfield i\nFigure 9.8\nOrgani'lation of Records with Fixed-Length Fields\n9.7.2\nVariable-Length Records\nIn the relational model, every record in a relation contains the same number\nof fields. If the number of fields is fixed, a record is of variable length only\nbecause some of its fields are of variable length.\nOne possible orga,nizatioll is to store fields consecutively, separated by delim-\niters (which are special characters that do not appear in the data itself). This\norganization requires a scan of the record to locate a desired field.\nAn alternative is to reserve some space at the beginning of a record for use 1:LS\nan array of integer offsets-the ith integer in this array is the starting address\nof the ith field value relative to the start of the record. Note that we also store\nan offset to the end of the record; this offset is needed to recognize where the\nlast held ends. Both alternatives are illustrated in Figure 9.9.\n\n332\nCHAPTERr9\nFields delimited by special symbol $\nArray of field offsets\nFigure 9.9\nAlternative Record Organizations for Variable-Length Fields\nThe second approach is typically superior. For the overhead of the offset array,\nwe get direct access to any field.\nWe also get a clean way to deal with null\nvalues. A mdl value is a special value used to denote that the value for a field\nis unavailable or inapplicable. If a field contains a null value, the pointer to the\nend of the field is set to be the same as the pointer to the beginning of the field.\nThat is, no space is used for representing the null value, and a comparison of\nthe pointers to the beginning and the end of the field is used to determine that\nthe value in the field is null.\nVariable~length record formats can obviously be used to store fixed-length\nrecords as well; sometimes, the extra overhead is justified by the added flexibil-\nity, because issues such as supporting n'ull values and adding fields to a recorcl\ntype arise with fixed-length records as well.\nI-laving variable-length fields in a record can raise some subtle issues, especially\nwhen a record is modified.\nIII\nModifying a field may cause it to grow, whieh requires us to shift all subse-\nquent fields to make space for the modification in all three record formats\njust presentcel.\nIII\nA modified record may no longer fit into the space remaining on its page.\nIf so, it may have to be moved to another page. If riels, which are used\nto 'point' to a record, include the page number (see Section 9.6), moving\na record to 'another page causes a problem. We may have to leave a 'for-\nwarding address' on this page identifying the ne\"v location of the record.\nAnd to ensure that space is ahvays available for this forwarding address,\nwe would have to allocate some minimum space for each record, regardless\nof its length.\n\nStoring Data: Disks and Files\nLarge Records in Real Systems: In Sybc1se ASE, a record can be at\nmost 1962 bytes. This limit is set by the 2KB log page size, since records\nare not allowed to be larger than a page. The exceptions to this rule. an~\nBLOBs and CLOBs, which consist of 1:1 set of bidirectionally linked pages.\nIBlvl DB2 and Microsoft SqL Server also do not allow records to span\npages, although large objects are allowed to span pages and are handled\nseparately from other data types. In DB2, record size is limited only by\nthe page size; in SQL Server, a record can be at most 8KB, excluding\nLOBs. Informix and Oracle 8 allow records to span pages. Informix allows\nrecords to be at most 32KB, while Oracle has no maximum record size;\nlarge records are organized as a singly directed list.\nIII\nA record may grow so large that it no longer fits on anyone page. We have\nto deal with this condition by breaking a record into smaller records. The\nsmaller records could be chained together-part of each smaller record is\na pointer to the next record in the chain---to enable retrieval of the entire\noriginal record.\n9.8\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\nExplain the term memory hierarchy.\nWhat are the differences between\nprimary, secondary, and tertiary storage? Give examples of each. Which\nof these is volatile, and which are pCf'sistenf?\nWhy is persistent storage\nmore important for a DBMS than, say, a program that generates prime\nnumbers? (Section 9.1)\nIII\nWhy are disks used so widely in a DBMS? What are their advantages\nover main memory and tapes?\n':Vhat are their relative disadvantages?\n(Section 9.1.1)\nIII\nWhat is a disk block or page?\nHow are blocks arranged in a disk?\nHow\ndoes this affect the time to access a block? Discuss seek tiTne, rotational\ndday, and transfer time. (Section 9.1.1)\nIII\nExplain how careful placement of pages on the disk to exploit the geometry\nof a disk can minimize the seek time and rotational delay when pages are\nread sequentially. (Section 9.1.2)\nIII\nExplain what a RAID systenl is and how it improves performance and\nreliability. Discuss str-iping and its impact on performance and nxlundancy\nand its irnpact on reliability.\nvVhat are the trade-offs between reliability\n\n334\nCHAPTER.£)\nand performance in the different RAID organizations called RAID levels'?\n(Section 9.2)\n..\nWnat is the role of the DBMS d'isk space manager'?\nvVhy do database\nsystems not rely on the operating system instead? (Section 9.3)\n..\nWhy does every page request in a DBMS go through the buffer manager?\nWhat is the buffer poor? '\\That is the difference between a frame in a buffer\npool, a page in a file, and a block on a disk? (Section 9.4)\n..\nWhat information does the buffer manager maintain for each page in the\nbuffer pool?\n·What information is maintained for each frame?\nWhat is\nthe significance of p'in_count and the d'irty flag for a page?\nUnder what\nconditions can a page in the pool be replaced?\nUnder what conditions\nmust a replaced page be written back to disk? (Section 9.4)\n..\nWhy does the buffer manager have to replace pages in the buffer pool?\nHow is a page chosen for replacement? vVhat is sequent'ial flood'ing, and\nwhat replacement policy causes it? (Section 9.4.1)\n..\nA DBMS buffer manager can often predict the access pattern for disk pages.\nHow does it utilize this ability to minimize I/O costs? Discuss prefetch-\n'ing.\n\\iVhat is forc'ing, and why is it required to support the write-ahead\nlog protocol in a DBMS? In light of these points, explain why database\nsystems reimplement many services provided by operating systems. (Sec-\ntion 9.4.2)\n..\nWhy is the abstraction of a file of records important? How is the software\nin a DBMS layered to take advantage of this? (Section 9.5)\n..\nWhat is a heap file? How are pages organized in a heap file? Discuss list\nversus directory organizations. (Section 9.5.1)\nIII\nDescribe how records are arranged on a page.\n\\i\\That is a slot, and how\nare slots used to identify records? How do slots ena.ble us to move records\non a page withont altering the record's identifier?\n·What arc the differ-\nences in page organizations for fixed-length and variable-length records?\n(Section 9.6)\niii\n·What are the differences in how fields are arranged within fixed-length and\nvariable-length records? For variable-length records, explain how the array\nof offsets organization provides direct access to a specific field and supports\nnull values. (Section 9.7)\n\nStoring Data: Disks and Files\nEXERCISES\nExercise 9.1 \\-Vhat is the most important difference behveen a disk and a tape?\nExercise 9.2 Explain the terms seek time, mtat'ional delay, and transfer t'ime.\n33:)\nExercise 9.3 Both disks and main memory support direct access to any desired location\n(page). On average, main memory accesses are faster, of course. \\\\That is the other important\ndifference (from the perspective of the time required to access a desired page)?\nExercise 9.4 If you have a large file that is frequently scanned sequentially, explain how you\nwould store the pages in the file on a disk.\nExercise 9.5 Consider a disk with a sector size of 512 bytes, 2000 tracks per surface, 50\nsectors per track, five double-sided platters, and average seek time of 10 msec.\n1. What is the capacity of a track in bytes? What is the capacity of each surface? What is\nthe capacity of the disk?\n2. How many cylinders does the disk have?\n:~. Give examples of valid block sizes. Is 256 bytes a valid block size? 2048? 51,200?\n4. If the disk platters rotate at 5400 rpm (revolutions per minute), what is the maximum\nrotational delay?\n5. If one track of data can be transferred per revolution, what is the transfer rate?\nExercise 9.6 Consider again the disk specifications from Exercise 9.5 and suppose that a\nblock size of 1024 bytes is chosen. Suppose that a file containing 100,000 records of 100 bytes\neach is to be stored on such a disk and that no record is allowed to span two blocks.\nL How many records fit onto a block?\n2. How many blocks are required to store the entire file? If the file is arranged sequentially\non disk, how lllallY surfaces are needed?\n:3. How many records of 100 bytes each can be stored using this disk?\n4. If pages are stored sequentially on disk, with page 1 on block 1 of track 1, what page is\nstored on block 1 of track 1 on the next disk surface? How would your answer change if\nthe disk were capable of reading and writing from all heads in parallel?\n5. VVhat titne is required to read a file containing 100,000 records of 100 bytes each sequen-\ntially? Again, how \\vould your answer change if the disk were capable of reading/writing\nfrom all heads in parallel (and the data was arranged optimally)?\n6. \\\\That is the time required to read a file containing 100,000 records of 100 bytes each in a\nrandom order? To read a record, the block containing the recOl'd has to be fetched from\ndisk. Assume that each block request incurs the average seek time and rotational delay.\nExercise 9.7 Explain what. the buffer manager JIms! do to process a read request for a page.\n\\Vhat happens if the requested page is in the pool but not pinned?\nExercise 9.8 When does a buffer manager write a page to disk?\nExercise 9.9 What. does it mean to say that a page is p'inned in the buffer pool? Who is\nresponsible for pinning pages? \\Vho is responsible for unpinning pages?\n\nCHAPTERJ9\nExercise 9.10 'When a page in the bulTer pool is modified, how does the DBMS ensure that\nthis change is propagated to disk?\n(Explain the role of the buffer manager as well as the\nmodifier of the page.)\nExercise 9.11 \\Vhat happens if a page is requested when all pages in the buffer pool are\ndirty?\nExercise 9.12 \\Vhat is sequential flooding of the buffer pool?\nExercise 9.13 Name an important capability of a DBIVIS buffer manager that is not sup-\nported by a typical operating system's buffer manager.\nExercise 9.14 Explain the term prefetching. \\Vhy is it important?\nExercise 9.15 Modern disks often have their own main memory caches, typically about\n1 MB, and use this to prefetch pages.\nThe rationale for this technique is the empirical\nobservation that, if a disk page is requested by some (not necessarily database!) application,\n80% of the time the next page is requested as well. So the disk gambles by reading ahead.\n1. Give a nontechnical reason that a DBMS may not want to rely on prefetching controlled\nby the disk.\n2. Explain the impact on the disk's cache of several queries running concurrently, each\nscanning a different file.\n3. Is this problem addressed by the DBMS buffer manager prefetching pages? Explain.\n4. Modern disks support segmented caches, with about four to six segments, each of which\nis used to cache pages from a different file.\nDoes this technique help, with respect to\nthe preceding problem? Given this technique, does it matter whether the DBMS buffer\nmanager also does prefetching?\nExercise 9.16 Describe two possible record formats. What are the trade-offs between them?\nExercise 9.17 Describe two possible page formats. What are the trade-offs between them?\nExercise 9.18 Consider the page format for variable-length records that uses a slot directory.\n1. One approach to managing the slot directory is to use a maximum size (i.e., a maximum\nnumber of slots) and allocate the directory array when the page is created. Discuss the\npros and cons of this approach with respect to the approach discussed in the text.\n2. Suggest a modification to this page format that would allow us to sort records (according\nto the value in some field) without moving records and without changing the record ids.\nExercise 9.19 Consider the two internal organizations for heap files (using lists of pages and\na directory of pages) discussed in the text.\n1. Describe them briefly and explain the trade-offs. \\Vhich organization would you choose\nif records are variable in length?\n2. Can you suggest a single page format to implement both internal file organizations'?\nExercise 9.20 Consider a list-based organizat.ion of the pages in a heap file in which two\nlists are maintained: a list of all pages in the file and a list of all pages with free space. In\ncontrast, the list-based organizatioll discussed in the text maintains a list of full pages and a\nlist of pages with free space.\n\nStoring Data: Disks and Files\n3&7\n1. VVhat are the trade-offs, if any'? Is one of them clearly superior?\n2. For each of these organizations, describe a suitable page format.\nExercise 9.21 Modern disk drives store more sectors on the outer tracks than the inner\ntracks. Since the rotation speed is constant, the sequential data transfer rate is also higher on\nthe outer tracks. The seek time and rotational delay are unchanged. Given this information,\nexplain good strategies for placing files with the following kinds of access patterns:\n1. rrequent, random accesses to a small file (e.g., catalog relations).\n2. Sequential scans of a large file (e.g., selection from a relation with no index).\n3. Random accesses to a large file via an index (e.g., selection from a relation via the index).\n4. Sequential scans of a small file.\nExercise 9.22 Why do frames in the buffer pool have a pin count instead of a pin flag?\nPROJECT-BASED EXERCISES\nExercise 9.23 Study the public interfaces for the disk space manager, the buffer manager,\nand the heap file layer in Minibase.\n1. Are heap files with variable-length records supported?\n2. What page format is used in Minibase heap files?\n3. What happens if you insert a record whose length is greater than the page size?\n4. How is free space handled in Minibase?\nBIBLIOGRAPHIC NOTES\nSalzberg [648] and Wiederhold [776] discuss secondary storage devices and file organizations\nin detail.\nRAID wa.s originally proposed by Patterson, Gibson, and Katz [587]. The article by Chen\net al. provides an excellent survey of RAID [171] .\nBooks about RAID include Gibson's\ndissertation [.317] and the publications from the RAID Advisory Board [605].\nThe design and implementation of storage managers is discussed in [65, 1:33, 219, 477, 718].\nWith the exception of [219], these systems emphasize el:tensibili.ty, anel the papers contain\nmuch of interest from that stanelpoint as well. Other papers that cover storage management\nissues in the context of significant implemented prototype systems are [480] and [588]. The\nDali storage Inanager, which is optimized for main memory databases, is described in [406].\nThree techniques for ilnplementing long fields are compared in [96]. The impact of processor\ncache misses 011 DBMS performallce ha.'i received attention lately, as complex queries have\nbecome increasingly CPU-intensive. [:33] studies this issue, and shows that performance can be\nsignificantly improved by using a new arrangement of records within a page, in which records\non a page are stored in a column~oriented format (all field values for the first attribute followed\nby values for the second attribute, etc.).\nStonebraker discusses operating systems issues in the context of databases in [715]. Several\nbuffer management policies for databa.se systems are compared in [181]. Buffer management\nis also studied in [119, 169, 2G1, 2:35].\n\n10\nTREE-STRUCTURED\nINDEXING\n...\nWhat is the intuition behind tree-structured indexes? Why are they\ngood for range selections?\n...\nHow does an ISAM index handle search, insert, and delete?\ni\"-\nHow does a B+ tree index handle search, insert, and delete?\n...\nWhat is the impact of duplicate key values on index implementation'?\n...\nWhat is key compression, and why is it important?\n...\nWhat is bulk-loading, and why is it important?\n...\nWhat happens to record identifiers when dynamic indexes are up-\ndated? How does this affect clustered indexes?\nItt\nKey concepts: ISAM, static indexes, overflow pages, locking issues;\nB+ trees, dynamic indexes, balance, sequence sets, node format; B+\ntree insert operation, node splits, delete operation, merge versus redis-\ntribution, minimum occupancy; duplicates, overflow pages, including\nrids in search keys; key compression; bulk-loading; effects of splits on\nrids in clustered indexes.\nOne that would have the fruit must climb the tree.\nI'homas Fuller\nVVe now consider two index data structures, called ISAM and B+ trees, b<:h':led\non tree organizations.\nThese structures provide efficient support for range\nsearches, including sorted file scans as a special c<h'3e. Unlike sorted files, these\n\nTree-StTuctuTed Indel:ing\nindex structures support efficient insertion and deletion.\nThey also provide\nsupport for equality selections, although they are not &'3 efficient in this case as\nhash-b::l.'3ed indexes, which are discussed in Chapter 11.\nAn ISAJVI1 tree is a static index structure that is effective when the file is\nnot frequently updated, but it is unsuitable for files that grow and shrink a\nlot.\n\\Ve discuss ISAM in Section 10.2.\nThe B+ tree is a dynamic structure\nthat adjusts to changes in the file gracefully. It is the most widely used index\nstructure because it adjusts well to changes and supports both equality and\nrange queries. We introduce B+ trees in Section 10.3. We cover B+ trees in\ndetail in the remaining sections. Section 10.3.1 describes the format of a tree\nnode.\nSection lOA considers how to search for records by using a B+ tree\nindex. Section 10.5 presents the algorithm for inserting records into a B+ tree,\nand Section 10.6 presents the deletion algorithm.\nSection 10.7 discusses how\nduplicates are handled. \\Ve conclude with a discussion of some practical issues\nconcerning B+ trees in Section 10.8.\nNotation: In the ISAM and B+ tree structures, leaf pages contain data entries,\naccording to the terminology introduced in Chapter 8.\nFor convenience, we\ndenote a data entry with search key value k as k*.\nNon-leaf pages conta.in\ninde:c entries of the form (search key 'Value., page id) and are used to direct the\nsea.rch for a desired data entry (which is stored in some leaf). We often simply\nuse entr'Y where the context makes the nature of the entry (index or data) clear.\n10.1\nINTUITION FOR TREE INDEXES\nConsider a file of Students recorcls sorted by gpa. To answer a range selection\nsuch as \"Find all students with a gpa higher than 3.0,\" we must identify the\nfirst such student by doing a binary search of the file and then scan the file\nfrom that point on. If the file is large, the initial binary search can be quite\nexpensive, since cost is proportional to the number of pages fetched; can we\nimprove upon this method'?\nOIle idea is to create a second file with OIle record per page in the original\n(data) file, of the form (first key on page, pointer to page), again sortecl by the\nkey attribute (which is gpa in our example). The format of a page in the second\ninde:c file is illustrated in Figure 10.1.\nWe refer to pairs of the form (key, pointer)\n~l.S indc:J: entries or just entries \\'\\'hen\nthe context is dear. Note that each index page contains OIle pointer more than\nI ISAM stands for Indexed Sequential Access Method.\n\n340\nindex entry\nr·.. ········\nFigUl'e 10.1\nFormat of an Index Page\nCHAPTER 10,\nthe number of keys---each key serves as a separator- for the contents of the pages\npointed to by the pointers to its left and right.\nThe simple index file data structure is illustrated in Figure 10.2.\nI\nPafJe 1 ·11\nPage 2.· II\nPage 31\nIndex file\nData file\nFigure 10.2\nOne-Level Index Structure\nWe can do a binary search of the index file to identify the page containing the\nfirst key (gpo.) value that satisfies the range selection (in our example, the first\nstudent with gpo. over 3.0) and follow the pointer to the page containing the first\ndata. record with that key value. We can then scan the data file sequentially\nfrom that point on to retrieve other qualifying records. This example uses the\nindex to find the first data page containing a Students record with gpo. greater\nthan 3.0, and the data file is scanned from that point on to retrieve other such\nStudents records.\nBecause the size of an entry in the index file (key value and page icl) is likely\nto be much smaller than the size of a page, and only one such entry exists per\npage of the data file, the index file is likely to be much smaller than the data\nfile; therefore, a binary search of the index file is much faster than a binary\nsearch of the data file.\nHowever, a binary search of the index file could still\nbe fairly expensive, and the index file is typically still large enough to make\ninserts and\nclelett~s expensive.\nThe potential large size of the index file motivates the tree indexing idea: Why\nnot apply the previous step of building an auxiliar:v structure all the collection\nof inde:l: records and so on recursively until the smallest auxiliary structure fits\nOIl one page? This repeated construction of a one-level index leads to a tree\nstructure with several levels of non-leaf pages.\n\nTrce-Stn.lduTed Inde:ring\n341\nJ\nAs we observed in Section 8.3.2, the power of the approach comes from the fact\nthat locating a record (given a search key value) involves a traversal from the\nroot to a leaf, with one I/O (at most; SCHne pages, e.g.) the root, are likely to be\nin the buffer pool) per level. Given the typical fan-out value (over 100), trees\nrarely have more than 3-4 levels.\nThe next issue to consider is how the tree structure can handle inserts and\ndeletes of data entries. Two distinct approaches have been used, leading to the\nISAM and B+ tree data structures, which we discuss in subsequent sections.\n10.2\nINDEXED SEQUENTIAL ACCESS METHOD (ISAM)\nThe ISAM data structure is illustrated in Figure 10.3. The data entries of the\nISAM index are in the leaf pages of the tree and additional overflow pages\nchained to some leaf page. Database systems carefully organize the layout of\npages so that page boundaries correspond closely to the physical characteristics\nof the underlying storage device.\nThe ISAM structure is completely static\n(except for the overflow pages, of which it is hoped, there will be few) and\nfacilitates such low-level optimizations.\nNon-leaf\npages\nLeaf\npages\nOverjlow P{~c::::J1\n~\n~\nPrimary pages\nFigure 10.3\nISAM Index Structure\nEach tree node is a disk page, and all the data resides in the leaf pages. This\ncorresponds to an index that uses Alternative (1) for data entries, in terms of\nthe alternatives described in Chapter 8; we can create an index with Alternative\n(2) by storing t.he data records in a separate file and storing (key, rid) pairs in\nthe leaf pages of the ISAM index. When the file is created, all leaf pages are\nallocated sequentially and sorted on the search key value. (If Alternative (2)\nor (3) is used, the data records are created and sorted before allocating the leaf\npages of the ISAM index.) The non-leaf level pages are then allocated. If there\nare several inserts to the file subsequently, so that more entries are inserted into\na leaf than will fit onto a single page, additional pages are needed because the\n\n342\nCHAPTER lO\nindex structure is static. These additional pages are allocated from an overflow\narea. The allocation of pages is illustrated in Figure 10.4.\nData Pages\nIndex Pages\nOverflow Pages\nFigure 10.4\nPage Allocation in ISAM\nThe basic operations of insertion, deletion, and search are all quite straightfor-\nward. J;\"'or an equality selection search, we start at the root node and determine\nwhich subtree to search by comparing the value in the search field of the given\nrecord with the key values in the node. (The search algorithm is identical to\nthat for a B+ tree; we present this algorithm in more detail later.) For a range\nquery, the starting point in the data (or leaf) level is determined similarly, and\ndata pages are then retrieved sequentially. For inserts and deletes, the appro-\npriate page is determined as for a search, and the record is inserted or deleted\nwith overflow pages added if necessary.\nThe following example illustrates the ISAM index structure. Consider the tree\nshown in Figure 10.5. All searches begin at the root. For example, to locate a\nrecord with the key value 27, we start at the root and follow the left pointer,\nsince 27 < 40. We then follow the middle pointer, since 20 <= 27 < 33. For a\nrange sea,rch, we find the first qualifying data entry as for an equality selection\nand then retrieve primary leaf pages sequentially (also retrieving overflow pages\nas needed by following pointers from the primary pages).\nThe primary leaf\npages are cL..ssumed to be allocated sequentially\nthis a..ssumption is reasonable\nbecause the number of such pages is known when the tree is created and does\nnot change subsequently under inserts and deletes-and so no 'next leaf page'\npointers are needed.\nvVe assume that each leaf page can contain two entries.\nIf we now insert a\nrecord with key value 23, the entry 23* belongs in the second data page, which\nalready contains 20* and 27* and has no more space. We deal with this situation\nby adding an overflow page and putting 23* in. the overflow page. Chains of\noverflow pages can easily develop.\nF'or instance, inserting 48*, 41*, and 42*\nleads to an overflow chain of two pages. The tree of Figure 10.5 with all these\ninsertions is shown ill Figure 10.6.\n\nTree~StrLLctuTed Inde:ri:ng\n343\n1\n10*1\n15*IEEl B3 1\n40*1\n46*11\n51*l 55*1 1\n63*1 97*I\nFigure 10.5\nSa.mple ISAM Tree\nNon-leaf\npages\nPrimary\nleaf\npages\nOverflow\npages\nFigure 10.6\nISAM Tree a.fter Inserts\n\n344\nCHAPTER Ii)\nThe deletion of an entry h\nis handled by simply removing the entry. If this\nentry is on an overflow page and the overflow page becomes empty, the page can\nbe removed. If the entry is on a primary page and deletion makes the primary\npage empty, the simplest approach is to simply leave the empty primary page\n~s it is; it serves as a placeholder for future insertions (and possibly lloll-empty\noverflow pages, because we do not move records from the overflow pages to the\nprimary page when deletions on the primary page create space).\nThus, the\nnumber of primary leaf pages is fixed at file creation time.\n10.2.1\nOverflow Pages, Locking Considerations\nNote that, once the ISAM file is created, inserts and deletes affect only the\ncontents of leaf pages. A consequence of this design is that long overflow chains\ncould develop if a number of inserts are made to the same leaf. These chains\ncan significantly affect the time to retrieve a record because the overflow chain\nhas to be searched as well when the search gets to this leaf. (Although data in\nthe overflow chain can be kept sorted, it usually is not, to make inserts fast.) To\nalleviate this problem, the tree is initially created so that about 20 percent of\neach page is free. However, once the free space is filled in with inserted records,\nunless space is freed again through deletes, overflow chains can be eliminated\nonly by a complete reorganization of the file.\nThe fact that only leaf pages are modified also has an important advantage with\nrespect to concurrent access. When a page is accessed, it is typically 'locked'\nby the requestor to ensure that it is not concurrently modified by other users\nof the page. To modify a page, it must be locked in 'exclusive' mode, which is\npermitted only when no one else holds a lock on the page. Locking can lead\nto queues of users (transactions, to be more precise) waiting to get access to a\npage. Queues can be a significant performance bottleneck, especially for heavily\naccessed pages near the root of an index structure.\nIn the ISAM structure,\nsince we know that index-level pages are never modified, we can safely omit\nthe locking step. Not locking index-level pages is an important advantage of\nISAM over a dynamic structure like a B+ tree. If the data distribution and\nsize are relatively static, which means overflow chains are rare, ISAM might be\npreferable to B+ trees due to this advantage.\n10.3\nB+ TREES: A DYNAMIC INDEX STRUCTURE\nA static structure such as the ISAI\\il index suffers from the problem that long\noverflow chains can develop a\"s the file grows, leading to poor performance. This\nproblem motivated the development of more flexible, dynamic structures that\nadjust gracefully to inserts and deletes. The B+ tree search structure, which\nis widely llsed, is a balanced tree in which the internal nodes direct the search\n\nTree~StT7lChtTed IndeTing\n345\nand the leaf nodes contain the data entries. Since the tree structure grows and\nshrinks dynamically, it is not feasible to allocate the leaf pages sequentially as in\nISAM, where the set of primary leaf pages was static. To retrieve all leaf pages\nefficiently, we have to link them using page pointers. By organizing them into a\ndoubly linked list, we can easily traverse the sequence of leaf pages (sometimes\ncalled the sequence set) in either direction. This structure is illustrated in\nFigure 10.7.2\nIndex entries\n(To direct search)\nIndex\nfile\nData entries\n(\"Sequence set\")\nFigure 10.7\nStructure of a B+ 'n'ee\nThe following are some of the main characteristics of a B+ tree:\n•\nOperations (insert, delete) on the tree keep it balanced.\n•\nA minimum occupancy of 50 percent is guaranteed for each node except\nthe root if the deletion algorithm discussed in Section 10.6 is implemented.\nHowever, deletion is often implemented by simply locating the data entry\nand removing it, without adjusting the tree &'3 needed to guarantee the 50\npercent occupancy, because files typically grow rather than shrink.\nl1li\nSearching for a record requires just a traversal from the root to the appro-\npriate leaf. Vie refer to the length of a path from the root to a leaf any\nleaf, because the tree is balanced\nas the height of the tree. For example,\na tree with only a leaf level and a single index level, such as the tree shown\nin Figure 10.9, has height 1, and a tree that h&'3 only the root node has\nheight O. Because of high fan-out, the height of a B+ tree is rarely more\nthan 3 or 4.\n\\Ve will study B+ trees in which every node contains Tn entries, where d :::;\nnJ, :::; 2d. The value d is a parameter of the B+ tree, called the order of the\n.._-\n2If the tree is created by IYll.lk..looding (see Section 10.8.2) an existing data set, the sequence set.\ncan be nHlde physically sequential, but this physical ordering is gradually destroyed as new data is\nadded and delet.ed over time.\n\n346\nCHAPTER \\0\ntree, and is a measure of the capacity of a tree node.\nThe root node is the\nonly exception to this requirement on the number of entries; for the root, it is\nsimply required that 1 ::; m ::; 2d.\nIf a file of records is updated frequently and sorted access is important, main-\ntaining a B+ tree index with data records stored as data entries is almost\nalways superior to maintaining a sorted file. For the space overhead of storing\nthe index entries, we obtain all the advantages of a sorted file plus efficient in-\nsertion and deletion algorithms. B+ trees typically maintain 67 percent space\noccupancy. B+ trees are usually also preferable to ISAM indexing because in-\nserts are handled gracefully without overflow chains. However, if the dataset\nsize and distribution remain fairly static, overflow chains may not be a major\nproblem. In this case, two factors favor ISAM: the leaf pages are allocated in\nsequence (making scans over a large range more efficient than in a B+ tree, in\nwhich pages are likely to get out of sequence on disk over time, even if they were\nin sequence after bulk-loading), and the locking overhead ofISAM is lower than\nthat for B+ trees. As a general rule, however, B+ trees are likely to perform\nbetter than ISAM.\n10.3.1\nFormat of a Node\nThe format of a node is the same as for ISAM and is shown in Figure 10.1.\nNon-leaf nodes with m 'index entr'ies contain m+ 1 pointers to children. Pointer\nPi points to a subtree in which all key va.lues K are such that Ki ::; K < K i+1.\nAs special ca\"Jes, Po points to a tree in which all key values are less than Kl'\nand Pm points to a tree in which all key values are greater than or equal to\nK m . For leaf nodes, entries arc denoted a\"J k*, as usual. Just as in ISAM, leaf\nnodes (and only leaf nodes!)\ncontain data entries. In the common ca.'se that\nAlternative (2) or (:3) is used, leaf entries are (K,I(K) ) pairs, just like non-leaf\nentries. Regardless of the alternative chosen for leaf entries, the leaf pages are\nchained together in a doubly linked list.\nThus, the leaves form a sequence,\nwhich can be used to answer range queries efficiently.\nThe reader should carefully consider how such a node organization can be\nachieved using the record formats presented in Section 9.7; after all, each key\npointer pair can be thought of as a record.\nIf the field being indexed is of\nfixed length, these index entries will be of fixed length; otherwise, we have\nvariable-length records. In either case the B+ tree can itself be viewed as a file\nof records. If the leaf pages do not contain the actual data records, then the\n13+ tree is indeed a file of records that is distinct from the file that contains the\ndata. If the leaf pages contain data. records, then a file contains the 13+ tree a...s\nwell as the data.\n\nTTee-St'r~uct'uTed Jude.ring\n10.4\nSEARCH\n347\nThe algorithm for sean:h finds the leaf node in which a given data entry belongs.\nA pseudocode sketch of the algorithm is given in Figure 10.8.\n\"\\Te use the\nnotation *ptT to denote the value pointed to by a pointer variable ptT and &\n(value) to denote the address of val'nc. Note that finding i in tTcc_seaTch requires\nus to search within the node, which can be done with either a linear search or\na binary search (e.g., depending on the number of entries in the node).\nIn discussing the search, insertion, and deletion algorithms for B+ trees, we\nassume that there are no duplicates. That is, no two data entries are allowed\nto have the same key value.\nOf course, duplicates arise whenever the search\nkey does not contain a candidate key and must be dealt with in practice. We\nconsider how duplicates can be handled in Section 10.7.\nfune find (search key value K) returns nodepointer\n/ / Given a seaTch key value, finds its leaf node\nreturn tree_search(root, K);\n/ / searches from root\nendfune\nfune tTee-seaTch (nodepointer, search key value K) returns nodepointer\n/ / Searches tree for entry\nif *nodepointer is a leaf, return nodepointer;\nelse,\nif K < K 1 then return tree_search(Po, K);\nelse,\nif K 2: K m then return tree_search(Pm , K);\n/ /\n171 = # entries\nelse,\nfind i such that K i :::; K < Ki+ 1 ;\nreturn tree_search(Pi , K)\nendfune\nFigure 10.8\nAlgorithm for Search\nConsider the sample B+ tree shown in Figure 10.9. This B+ tree is of order\nd=2. That is, each node contains between 2 and 4 entries. Each non--leaf entry\nis a (key valuc.' nodepointcT) pair; at the leaf level, the entries are data records\nthat we denote by k*.\nTo search for entry 5*, we follow the left-most child\npointer, since 5 < 13. To search for the entries 14* or 15*, we follow the second\npointer, since 1:3 :::; 14 < 17, and 1:3 :::; 15 < 17.\n(vVe do not find 15* on the\nappropriate leaf and can conclude that it is not present in the tree.) To find\n24*, we follow the fourth child pointer, since 24 :::; 24 < :30.\n\n348\nCHAPTER 10\nFigure 10.9\nExample of a B+ Tree, Order d=2\n10.5\nINSERT\nThe algorithm for insertion takes an entry, finds the leaf node where it belongs,\nand inserts it there. Pseudocode for the B+ tree insertion algorithm is given\nin Figure HUG.\nThe basic idea behind the algorithm is that we recursively\ninsert the entry by calling the insert algorithm on the appropriate child node.\nUsually, this procedure results in going down to the leaf node where the entry\nbelongs, placing the entry there, and returning all the way back to the root\nnode. Occasionally a node is full and it must be split. When the node is split,\nan entry pointing to the node created by the split must be inserted into its\nparent; this entry is pointed to by the pointer variable newchildentry. If the\n(old) root is split, a new root node is created and the height of the tree increa..<;es\nby 1.\nTo illustrate insertion, let us continue with the sample tree shown in Figure\n10.9. If we insert entry 8*, it belongs in the left-most leaf, which is already\nfull. This insertion causes a split of the leaf page; the split pages are shown in\nFigure 10.11. The tree must now be adjusted to take the new leaf page into\naccount, so we insert an entry consisting of the pair (5, pointer to new page)\ninto the parent node.\nNote how the key 5, which discriminates between the\nsplit leaf page and its newly created sibling, is 'copied up.'\n\\\\Te cannot just\n'push up' 5, because every data entry must appear in a leaf page.\nSince the parent node is also full, another split occurs. In general we have to\nsplit a non-leaf node when it is full, containing 2d keys and 2d+ 1 pointers, and\nwe have to add another index entry to account for a child split. We now have\n2d+ 1 keys and 2d+2 pointers, yielding two minimally full non-leaf nodes, each\ncontaining d keys and d + 1 pointers, and an extra key, which we choose to be\nthe 'middle' key. This key and a pointer to the second non-leaf node constitute\nan index entry that must be inserted into the parent of the split non-leaf node.\nThe middle key is thus 'pushed up' the tree, in contrast to the case for a split\nof a leaf page.\n\nTree~Structured Inde.1:ing\n349\nproc inseTt (nodepointel', entry, newchildentry)\n/ / InseTts entry into subtree with TOot '*nodepointer'; degree is d;\n/ /'newchildentTy' null initially, and null on retUTn unless child is split\nif *nodepointer is a non-leaf node, say N,\nfind'i such that J(i S entry's key value < J(i+l;\n/ / choose subtree\ninsert(.R;, entry, newchildentry);\n/ / recurs'ively, insert entry\nif newchildentry is null, return;\n/ / usual case; didn't split child\nelse,\n/ / we split child, must insert *newchildentry in N\nif N has space,\n/ / usual case\nput *newchildentry on it, set newchildentry to null, return;\nelse,\n/ / note difference wrt splitting of leaf page!\nsplit N:\n/ / 2d + 1 key values and 2d + 2 nodepointers\nfirst d key values and d + 1 nodepointers stay,\nlast d keys and d + 1 pointers move to new node, N2;\n/ / *newchildentry set to guide searches between Nand N2\nnewchildentry = & ((smallest key value on N2,\npointer to N2));\nif N is the root,\n/ / root node was just split\ncreate new node with (pointer to N, *newchildentry);\nmake the tree's root-node pointer point to the new node;\nreturn;\nif *nodepointer is a leaf node, say L,\nif L has space,\n/ / usual case\nput entry on it, set newchildentry to null, and return;\nelse,\n/ / once in a while, the leaf is full\nsplit L: first d entries stay, rest move to brand new node L2;\nnewchildentry = & ((smallest key value on L2, pointer to L2));\nset sibling pointers in Land L2;\nreturn;\nendproc\nFigure 10.1.0\nAlgorithrn for Insertion into B+ Tree of Order d\n\n350\nCHAPTER jO\n/\n,_ - - Entry to be inserted in parent 11(.)de.\n[i]1\n<-- -\n(Note that 5 is 'copied up' and\n_-....... \"---\\\n,ontin.\", to ,ppcM;n the lenf.)\nEEf-rJ-~r\nFigure 10.11\nSplit Leaf Pages during Insert of Entry 8*\nThe split pages in our example are shown in Figure 10.12. The index entry\npointing to the new non-leaf node is the pair (17, pointer to new index-level\npage); note that the key value 17 is 'pushed up' the tree, in contrast to the\nsplitting key value 5 in the leaf split, which was 'copied up.'\n/\nEntry to be inserted in parent node.\n~\n7\n..£~\n_ :' -\n-\n(Note that 17 is 'pushed up' and\nand appears once In the index. Contrast\nthIS with a leaf spILt.)\n)EffJD HPJ\nFigure 10.12\nSplit Index Pages during Insert of Entry 8*\nThe difference in handling leaf-level and index-level splits arises from the B+\ntree requirement that all data entries h\nmust reside in the leaves.\nThis re-\nquirement prevents us from 'pushing up' 5 and leads to the slight redundancy\nof having some key values appearing in the leaf level as well as in some index\nleveL However, range queries can be efficiently answered by just retrieving the\nsequence of leaf pages; the redundancy is a small price to pay for efficiency. In\ndealing with the index levels, we have more flexibility, and we 'push up' 17 to\navoid having two copies of 17 in the index levels.\nNow, since the split node was the old root, we need to create a new root node\nto hold the entry that distinguishes the two split index pages. The tree after\ncompleting the insertion of the entry 8* is shown in Figure 10.13.\nOne variation of the insert algorithm tries to redistribute entries of a node N\nwith a sibling before splitting the node; this improves average occupancy. The\nsibling of a node N, in this context, is a node that is immediately to the left\nor right of N and has the same pare'nt as N.\n\nTree-Structured Index'ing\n351\nFigure 10.13\nB+ Tree after Inserting Entry 8*\nTo illustrate redistribution, reconsider insertion of entry 8* into the tree shown\nin Figure 10.9. The entry belongs in the left-most leaf, which is full. However,\nthe (only) sibling of this leaf node contains only two entries and can thus\naccommodate more entries. We can therefore handle the insertion of 8* with a\nredistribution. Note how the entry in the parent node that points to the second\nleaf has a new key value; we 'copy up' the new low key value on the second\nleaf. This process is illustrated in Figure 10.14.\nFigure 10.14\nB+ Tree after Inserting Entry 8* Using Redistribution\nTo determine whether redistribution is possible, we have to retrieve the sibling.\nIf the sibling happens to be full, we have to split the node anyway. On average,\nchecking whether redistribution is possible increases I/O for index node splits,\nespecially if we check both siblings. (Checking whether redistribution is possible\nmay reduce I/O if the redistribution succeeds whereas a split propagates up the\ntree, but this case is very infrequent.) If the file is growing, average occupancy\nwill probably not be affected much even if we do not redistribute. Taking these\nconsiderations ,into account, not redistributing entries at non-leaf levels usually\npays off.\nIf a split occurs at the leaf level, however, we have to retrieve a neighbor\nto adjust the previous and next-neighbor pointers with respect to the newly\ncreated leaf node. Therefore, a limited form of redistribution makes sense: If a\nleaf node is full, fetch a neighbor node; if it ha.'3 space and has the same parent,\n\n352\nCHAPTER :40\nredistribute the entries. Othenvise (the neighbor has diflerent parent, Le., it is\nnot a sibling, or it is also full) split the leaf node and a,djust the previous and\nnext-neighbor pointers in the split node, the newly created neighbor, and the\nold neighbor.\n10.6\nDELETE\nThe algorithm for deletion takes an entry, finds the leaf node where it belongs,\nand deletes it.\nPseudocode for the B+ tree deletion algorithm is given in\nFigure 10.15. The basic idea behind the algorithm is that we recursively delete\nthe entry by calling the delete algorithm on the appropriate child node. We\nusually go down to the leaf node where the entry belongs, remove the entry\nfrom there, and return all the way back to the root node.\nOccasionally a\nnode is at minimum occupancy before the deletion, and the deletion causes\nit to go below the occupancy threshold. When this happens, we must either\nredistribute entries from an adjacent sibling or merge the node with a sibling to\nmaintain minimum occupancy. If entries are redistributed between two nodes,\ntheir parent node must be updated to reflect this; the key value in the index\nentry pointing to the second node must be changed to be the lowest search key\nin the second node. If two nodes are merged, their parent must be updated to\nreflect this by deleting the index entry for the second node; this index entry is\npointed to by the pointer variable oldchildentry when the delete call returns to\nthe parent node. If the last entry in the root node is deleted in this manner\nbecause one of its children was deleted, the height of the tree decreases by 1.\nTo illustrate deletion, let us consider the sample tree shown in Figure 10.13. To\ndelete entry 19*, we simply remove it from the leaf page on which it appears,\nand we are done because the leaf still contains two entries. If we subsequently\ndelete 20*, however, the leaf contains only one entry after the deletion. The\n(only) sibling of the leaf node that contained 20* has three entries, and we can\ntherefore deal with the situation by redistribution; we move entry 24* to the\nleaf page that contained 20* and copy up the new splitting key (27, which is\nthe new low key value of the leaf from which we borrowed 24*) into the parent.\nThis process is illustrated in Figure 10.16.\nSuppose that we now delete entry 24*. The affected leaf contains only one entry\n(22*) after the deletion, and the (only) sibling contains just two entries (27*\nand 29*). Therefore, we cannot redistribute entries. However, these two leaf\nnodes together contain only three entries and can be merged. \\Vhile merging,\nwe can 'tos::;' the entry ((27, pointer' to second leaf page)) in the parent, which\npointed to the second leaf page, because the second leaf page is elnpty after the\nmerge and can be discarded. The right subtree of Figure 10.16 after thi::; step\nin the deletion of entry 2!1* is shown in Figure 10.17.\n\nTree-Structured Inde:l:ing\n353\n,\nproc delete (parentpointer, nodepointer, entry, oldchiIdentry)\n/ / Deletes entry from s'ubtree w'ith TOot '*nodepointer '; degree is d;\n/ /\n'oldchildentry' null initially, and null upon ret1lrn unless child deleted\nif *nodepointer is a non-leaf node, say N,\nfind i such that K i ::; entry's key value < K i+l;\n/ / choose subtree\ndelete(nodepointer, Pi, entry, oldchildentry);\n/ / recursive delete\nif oldchildentry is null, return;\n/ / usual case: child not deleted\nelse,\n/ / we discarded child node (see discussion)\nremove *oldchildentry from N,\n/ / next, check for underflow\nif N has entries to spare,\n/ / usual case\nset oldchildentry to null, return; / / delete doesn't go further\nelse,\n/ / note difference wrt merging of leaf pages!\nget a sibling S of N:\n/ / parentpointer arg used to find S\nif S has extra entries,\nredistribute evenly between Nand S through parent;\nset oldchildentry to null, return;\nelse, merge Nand S\n/ / call node on rhs 111\noldchildentry = & (current entry in parent for M);\npull splitting key from parent down into node on left;\nmove all entries from 1\\11 to node on left;\ndiscard empty node M, return;\nif *nodepointer is a leaf node, say L,\nif L h&<; entries to spare,\n/ / usual case\nremove entry, set oldchildentry to null, and return;\nelse,\n/ / once in a while, the leaf becomes underfull\nget a sibling S of L;\n/ / parentpointer used to find S\nif S has extra entries,\nredistribute evenly between Land S;\nfind entry in parent for node on right;\n/ / call it A;J\nreplace key value in parent entry by new low-key value in 1\\11;\nset oldchildentry to null, return;\nelse, merge Land S\n/ / call node on rhs 1\\11\noldchildentry = & (current entry in parent for M);\nmove all entries from 1\\11 to node on left;\ndiscard empty node AI, adjust sibling pointers, return;\nendproc\nFigure 10.15\nAlgorithm for Deletion from B+ Tree of Order r1\n\n354\nCHAPTER 1-0\nFigure 10.16\nB+ Tree after Deleting Entries 19* and 20*\nFigure 10.17\nPartial B+ Tree during Deletion of Entry 24*\nDeleting the entry (27, pointer to second leaf page) has created a non-Ieaf-Ievel\npage with just one entry, which is below the minimum of d = 2. To fix this\nproblem, we must either redistribute or merge. In either case, we must fetch a\nsibling. The only sibling of this node contains just two entries (with key values\n5 and 13), and so redistribution is not possible; we must therefore merge.\nThe situation when we have to merge two non-leaf nodes is exactly the opposite\nof the situation when we have to split a non-leaf node. We have to split a non-\nleaf node when it contains 2d keys and 2d + 1 pointers, and we have to add\nanother key--pointer pair. Since we resort to merging two non-leaf nodes only\nwhen we cannot redistribute entries between them, the two nodes must be\nminimally full; that is, each must contain d keys and d + 1 pointers prior to\nthe deletion. After merging the two nodes and removing the key--pointer pair\nto be deleted, we have 2d - 1 keys and 2d + 1 pointers: Intuitively, the left-\nmost pointer on the second merged node lacks a key value. To see what key\nvalue must be combined with this pointer to create a complete index entry,\nconsider the parent of the two nodes being merged. The index entry pointing\nto one of the merged nodes must be deleted from the parent because the node\nis about to be discarded. The key value in this index entry is precisely the key\nvalue we need to complete the new merged node: The entries in the first node\nbeing merged, followed by the splitting key value that is 'pulled down' from the\nparent, followed by the entries in the second non-leaf node gives us a total of 2d\nkeys and 2d + 1 pointers, which is a full non-leaf node. Note how the splitting\n\nTree-Structured Indexing\n355\nkey value in the parent is pulled down, in contrast to the case of merging two\nleaf nodes.\nConsider the merging of two non-leaf nodes in our example. Together, the non-\nleaf node and the sibling to be merged contain only three entries, and they have\na total of five pointers to leaf nodes. To merge the two nodes, we also need to\npull down the index entry in their parent that currently discriminates between\nthese nodes. This index entry has key value 17, and so we create a new entry\n(17, left-most child pointer in sibling). Now we have a total of four entries and\nfive child pointers, which can fit on one page in a tree of order d = 2. Note that\npulling down the splitting key 17 means that it will no longer appear in the\nparent node following the merge.\nAfter we merge the affected non-leaf node\nand its sibling by putting all the entries on one page and discarding the empty\nsibling page, the new node is the only child of the old root, which can therefore\nbe discarded. The tree after completing all these steps in the deletion of entry\n24* is shown in Figure 10.18.\nFigure 10.18\nB+ Tree after Deleting Entry 24*\nThe previous examples illustrated redistribution of entries across leaves and\nmerging of both leaf-level and non-leaf-level pages. The remaining case is that\nof redistribution of entries between non-leaf-level pages.\nTo understand this\ncase, consider the intermediate right subtree shown in Figure 10.17. We would\narrive at the same intermediate right subtree if we try to delete 24* from a\ntree similar to the one shown in Figure 10.16 but with the left subtree and\nroot key value as shown in Figure 10.19. The tree in Figure 10.19 illustrates\nan intermediate stage during the deletion of 24*. (Try to construct the initial\ntree. )\nIn contrast to the caf.;e when we deleted 24* from the tree of Figure HUG, the\nnon-leaf level node containing key value :30 now ha..s a sibling that can spare\nentries (the entries with key values 17 and 20).\nvVe move these entries3 over\nfrom the sibling. Note that, in doing so, we essentially push them through the\n:11t is sufficient to move over just the entry with key value 20, hut we are moving over two entries\n°0 illustrate what happens when several entries are redistributed.\n\n356\nCHAPTER ;'0\nFigure 10.19\nA B+ Tree during a Deletion\nsplitting entry in their parent node (the root), which takes care of the fact that\n17 becomes the new low key value on the right and therefore must replace the\nold splitting key in the root (the key value 22). The tree with all these changes\nis shown in Figure 10.20.\nFigure 10.20\nB+ Tree after Deletion\nIn concluding our discussion of deletion, we note that we retrieve only one\nsibling of a node. If this node has spare entries, we use redistribution; otherwise,\nwe merge. If the node has a second sibling, it may be worth retrieving that\nsibling as well to check for the possibility of redistribution. Chances are high\nthat redistribution is possible, and unlike merging, redistribution is guaranteed\nto propagate no further than the parent node.\nAlso, the pages have more\nspace on them, which reduces the likelihood of a split on subsequent insertions.\n(Remember, files typically grow, not shrink!)\nHowever, the number of times\nthat this case arises (the node becomes less than half-full and the first sibling\ncannot spare an entry) is not very high, so it is not essential to implement this\nrefinement of the bct.'3ic algorithm that we presented.\n10.7\nDUPLICATES\nThe search, insertion, and deletion algorithms that we presented ignore the\nissue of duplicate keys, that is, several data entries with the same key value.\nvVe now discuss how duplica.tes can be handled.\n\nTree-StT'llct'uTed Inde:ring\n35~\nDuplicate Handling in COlllmercial Systems: In a clustered index in\nSybase ASE, the data rows are maintained in sorted order onthe page and\nin the eollection of data pages. The data pages are bidireetionally linked\nin sort order. Rows with duplicate keys are inserted into (or deleted from}\nthe ordered set of rows. This may result in overflow pages of rows with\nduplieate keys being inserted into the page chain or empty overflow pages\nremoved from the page chain. Insertion or deletion of a duplicate key does\nnot affect the higher index level'> unless a split or. lIlergy ofa .non.-overflow\npage occurs. In IBM DB2, Oracle 8, and Miero§oft'SQL'Server; dupliclltes\nare handled by adding a row id if necessary to eliminate duplicate key\nvalues.\n.\nThe basic search algorithm assumes that all entries with a given key value reside\non a single leaf page.\nOne way to satisfy this assumption is to use overflow\npages to deal with duplicates. (In ISAM, of course, we have overflow pages in\nany case, and duplicates are easily handled.)\nTypically, however, we use an alternative approach for duplicates. We handle\nthem just like any other entries and several leaf pages may contain entries with\na given key value. To retrieve all data entries with a given key value, we must\nsearch for the left-most data entry with the given key value and then possibly\nretrieve more than one leaf page (using the leaf sequence pointers). Modifying\nthe search algorithm to find the left-most data entry in an index with duplicates\nis an interesting exercise (in fact, it is Exercise 10.11).\nOne problem with this approach is that, when a record is deleted, if we use\nAlternative (2) for data entries, finding the corresponding data entry to delete\nin the B+ tree index could be inefficient because we may have to check several\nduplicate entries (key, rid) with the same key value.\nThis problem can be\naddressed by considering the rid value in the data entry to be part of the\nsearch key, for purposes of positioning the data entry in the tree. This solution\neffectively turns the index into a uniq71,e index (i.e\" no duplicates), Remember\nthat a search key can be any sequence of fields\nin this variant, the rid of the\ndata record is essentially treated as another field while constructing the search\nkey.\nAlternative (3) f'or data entries leads to a natural solution for duplicates, but if\nwe have a large number of duplicates, a single data entry could span multiple\npages. And of course, when a data record is deleted, finding the rid to delete\nfrom the corresponding data entry can be inefficient,\nThe solution to this\nproblem is similar to the one discussed previously for Alternative (2): vVe can\n\n358\nCHAPTER 10\nmaintain the list of rids within each data entry in sorted order (say, by page\nnumber and then slot number if a rid consists of a page id and a slot id).\n10.8\nB+ TREES IN PRACTICE\nIn this section we discuss several important pragmatic issues.\n10.8.1\nKey Compression\nThe height of a B+ tree depends on the number of data entries and the size of\nindex entries. The size of index entries determines the number of index entries\nthat will fit on a page and, therefore, the fan-out of the tree. Since the height\nof the tree is proportional to logfan-oud# of data entries), and the number of\ndisk l/Os to retrieve a data entry is equal to the height (unless some pages are\nfound in the buffer pool), it is clearly important to maximize the fan-out to\nminimize the height.\nAn index entry contains a search key value and a page pointer.\nHence the\nsize depends primarily on the size of the search key value.\nIf search key\nvalues are very long (for instance, the name Devarakonda Venkataramana\nSathyanarayana Seshasayee Yellamanchali Murthy, or Donaudampfschifffahrts-\nkapitansanwiirtersmiitze), not many index entries will fit on a page: Fan-out is\nlow, and the height of the tree is large.\nOn the other hand, search key values in index entries are used only to direct\ntraffic to the appropriate leaf.\nWhen we want to locate data entries with a\ngiven search key value, we compare this search key value with the search key\nvalues of index entries (on a path from the root to the desired leaf). During\nthe comparison at an index-level node, we want to identify two index entries\nwith search key values kl and k2 such that the desired search key value k falls\nbetween k1 and k2. To accomplish this, we need not store search key values in\ntheir entirety in index entries.\nFor example, suppose we have two adjacent index entries in a node, with search\nkey values 'David Smith' and 'Devarakonda ... ' To discriminate between these\ntwo values, it is sufficient to store the abbreviated forms 'Da' and 'De.' More\ngenerally, the lneaning of the entry 'David Smith' in the B+ tree is that every\nvalue in the subtree pointed to by the pointer to the left of 'David Smith' is less\nthan 'David Smith,' and every value in the subtree pointed to by the pointer\nto the right of 'David Smith' is (greater than or equal to 'David Smith' and)\nless than 'Devarakonda ... '\n\nTree-Struct'ured Indexing\n359\nB+ Trees in Real Systems: IBM DB2, Infol:mLx, Microsoft SQL Server,\nOracle 8, and Sybase ASE all support clustered ~d unclustered B+ tree\nindexes, with some differencesin how they handle deletions and duplicate\nkey values. In Sybase ASE, depending on the concurrency control schelne\nbeing used for· the index, the deleted row is removed (with merging if\nthe page occupancy goes below threshold) or simply 111arkedas deleted; a\ngarbage collection scheme is used to recover space . in th~ latter case,\nIn\nOracle 8, deletions are handled by marking the row as deleted. 1'0 reclaim\nthe space occupied by deleted records, we can rebuild the index online (i.e.,\nwhile users continue to use the index) or coalesce underfull pages (which\ndoes not reduce tree height). Coalesce is in-place, rebuild creates a copy.\nInformix handles deletions by simply marking records as deleted. DB2 and\nSQL Server remove deleted records and merge pages when occupancy goes\nbelow threshold.\nOracle 8 also allows records from multiple relations to be co-clustered on\nthe same page. The co-clustering can be based on a B+ tree search key or\nstatic hashing and up to 32 relations can be stored together.\nTo ensure such semantics for an entry is preserved, while compressing the entry\nwith key 'David Smith,' we must examine the largest key value in the subtree to\nthe left of 'David Smith' and the smallest key value in the subtree to the right\nof 'David Smith,' not just the index entries ('Daniel Lee' and 'Devarakonda\n... ') that are its neighbors. This point is illustrated in Figure 10.21; the value\n'Davey Jones' is greater than 'Dav,' and thus, 'David Smith' can be abbreviated\nonly to 'Davi,' not to 'Dav.'\n000\nDevarakonda ...\n000\no\n0\n0\nFigure 10.21\nExample Illustrating Prefix Key Compression\n000\nThis technique.\ncalled prefix key compression or simply key compres-\nsion, is supported in many commercial implementations of B+ trees. It can\nsubstantially increCL')e the fan-out of a tree. We do not discuss the details of\nthe insertion and deletion algorithms in the presence of key compression.\n\n360\nCHAPTER 10\n10.8.2\nBulk-Loading a B+ Tree\nEntries are added to a B+ tree in two ways. First, we may have an existing\ncollection of data records with a B+ tree index on it; whenever a record is\nadded to the collection, a corresponding entry must be added to the B+ tree\nas well. (Of course, a similar comment applies to deletions.) Second, we may\nhave a collection of data records for which we want to create a B+ tree index\non some key field(s). In this situation, we can start with an empty tree and\ninsert an entry for each data record, one at a time, using the standard insertion\nalgorithm. However, this approach is likely to be quite expensive because each\nentry requires us to start from the root and go down to the appropriate leaf\npage. Even though the index-level pages are likely to stay in the buffer pool\nbetween successive requests, the overhead is still considerable.\nFor this reason many systems provide a bulk-loading utility for creating a B+\ntree index on an existing collection of data records. The first step is to sort\nthe data entries k* to be inserted into the (to be created) B+ tree according to\nthe search key k. (If the entries are key-pointer pairs, sorting them does not\nmean sorting the data records that are pointed to, of course.) We use a running\nexample to illustrate the bulk-loading algorithm. We assume that each data\npage can hold only two entries, and that each index page can hold two entries\nand an additional pointer (i.e., the B+ tree is assumed to be of order d = 1).\nAfter the data entries have been sorted, we allocate an empty page to serve as\nthe root and insert a pointer to the first page of (sorted) entries into it. We\nillustrate this process in Figure 10.22, using a sample set of nine sorted pages\nof data entries.\n~!--==_~~~~\"\",L:-=o-s_c_,:_.te_d_p~a~g_e_s_()f_(_la_t_a_e_nt_rie_s~n~ot_Y_\"_il_l_B_+_t_re_e~~_----,\nffi EEJ 110*~~ 112j~\n120*[221\nFigure 10.22\nInitial Step in B+ Tree Bulk-Loading\nvVe then add one entry to the root page for each page of the sorted data entries.\nThe new entry consists of \\low key value on page, pointer' to page). vVe proceed\nuntil the root page is full; see Figure 10.23.\nTo insert the entry for the next page of data entries, we must split the root and\ncreate a new root page. vVe show this step in Figure 10.2/1.\n\nTr'ee-8iruci'ured Index'ing\nData entry pages not yet in B+ tree\nFigure 10.23\nRoot Page Fills up in B+ Tree Bulk-Loading\nData entry pages Ilot yet ill B+ tree\nFigure 10.24\nPage Split during B+ 'fi'ee Bulk-Loading\n361\n\n362\nCHAPTER :FO\n\"We have redistributed the entries evenly between the two children of the root,\nin anticipation of the fact that the B+ tree is likely to grow. Although it is\ndifficult (!) to illustrate these options when at most two entries fit on a page,\nwe could also have just left all the entries on the old page or filled up some\ndesired fraction of that page (say, 80 percent). These alternatives are simple\nvariants of the basic idea.\nTo continue with the bulk-loading example, entries for the leaf pages are always\ninserted into the right-most index page just above the leaf level. 'When the right-\nmost index page above the leaf level fills up, it is split. This action may cause\na split of the right-most index page one step closer to the root, as illustrated\nin Figures 10.25 and 10.26.\nData entry pages\nnot yet in B+ tree\nFigure 10.25\nBefore Adding Entry for Leaf Page Containing 38*\nData entry pages\nnot yet in B+ tree\nI\nI\nI\nITIf IT,fT ?'\nr---.----'i\n12113{j2'122:J123*EJ ~5*136*~ '141*1!f'*1 ]\nFigure 10.26\nAfter Adding Entry for Leaf Page Containing :38*\n\nTiee-Structured Inde:ring\n36,3\nNote that splits occur only on the right-most path from the root to the leaf\nlevel. \\Ve leave the completion of the bulk-loading example as a simple exercise.\nLet us consider the cost of creating an index on an existing collection of records.\nThis operation consists of three steps: (1) creating the data entries to insert\nin the index, (2) sorting the data entries, and (3) building the index from the\nsorted entries. The first step involves scanning the records and writing out the\ncorresponding data entries; the cost is (R + E) I/Os, where R is the number of\npages containing records and E is the number of pages containing data entries.\nSorting is discussed in Chapter 13; you will see that the index entries can be\ngenerated in sorted order at a cost of about 3E I/Os. These entries can then be\ninserted into the index as they are generated, using the bulk-loading algorithm\ndiscussed in this section. The cost of the third step, that is, inserting the entries\ninto the index, is then just the cost of writing out all index pages.\n10.8.3\nThe Order Concept\nWe presented B+ trees using the parameter d to denote minimum occupancy. It\nis worth noting that the concept of order (i.e., the parameter d), while useful for\nteaching B+ tree concepts, must usually be relaxed in practice and replaced\nby a physical space criterion; for example, that nodes must be kept at lea..c;t\nhalf-full.\nOne reason for this is that leaf nodes and non-leaf nodes can usually hold\ndifferent numbers of entries.\nRecall that B+ tree nodes are disk pages and\nnon-leaf nodes contain only search keys and node pointers, while leaf nodes can\ncontain the actual data records. Obviously, the size of a data record is likely\nto be quite a bit larger than the size of a search entry, so many more search\nentries than records fit on a disk page.\nA second reason for relaxing the order concept is that the search key may\ncontain a character string field (e.g., the name field of Students) whose size\nvaries from record to record; such a search key leads to variable-size data entries\nand index entries, and the number of entries that will fit on a disk page becomes\nvariable.\nFinally, even i{ the index is built on a fixed-size field, several records may still\nhave the same search key value (e.g., several Students records may have the\nsame gpa or name value). This situation can also lead to variable-size leaf entries\n(if we use Alternative (3) for data entries). Because of all these complications,\nthe concept of order is typically replaced by a simple physical criterion (e.g.,\nmerge if possible when more than half of the space in the node is unused).\n\n364\nCHAPTER 1()\n10.8.4\nThe Effect of Inserts and Deletes on Rids\nIf the leaf pages contain data records-that is, the B+ tree is a clustered index-\nthen operations such as splits, merges, and redistributions can change rids.\nRecall that a typical representation for a rid is some combination of (physical)\npage number and slot number. This scheme allows us to move records within\na page if an appropriate page format is chosen but not across pages, as is the\ncase with operations such as splits. So unless rids are chosen to be independent\nof page numbers, an operation such as split or merge in a clustered B+ tree\nmay require compensating updates to other indexes on the same data.\nA similar comment holds for any dynamic clustered index, regardless of whether\nit is tree-based or hash-based.\nOf course, the problem does not arise with\nnonclustered indexes, because only index entries are moved around.\n10.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhy are tree-structured indexes good for searches, especially range selec-\ntions? (Section 10.1)\n•\nDescribe how search, insert, and delete operations work in ISAM indexes.\nDiscuss the need for overflow pages, and their potential impact on perfor-\nmance. What kinds of update workloads are ISAM indexes most vulnerable\nto, and what kinds of workloads do they handle well? (Section 10.2)\n•\nOnly leaf pages are affected in updates in ISAM indexes.\nDiscuss the\nimplications for locking and concurrent access.\nCompare ISAM and B+\ntrees in this regard. (Section 10.2.1)\n•\nWhat are the main differences between ISAM and B+ tree indexes? (Sec-\ntion 10.3)\n•\nWhat is the order of a B+ tree?\nDescribe the format of nodes in a B+\ntree. Why are nodes at the leaf level linked? (Section 10.3)\n•\nHow rnany nodes must be examined for equality search in a B+ tree? How\nmany for a range selection? Compare this with ISAM. (Section 10.4)\n•\nDescribe the B+ tree insertion algorithm, and explain how it eliminates\noverflow pages. Under what conditions can an insert increase the height of\nthe tree? (Section 10.5)\n•\nDuring deletion, a node might go below the minimum occupancy threshold.\nHow is this handled? Under what conditions could a deletion decrease the\nheight of the tree? (Section 10.6)\n\nTree-Str\"l.tct1lTcd Indccing\nFigure 10.27\nTree for Exercise 10.1\n•\nWhy do duplicate search keys require modifications to the implementation\nof the basic B+ tree operations? (Section 10.7)\n•\n\\Vhat is key compression, and why is it important? (Section 10.8.1)\n•\nHow can a new B+ tree index be efficiently constructed for a set of records?\nDescribe the bulk-loading algorithm. (Section 10.8.2)\n•\nDiscuss the impact of splits in clustered B+ tree indexes. (Section 10.8.4)\nEXERCISES\nExercise 10.1 Consider the B+ tree index of order d = 2 shown in Figure 10.27.\n1. Show the tree that would result from inserting a data entry with key 9 into this tree.\n2. Show the B+ tree that would result from inserting a data entry with key 3 into the\noriginal tree. How many page reads and page writes does the insertion require?\n:3. Show the B+ tree that would result from deleting the data entry with key 8 from the\noriginal tree, assuming that the left sibling is checked for possible redistribution.\n4. Show the B+ tree that would result from deleting the data entry with key 8 from the\noriginal tree, assuming that the right sibling is checked for possible redistribution.\n5. Show the B+ tree that would result from starting with the original tree, inserting a data\nentry with key 46 and then deleting the data entry with key 52.\n6. Show the B+ tree that would result from deleting the data entry with key 91 from the\noriginal tree.\n7. Show the B+ tree that would result from starting with the original tree, inserting a data\nentry with key 59, and then deleting the data entry with key 91.\n8. Show the B+ tree that \\vould result from successively deleting the data entries with keys\n32, 39, 41, 45, and 73 from the original tree.\nExercise 10.2 Consider the B+ tree index shown in Figure 10.28, which uses Alternative\n(1) for data entries. Each intermediate node can hold up to five pointers and four key values.\nEach leaf can hold up to four records, and leaf nodes are doubly linked as usual, although\nthese links are not shown in the figure. Answer the following questions.\n1. Name all the tree nodes that mllst be fetched to answer the following query: \"Get all\nrecords with search key greater than :38.\"\n\n366\nCHAPTER 10\nL3\nL6\nFigure 10.28\nTree for Exercise 10.2\n2. Insert a record with search key 109 into the tree.\n3. Delete the record with search key 81 from the (original) tree.\n4. Name a search key value such that inserting it into the (original) tree would cause an\nincrease in the height of the tree.\n5. Note that subtrees A, B, and C are not fully specified. Nonetheless, what can you infer\nabout the contents and the shape of these trees?\n6. How would your answers to the preceding questions change if this were an ISAM index?\n7. Suppose that this is an ISAM index. What is the minimum number of insertions needed\nto create a chain of three overflow pages?\nExercise 10.3 Answer the following questions:\n1. What is the minimum space utilization for a B+ tree index?\n2. What is the minimum space utilization for an ISAM index?\n3. If your database system supported both a static and a dynamic tree index (say, ISAM and\nB+ trees), would you ever consider using the static index in preference to the dynamic\nindex?\nExercise 10.4 Suppose that a page can contain at most four data values and that aU data\nvalues are integers. Using only B+ trees of order 2, give examples of each of the following:\n1. A B+ tree whose height changes from 2 to 3 when the value 25 is inserted. Show your\nstructure before and after the insertion.\n2. A B+ tree in which the deletion of the value 25 leads to a redistribution.\nShow your\nstructure before and aft.er the deletion.\n3. A B+ tree in which t.he delet.ion of the value 25 causes a merge of two nodes but without.\naltering the height of the tree.\n4. An ISAM structure with four buckets, none of which has an overflow page.\nFurther,\nevery bucket has space for exactly one more entry. Show your structure before and aft.er\ninserting t.wo additional values, chosen so that. an overflow page is created.\n\nTree-Structured Index'ing\n367\nFigure 10.29\nTree for Exercise 10.5\nExercise 10.5 Consider the B+ tree shown in Figure 10.29.\n1. Identify a list of five data entries such that:\n(a) Inserting the entries in the order shown and then deleting them in the opposite\norder (e.g., insert a, insert b, delete b, delete a) results in the original tree.\n(b) Inserting the entries in the order shown and then deleting them in the opposite\norder (e.g., insert a, insert b, delete b, delete a) results in a different tree.\n2. What is the minimum number of insertions of data entries with distinct keys that will\ncause the height of the (original) tree to change from its current value (of 1) to 3?\n3. Would the minimum number of insertions that will cause the original tree to increase to\nheight 3 change if you were allowed to insert duplicates (multiple data entries with the\nsame key), assuming that overflow pages are not used for handling duplicates?\nExercise 10.6 Answer Exercise 10.5 assuming that the tree is an ISAM tree! (Some of the\nexamples asked for may not exist-if so, explain briefly.)\nExercise 10.7 Suppose that you have a sorted file and want to construct a dense primary\nB+ tree index on this file.\n1. One way to accomplish this task is to scan the file, record by record, inserting each\none using the B+ tree insertion procedure. What performance and storage utilization\nproblems are there with this approach?\n2. Explain how the bulk-loading algorithm described in the text improves upon this scheme.\nExercise 10.8 Assume that you have just built a dense B+ tree index using Alternative (2)\non a heap file containing 20,000 records.\nThe key field for this B+ tree index is a 40-byte\nstring, and it is a candidate key. Pointers (Le., record ids and page ids) are (at most) 10-\nbyte values. The size of one disk page is 1000 bytes. The index wa9 built in a bottom-up\nfashion using the bulk-loading algorithm, and the nodes at each level were filled up a..9 much\nas possible.\n1. Ho\\v many levels does the resulting tree have?\n2. For each level of the trec, how many nodes are at that level?\n3. How many levels would the resulting tree have if key compression is llsed and it reduces\nthe average size of each key in an entry to 10 bytes?\n\n368\nsid\nname\nlogin\nage\ngpa\nCHAPTER to\n53831\nMaclayall\nmaclayan@music\n11\n1.8\n53832\nGuldu\nguldu@music\n12\n3.8\n53666\nJones\njones(gcs\n18\n3.4\n53901\nJones\njones({'!Jtoy\n18\n3A\n53902\nJones\njones@physics\n18\n3.4\n53903\nJones\njones(Q)english\n18\n3.4\n53904\nJones\njones(ggenetics\n18\n3.4\n53905\nJones\njones@astro\n18\n3.4\n53906\nJones\njones@chem\n18\n3.4\n53902\nJones\njones(Qlsanitation\n18\n3.8\n53688\nSmith\nsmith@ee\n19\n3.2\n53650\nSmith\nsmith@math\n19\n3.8\n54001\nSmith\nsmith@ee\n19\n3.5\n54005\nSmith\nsmith@cs\n19\n3.8\n54009\nSmith\nsmith@a.'3tro\n19\n2.2\nFigure 10.30\nAn Instance of the Students Relation\n4. How many levels would the resulting tree have without key compression but with all\npages 70 percent full?\nExercise 10.9 The algorithms for insertion and deletion into a B+ tree are presented as\nrecursive algorithms. In the code for inseTt, for instance, a call is made at the parent of a\nnode N to insert into (the subtree rooted at) node N, and when this call returns, the current\nnode is the parent of N.\nThus, we do not maintain any 'parent pointers' in nodes of B+\ntree. Such pointers are not part of the B+ tree structure for a good reason, as this exercise\ndemonstrates. An alternative approach that uses parent pointers--again, remember that such\npointers are not part of the standard B+ tree structure!-in each node appears to be simpler:\nSearch to the appropriate leaf using the search algorithm; then insert the entry and\nsplit if necessary, with splits propagated to parents if necessary (using the parent\npointers to find the parents).\nConsider this (unsatisfactory) <dternative approach:\nI. Suppose that an internal node N is split into nodes Nand N2. What can you say about\nthe parent pointers in the children of the original node N?\n2. Suggest two \\\\rays of dealing with the inconsistent parent pointers in the children of node\nN.\n3. For each of these suggestions, identify a potential (major) disadvantage.\n4. \\Vhat conclusions can you draw from this exercise?\nExercise 10.10 Consider the instance of the Students relation shown in Figure 10.30. Show\na B+ tree of order 2 in each of these cases, assuming that duplicates are handled using overflow\npages. Clearly indicate what the data entries are (i.e., do not use the k* convention).\n\nTree-StIlJ,ct7.1red Indexing\n369\n1. A B+ tree index on age using Alternative (1) for data entries.\n2. A dense B+ tree index on gpa using Alternative (2) for data entries. For this question,\nassume that these tuples are stored in a sorted file in the order shown in the figure: The\nfirst tuple is in page 1, slot 1; the second tuple is in page 1, slot 2; and so on. Each page\ncan store up to three data records. You can use (page-id, slot) to identify a tuple.\nExercise 10.11 Suppose that duplicates are handled using the approach without overflow\npages discussed in Section 10.7. Describe an algorithm to search for the left-most occurrence\nof a data entry with search key value K.\nExercise 10.12 Answer Exercise 10.10 assuming that duplicates are handled without using\noverflow pages, using the alternative approach suggested in Section 9.7.\nPROJECT-BASED EXERCISES\nExercise 10.13 Compare the public interfaces for heap files, B+ tree indexes, and linear\nhashed indexes. What are the similarities and differences? Explain why these similarities and\ndifferences exist.\nExercise 10.14 This exercise involves using Minibase to explore the earlier (non-project)\nexercises further.\n1. Create the trees shown in earlier exercises and visualize them using the B+ tree visualizer\nin Minibase.\n2. Verify your answers to exercises that require insertion and deletion of data entries by\ndoing the insertions and deletions in Minibase and looking at the resulting trees using\nthe visualizer.\nExercise 10.15 (Note to instructors: Additional details must be pT'Ovided if this cxer'Cise is\nassigned; see Appendix 30.) Implement B+ trees on top of the lower-level code in Minibase.\nBIBLIOGRAPHIC NOTES\nThe original version of the B+ tree was presented by Bayer and McCreight [69].\nThe B+\ntree is described in [442] and [194]. B tree indexes for skewed data distributions are studied\nin [260].\nThe VSAM indexing structure is described in [764].\nVarious tree structures for\nsupporting range queries are surveyed in [79]. An early paper on multiattribute search keys\nis [498].\nReferences for concurrent access to B+ trees are in the bibliography for Chapter 17.\n\n11\nHASH-BASED INDEXING\n...\nWhat is the intuition behind hash-structured indexes? Why are they\nespecially good for equality searches but useless for range selections?\n...\nWhat is Extendible Hashing? How does it handle search, insert, and\ndelete?\n...\nWhat is Linear Hashing?\nHow does it handle search, insert, and\ndelete?\n...\nWhat are the similarities and differences between Extendible and Lin-\near Hashing?\nItt\nKey concepts: hash function, bucket, primary and overflow pages,\nstatic versus dynamic hash indexes; Extendible Hashing, directory of\nbuckets, splitting a bucket, global and local depth, directory doubling,\ncollisions and overflow pages; Linear Hashing, rounds ofsplitting, fam-\nily of hash functions, overflow pages, choice of bucket to split and time\nto split; relationship between Extendible Hashing's directory and Lin-\near Hashing's family of hash functiolis, need for overflow pages in both\nschemes in practice, use of a directory for Linear Hashing.\nL.~~_~__\nNot chaos-like, together crushed and bruised,\nBut, as the wo~ld harmoniously confused:\nWhere order in variety we see.\n___ J\nAlexander Pope, Windsor Forest\nIn this chapter we consider file organizations that are excellent for equality\nselections.\nThe basic idea is to use a hashing function, which maps values\n370\n\nHash-Based Indexing\n371\nin a search field into a range of b'ucket numbers to find the page on which a\ndesired data entry belongs. \\Ve use a simple scheme called Static Hashing to\nintroduce the idea. This scheme, like ISAM, suffers from the problem of long\noverflow chains, which can affect performance. Two solutions to the problem\nare presented.\nThe Extendible Hashing scheme uses a directory to support\ninserts and deletes efficiently with no overflow pages.\nThe Linear Hashing\nscheme uses a clever policy for creating new buckets and supports inserts and\ndeletes efficiently without the use of a directory. Although overflow pages are\nused, the length of overflow chains is rarely more than two.\nHash-based indexing techniques cannot support range searches, unfortunately.\nn'ee-based indexing techniques, discussed in Chapter 10, can support range\nsearches efficiently and are almost as good as ha...,h-based indexing for equality\nselections. Thus, many commercial systems choose to support only tree-based\nindexes.\nNonetheless, hashing techniques prove to be very useful in imple-\nmenting relational operations such as joins, as we will see in Chapter 14. In\nparticular, the Index Nested Loops join method generates many equality se-\nlection queries, and the difference in cost between a hash-based index and a\ntree-based index can become significant in this context.\nThe rest of this chapter is organized as follows.\nSection 11.1 presents Static\nHashing. Like ISAM, its drawback is that performance degrades as the data\ngrows and shrinks. We discuss a dynamic hashing technique, called Extendible\nHashing, in Section 11.2 and another dynamic technique, called Linear Hashing,\nin Section 11.3. vVe compare Extendible and Linear Hashing in Section 11.4.\n11.1\nSTATIC HASHING\nThe Static Hashing scheme is illustrated in Figure 11.1. The pages containing\nthe data can be viewed as a collection of buckets, with one primary page\nand possibly additional overflow pages per bucket.\nA file consists of buckets\na through N - 1, with one primary page per bucket initially. Buckets contain\ndata entTies, which can be any of the three alternatives discussed in Chapter\n8.\nTo search for a data entry, we apply a hash function h to identify the bucket\nto which it belongs and then search this bucket.\nTo speed the search of a\nbucket, we can maintain data entries in sorted order by search key value; in\nthis chapter, we do not sort entries, and the order of entries within a bucket\nhas no significance. To insert a data entry, we use the hash function to identify\nthe correct bucket and then put the data entry there. If there is no space for\nthis data entry, we allocate a new overflow page, put the data entry on this\npage, and add the page to the overflow chain of the bucket. To delete a data\n\n372\nh(key) mod N /\n~-~-§=l~\n// __\n~L~--.-J~· . -\n~G\\---I\n...\n,INd-\nPrimary bucket pages\nOverflow pages\nFigure 11.1\nStatic Hashing\nCHAPTER 11\n$\nentry, we use the hashing function to identify the correct bucket, locate the\ndata entry by searching the bucket, and then remove it. If this data entry is\nthe last in an overflow page, the overflow page is removed from the overflow\nchain of the bucket and added to a list of free pages.\nThe hash function is an important component of the hashing approach. It must\ndistribute values in the domain of the search field uniformly over the collection\nof buckets. If we have N buckets, numbered a through N ~ 1, a hash function\nh of the form h(value)\n=\n(a * value + b) works well in practice. (The bucket\nidentified is h(value) mod N.) The constants a and b can be chosen to 'tune'\nthe hash function.\nSince the number of buckets in a Static Hashing file is known when the file\nis created, the primary pages can be stored on successive disk pages. Hence,\na search ideally requires just one disk I/O, and insert and delete operations\nrequire two I/Os (read and write the page), although the cost could be higher\nin the presence of overflow pages. As the file grows, long overflow chains can\ndevelop. Since searching a bucket requires us to search (in general) all pages\nin its overflow chain, it is easy to see how performance can deteriorate.\nBy\ninitially keeping pages 80 percent full, we can avoid overflow pages if the file\ndoes not grow too IIluch, but in general the only way to get rid of overflow\nchains is to create a new file with more buckets.\nThe main problem with Static Hashing is that the number of buckets is fixed.\nIf a file shrinks greatly, a lot of space is wasted; more important, if a file grows\na lot, long overflow chains develop, resulting in poor performance. Therefore,\nStatic Hashing can be compared to the ISAM structure (Section 10.2), which\ncan also develop long overflow chains in case of insertions to the same leaf.\nStatic Hashing also has the same advantages as ISAM with respect to concur-\nrent access (see Section 10.2.1).\n\nHash-Based Inrle:ring\n373\nOne simple alternative to Static Hashing is to periodically 'rehash' the file to\nrestore the ideal situation (no overflow chains, about 80 percent occupancy).\nHowever, rehashing takes time and the index cannot be used while rehashing\nis in progress.\nAnother alternative is to use dynamic hashing techniques\nsuch as Extendible and Linear Hashing, which deal with inserts and deletes\ngracefully. vVe consider these techniques in the rest of this chapter.\n11.1.1\nNotation and Conventions\nIn the rest of this chapter, we use the following conventions. As in the previous\nchapter, record with search key k, we denote the index data entry by k*. For\nhash-based indexes, the first step in searching for, inserting, or deleting a data\nentry with search key k is to apply a hash function h to k; we denote this\noperation by h(k), and the value h(k) identifies the bucket for the data entry\nh. Note that two different search keys can have the same hash value.\n11.2\nEXTENDIBLE HASHING\nTo understand Extendible Hashing, let us begin by considering a Static Hashing\nfile. If we have to insert a new data entry into a full bucket, we need to add\nan overflow page. If we do not want to add overflow pages, one solution is\nto reorganize the file at this point by doubling the number of buckets and\nredistributing the entries across the new set of buckets. This solution suffers\nfrom one major defect--the entire file has to be read, and twice (h') many pages\nhave to be written to achieve the reorganization. This problem, however, can\nbe overcome by a simple idea: Use a directory of pointers to bucket.s, and\ndouble t.he size of the number of buckets by doubling just the directory and\nsplitting only the bucket that overflowed.\nTo understand the idea, consider the sample file shown in Figure 11.2. The\ndirectory consists of an array of size 4, with each element being a point.er to\na bucket.. (The global depth and local depth fields are discussed shortly, ignore\nthem for now.) To locat.e a data entry, we apply a hash funct.ion to the search\nfield and take the last. 2 bit.s of its binary represent.ation t.o get. a number\nbetween 0 and\n~~. The pointer in this array position gives us t.he desired bucket.;\nwe assume that each bucket can hold four data ent.ries. Therefore, t.o locate a\ndata entry with hash value 5 (binary 101), we look at directory element 01 and\nfollow the pointer to the data page (bucket B in the figure).\nTo insert. a dat.a entry, we search to find the appropriate bucket.. For example,\nto insert a data entry with hash value 13 (denoted as 13*), we examine directory\nelement 01 and go to the page containing data ent.ries 1*, 5*, and 21*. Since\n\n374\nCHAPTER 11\nBucketC\nBucket B\nBucket A\n1f L~--L----i----4.,J.--'\n~ Data entry r\nwith h(r)=32\n00\n01\n10\n11\nLOCAL DEPTH~\nGLOBAL DEPTH\nDIRECTORY\nBucket D\nDATA PAGES\nFigure 11.2\nExample of an Extendible Ha.~hed File\nthe page has space for an additional data entry, we are done after we insert the\nentry (Figure 11.3).\nLOCAL DEPTH~\nGLOBAL DEPTH\n00\n01\n10\n11\nDIRECTORY\nBucket A\nBucket B\nBucket C\nBucketD\nDATA PAGES\nFigure 11.3\nAfter Inserting Entry T with h(T) = 1:3\nNext, let us consider insertion of a data entry into a full bucket. The essence\nof the Extcndible Hashing idea lies in how we deal with this case. Consider the\ninsertion of data entry 20* (binary 10100). Looking at directory clement 00,\nwe arc led to bucket A, which is already full. We 111Ust first split the bucket\n\nHash-Based Indexing\n375\nby allocating a new bucketl and redistributing the contents (including the new\nentry to be inserted) across the old bucket and its 'split image.' To redistribute\nentries across the old bucket and its split image, we consider the last three bits\nof h(T); the last two bits are 00, indicating a data entry that belongs to one of\nthese two buckets, and the third bit discriminates between these buckets. The\nredistribution of entries is illustrated in Figure 11.4.\nLOCAL DEPTH~:>\nGLOBAL DEPTH\n00\n01\n10\n11\nDIRECTORY\nBucket D\nBucket A2 (split image of bucket A)\nFigure 11.4\nWhile Inserting Entry r with h(r}=20\nNote a problem that we must now resolve\"\"\" \"\"we need three bits to discriminate\nbetween two of our data pages (A and A2), but the directory has only enough\nslots to store all two-bit patterns. The solution is to double the directory. El-\nements that differ only in the third bit from the end are said to 'correspond':\nCOT-r'esponding elements of the directory point to the same bucket with the\nexception of the elements corresponding to the split bucket. In our example,\nbucket awas split; so, new directory element 000 points to one of the split ver-\nsions and new element 100 points to the other. The sample file after completing\nall steps in the insertion of 20* is shown in Figure 11.5.\nTherefore, doubling the file requires allocating a new bucket page, writing both\nthis page and the old bucket page that is being split, and doubling the directory\narray.\nThe directory is likely to be much smaller than the file itself because\neach element is just a page-id, and can be doubled by simply copying it over\nlSince there are 'no overflow pages in Extendible Hashing, a bucket can be thought of a.~ a single\npage.\n\n376\nLOCAL DEPTH~\nGLOBAL DEPTH\n000\n001\n010\n011\n100\n101\n110\n111\nCHAPTER 11\nBucket A\nBucket B\nBucket C\nBucket 0\nDIRECTORY\nBucket A2\n(split image of bucket A)\nFigure 11.5\nAfter Inserting Entry r with h(r) = 20\n(and adjusting the elements for the split buckets). The cost of doubling is now\nquite acceptable.\nWe observe that the basic technique used in Extendible Hashing is to treat the\nresult of applying a hash function h a\" a binary number and interpret the last d\nbits, where d depends on the size of the directory, as an offset into the directory.\nIn our example, d is originally 2 because we only have four buckets; after the\nsplit, d becomes 3 because we now have eight buckets.\nA corollary is that,\nwhen distributing entries across a bucket and its split image, we should do so\non the basis of the dth bit. (Note how entries are redistributed in our example;\nsee Figure 11.5.) The number d, called the global depth of the hashed file, is\nkept as part of the header of the file. It is used every time we need to locate a\ndata entry.\nAn important point that arises is whether splitting a bucket necessitates a\ndirectory doubling. Consider our example, as shown in Figure 11.5. If we now\ninsert 9*, it belongs in bucket B; this bucket is already full. \\Ve can deal with\nthis situation by splitting the bucket and using directory elements 001 and 10]\nto point to the bucket and its split image, as shown in Figure 11.6.\nHence, a bucket split does not necessarily require a directory doubling. How-\never, if either bucket A or A2 grows full and an insert then forces a bucket split,\nwe are forced to double the directory again.\n\nHash-Based Inde:ring\nLOCAL DEPTH---L..-->\nGLOBAL DEPTH\n000\n001\n010\n011\n100\n101\n110\n111\nDIRECTORY\nBucket A\nBucket B\nBucket C\nBucket 0\nBucket A2 (split image of bucket A)\nBucket B2 (split image of bucket B)\n377\nFigure 11.6\nAfter Inserting Entry l' with h(r) = 9\nTo differentiate between these cases and determine whether a directory doubling\nis needed, we maintain a local depth for each bucket. If a bucket whose local\ndepth is equal to the global depth is split, the directory must be doubled. Going\nback to the example, when we inserted 9* into the index shown in Figure 11.5,\nit belonged to bucket B with local depth 2, whereas the global depth was 3.\nEven though the bucket was split, the directory did not have to be doubled.\nBuckets A and A2, on the other hand, have local depth equal to the global\ndepth, and, if they grow full and are split, the directory must then be doubled.\nInitially, all local depths are equal to the global depth (which is the number of\nbits needed to express the total number of buckets). vVe increment the global\ndepth by 1 each time the directory doubles, of course. Also, whenever a bucket\nis split (whether or not the split leads to a directory doubling), we increment\nby 1 the local depth of the split bucket and assign this same (incremented)\nlocal depth to its (newly created) split image. Intuitively, if a bucket has local\ndepth l, the hash values of data entries in it agree on the la.st l bits; further, no\ndata entry in any other bucket of the file has a hash value with the same last I\nbits. A total of 2dl directory elernents point to a bucket with local depth I; if\nd = l, exactly one directory element points to the bucket and splitting such a\nbucket requires directory doubling.\n\n378\nCHAPTER hI\nA final point to note is that we can also use the first d bits (the most significant\nbits) instead of the last d (least s'ignificant bits), but in practice the last d bits\nare used. The reason is that a directory can then be doubled simply by copying\nit.\nIn summary, a data entry can be located by computing its hash value, taking\nthe last d bits, and looking in the bucket pointed to by this directory element.\nFor inserts, the data entry is placed in the bucket to which it belongs and the\nbucket is split if necessary to make space. A bucket split leads to an increase in\nthe local depth and, if the local depth becomes greater than the global depth\nas a result, to a directory doubling (and an increase in the global depth) as\nwell.\nFor deletes, the data entry is located and removed. If the delete leaves the\nbucket empty, it can be merged with its split image, although this step is\noften omitted in practice. Merging buckets decreases the local depth. If each\ndirectory element points to the same bucket as its split image (i.e., 0 and 2d- 1\npoint to the same bucket, namely, A; 1 and 2d- 1 + 1 point to the same bucket,\nnamely, B, which mayor may not be identical to A; etc.), we can halve the\ndirectory and reduce the global depth, although this step is not necessary for\ncorrectness.\nThe insertion examples can be worked out backwards as examples of deletion.\n(Start with the structure shown after an insertion and delete the inserted ele-\nment. In each case the original structure should be the result.)\nIf the directory fits in memory, an equality selection can be answered in a\nsingle disk access, as for Static Hashing (in the absence of overflow pages), but\notherwise, two disk I/Os are needed. As a typical example, a 100MB file with\n100 bytes per data entry and a page size of 4KB contains 1 million data entries\nand only about 25,000 elements in the directory. (Each page/bucket contains\nroughly 40 data entries, and we have one directory element per bucket.) Thus,\nalthough equality selections can be twice as slow as for Static Hashing files,\nchances are high that the directory will fit in memory and performance is the\nsame as for Static Ha.<;hing files.\nOn the other hand, the directory grows in spurts and can become large for\nskewed data distTibutions (where our assumption that data pages contain roughly\nequal numbers of data entries is not valid). In the context of hashed files, in a\nskewed data distribution the distribution of hash values of seaTch field values\n(rather than the distribution of search field values themselves) is skewed (very\n'bursty' or nonuniform). Even if the distribution of search values is skewed, the\nchoice of a good hashing function typically yields a fairly uniform distribution\nof lw\"sh va.lues; skew is therefore not a problem in practice.\n\nHash~Based Inde:cing\n379\nF\\lrther, collisions, or data entries with the same hash value, cause a problem\nand must be handled specially: \\Vhen more data entries th311 \\vill fit on a page\nhave the same hash value, we need overflow pages.\n11.3\nLINEAR HASHING\nLinear Hashing is a dynamic hashing technique, like Extendible Hashing, ad-\njusting gracefully to inserts and deletes.\nIn contrast to Extendible Hashing,\nit does not require a directory, deals naturally with collisions, and offers a lot\nof flexibility with respect to the timing of bucket splits (allowing us to trade\noff slightly greater overflow chains for higher average space utilization). If the\ndata distribution is very skewed, however, overflow chains could cause Linear\nHashing performance to be worse than that of Extendible Hashing.\nThe scheme utilizes a family of hash functions ha, hI, h2, ... , with the property\nthat each function's range is twice that of its predecessor. That is, if hi maps\na data entry into one of M buckets, h i+I maps a data entry into one of 2lv!\nbuckets. Such a family is typically obtained by choosing a hash function hand\nan initial number N ofbuckets,2 and defining hi(value) \"'= h(value) mod (2i N).\nIf N is chosen to be a power of 2, then we apply h and look at the last di bits;\ndo is the number of bits needed to represent N, and di = da+ i. Typically we\nchoose h to be a function that maps a data entry to some integer.\nSuppose\nthat we set the initial number N of buckets to be 32. In this case do is 5, and\nha is therefore h mod 32, that is, a number in the range 0 to 31. The value of\nd l is do + 1 = 6, and hI is h mod (2 * 32), that is, a number in the range 0 to\n63. Then h2 yields a number in the range 0 to 127, and so OIl.\nThe idea is best understood in terms of rounds of splitting.\nDuring round\nnumber Level, only hash functions hLeud and hLevel+1 are in use. The buckets\nin the file at the beginning of the round are split, one by one from the first to\nthe last bucket, thereby doubling the number of buckets. At any given point\nwithin a round, therefore, we have buckets that have been split, buckets that\nare yet to be split, and buckets created by splits in this round, as illustrated in\nFigure 11.7.\nConsider how we search for a data entry with a given search key value.\n\\Ve\napply ha..:sh function h Level , and if this leads us to one of the unsplit buckets,\nwe simply look there. If it leads us to one of the split buckets, the entry may\nbe there or it may have been moved to the new bucket created earlier in this\nround by splitting this bucket; to determine which of the two buckets contains\nthe entry, we apply hLevel+I'\n2Note that 0 to IV - 1 is not the range of fl.!\n\n380\nBucket to be split\nNext\nBuckets that existed at the\nbeginning of this round:\nthis is the range of\nh Level\n-\n1\nr\nI\nI\n-<-\nCHAPTER \\1\nBuckets split in this round:\nIf h Le~'el ( search key mil,e\nis in this range, must use\nh Level+1 (search key vallie\nto decide if entry is in\nsplit image bucket.\n'split image' buckets:\ncreated (tlrrough splitting\nof other buckets) in this round\nFigure 11.7\nBuckets during a Round in Linear Hashing\nUnlike Extendible Hashing, when an insert triggers a split, the bucket into\nwhich the data entry is inserted is not necessarily the bucket that is split. An\noverflow page is added to store the newly inserted data entry (which triggered\nthe split), as in Static Hashing. However, since the bucket to split is chosen\nin round-robin fashion, eventually all buckets are split, thereby redistributing\nthe data entries in overflow chains before the chains get to be more than one\nor two pages long.\nWe now describe Linear Hashing in more detail.\nA counter Level is used to\nindicate the current round number and is initialized to O. The bucket to split\nis denoted by Next and is initially bucket °(the first bucket). We denote the\nnumber of buckets in the file at the beginning of round Level by N Level. We\ncan easily verify that N Level = N * 2Level. Let the number of buckets at the\nbeginning of round 0, denoted by No, be N.\nWe show a small linear hashed\nfile in Figure 11.8. Each bucket can hold four data entries, and the file initially\ncontains four buckets, as shown in the figure.\nWe have considerable flexibility in how to trigger a split, thanks to the use of\noverflow pages. We can split whenever a new overflow page is added, or we can\nimpose additional conditions based all conditions such as space utilization. For\nour examples, a split is 'triggered' when inserting a new data entry causes the\ncreation of an Qverftow page.\n\\Vhenever a split is triggered the Next bucket is split, and hash function hLevel+l\nredistributes entries between this bucket (say bucket number b) and its split\nimage; the split image is therefore bucket number b+ NLeve/. After splitting a\nbucket, the value of Next is incremented by 1. In the example file, insertion of\n\nHash-Based Indexing\n38J\nLevel=O. N=4\nPRIMARY\nPAGES\n000\n00\nNext=O\n\"'1 32'1 44'1 36'1 1\n001\n010\n01\n10\nI 9\" 1 2S\"i S'f~~\n_\nData entry r\nwith h{r)mS\n~\n4:\n18\"\n10\"\n30\"\n.\n,\nPrJ.mary\n_\nbucket page\n11\n011\n~\nThis information is\nThe actual contelJts\nfor illustratiolJ only\nofthe linear hashedjile\nFigure 11.8\nExample of a Linear Hashed File\ndata entry 43* triggers a split. The file after completing the insertion is shown\nin Figure 11.9.\nLevel=O\nPRIMARY\nOVERFLOW\nh 1\nh O\nPAGES\nPAGES\n000\n00\n~\nNext=1\n-\n001\n01\n\"'~\n010\n10\n~\n011\n11\n100\n00\nFigure 11.9\nAfter Inserting Record T with h(T) = 43\nAt any time in .the middle of a round Level, all buckets above bucket Ne:rt have\nbeen split, and the file contains buckets that are their split images, as illustrated\nin Figure 11.7. Buckets Next through NLevcl have not yet been split. If we use\nhLevel on a data entry and obtain a number b in the range Next through NLevel,\nthe data entry belongs to bucket b. For example, ho(18) is 2 (binary 10); since\nthis value is between the current values of Ne:r:t (= 1) and N 1 (=,-:': 4), this bucket\nhas not been split. However, if we obtain a number b in the range 0 through\n\n382\nCHAPTER 11\nNext, the data entry may be in this bucket or in its split image (which is bucket\nnumber b+NLevet}; we have to use hLevel+1 to determine to which of these two\nbuckets the data entry belongs. In other words, we have to look at one more\nbit of the data entry's hash value. For example, ho(32) and ho(44) are both a\n(binary 00). Since Next is currently equal to 1, which indicates a bucket that\nhas been split, we have to apply hI'\nWe have hI (32) = 0 (binary 000) and\nh1(44) = 4 (binary 100). Therefore, 32 belongs in bucket A and 44 belongs in\nits split image, bucket A2.\nNot all insertions trigger a split, of course. If we insert 37* into the file shown\nin Figure 11.9, the appropriate bucket has space for the new data entry. The\nfile after the insertion is shown in Figure 11.10.\nLevel=O\nPRIMARY\nOVERFLOW\nh1\nho\nPAGES\nPAGES\n000\n00\n~\nNext=1\n-\n001\n01\n~~\n010\n10\n~\n011\n11\n100\n00\nEEITl\nFigure 11.10\nAfter Inserting Record r with h(r) = 37\nSometimes the bucket pointed to by Next (the current candidate for splitting)\nis full, and a new data entry should be inserted in this bucket. In this case, a\nsplit is triggered, of course, but we do not need a new overflow bucket. This\nsituation is illustrated by inserting 29* into the file shown in Figure 11.10. The\nresult is shown in Figure 11.11.\nWhen Next is equal to NLevel - 1 and a split is triggered, we split the last of\nthe buckets present in the file at the beginning of round Level. The number\nof buckets after the split is twice the number at the beginning of the round,\nand we start a new round with Level incremented by 1 and Next reset to O.\nIncrementing Level amounts to doubling the effective range into which keys are\nhashed. Consider the example file in Figure 11.12, which was obtained from the\nfile of Figure 11.11 by inserting 22*, 66*, and 34*. (The reader is encouraged to\ntry to work out the details of these insertions.) Inserting 50* causes a split that\n\nHash-Based l'ndex'ing\nLevel=O\nPRIMARY\nOVERFLOW\nh 1\nho\nPAGES\nPAGES\n000\n00\n132'1\n1\n001\n01\n025'\\\n1\nNext\",2\n010\n10\n\"'114'\\18'110'13°1\n011\n11\n~EITQ\n100\n00\nE8Il\n101\n01\n~\nFigure 11.11\nAfter Inserting Record r with h(r\") = 29\n383\nleads to incrementing Level, as discussed previously; the file after this insertion\nis shown in Figure 11.13.\nLevel=O\nPRIMARY\nh 1\nh O\nPAGES\n000\n00\n~C[l\n001\n01\n~[J=l\n010\n10\n~\nNext=3\n011\n11\n~17'll1'1\n100\n00\n~J.:~lI{\n101\n01\nEL~1i?I=]\n~\n110\n10\n~\n~\nOVERFLOW\nPAGES\nFigure 11.12\nAfter Inserting Records with h(r) = 22,66,and34\nIn summary, an equality selection costs just one disk I/O unless the bucket has\noverflow pages; in practice, the cost on average is about 1.2 disk accesses for\n\n384\nLevel:1\nPRIMARY\nOVERFLOW\nh 1\nh o\nPAGES\nPAGES\nNext=O\n000\n00\n001\n01\nLd~I~]~\n010\n10\n~~8]2or;~!\n. ~l~j\n011\n11\n~5J11I-1\n'---=--L_~_\ni\n100\n00\n[44'!3sTT-l\n101\n11\n1..~13~J2::L~l\n110\n10\nr-i4I30:@I1\n111\n11\nFigure 11.13\nAfter Inserting Record r with h(r) = 50\nreasonably uniform data distributions. (The cost can be considerably worse--\nlinear in the number of data entries in the file----if the distribution is very skewed.\nThe space utilization is also very poor with skewed data distributions.) Inserts\nrequire reading and writing a single page, unless a split is triggered.\n'We not discuss deletion in detail, but it is essentially the inverse of insertion.\nIf the last bucket in the file is empty, it can be removed and Next can be\ndecremented. (If Next is 0 and the last bucket becomes empty, Next is made to\npoint to bucket (AI/2) ~ 1, where !vI is the current number of buckets, Level is\ndecremented, and the empty bucket is removed.) If we wish, we can combine the\nlast bucket with its split image even when it is not empty, using some criterion\nto trigger this merging in essentially the same way. The criterion is typically\nbased on the occupancy of the file, and merging can be done to improve space\nutilization.\n11.4\nEXTENDIBLE VS. LINEAR HASHING\nTo understand the relationship between Linear Hashing and Extendible Hash-\ning, imagine that we also have a directory in Linear Hashing with elements 0\nto N - 1. The first split is at bucket 0, and so we add directory element N. In\nprinciple, we may imagine that the entire directory has been doubled at this\npoint; however, because element 1 is the same as element N + 1, elernent 2 is\n\nHash~Based Indexing\n385\n!\nthe same a.'3 element N + 2, and so on, we can avoid the actual copying for\nthe rest of the directory. The second split occurs at bucket 1; now directory\nelement N + 1 becomes significant and is added. At the end of the round, all\nthe original N buckets are split, and the directory is doubled in size (because\nall elements point to distinct buckets).\n\\Ve observe that the choice of hashing functions is actually very similar to\nwhat goes on in Extendible Hashing---in effect, moving from hi to hi+1 in\nLinear Hashing corresponds to doubling the directory in Extendible Hashing.\nBoth operations double the effective range into which key values are hashed;\nbut whereas the directory is doubled in a single step of Extendible Hashing,\nmoving from hi to hi+l, along with a corresponding doubling in the number\nof buckets, occurs gradually over the course of a round in Linear Ha.'3hing.\nThe new idea behind Linear Ha.'3hing is that a directory can be avoided by a\nclever choice of the bucket to split. On the other hand, by always splitting the\nappropriate bucket, Extendible Hashing may lead to a reduced number of splits\nand higher bucket occupancy.\nThe directory analogy is useful for understanding the ideas behind Extendible\nand Linear Hashing. However, the directory structure can be avoided for Linear\nHashing (but not for Extendible Hashing) by allocating primary bucket pages\nconsecutively, which would allow us to locate the page for bucket i by a simple\noffset calculation.\nFor uniform distributions, this implementation of Linear\nHashing has a lower average cost for equality selections (because the directory\nlevel is eliminated). For skewed distributions, this implementation could result\nin any empty or nearly empty buckets, each of which is allocated at least one\npage, leading to poor performance relative to Extendible Hashing, which is\nlikely to have higher bucket occupancy.\nA different implementation of Linear Hashing, in which a directory is actually\nmaintained, offers the flexibility of not allocating one page per bucket; null\ndirectory elements can be used as in Extendible Hashing. However, this imple-\nmentation introduces the overhead of a directory level and could prove costly\nfor large, uniformly distributed files. (Also, although this implementation alle-\nviates the potential problem of low bucket occupancy by not allocating pages\nfor empty buckets, it is not a complete solution because we can still have many\npages with very few entries.)\n11.5\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n\n386\nCHAPTER).l\n•\nHow does a hash-ba..sed index handle an equality query? Discuss the use of\nthe hash function in identifying a bucket to search. Given a bucket number,\nexplain how the record is located on disk.\n•\nExplain how insert and delete operations are handled in a static hash index.\nDiscuss how overflow pages are used, and their impact on performance.\nHow many disk l/Os does an equality search require, in the absence of\noverflow chains? What kinds of workload does a static hash index handle\nwell, and when it is especially poor? (Section 11.1)\n•\nHow does Extendible Hashing use a directory of buckets? How does Ex-\ntendible Hashing handle an equality query? How does it handle insert and\ndelete operations? Discuss the global depth of the index and local depth of\na bucket in your answer. Under what conditions can the directory can get\nlarge? (Section 11.2)\n•\nWhat are collisions?\nWhy do we need overflow pages to handle them?\n(Section 11.2)\n•\nHow does Linear Hashing avoid a directory? Discuss the round-robin split-\nting of buckets. Explain how the split bucket is chosen, and what triggers\na split. Explain the role of the family of hash functions, and the role of\nthe Level and Next counters. When does a round of splitting end? (Sec-\ntion 11.3)\n•\nDiscuss the relationship between Extendible and Linear Hashing. What are\ntheir relative merits? Consider space utilization for skewed distributions,\nthe use of overflow pages to handle collisions in Extendible Hashing, and\nthe use of a directory in Linear Hashing. (Section 11.4)\nEXERCISES\nExercise 11.1 Consider the Extendible Hashing index shown in Figure 1l.14. Answer the\nfollowing questions about this index:\n1. What can you say about the last entry that was inserted into the index?\n2. What can you say about the last entry that was inserted into the index if you know that\nthere have been no deletions from this index so far?\n3. Suppose you are told that there have been no deletions from this index so far. What can\nyou say about the last entry whose insertion into the index caused a split?\n4. Show the index after inserting an entry with hash value 68.\n5. Show the original index after inserting entries with ha.sh values 17 and 69.\n6. Show the original index after deleting the entry with hash value 21. (Assume that the\nfull deletion algorithm is used.)\n7. Show the original index after deleting the entry with ha,,;h value 10. Is a merge triggered\nby this deletion? If not, explain why. (Assume that the full deletion algorithm is used.)\n\nHash-Based Indexing\n387\n000\n001\n010\n011\n100\n101\n110\n111\nDIRECTORY\nBucket A\nBucket B\nBucketC\nBucket 0\nBucketA2\nFigure 11.14\nFigure for Exercise 11.1\nLevel=O\nPRIMARY\nOVERFLOW\nh(l)\nh(O)\nPAGES\nPAGES\n000\n00\n~\nNext=l\n-\n001\n01\n\"~\n010\n10\n~\n011\n11\nI~ 35 1 7[11\\\n1-\n100\n00\nEFrq\nFigure 11.15\nFigure for Exercise 11.2\nExercise 11.2 Consider the Linear Hashing index shown in Figure 11.15. Assume that we\nsplit whenever an overflow page is created. Answer the following questions about this index:\n1. What can you say about the last entry that was inserted into the index?\n2. What can you say about the last entry that was inserted into the index if you know that\nthere llave been no deletions from this index so far?\n:t Suppose you know that there have been no deletions from this index so far. What can\nyou say about the last entry whose insertion into the index caused a split?\n4. Show the index after inserting an entry with hash value 4.\n\n388\nCHAPTER 11\n5, Show the original index after inserting an entry with hash value 15.\n6. Show the original index after deleting the entries with hash values 36 and 44. (Assume\nthat the full deletion algorithm is used.)\n7. Find a list of entries whose insertion into the original index would lead to a bucket with\ntwo overflow pages.\nUse as few entries as possible t.o accomplish this.\n\"Vhat is the\nmaximum number of entries that can be inserted into this bucket before a split occurs\nthat reduces the length of this overflow chain?\nExercise 11.3 Answer the following questions about Extendible Hashing:\n1. Explain why local depth and global depth are needed.\n2. After an insertion that causes the directory size to double, how many buckets have\nexactly one directory entry pointing to them? If an entry is then deleted from one of\nthese buckets, what happens to the directory size? Explain your answers briefly.\n3. Does Extendible I-lashing guarantee at most one disk access to retrieve a record with a\ngiven key value?\n4. If the hash function distributes data entries over the space of bucket numbers in a very\nskewed (non-uniform) way, what can you say about the size of the directory? What can\nyou say about the space utilization in data pages (i.e., non-directory pages)?\n5. Does doubling the directory require us to examine all buckets with local depth equal to\nglobal depth?\n6. Why is handling duplicate key values in Extendible Hashing harder than in ISAM?\nExercise 11.4 Answer the following questions about Linear Hashing:\n1. How does Linear Hashing provide an average-case search cost of only slightly more than\none disk I/O, given that overflow buckets are part of its data structure?\n2. Does Linear Hashing guarantee at most one disk access to retrieve a record with a given\nkey value?\n3. If a Linear Hashing index using Alternative (1) for data entries contains N records, with\nP records per page and an average storage utilization of 80 percent, what is the worst-\ncase cost for an equality search? Under what conditions would this cost be the actual\nsearch cost?\n4. If the hash function distributes data entries over the space of bucket numbers in a very\nskew(,d (non-uniform) way, what can you say about thc space utilization in data pages?\nExercise 11.5 Give an example of when you would use each element (A or B) for each of\nthe following 'A versus B' pairs:\n1. A hashed index using Alternative (1) versus heap file organization.\n2. Extendible Hashing versus Linear Hashing.\n3. Static Hashing versus Linear Hashing.\n4. Static Hashing versus ISAIVI.\n5. Linear Hashing versus B+ trees.\nExercise 11.6 Give examples of the following:\n1. A Linear Hashing index and an Extendible Hashing index with the same data entries,\nsuch that the Linear Hashing index has more pages.\n\nHash-Based Indf1;ing\nLevel=O, N=4\nh 1\nho\nPRIMARY\nPAGES\nNext=O\n000\n00\n\"I 64144 1 1\n001\n01\nliliEI~\n010\n10\nGIQ\n011\n11\n~\nFigure 11.16\nFigure for Exercise 11.9\n389\n2. A Linear H&shing index and an Extendible Hashing index with the same data entries,\nsuch that the Extendible Hashing index has more pages.\nExercise 11.7 Consider a relation R( [L, b, c, rf) containing 1 million records, where each page\nof the relation holds 10 records. R is organized as a heap file with unclustered indexes, and\nthe records in R are randomly ordered. Assume that attribute a is a candidate key for R, with\nvalues lying in the range 0 to 999,999. For each of the following queries, name the approach\nthat would most likely require the fewest l/Os for processing the query. The approaches to\nconsider follow:\n•\nScanning through the whole heap file for R.\n•\nUsing a B+ tree index on attribute R.a.\n•\nUsing a hash index on attribute R.a.\nThe queries are:\n1. Find all R tuples.\n2. Find all R tuples such that a < 50.\n3. Find all R tuples such that a = 50.\n4. Find all R tuples such that a > 50 and a < 100.\nExercise 11.8 How would your answers to Exercise 11.7 change if a is not a candidate key\nfor R? How would thcy change if we assume that records in R are sorted on a?\nExercise 11.9 Consider the snapshot of the Linear Hashing index shown in Figure 11.16.\nAssume that a bucket split occurs whcnever an overflow page is created.\n1. vVhat is the mll1:imwn number of data entries that call be inserted (given the best possible\ndistribution of keys) before you have to split a bucket? Explain very briefly.\n2. Show the file after inserting a singlc record whose insertion causes a bucket split.\n\n390\nCHAPTER 11\n3.\n(a) What is the minimum number of record insertions that will cause a split of all four\nbuckets? Explain very briefly.\n(b) What is the value of Next after making these insertions?\n(c) What can you say about the number of pages in the fourth bucket shown after this\nseries of record insertions?\nExercise 11.10 Consider the data entries in the Linear Hashing index for Exercise 11.9.\n1. Show an Extendible Hashing index with the same data entries.\n2. Answer the questions in Exercise 11.9 with respect to this index.\nExercise 11.11 In answering the following questions, assume that the full deletion algorithm\nis used. Assume that merging is done when a bucket becomes empty.\n1. Give an example of Extendible Hashing where deleting an entry reduces global depth.\n2. Give an example of Linear Hashing in which deleting an entry decrements Next but leaves\nLevel unchanged. Show the file before and after the deletion.\n3. Give an example of Linear Hashing in which deleting an entry decrements Level. Show\nthe file before and after the deletion.\n4. Give an example of Extendible Hashing and a list of entries el, e2, e3 such that inserting\nthe entries in order leads to three splits and deleting them in the reverse order yields the\noriginal index. If such an example does not exist, explain.\n5. Give an example of a Linear Hashing index and a list of entries el, e2, e3 such that\ninserting the entries in order leads to three splits and deleting them in the reverse order\nyields the original index. If such an example does not exist, explain.\nPROJECT-BASED EXERCISES\nExercise 11.12 (Note to inst1'7u:toTS: Additional details must be provided if this question is\nassigned. See Appendi:c 30.) Implement Linear Hashing or Extendible Hashing in Minibase.\nBIBLIOGRAPHIC NOTES\nHashing is discussed in detail in [442].\nExtendible Hashing is proposed in [256].\nLitwin\nproposed Linear Hashing in [483].\nA generalization of Linear Hashing for distributed\nenvi~\nronments is described in [487]. There has been extensive research into hash-based indexing\ntechniques. Larson describes two variations of Linear Hashing in [469] and [470]. Ramakr-\nishna presents an analysis of hashing techniques in [607]. Hash functions that do not produce\nbucket overflows are studied in [608]. Order-preserving hashing techniques are discussed in\n[484] and [308] .\nPartitioned-hashing, in which each field is hashed to obtain some bits of\nthe bucket address, extends hashing for the case of queries in which equality conditions are\nspecified only for some of the key fields. This approach was proposed by Rivest [628] and is\ndiscussed in [747]; a further development is described in [616].\n\nPARTN\nQUERY EVALUATION\n\n12\nOVERVIEW OF QUERY\nEVALUATION\n....\nWhat descriptive information does a DBMS store in its catalog?\n....\nWhat alternatives are considered for retrieving rows from a table?\n....\n~Why does a DBMS implement several algorithms for each algebra\noperation? What factors affect the relative performance of different\nalgorithms?\n....\nWhat are query evaluation plans and how are they represented?\n....\nWhy is it important to find a good evaluation plan for a query? How\nis this done in a relational DBMS?\n..\nKey concepts: catalog, system statistics; fundamental techniques,\nindexing, iteration, and partitioning; access paths, matching indexes\nand selection conditions; selection operator, indexes versus scans, im-\npact of clustering; projection operator, duplicate elimination; join op-\nerator, index nested-loops join, sort-merge join; query evaluation plan;\nmaterialization vs.\npipelinining; iterator interface; query optimiza-\ntion, algebra equivalences, plan enumeration; cost estimation\nThis very remarkable man, commends a most practical plan:\nYou can do what you want, if you don't think you can't,\nSo c1on't think you can't if you can.\n~~~--~Charles Inge\nIn this chapter, we present an overview of how queries are evaluated in a rela-\ntional DBMS. Vve begin with a discussion of how a DBMS describes the data\n393\n\n394\nCHAPTER 12\nthat it manages, including tables and indexes, in Section 12.1. This descriptive\ndata, or metadata, stored in special tables called the system catalogs, is\nused to find the best way to evaluate a query.\nSQL queries are translated into an extended form of relational algebra, and\nquery evaluation plans are represented as trees of relational operators, along\nwith labels that identify the algorithm to use at each node. Thus, relational op-\nerators serve as building blocks for evaluating queries, and the implementation\nof these operators is carefully optimized for good performance. We introduce\noperator evaluation in Section 12.2 and describe evaluation algorithms for var-\nious operators in Section 12.3.\nIn general, queries are composed of several operators, and the algorithms for\nindividual operators can be combined in many ways to evaluate a query. The\nprocess of finding a good evaluation plan is called query optimization. We intro-\nduce query optimization in Section 12.4. The basic task in query optimization,\nwhich is to consider several alternative evaluation plans for a query, is moti-\nvated through examples in Section 12.5. In Section 12.6, we describe the space\nof plans considered by a typical relational optimizer.\nThe ideas are presented in sufficient detail to allow readers to understand\nhow current database systems evaluate typical queries. This chapter provides\nthe necessary background in query evaluation for the discussion of physical\ndatabase design and tuning in Chapter 20.\nRelational operator implementa-\ntion and query optimization are discussed further in Chapters 13, 14, and 15;\nthis in-depth coverage describes how current systems are implemented.\nWe consider a number of example queries using the following schema:\nSailors(sid: integer, .mame: string, rating: integer, age: real)\nReserves(sid: integer, bid: integer, day: dates, marne: string)\nWe aSSUlne that each tuple of Reserves is 40 bytes long, that a page can hold\n100 Reserves tuples, and that we have 1000 pages of such tuples.\nSimilarly,\nwe assume that each tuple of Sailors is 50 bytes long, that a page can hold 80\nSailors tuples, and that we have 500 pages of such tuples.\n12.1\nTHE SYSTEM CATALOG\n\\Ve can store a table using one of several alternative file structures, and we can\ncreate one or more indexes -each stored as a file\n011 every tal)le. Conversely,\nin a relational DBMS, every file contains either the tuples in a table or the\n\nOverview of Query Evaluation\n395\nentries in an index. The collection of filE'-s corresponding to users' tables and\nindexes represents the data in the databa.<;e.\nA relational DBMS maintains information about every table and index that it\ncontains. The descriptive information is itself stored in a collection of special\ntables called the catalog tables.\nAn example of a catalog table is shown\nin Figure 12.1. The catalog tables are also called the data dictionary, the\nsystem catalog, or simply the catalog.\n12.1.1\nInformation in the Catalog\nLet us consider what is stored in the system catalog. At a minimum, we\"have\nsystem-wide information, such as the size of the buffer pool and the page size,\nand the following information about individual tables, indexes, and views:\n•\nFor each table:\n- Its table name, the file name (or some identifier), and the file structure\n(e.g., heap file) of the file in which it is stored.\n- The attribute name and type of each of its attributes.\n- The index name of each index on the table.\n- The integrity constmints (e.g., primary key and foreign key constraints)\non the table.\n•\nFor each index:\n- The inde:I: name and the structure (e.g., B+ tree) of the index.\n- The search key attributes.\n•\nFor each view:\n- Its view name and definition.\nIn addition, statistics about tables and indexes are stored in the system catalogs\nand updated periodically (not every time the underlying tables are modified).\nThe following information is commonly stored:\n•\nCardinality: The number of tuples NTaplcs(R) for each table R.\n•\nSize: The number of pages NPages(R) for each table R.\n•\nIndex Cardinality: The number of distinct key values NKeys(I) for each\nindex I.\n•\nIndex Size: The nUluber of pages INPages(I) for each index I. (For a B+\ntree index I, we take INPagcs to be the number of leaf pages.)\n\n396\nCHAPTER 12\n•\nIndex Height: The number of nonleaf levels IHe'ight(I) for each tree index\nI.\n•\nIndex Range: The minimum present key value ILow(I) and the maximum\npresent key value INigh(I) for each index I.\nvVe assume that the database architecture presented in Chapter 1 is used.\n.Further, we assume that each file of records is implemented as a separate file of\npages. Other file organizations are possible, of course. For example, a page file\ncan contain pages that store records from more than one record file. If such a\nfile organization is used, additional statistics must be maintained, such as the\nfraction of pages in a file that contain records from a given collection of records.\nThe catalogs also contain information about users, such as accounting infor-\nmation and authorization information (e.g., Joe User can modify the Reserves\ntable but only read the Sailors table).\nHow Catalogs are Stored\nAn elegant aspect of a relational DBMS is that the system catalog is itself\na collection of tables.\nFor example, we might store information about the\nattributes of tables in a catalog table called Attribute_Cat:\nAttribute_Cat( attLnatne: string, reLname: string,\ntype: string, position: integer)\nSuppose that the database contains the two tables that we introduced at the\nbegining of this chapter:\nSailors(sid: integer, sname: string, rating: integer, age: real)\nReserves(sid: integer, bid: integer, day: dates, mame: string)\nFigure 12.1 shows the tuples in the Attribute_Cat table that describe the at--\ntributes of these two tables.\nNote that in addition to the tuples describing\nSailors and Reserves, other tuples (the first four listed) describe the four at-\ntributes of the Attribute_Cat table itself[ These other tuples illustrate an im-\nportant Point: the catalog tables describe all the tables in the database, includ-\ning the catalog tables themselves. When information about a table is needed,\nit is obtained from the system catalog. Of course, at the implementation level,\nwhenever the DBMS needs to find the schema of a catalog table, the code\nthat retrieves this information must be handled specially. (Otherwise, the code\nha\",> to retrieve this information from the catalog tables without, presumably,\nknowing the schema of the catalog tables.)\n\nOverview of Q'lLC'l~1j E1Jaluotion\nattT_narne\nrei\n·.>./ ...·.i .·>ttJTJe:<>\nattr_name\nAttribute_Cat\nstring\n1\nreLname\nAttribute_Cat\nstring\n2\n-_.-\ntype\nAttribute_Cat\nstring\n3\nposition\nAttribute_Cat\ninteger\n4\nsid\nSailors\ninteger\n1\nsname\nSailors\nstring\n2\nrating\nSailors\ninteger\n3\nage\nSailors\nreal\n4\nsid\nReserves\ninteger\n1\nbid\nReserves\ninteger\n2\nday\nReserves\ndates\n3\nrname\nReserves\nstring\n4\nFigure 12.1\nAn Instance of the Attribute_Cat Relation\n~397\nThe fact that the system catalog is also a collection of tables is very useful. For\nexample, catalog tables can be queried just like any other table, using the query\nlanguage of the DBMS! Further, all the techniques available for implementing\nand managing tables apply directly to catalog tables.\nThe choice of catalog\ntables and their schema..., is not unique and is made by the implementor of the\nDBMS. Real systems vary in their catalog schema design, but the catalog is\nalways implemented as a collection of tables, and it essentially describes all the\ndata stored in the database. 1\n12.2\nINTRODUCTION TO OPERATOR EVALUATION\nSeveral alternative algorithms are available for implementing each relational\noperator, and for most operators no algorithm is universally superior. Several\nfactors influence which algorithm performs best, including the sizes of the tables\ninvolved, existing indexes and sort orders, the size of the available buffer pool,\nand the buffer replacement policy.\nIn this section, we describe some common techniques used in developing eval-\nuation algorithms for relational operators, and introduce the concept of access\npaths, which are the different ways in which rows of a table can be retrieved.\nISome systems may store additional information in a non-relational form. For example, a system\nwith a sophisticated query optimizer may maintain histograms or other statistical information about\nthe distribution of values in certain attributes of a table. \\Ve can think of such information, when it.\nis maintained, as a supplement to the catalog tables.\n\n398\nCHAPTER 12\n12.2.1\nThree Common Techniques\nThe algorithms for various relational operators actually have a lot in common.\nA few simple techniques are used to develop algorithms for each operator:\nIII\nIndexing: If a selection or join condition is specified, use an index to\nexamine just the tuples that satisfy the condition.\nIII\nIteration: Examine all tuples in an input table, one after the other. If\nwe need only a few fields from each tuple and there is an index whose key\ncontains all these fields, instead of examining data tuples, we can scan all\nindex data entries.\n(Scanning all data entries sequentially makes no use\nof the index's ha.8h- or tree-based search structure; in a tree index, for\nexample, we would simply examine all leaf pages in sequence.)\nIII\nPartitioning: By partitioning tuples on a sort key, we can often decom-\npose an operation into a less expensive collection of operations on parti-\ntions. Sorting and hashing are two commonly used partitioning techniques.\nWe discuss the role of indexing in Section 12.2.2. The iteration and partitioning\ntechniques are seen in Section 12.3.\n12.2.2\nAccess Paths\nAn access path is a way of retrieving tuples from a table and consists of\neither (1) a file scan or (2) an index plus a matching selection condition. Every\nrelational operator accepts one or more tables as input, and the access methods\nused to retrieve tuples contribute significantly to the cost of the operator.\nConsider a simple selection that is a conjunction of conditions of the form\nattT op 'ualue, where op is one of the comparison operators <, ::;,\n=,\n=f.,\n~,\nor >. Such selections are said to be in conjunctive normal form (CNF),\nand each condition is called a conjunct.2\nIntuitively, an index matches a\nselection condition if the index can be used to retrieve just the tuples that\nsatis(y the condition.\nIII\nA hash index matches a CNF selection if there is a term of the form\nattribute=1wJue in the selection for each attribute in the index's search key.\nIII\nA tree index matches a CNF selection if there is a term of the form\nattTibute op value for each attribute in a prefLr of the index's search key.\n((eL; and (a,b; are prefixes of key (a,b,e), but (a,e) and (b,e) are not.)\n2We consider more complex selection conditions in Section 14.2.\n\nOverview of Que7'.I! Evaluation\n399\nNote that op can be any comparison; it is not n:Btricted to he equality as\nit is for matching selections on a h&\"h index.\nAn index can match some subset of the conjuncts in a selection condition (in\nCNP), even though it does not match the entire condition.\n\\Ve refer to the\nconjuncts that the index matches as the primary conjuncts in the selection.\nThe following examples illustrate access paths.\n•\nIf we have a hash index H on the search key Cmarne, bid,sirf) , we can\nuse the index to retrieve just the Sailors tuples that satisfy the condition\nrnarne='Joe'l\\ bid=5 1\\ sid=3.\nThe index matches the entire condition\n77wme= 'Joe' 1\\ bid=5 1\\ sid= 3. On the other hand, if the selection con-\ndition is rname= 'Joe' 1\\ bid=5, or some condition on date, this index does\nnot match. That is, it cannot be used to retrieve just the tuples that satisfy\nthese conditions.\nIn contrast, if the index were a B+ tree, it would match both rname= 'Joe'\n1\\ bid=51\\ 8id=3 and mame='Joe' 1\\ bid=5. However, it would not match\nbid=5 1\\ sid=8 (since tuples are sorted primarily by rnarne).\n•\nIf we have an index (hash or tree) on the search key (bid,sid'; and the se-\nlection condition 'rname= 'Joe' 1\\ bid=5 1\\ sid=3, we can use the index to\nretrieve tuples that satisfy bid=51\\ sid=3; these are the primary conjuncts.\nThe fraction of tuples that satisfy these conjuncts (and whether the index\nis clustered) determines the number of pages that are retrieved. The ad-\nditional condition on Tna7ne must then be applied to each retrieved tuple\nand will eliminate some of the retrieved tuples from the result.\n1iI\nIf we have an index on the search key (bid,\nsi(~ and we also have a B+ tree\nindex on day, the selection condition day < 8/9/2002 1\\ bid=5 1\\ sid=3\noffers us a choice.\nBoth indexes match (part of) the selection condition,\nand we can use either to retrieve Reserves tuples. \\Vhichever index we use,\nthe conjuncts in the selection condition that are not matched by the index\n(e.g., bid=51\\ sid=3 if we use the B+ tree index on day) must be checked\nfor each retrieved tuple.\nSelectivity of Access Paths\nThe selectivity of an access path is the number of pages retrieved (index pages\nplus data pages) if we usc this access path to retrieve all desired tuples. If a\ntable contains an index that matches a given selection, there are at lea.st two\naccess paths: the index and a scan of the data file. Sometimes, of course, we\ncan scan the index itself (rather than scanning the data file or using the index\nto probe the file), giving us a third ,'lccess path.\n\n400\nCHAPTER l2\nThe most selective access path is the one that retrieves the fewest pages;\nusing the most selective access path minimizes the cost of data retrieval. The\nselectivity of an l:lCCeSS path depends on the primary conjuncts in the selection\ncondition (with respect to the index involved). Each conjunct acts as a filter\non the table. The fraction of tuples in the table that satisfy a given conjunct is\ncalled the reduction factor. 'When there are several primary conjuncts, the\nfraction of tuples that satisfy all of them can be approximated by the product\nof their reduction factors; this effectively treats them as independent filters,\nand while they may not actually be independent, the approximation is widely\nused in practice.\nSupose we have a hash index H on Sailors with search key (rname,bid,sid:), and\nwe are given the selection condition rname='Joe' 1\\ bid=5 1\\ sid=3. We can\nuse the index to retrieve tuples that satisfy all three conjuncts. The catalog\ncontains the number of distinct key values, N K eys(H), in the hash index, as\nwell as the number of pages, N Pages, in the Sailors table.\nThe fraction of\npages satisfying the primary conjuncts is Npages(Sailors) . NI<e;s(H)'\nIf the index has search key (bid,sid:) , the primary conjuncts are bid=51\\ sid=3.\nIf we know the number of distinct values in the bid column, we can estimate\nthe reduction factor for the first conjunct.\nThis information is available in\nthe catalog if there is an index with bid as the search key; if not, optimizers\ntypically use a default value such as 1/10. Multiplying the reduction factors\nfor bid=5 and sid=3 gives us (under the simplifying independence assumption)\nthe fraction of tuples retrieved; if the index is clustered, this is also the fraction\nof pages retrieved. If the index is not clustered, each retrieved tuple could be\non a different page. (Review Section 8.4 at this time.)\nvVe estimate the reduction factor for a range condition such a.s day> 8/9/2002\nby assuming that values in the column cLre uniformly distributed. If there is a\nB\nt\nT\n'th k\nd\ntl\nd\nt'\nft'\nHigh(T)\n~ value\n+ ree\nWI\ney\nay,\n1e re uc IOn ac or IS H' ! (T)\nL· (T)'\ntg 1.\n-\nAJW\n12.3\nALGORITHMS FOR RELATIONAL OPERATIONS\nvVe now briefly discuss evaluation algorithms for the main relational operators.\n~'hile the important idea.s are introduced here, a more in-depth treatment is\ndeferred to Chapter 14.\nAs in Chapter 8, we consider only I/O costs and\nmea.'mre I/O costs in terms of the number of page I/Os. In this chapter, we\nuse detailed examples to illustrate how to compute the cost of an algorithm.\nAlthough we do not present rigorous cost formulas in this chapter, the reader\nshould be able to apply the underlying icleas to do cost calculations on other\nsimilar examples.\n\nOverview of Query Evaluation\n12.3.1\nSelection\n40)\nThe selection operation is a simple retrieval of tuples from a table, and its\nimplementation is essentially covered in our discussion of access paths.\nTo\nsummarize, given a selection of the form erRattr op value(R), if there is no index\non R.attr, we have to scan R.\nIf one or more indexes on R match the selection, we can use the index to re-\ntrieve matching tuples, and apply any remaining selection conditions to further\nrestrict the result set. As an example, consider a selection of the form rname\n< 'C%' on the Reserves table. Assuming that names are uniformly distributed\nwith respect to the initial letter, for simplicity, we estimate that roughly 10%\nof Reserves tuples are in the result.\nThis is a total of 10,000 tuples, or 100\npages. If we have a clustered B+ tree index on the rname field of Reserves, we\ncan retrieve the qualifying tuples with 100 l/Os (plus a few l/Os to traverse\nfrom the root to the appropriate leaf page to start the scan). However, if the\nindex is unclustered, we could have up to 10,000 l/Os in the worst case, since\neach tuple could cause us to read a page.\nAs a rule of thumb, it is probably cheaper to simply scan the entire table\n(instead of using an unclustered index) if over 5% of the tuples are to be\nretrieved.\nSec Section 14.1 for more details on implementation of selections.\n12.3.2\nProjection\nThe projection operation requires us to drop certain fields of the input, which\nis easy to do.\nThe expensive aspect of the operation is to ensure that no\nduplicates appear in the result. For example, if we only want the sid and bid\nfields from Reserves, we could have duplicates if a sailor has reserved a given\nboat on several days.\nIf duplicates need not be eliminated (e.g., the DISTINCT keyword is not in-\ncluded in the SELECT clause), projection consists of simply retrieving a subset\nof fields from each tuple of the input table. This can be accomplished by simple\niteration on either the table or an index whose key contains all necessary fields.\n(Note that we do not care whether the index is clustered, since the values we\nwant are in the data entries of the index itself!)\nIf we have to eliminate duplicates, we typically have to use partitioning. Sup-\npose we want to obtain (sid, bid') by projecting from Reserves. \\Ve can partition\nby (1) scanning H.eserves to obtain (sid, b'id'; pairs and (2) sorting these pairs\n\n402\nCHAPTER 12\nusing (s'id, bid) as the sort key.\n\"\"Ve can then scan the sorted pairs and easily\ndiscard duplicates, which are now adjacent.\nSorting large disk-resident datasets is a very important operation in database\nsystems, and is discussed in Chapter 13. Sorting a table typically requires two\nor three passes, each of which reads and writes the entire table.\nThe projection operation can be optimized by combining the initial scan of\nReserves with the scan in the first pass of sorting.\nSimilarly, the scanning\nof sorted pairs can be combined with the last pass of sorting. With such an\noptimized implemention, projection with duplicate elimination requires (1) a\nfirst pass in which the entire table is scanned, and only pairs (s'id, bid) are\nwritten out, and (2) a final pass in which all pairs are scanned, but only one\ncopy of each pair is written out. In addition, there might be an intermediate\npass in which all pairs are read from and written to disk.\nThe availability of appropriate indexes can lead to less expensive plans than\nsorting for duplicate elimination. If we have an index whose search key contains\nall the fields retained by the projection, we can sort the data entries in the\nindex, rather than the data records themselves. If all the retained attributes\nappear in a prefix of the search key for a clustered index, we can do even\nbetter; we can simply retrieve data entries using the index, and duplicates are\neasily detected since they are adjacent.\nThese plans are further examples\nof~\n'index-only evaluation strategies, which we discussed in Section 8.5.2.\nSee Section 14.3 for more details on implementation of projections.\n12.3.3\nJoin\nJoins are expensive operations and very common. Therefore, they have been\nwidely studied, and systems typically support several algorithms to carry out\njoins.\nConsider the join of Reserves and Sailors, with the join conclition Reserves.sid =\nSa'ilors.sid. Suppose that one of the tables, say Sailors, has an index on the\nsid column. \"\"Ve can scan Reserves and, for each tuple, use the index to pTObe\nSailors for matGhing tuples. This approach is called index nested loops join.\nSuppose that we have a ha.'3h-based index using Alternative (2) on the sid\nattribute of Sailors and that it takes about 1.2 1/0s on average:J to retrieve\nthe appropriate page of the index.\nSince s'id is a key for Sailors, we have at\n:IThis is a typical cost for hash-based indexes.\n\nOvenl'iew of Query E1.1aluat'ion\n403\nmost one matching tuple,\nIndeed, sid in Reserves is a foreign key referring\nto Sailors, and therefore we have exactly one matching Sailors tuple for each\nReserves tuple,\nLet us consider the cost of scanning Reserves and using the\nindex to retrieve the matching Sailors tuple for each Reserves tuple, The cost of\nscanning Reserves is 1000. There are 100 * 1000 tuples in Reserves. For each of\nthese tuples, retrieving the index page containing the rid of the matching Sailors\ntuple costs 1.2 I/Os (on average); in addition, we have to retrieve the Sailors\npage containing the qualifying tuple, Therefore, we have 100,000 * (1 + 1.2)\nI/Os to retrieve matching Sailors tuples. The total cost is 221,000 I/Os. 4\nIf we do not have an index that matches the join condition on either table, we\ncannot use index nested loops, In this case, we can sort both tables on the join\ncolumn, and then scan them to find matches. This is called sort-merge join..\nAssuming that we can sort Reserves in two passes, and Sailors in two passes\nas well, let us consider the cost of sort-merge join.\nConsider the join of the\ntables Reserves and Sailors. Because we read and write Reserves in each pass,\nthe sorting cost is 2 * 2 * 1000 = 4000 I/Os. Similarly, we can sort Sailors at a\ncost of 2*2*500 = 2000 I/Os. In addition, the second phase of the sort-merge\njoin algorithm requires an additional scan of both tables. Thus the total cost\nis 4000 + 2000 + 1000 + 500 = 7500 I/Os.\nObserve that the cost of sort-merge join, which does not require a pre-existing\nindex, is lower than the cost of index nested loops join, In addition, the result\nof the sort-merge join is sorted on the join column(s). Other join algorithms\nthat do not rely on an existing index and are often cheaper than index nested\nloops join are also known (block nested loops and hash joins; see Chapter 14).\nGiven this, why consider index nested loops at all?\nIndex nested loops has the nice property that it is incremental. The cost of our\nexample join is incremental in the number of Reserves tuples that we process.\nTherefore, if some additional selection in the query allows us to consider only\na small subset of Reserves tuples, we can avoid computing the join of Reserves\nand Sailors in its entirety. For instance, suppose that we only want the result\nof the join for boat 101, and there are very few such reservations.\nfor each\nsuch Reserves tuple, we probe Sailors, and we are clone. If we use sort-merge\njoin, on the other hand, we have to scan the entire Sailors table at least once,\nand the cost of this step alone is likely to be much higher than the entire cost\nof index nested loops join.\nObserve that the choice of index nested loops join is based on considering the\nquery as a whole, including the extra selection all Reserves, rather than just\n-~~~~-~---~---\n4 As an exercise, the reader should write formulas for the cost estimates in this example in terms\nof the properties\ne.g.• NPages-of the tables and indexes involved.\n\n404\nCHAPTER i2\nthe join operation by itself. This leads us to our next topic, query optimization,\nwhich is the process of finding a good plan for an entire query.\nSee Section 14.4 for more details.\n12.3.4\nOther Operations\nA SQL query contains group-by and aggregation in addition to the basic re-\nlational operations. Different query blocks can be combined with union, set-\ndifference, and set-intersection.\nThe expensive aspect of set operations such as union and intersection is du-\nplicate elimination, just like for projection. The approach used to implement\nprojection is easily adapted for these operations a..s well. See Section 14.5 for\nmore details.\nGroup-by is typically implemented through sorting. Sometimes, the input table\nhas a tree index with a search key that matches the grouping attributes. In this\ncase, we can retrieve tuples using the index in the appropriate order without\nan explicit sorting step. Aggregate operations are carried out using temporary\ncounters in main memory as tuples are retrieved.\nSee Section 14.6 for more\ndetails.\n12.4\nINTRODUCTION TO QUERY OPTIMIZATION\nQuery optimization is one of the most important tasks of a relational DBMS.\nOne of the strengths of relational query languages is the wide variety of ways in\nwhich a user can express and thus the system can evaluate a query. Although\nthis flexibility makes it easy to write queries, good performance relies greatly\non the quality of the query optimizer···a given query can be evaluated in many\nways, and the difference in cost between the best and worst plans may be\nseveral orders of magnitude. Realistically, we cannot exped to always find the\nbest plan, but we expect to consistently find a plan that is quite good.\nA more detailed view of the query optimization and execution layer in the\nDBMS architecture from Section 1.8 is shown in Figure 12.2.\nQueries are\nparsed and then presented to a query optimizer, which is responsible for\nidentifying an efficient execution plan.\nThe optimizer generates alternative\nplans and chooses the plan wit.h the least estimated cost.\nThe space of plans considered by a typical relational query optimizer can be\nunderstood by recognizing that a query is essentially treated as a a - Ii-\nCXJ\nalgebra c;r;prc88'lon, with the remaining operations (if any, in a given query)\n\nOverview of Qucry EvaJuat'lon\n4115\n•\nQUet}'\nI'--\"'---'-'~'-\"'-'--'-\nI\nQuery Parser\nI~__._.•.•_~~\n__\ntParsed query\nEvaluation plan\n---1\nr\nPlan Cost\nI Catalog\nEstimator\nI\nManager\nQuery Optimizer\nPlan\nQuery Plan Evaluator\nFigure 12.2\nQuery Parsing, Optimization, and Execution\nCommercialOptimizers: Current relational DBMS optimizers are very\ncomplex pieces of software with many closely guarded details, and they\ntypically represent 40 to 50 man-years of development effort!\ncarried out on the result of the\n(J\"\n-\n7f-\n[Xl expression.\nOptimizing such a\nrelational algebra expression involves two basic steps:\n•\nEnumerating alternative plans for evaluating the expression. Typically, an\noptimizer considers a subset of all possible plans because the number of\npossible plans is very large.\n•\nEstimating the cost of each enumerated plan and choosing the plan with\nthe lowest estimated cost.\nIn this section we lay the foundation for our discussion of query optimization\nby introducing evaluation plans.\n12.4.1\nQuery Evaluation Plans\nA query evaluation plan (or simply plan) consists of an extended relational\nalgebra tree, with additional annotations at each node indicating the access\nmethods to use for each table and the implementation method to use for each\nrelational operator.\nConsider the following SQL query:\n\n406\nSELECT\nFROM\nWHERE\nS.sname\nReserves R, Sailors S\nR.sid = S.sid\nAND R.bid = 100 AND S.rating > 5\nCHAPTERi2\nThis query can be expressed in relational algebra as follows:\n7fsname (O'bid=100/\\mting>5 (ReservesMsid=sidSailor s))\nThis expression is shown in the form of a tree in Figure 12.3.\nThe algebra\nexpression partially specifies how to evaluate the query-owe first compute the\nnatural join of Reserves and Sailors, then perform the selections, and finally\nproject the snarne field.\nITsname\nI\n0- bid=100 A rating> 5\nI\nReserves\nSailors\nFigure 12.3\nQuery Expressed as a Relational Algebra Tree\nTo obtain a fully specified evaluation plan, we must decide on an implemen-\ntation for each of the algebra operations involved.\n}or example, we can use\na page-oriented simple nested loops join with Reserves as the outer table and\napply selections and projections to each tuple in the result of the join as it is\nproduced; the result of the join before the selections and projections is never\nstored in its entirety. This query evaluation plan is shown in Figure 12.4.\nITsname\n(Orl-/he-}7y)\nI\nI\nO' bid=100 A rating> 5\n(Oll-Ihe-fly)\n[><.::J\n(Simple IIcslcd loops)\nsid=sid\n//~/\n(File SCOII)\nReserves\nSailors\n(File ,,'um)\nFigure 12.4\nQuery Evaluation Plan for Sample Query\n\nOverview of Query Eval'uation\n407\nJ\nIn drawing the query evaluation plan, we have used the convention that the\nouter table is the left child of the join operator.\nvVe adopt this convention\nhenceforth.\n12.4.2\nMulti-operator Queries: Pipelined Evaluation\nWhen a query is composed of several operators, the result of one operator is\nsometimes pipelined to another operator without creating a temporary table\nto hold the intermediate result. The plan in Figure 12.4 pipelines the output of\nthe join of Sailors and Reserves into the selections and projections that follow.\nPipelining the output of an operator into the next operator saves the cost of\nwriting out the intermediate result and reading it back in, and the cost sav-\nings can be significant. If the output of an operator is saved in a temporary\ntable for processing by the next operator, we say that the tuples are material-\nized. Pipelined evaluation has lower overhead costs than materialization and\nis chosen whenever the algorithm for the operator evaluation permits it.\nThere are many opportunities for pipelining in typical query plans, even simple\nplans that involve only selections..\nConsider a selection query in which only\npart of the selection condition matches an index. We can think of such a query\nas containing two instances of the selection operator: The first contains the\nprimary, or matching, part of the original selection condition, and the second\ncontains the rest of the selection condition.\nWe can evaluate such a query\nby applying the primary selection and writing the result to a temporary table\nand then applying the second selection to the temporary table.\nIn contrast,\na pipelined evaluation consists of applying the second selection to each tuple\nin the result of the primary selection as it is produced and adding tuples that\nqualify to the final result.\nWhen the input table to a unary operator (e.g.,\nselection or projection) is pipelined into it, we sometimes say that the operator\nis applied on-the-fly.\nAs a second and more general example, consider a join of the form (A CXJ B) 1><1\nC, shown in Figure 12.5 &'3 a tree of join operations.\nResult tuples\nof first join\npipelined into\njoin with C\nFigure 12.5\nA Query Tree Illustrating Pipelilling\n\n408\nCHAPTER\n~2\nBoth joins can be evaluated in pipelined fa.<;hion using some version of a nested\nloops join. Conceptually, the evaluation is initiated from the root, and the node\njoining A and B produces tuples as and when they are requested by its parent\nnode. 'When the root node gets a page of tuples from its left child (the outer\ntable), all the matching inner tuples are retrieved (using either an index or a\nscan) and joined with matching outer tuples; the current page of outer tuples\nis then discarded. A request is then made to the left child for the next page\nof tuples, and the process is repeated. Pipelined evaluation is thus a control\nstrategy governing the rate at which different joins in the plan proceed. It has\nthe great virtue of not writing the result of intermediate joins to a temporary\nfile because the results are produced, consumed, and discarded one page at a\ntime.\n12.4.3\nThe Iterator Interface\nA query evaluation plan is a tree of relational operators and is executed by\ncalling the operators in some (possibly interleaved) order. Each operator has\none or more inputs and an output, which are also nodes in the plan, and tuples\nmust be pa.<;sed between operators according to the plan's tree structure.\nTo simplify the code responsible for coordinating the execution of a plan, the\nrelational operators that form the nodes of a plan tree (which is to be evaluated\nusing pipelining) typically support a uniform iterator interface, hiding the\ninternal implementation details of each operator.\nThe iterator interface for\nan operator includes the functions open, geLnext, and close.\nThe open\nfunction initializes the state of the iterator by allocating buffers for its inputs\nand output, and is also used to pa..\"s in arguments such ac; selection conditions\nthat modify the behavior of the operator. The code for the get-next function\ncalls the get-next function on each input node and calls operator-specific code\nto process the input tuples.\nThe output tuples generated by the processing\nare placed in the output buffer of the operator, and the state of the iterator is\nupdated to keep track of how much input hac; been consumed. \\i\\1hen all output\ntuples have been produced through repeated calls to get-ne:rt, the close function\nis called (by the code that initiated execution of this operator) to deallocate\nstate information.\nThe iterator interface supports pipelining of results naturally: the decision to\npipeline or mat(~rialize input tuples is encapsulated in the operator-specific code\nthat processes input tuples. If the algorithm implemented for the operator\nallows input tuples to be processed completely when they are received, input\ntuples are not Inaterialized and the evaluation is pipelined. If the algorithm\nexamines the same input tuples several times, they are materialized.\nThis\n\nOv(;;'rvieu,) of query Eval'uaiion\n409\n~\ndecision, like other details of the operator's implementation, is hidden by the\niterator interface for the operator.\nThe iterator interface is also used to encapsulate access methods such as B+\ntrees and hash-ba.\"ied indexes. Externally, access methods can be viewed simply\nas operators that produce a stream of output tuples.\nIn this case, the open\nfunction can be used to pass the selection conditions that match the access\npath.\n12.5\nALTERNATIVE PLANS: A MOTIVATING EXAMPLE\nConsider the example query from Section 12.4.\nLet us consider the cost of\nevaluating the plan shown in Figure 12.4. We ignore the cost of writing out\nthe final result since this is common to all algorithms, and does not affect\ntheir relative costs. The cost of the join is 1000 + 1000 * 500 = 501,000 page\nl/Os. The selections and the projection are done on-the-fly and do not incur\nadditional l/Os.\nThe total cost of this plan is therefore 501,000 page l/Os.\nThis plan is admittedly naive; however, it is possible to be even more naive by\ntreating the join as a cross-product followed by a selection.\nvVe now consider several alternative plans for evaluating this query. Each al-\nternative improves on the original plan in a different way and introduces some\noptimization idea.<; that are examined in more detail in the rest of this chapter.\n12.5.1\nPushing Selections\nA join is a relatively expensive operation, and a good heuristic is to reduce\nthe sizes of the tables to be joined as much as possible.\nOne approach is to\napply selections early; if a selection operator appears after a join operator, it is\nworth examining whether the selection can be 'pushed' ahead of the join. As\nan example, the selection bid=1()(} involves only the attributes of Reserves and\ncan be applied to Reserves befoTe the join. Similarly, the selection mting> 5\ninvolves only attributes of Sailors and can be applied to Sailors before the join.\nLet us suppose that the selections are performed using a simple file scan, that\nthe result of each selection is written to a temporary table on disk, and that\nthe temporary tables are then joined using a sort-merge join.\nThe resulting\nquery evaluation plan is shown in Figure 12.6.\nLet us assume that five buffer pages are available and estimate the cost of\nthis query evaluation plan.\n(It is likely that more buffer pages are available\nin practice. vVe chose a small number simply for illustration in this example.)\nThe cost of applying bid=100 to Reserves is the cost of scanning Reserves\n(1000 pages) plus the cost of writing the result to a temporary table, say Tl.\n\n410\nTTsname\n(On-the-fly)\nC><J\n(Sort-merge join)\nsid=sid\n(Scan;\nU bid=100\nUrating > 5\n(Scan;\nwrite to\nwrite to\ntemp Tl)\nI\nI\ntemp 12)\nFile scan\nReserves\nSailors\nFile scan\nFigure 12.6\nA Second Query Evaluation Plan\nCHAPTER J2\n(Note that the cost of writing the temporary table cannot be ignored-we can\nignore only the cost of writing out the final result of the query, which is the\nonly component of the cost that is the same for· all plans.)\nTo estimate the\nsize of Tl, we require additional information. For example, if we assume that\nthe maximum number of reservations of a given boat is one, just one tuple\nappears in the result. Alternatively, if we know that there are 100 boats, we\ncan assume that reservations are spread out uniformly across all boats and\nestimate the number of pages in Tl to be 10. For concreteness, assume that\nthe number of pages in Tl is indeed 10.\nThe cost of applying rating > 5 to Sailors is the cost of scanning Sailors (500\npages) plus the cost of writing out the result to a temporary table, say, T2. If\nwe assume that ratings are uniformly distributed over the range 1 to 10, we\ncan approximately estimate the size of T2 as 250 pages.\nTo do a sort-merge join of Tl and T2, let us assume that a straightforward\nimplementation is used in which the two tables are first completely sorted and\nthen merged. Since five buffer pages are available, we C8Jl sort Tl (which ha..s\n10 pages) in two pa..'3ses. Two runs of five pages each are produced in the first\npass and these are merged in the second pass. In each pass, we read and write\n10 pages; thus, the cost of sorting Tl is 2 * 2 * 10 = 40 page l/Os. We need\nfour pa..'3ses to sort T2, which ha..s 250 pages. The cost is 2 * 4 * 250 = 2000\npage l/Os. To, merge the sorted versions of Tl and T2, we need to scan these\ntables, and the cost of this step is 10 + 250 = 260. The final projection is done\non-the-fly, and by convention we ignore the cost of writing the final result.\nThe total cost of the plan shown in Figure 12.6 is the sum of the cost of the\nselection (1000+10+500+250 = 1760) and the cost of the join (40+2000+260 =\n23(0), that is, 4060 page l/Os.\n\nOverview of query Evaluation\n411\nSort-merge join is one of several join methods. \\Ve may be able to reduce the\ncost of this plan by choosing a different join method. As an alternative, suppose\nthat \\ve used block nested loops join instead of sort-merge join.. Using T1 as\nthe outer table, for every three-page block of T1, we scan all of T2; thus, we\nscan T2 four times. The cost of the join is therefore the cost of scanning T1\n(10) plus the cost of scanning T2 (4 * 250 = 1000). The cost of the plan is now\n1760 + 1010 = 2770 page I/Os.\nA further refinement is to push the projection, just like we pushed the selec-\ntions past the join. Observe that only the sid attribute of T1 and the sid and\nsname attributes of T2 are really required. As we scan Reserves and Sailors to\ndo the selections, we could also eliminate unwanted columns. This on-the-fly\nprojection reduces the sizes of the temporary tables T1 and T2. The reduction\nin the size of T1 is substantial because only an integer field is retained. In fact,\nT1 now fits within three buffer pages, and we can perform a block nested loops\njoin with a single scan of T2. The cost of the join step drops to under 250 page\nI/Os, and the total cost of the plan drops to about 2000 I/Os.\n12.5.2\nUsing Indexes\nIf indexes are available on the Reserves and Sailors tables, even better query\nevaluation plans may be available. For example, suppose that we have a clus-\ntered static hash index on the bid field of Reserves and another hash index on\nthe sid field of Sailors. We can then use the query evaluation plan shown in\nFigure 12.7.\n(Use hash\nindex; do\nnot write\nresult 10\nTemp)\nHash index on bid\nITsname\n(Jrating > 5\nsid=sid\nI\nReserves\n(Oil-the-fly)\n(OIl-the-f1y)\n(Illdex Ilested loops.\nwith pipelilling )\nSailors\nHash illdex all sid\nFigure 12.7\nA Query Evaluation Plan Using Indexes\nThe selection bid.=100 is performed on Reserves by using the hash index on\nbid to retrieve only matching tuples. As before, if we know that 100 boats are\navailable and assume that reservations are spread out uniformly across all boats,\n\n412\nCHAPTER 12\n\\ve can estimate the number of selected tuples to be 100, 000/100 = lOOO. Since\nthe index on b'id is clustered, these 1000 tuples appear consecutively within the\nsame bucket; therefore, the cost is 10 page l/Os.\n:For each selected tuple, we retrieve matching Sailors tuples using the hash index\non the sid field; selected Reserves tuples are not materialized and the join is\npipelined.\nFor each tuple in the result of the join, we perform the selection\nmting>5 and the projection of sname on-the-fly. There are several important\npoints to note here:\n1. Since the result of the selection on Reserves is not materialized, the opti-\nmization of projecting out fields that are not needed subsequently is un-\nnecessary (and is not used in the plan shown in Figure 12.7).\n2. The join field sid is a key for Sailors. Therefore, at most one Sailors tuple\nmatches a given Reserves tuple. The cost of retrieving this matching tuple\ndepends on whether the directory of the hash index on the sid column of\nSailors fits in memory and on the presence of overflow pages (if any). How-\never, the cost does not depend on whether this index is clustered because\nthere is at most one matching Sailors tuple and requests for Sailors tuples\nare made in random order by sid (because Reserves tuples are retrieved by\nbid and are therefore considered in random order by sid). For a hash index,\n1.2 page l/Os (on average) is a good estimate of the cost for retrieving a\ndata entry. Assuming that the sid hash index on Sailors uses Alternative\n(1) for data entries, 1.2 l/Os is the cost to retrieve a matching Sailors tu-\nple (and if one of the other two alternatives is used, the cost would be 2,2\nl/Os).\n3. vVe have chosen not to push the selection mt'ing>5 ahead of the join, and\nthere is an important reason for this decision. If we performed the selection\nbefore the join, the selection would involve scanning Sailors, assuming that\nno index is available on the mt'ing field of Sailors.\nFurther, whether or\nnot such an index is available, once we apply such a selection, we have\nno index on the sid field of the result of the selection (unless we choose\nto build such an index solely for the sake of the subsequent join). Thus,\npushing selections ahead of joins is a good heuristic, but not always the\nbest strategy. Typically, as in this example, the existence of useful indexes\nis the reason a selection is not pushed. (Otherwise, selections are pushed.)\nLet us estimate the cost of the plan shown in Figure 12.7.\nThe selection of\nReserves tuples costs 10 l/Os, as we saw earlier. There are 1000 such tuples,\nand for each, the cost of finding the matching Sailors tuple is 1.2 l/Os, on\naverage. The cost of this step (the join) is therefore 1200 l/Os. All remaining\nselections and projections are performed on~the-fly. The total cost of the plan\nis 1210 l/Os.\n\nOverview of quer7! Evaluation\n413\nAs noted earlier, this plan does not utilize clustering of the Sailors index. The\nplan can be further refined if the index on the sid field of Sailors is clustered.\nSuppose we materialize the result of performing the selection bid=100 on Re-\nserves and sort this temporary table. This table contains 10 pages. Selecting\nthe tuples costs 10 page l/Os (as before), writing out the result to a temporary\ntable costs another 10 l/Os, and with five buffer pages, sorting this temporary\ncosts 2 * 2 * 10 = 40 l/Os.\n(The cost of this step is reduced if we push the\nprojection on sid. The sid column of materialized Reserves tuples requires only\nthree pages and can be sorted in memory with five buffer pages.) The selected\nReserves tuples can now be retrieved in order by 8'id.\nIf a sailor has reserved the same boat many times, all corresponding Reserves\ntuples are now retrieved consecutively; the matching Sailors tuple will be found\nin the bufFer pool on all but the first request for it. This improved plan also\ndemonstrates that pipelining is not always the best strategy.\nThe combination of pushing selections and using indexes illustrated by this\nplan is very powerful. If the selected tuples from the outer table join with a\nsingle inner tuple, the join operation may become trivial, and the performance\ngains with respect to the naive plan in Figure 12.6 are even more dramatic.\nThe following variant of our example query illustrates this situation:\nSELECT\nFROM\nWHERE\nS.sname\nReserves R, Sailors S\nRsid = S.sid\nAND R.bid = 100 AND S.rating > G\nAND Rday = '8/9/2002'\nA slight variant of the plan shown in Figure 12.7, designed to answer this query,\nis shown in Figure 12.8. The selection day='8/9/2002' is applied on-the-fly to\nthe result of the selection bid=100 on the Reserves table.\nSuppose that bid and day form a key for Reserves. (Note that this assumption\ndiffers from the schema presented earlier in this chapter.) Let us estimate the\ncost of the plan shown in Figure 12.8.\nThe selection bid=100 costs 10 page\nl/Os, as before, and the additional selection day='8j.9/2002' is applied on-the-\nfly, eliminating all but (at most) one Reserves tuple.\nThere is at most one\nrnatching Sailors tuple, and this is retrieved in 1.2 l/Os (an average value).\nThe selection on rrding and the projection on sname are then applied on-the-\nfly at no additional cost. The total cost of the plan in Figure 12.8 is thus about\n11 I/Os. In contrast, if we modify the naive plan in Figure 12.6 to perform\nthe additional selection on day together with the selection bid=100, the cost\nremains at 501,000 l/Os.\n\n414\nTT sname\n1><1\nIO\"-the-jly)\nIOn-lhe-jlyj\nIIlIde.' lIested [oops,\nwilhl'il'eli\"i\"g)\nCHAPTER J2\n(On·the-fly)\n(Use hash\nindex; do\nf10twrite\nresult to\ntemp)\naday='819194'\nI\na bid=100\nSailors\nHash i\"dex 0\" sid\nHash illdex 011 bid\nReserves\nFigure 12.8\nA Query Evaluation Plan for the Second Example\n12.6\nWHAT A TYPICAL OPTIMIZER DOES\nA relational query optimizer uses relational algebra equivalences to identify\nmany equivalent expressions for a given query. For each such equivalent ver-\nsion of the query, all available implementation techniques are considered for the\nrelational operators involved, thereby generating several alternative queryeval-\nuation plans. The optimizer estimates the cost of each such plan and chooses\nthe one with the lowest estimated cost.\n12.6.1\nAlternative Plans Considered\nTwo relational algebra expressions over the same set of input tables are said\nto be equivalent if they produce the same result on all instances of the in-\nput tables.\nRelational algebra equivalences playa central role in identifying\nalternative plans.\nConsider a basic SQL query consisting of a SELECT clause, a FROM clause, and\na WHERE clause, This is easily represented as an algebra expression; the fields\nmentioned in the SELECT are projected from the cross-product of tables in\nthe FROM clause, after applying the selections in the WHERE clause.\nThe use\nof equivalences enable us to convert this initial representation into equivalent\nexpressions. In particular:\n•\nSelections and cross-products can be combined into joins.\n•\nJoins can be extensively reordered.\n\nOverview of quer7J Evalnation\n•\nSelections and projections, which reduce the size of the input, can be\n\"pushed\" ahead of joins.\nThe query discussed in Section 12.5 illustrates these points; pushing the selec-\ntion in that query ahead of the join yielded a dramatically better evaluation\nplan. \\Ve discuss relational algebra equivalences in detail in Section 15.3.\nLeft-Deep Plans\nConsider a query of the form A\n[Xl B\n[Xl C [Xl D; that is, the natural join of\nfour tables. Three relational algebra operator trees that are equivalent to this\nquery (based on algebra equivalences) are shown in Figure 12.9. By convention,\nthe left child of a join node is the outer table and the right child is the inner\ntable.\nBy adding details such as the join method for each join node, it is\nstraightforward to obtain several query evaluation plans from these trees.\nC><J\n/~\nC><J\nD\n/~\nC><J\nc\n/~\nA\nB\nC><J\n/~\nI><J\nI><J\n/~\n/~\nABC\nD\nFigure 12.9\nThree Join Trees\nThe first two trees in Figure 12.9 are examples of linear trees. In a linear tree,\nat least one child of a join node is a base table. The first tree is an example of\na left-deep tree-the right child of each join node is a base table. The third\ntree is an example of a non-linear or bushy tree.\nOptimizers typically use a dynamic-programming approach (see Section 15.4.2)\nto efficiently search the class of aU left-deep plans. The second and third kinds\nof trees are therefore never considered. Intuitively, the first tree represents a\nplan in which we join A and B first, then join the result with C, then join\nthe result with D. There are\n2~35 other left-deep plans that differ only in the\norder that tables are joined. If any of these plans has selection and projection\nconditions other than the joins themselves, these conditions are applied as early\nas possible (consitent with algebra equivalences) given the choice of a join order\nfor the tables.\nOf course, this decision rules out many alternative plans that may cost less\nthan the best plan using a left-deep tree; we have to live with the fact that\n\"The reader should think through the number 2:3 in this example.\n\n416\nCHAPTER ,12\nthe optimizer will never find such plans. There are two main reasons for this\ndecision to concentrate on left-deep plans, or plans ba.<;ed on left-deep trees:\n1. As the number of joins increases, the number of alternative plans increa..:.;es\nrapidly and it becomes necessary to prune the space of alternat.ive plans.\n2. Left-deep trees allow us to generate all fully pipelined plans; that is,\nplans in which all joins are evaluated using pipelining. (Inner tables must\nalways be materialized because we must examine the entire inner table for\neach tuple of the outer table.\nSo, a plan in which an inner table is the\nresult of a join forces us to materialize the result of that join.)\n12.6.2\nEstimating the Cost of a Plan\nThe cost of a plan is the sum of costs for the operators it contains. The cost\nof individual relational operators in the plan is estimated using information,\nobtained from the system catalog, about properties (e.g., size, sort order) of\ntheir input tables. We illustrated how to estimate the cost of single-operator\nplans in Sections 12.2 and 12.3, and how to estimate the cost of multi-operator\nplans in Section 12.5.\nIf we focus on the metric of I/O costs, the cost of a plan can be broken down\ninto three parts: (1) reading the input tables (possibly rnultiple times in the\ncase of some join and sorting algorithms), (2) writing intermediate tables, and\n(possibly) (3) sorting the final result (if the query specifies duplicate elimination\nor an output order). The third part is common to all plans (unless one of the\nplans happens to produce output in the required order), and, in the common\ncase that a fully-pipelined plan is chosen, no intermediate tables are written.\nThus, the cost for a fully-pipelined plan is dominated by part (1). This cost\ndepends greatly on the access paths used to read input tables; of course, access\npaths that are used repeatedly to retrieve matching tuples in a join algorithm\nare especially important.\nFor plans that are not fully pipelined, the cost of rnaterializing temporary tables\ncan be significant.\nThe cost of materializing an intermediate result depends\non its size, and the size also infiuences the cost of the operator for which the\ntemporary is hn input table. The number of tuples in the result of a selection is\nestimated by multiplying the input size by the reduction factor for the selection\nconditions. The number of tuples in the result of a projection is the same as\nthe input, a.ssuming that duplicates are not eliminated; of course, each result\ntuple is smaller since it contains fewer fields.\n\nOverview of q1lery Eval'llat'ion\n417\nThe result size for a join can be estimated by multiplying the maximum result\nsize, which is the product of the input table sizes, by the reduction factor of the\njoin condition. The reduction factor for join condition columni = column2 can\nbe approximated by the formula ~(NJ{eY,~(~1),NKeys(I2)) if there are indexes\n11 and 12 on columni and colwnn2, respectively. This formula assumes that\neach key value in the smaller index, say 11, has a matching value in the other\nindex.\nGiven a value for columni, we assume that each of the NKeys(I2)\nvalues for column2 is equally likely. Thus, the number of tuples that have the\nsame value in column2 as a given value in columni is N K e~s(I2) .\n12.7\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nII\nWhat is metadata? What metadata is stored in the system catalog? De-\nscribe the information stored per relation, and per index. (Section 12.1)\nII\nThe catalog is itself stored as a collection of relations. Explain why. (Sec-\ntion 12.1)\nII\nWhat three techniques are commonly used in algorithms to evaluate rela-\ntional operators? (Section 12.2)\nII\nWhat is an access path? When does an index match a search condition?\n(Section 12.2.2)\nII\nWhat are the main approaches to evaluating selections? Discuss the use of\nindexes, in particular. (Section 12.3.1)\nII\nWhat are the main approaches to evaluating projections?\nWhat makes\nprojections potentially expensive? (Section 12.3.2)\nII\nWhat are the main approaches to evaluating joins? Why are joins expen-\nsive? (Section 12.3.3)\nII\nWhat is the goal of query optimization? Is it to find the best plan? (Sec-\ntion 12.4)\nII\nHow does a DBMS represent a relational query evaluation plan?\n(Sec-\ntion 12.4.1)\nII\nWhat is pipelined evaluation? What is its benefit? (Section 12.4.2)\nII\nDescribe the iterator interface for operators and access methods. 'What is\nits purpose? (Section 12.4.3)\n\n418\nCHAPTER 12\n•\nDiscuss why the difference in cost between alternative plans for a query CI:Ul\nbe very large. Give specific examples to illustrate the impact of pushing\nselections, the choice of join methods, and the availability of appropriate\nindexes. (Section 12.5)\n•\nWhat is the role of relational algebra equivalences in query optimization?\n(Section 12.6)\n•\nWhat is the space of plans considered by a typical relational query opti-\nmizer? Justify the choice of this space of plans. (Section 12.6.1)\n•\nHow is the cost of a plan estimated? What is the role of the system catalog?\nWhat is the selectivity of an access path, and how does it influence the cost\nof a plan? Why is it important to be able to estimate the size of the result\nof a plan? (Section 12.6.2)\nEXERCISES\nExercise 12.1 Briefly answer the following questions:\n1. Describe three techniques commonly used when developing algorithms for relational op-\nerators. Explain how these techniques can be used to design algorithms for the selection,\nprojection, and join operators.\n2. What is an access path? When does an index match an access path? What is a primar1J\nconj1Lnct, and why is it important?\n3. What information is stored in the system catalogs?\n4. What are the benefits of making the system catalogs be relations?\n5. What is the goal of query optimization? Why is optimization important?\n6. Describe pipelining and its advantages.\n7. Give an example query and plan in which pipelining cannot be used.\n8. Describe the itemto1' interface and explain its advantages.\n9. What role do statistics gathered from the database play in query optimization?\n10. What were the important design decisions made in the System R optimizer?\n11. Why do query optimizers consider only left-deep join trees? Give an example of a query\nand a plan that would not be considered because of this restriction.\nExercise 12.2 Consider a relation R( a,b,e,.d,e) containing 5,000,000 records, where each data\npage of the relation holds 10 records. R is organized as a sorted file with secondary indexes.\nAssume that R.a is a candidate key for R, with values lying in the range 0 to 4,999,999, and\nthat R is stored in R.o, order. For each of the following relational algebra queries, state which\nof the following three approaches is most likely to be the cheapest:\n•\nAccess the sorted file for R directly.\n•\nUse a (clustered) B+ tree index on attribute R.o,.\n•\nUsc a linear hashed index on attribute R.a..\n\nOverview of Query Evaluation\n1. (7a<50,000(R)\n2.\n(Ta=50,OOO (R)\n3.\n(Ta>50,000Ao<50,OlO (R)\n4.\n(Ta;>'50,000 (R)\n419\nExercise 12.3 For each of the following SQL queries, for each relation involved, list the\nattributes that must be examined to compute the answer. All queries refer to the following\nrelations:\nEmp(eid: integer, did: integer, sal: integer, hobby: char(20))\nDept(did: integer, dname: char(20), floor: integer, budget: real)\n1. SELECT * FROM Emp\n2. SELECT * FROM Emp, Dept\n3, SELECT * FROM Emp E, Dept D WHERE E.did = D.did\n4. SELECT E.eid, D,dname FROM Emp E, Dept D WHERE E.did = D.did\nExercise 12.4 Consider the following schema with the Sailors relation:\nSailors(sid: integer, sname: string, rating: integer, age: real)\nFor each of the following indexes, list whether the index matches the given selection conditions.\nIf there is a match, list the primary conjuncts.\n1. A B+-tree index on the search key ( Sailors.sid ).\n(a) (7Sailors.sid<50,OOO (Sailor s)\n(b) (7Sailor.uid=f,o,ooo(Sailors)\n2. A hash index on the search key ( Sailors.sid ).\n(a) O'Sailo'·s.sid<50,OOO (Sailors)\n(b) (7Sailon.S1d=5o,ooo(Sailors)\n3. A B+-tree index on the search key ( Sailors.sid, Sailors.age ).\n(a) O'Sallors.8icl<50,OOOASai.loT's.ag,,=21 (Sailors)\n(b) O'Sailor.5.si.d=.SO,OOOASallors.age>21 (Sailors)\n(c) (7Sai/oTS.sid=5o,ooo(Sailors)\n(d) 0'!3ai/o·rs.ag('=21(Sailors)\n4. A ha.'lh-tree hidex on the search key ( Sailors.sid, Sailors.age ).\n(a) O'S\"Il\",·s.sid=50,OOOASo.ilors.ag,,=21 (Sailors)\n(b)\nO'S\",i/01·s ..,i.d=50,O(JOAS,,·i!or.,.age>21 (Sailors)\n(c) O's'd{ors,,\"d=5o,ooo(Sailors)\n(d) O'S'''/01's.<l.()''=21 (Sa'ilors)\nExercise 12.5 Consider again the schema with the Sailors relation:\n\n420\nSailors(sid: integer, sname: string, mUng: integer, age: real)\nCHAPTERfi12\nAssume that each tuple of Sailors is 50 bytes long, that a page can hold 80 Sailors tuples, and\nthat we have 500 pages of such tuples. For each of the following selection conditions, estimate\nthe number of pages retrieved, given the catalog information in the question.\n1. Assume that we have a B+-tree index 7' on the search key ( Sailors.sid ), and assume\nthat IHeight(T) = 4, INPages(T) = 50, Low(7')\n=\n1, and High(T)\n=\n100,000.\n(a) aSailors.'id<5o,ooo(S'a'ilors)\n(b) aSailorssid=50,OOO(Sa'ilors)\n2. Assume that we have a hash index 7' on the search key ( Sailors.sid ), and assume that\nIHeight(7') = 2, INPages(7') = 50, Low(7') = 1, and High(T)\n=\n100,000.\n(a) aSa'lor's.sid<50,OOo(Sailors)\n(b) aSailor·s.sid=5o,ooo(5ailors)\nExercise 12.6 Consider the two join methods described in Section 12.3.3. Assume that we\njoin two relations Rand 5, and that the systems catalog contains appropriate statistics about\nRand S. Write formulas for the cost estimates of the index nested loops join and sort-merge\njoin using the appropriate variables from the systems catalog in Section 12.1. For index nested\nloops join, consider both a B+ tree index and a hash index.\n(For the hash index, you can\nassume that you can retrieve the index page containing the rid of the matching tuple with\n1.2 l/Os on average.)\nNote.' Additional exercises on the material covered in this chapter can be found in the exercises\nfor Chapters 14 and 15.\nBIBLIOGRAPHIC NOTES\nSee the bibliograpic notes for Chapters 14 and 15.\n\n13\nEXTERNAL SORTING\n...\nWhy is sorting important in a DBMS?\n...\nWhy is sorting data on disk different from sorting in-memory data?\n...\nHow does external merge-sort work?\n...\nHow do techniques like blockecl I/O and overlapped I/O affect the\ndesign of external sorting algorithms?\n...\nWhen can we use a B+ tree to retrieve records in sorted order?\n..\nKey concepts: motivation, bulk-loading, duplicate elimination, sort-\nmerge joins; external merge sort, sorted runs, merging runs; replace-\nment sorting, increasing run length; I/O cost versus number of I/Os,\nblocked I/Os, double buffering; B+ trees for sorting, impact of clus-\ntering.\nGood order is the foundation of all things.\nEdmund Burke\nIn this chapter, we consider a widely used and relatively expensive operation,\nsorting records according to a search key. vVe begin by considering the lnany\nuses of sorting In a database system in Section 13.1. \\;Ye introduce the idea of\nexternal sorting by considering a very simple algorithm in Section 1:3.2; using\nrepeated passes over the data, even very large datasets can be sorted with a\nsmall amount of rnemory. This algol'ithrn is generalized to develop a realistic\nexternal sorting algorithrn in Section 1:3.3.\nThree important refinements are\n421\n\n422\nCHAPTER\n1~~\ndiscussed. The first, discussed in Section 13.3.1, enables us to reduce the num-\nber of passes. The next two refinements, covered in Section 13.4, require us\nto consider a more detailed model of I/O costs than the number of page I/Os.\nSection 13.4.1 discusses the effect of blocked I/O, that is, reading and writing\nseveral pages at a time; and Section 13.4.2 considers how to use a technique\ncalled double buffering to minimize the time spent waiting for an I/O operation\nto complete. Section 13.5 discusses the use of B+ trees for sorting.\nWith the exception of Section 13.4, we consider only I/O costs, which we ap-\nproximate by counting the number of pages read or written, as per the cost\nmodel discussed in Chapter 8. Our goal is to use a simple cost model to convey\nthe main ideas, rather than to provide a detailed analysis.\n13.1\nWHEN DOES A DBMS SORT DATA?\nSorting a collection of records on some (search) key is a very useful operation.\nThe key can be a single attribute or an ordered list of attributes, of course.\nSorting is required in a variety of situations, including the following important\nones:\nII\nUsers may' want answers in some order; for example, by increa..\"iing age\n(Section 5.2).\nII\nSorting records is the first step in bulk loading a tree index (Section 10.8.2).\nII\nSorting is useful for eliminating duplicate copies in a collection of records\n(Section 14.3).\n\nExternal Sorting\n•\nA widely used algorithm for performing a very important relational algebra\noperation, called jo'in, requires a sorting step (Section 14.4.2).\nAlthough main memory sizes are growing rapidly the ubiquity of database\nsystems has lead to increasingly larger datasets as well.\n'\\Then the data to\nbe sorted is too large to fit into available main memory, we need an external\nsorting algorithm. Such algorithms seek to minimize the cost of disk accesses.\n13.2\nA SIMPLE TWO-WAY MERGE SORT\nWe begin by presenting a simple algorithm to illustrate the idea behind external\nsorting.\nThis algorithm utilizes only three pages of main memory, and it is\npresented only for pedagogical purposes.\nIn practice, many more pages of\nmemory are available, and we want our sorting algorithm to use the additional\nmemory effectively; such an algorithm is presented in Section 13.3.\nWhen\nsorting a file, several sorted subfiles are typically generated in intermediate\nsteps. In this chapter, we refer to each sorted subfile as a run.\nEven if the entire file does not fit into the available main memory, we can sort\nit by breaking it into smaller subfiles, sorting these subfiles, and then merging\nthem using a minimal amount of main memory at any given time. In the first\npass, the pages in the file are read in one at a time. After a page is read in,\nthe records on it are sorted and the sorted page (a sorted run one page long) is\nwritten out. Quicksort or any other in-memory sorting technique can be used\nto sort the records on a page.\nIn subsequent passes, pairs of runs from the\noutput of the previous pass are read in and merged to produce runs that are\ntwice as long. This algorithm is shown in Figure 13.1.\nIf the number of pages in the input file is 2k , for some k, then:\nPass 0 produces 2k sorted runs of one page each,\nPass 1 produces 2k~1 sortecl runs of two pages each,\nPass 2 produces 2k - 2 sortecl runs of four pages each,\nand so on, until\nPass k produces one sorted run of 2k: pages.\nIn each pass, we read every page in the file, process it, and write it out.\nTherefore we have two disk I/Os per page, per pass.\nThe number of passes\nis flog2Nl -+- 1, where N is the number of pages in the file. The overall cost is\n2N( ilog2Nl + 1) l/Os.\nThe algorithm is illustrated on all example input file containing seven pages\nin Figure 13.2.\nThe sort takes four passes, and in each pass, we read and\n\n424\nCHAPTER\nl~\nproc 2-'ulay_cxtsort (file)\n/ / Oiven a file on disk) sorts it 'using three buffeT' pages\n/ / Produce runs that are one page long: Pass 0\nRead each page into memory, sort it, write it out.\n/ / Merge pairs of runs to produce longer runs until only\n/ / one run (containing all records of input file) is left\n\\Vhile the number of runs at end of previous pass is > 1:\n/ / Pass i = 1, 2, ...\nWhile there are runs to be merged from previous pass:\nChoose next two runs (from previous pass).\nRead each run into an input buffer; page at a time.\nMerge the runs and write to the output buffer;\nforce output buffer to disk one page at a time.\nendproc\nFigure 13.1\nTwo-Way Merge Sort\nwrite seven pages, for a total of 56 l/Os. This result agrees with the preceding\nanalysis because 2· 7( llo92 71 +1)\n=\n56. The dark pages in the figure illustrate\nwhat would happen on a file of eight pages; the number of passes remains at\nfour (llo9281 + 1 =\n4), but we read and write an additional page in each pass\nfor a total of 64 l/Os. (Try to work out what would happen on a file with, say,\nfive pages.)\nThis algorithm requires just three buffer pages in lnain memory,\nCl\"S Figure 13.3\nillustrates. This observation raises an important point: Even if we have more\nbuffer space available, this simple algorithm does not utilize it effectively. The\nexternal merge sort algorithm that we discuss next addresses this problem.\n13.3\nEXTERNAL MERGE SORT\nSuppose that 13 buffer pages are available in memory and that we need to sort\na large file with N pages.\nHow can we improve on the t\\vo-way merge sort\npresented in the previous section? The intuition behind the generalized algo-\nrithm that we now present is to retain the basic structure of making multiple\npasses while trying to minimize the number of passes. There are two important\nmodifications to the two-way merge sort algorithm:\n1. In Pass 0, read in 13 pages at a time and sort internally to produce IN/131\nruns of 13 pages each (except for the last run, which lnay contain fewer\n\nRTtemal Sorting\nI\nInputfile\nPASS\n0\nI-page runs\n~-,\nPASS\n2-page runs\n-----C~--~,.._L----------\"'>....,__---__O>\"L.---~PASS\n2\n4-page runs\n---------\"\"'----=::-------=-~-------PASS3\n1,2\n2,3\n425\n;~\n3,4\n4,5\n6,6\n7,8\n8-page runs\nFigure 13.2\nTwo-Way Merge Sort of a Seven-Page File\nc\n~\nr:=:-\n-------\n[iNPUT 1 I~I\nOUTPUT )\n..\n[!NPUT2 1/\nDisk\nMain memory buffers\nDisk\nFigure 13.3\nTwo-'Way Merge Sort with Three Buffer Pages\n\n426\nCHAPTER~3\npages). This modification is illustrated in Figure 13.4, using the input file\nfrom Figure 13.2 and a buffer pool with four pages.\n2. In passes i = 1,2, ... use B-1 buffer pages for input and use the remaining\npage for output; hence, you do a (B - I)-way merge in each pass.\nThe\nutilization of buffer pages in the merging passes is illustrated in Figure\n13.5.\n2,3\n8,9\n2nd output run\n4,4\n6 , 7\n1st output run\n!\n'2\n3,5\n6\nBuffer pool with B:::4 pages\nInput file\nFigure 13.4\nExternal Merge Sort with B Buffer Pages: Pass 0\n¢\n¢\n¢\nDisk\n! ~UTl ~\nIINPUT2 I\n> I~I\nOUTPUT\nB main memory buffers\n¢\n¢\n¢\nDisk\nFigure 13.5\nExternal IVlerge Sort with B Buffer Pages: Pass 'i > 0\nThe first refinement reduces the number of runs produced by Pass 0 to NI\nrN / Bl, versus N for the two-way merge. l The second refinement is even more\nimportant.\nBy doing a (B ~ I)-way merge, the number of passes is reduced\ndramatically\nincluding the initial pass, it becomes rZ0.9B- 1NIl + 1 versus\n[loY2Nl + 1 for the two-way merge algorithm presented earlier. Because B is\n1Note that the technique used for sorting data in buffer pages is orthogonal to external sorting.\nYou could use, say, Quicksort for sorting data in buffer pages.\n\nExternal Sorting\n427\ntypically quite large, the savings can be substantial. The external merge sort\nalgorithm is shown is Figure 13.6.\nproc extsort (file)\n/ / Given a file on disk, sorts it using three buffer pages\n/ / Produce runs that are B pages long: Pass 0\nRead B pages into memory, sort them, write out a run.\n/ / Merge B-1 runs at a time to produce longer runs until only\n/ / one run (containing all records of input file) is left\nWhile the number of runs at end of previous pass is > 1:\n// Pass i = 1,2, ...\nWhile there are runs to be merged from previous pass:\nChoose next B ~ 1 runs (from previous pass).\nRead each rUll into an input buffer; page at a time.\nMerge the rUllS and write to the output buffer;\nforce output buffer to disk one page at a time.\nendproc\nFigure 13.6\nExternal Merge Sort\nAs an example, suppose that we have five buffer pages available and want to\nsort a file with lOS pages.\nPac'Ss 0 produces POS/51\n=\n22 sorted runs of five pages each, except\nfor the last run, which is only three pages long.\nPass 1 does a four-way merge to produce 122/41\n= six sorted runs of\n20 pages each, except for the iast run, which is only eight pages long.\nPass 2 produces 16/41 = two sorted runs; one with SO pages and one\nwith 28 pages.\nPass 3 merges the two runs produced in Pass 2 to produce the sorted\nfile.\nIn each pass we read and write 108 pages; thus the total cost is 2* 108*4 = 864\nl/Os.\nApplying our formula, we have Nl\n1108/51\n22 and cost\n2 * N * (llogB~lNll + 1) = 2 * 108 * (llog4221 + 1) = 864, &'3 expected.\nTo emphasize the potential gains in using all available buffers, in Figure 13.7,\nwe show the number of passes, computed using our formula., for several values\nof Nand B.\nTo obtain the cost, the number of passes should be multiplied\nby 2N. In practice, one would expect to have more than 257 buffers, but this\ntable illustrates the importance of a high fan-in during merging.\n\n428\nCHAPTER +3\n\"\n....\n..;:;.-,., ....\nB+F ••9i\n.n\n.,,..\nn\n..\n·.·\\~.:tt.:· •••• &Q'i\" ••il\nJ.\nJ]2'i'i'u\n·.£\">;;;0;;;,[1\nLJTfif\",a\n100\n7\n4\n3\n2\n1\n1\n1000\n10\n5\n4\n3\n2\n2\n10,000\n13\n7\n5\n4\n2\n2\n100,000\n17\n9\n6\n5\n3\n3\n1,000,000\n20\n10\n7\n5\n3\n3\n10,000,000\n23\n12\n8\n6\n4\n3\n100,000,000\n26\n14\n9\n7\n4\n4\n1,000,000,000\n30\n15\n10\n8\n5\n4\nFigure 13.7\nNumber of Passes of External Merge Sort\nOf course, the CPU cost of a multiway merge can be greater than that for\na two-way merge, but in general the I/O costs tend to dominate.\nIn doing\na (B - I)-way merge, we have to repeatedly pick the 'lowest' record in the\nB-1 runs being merged and write it to the output buffer. This operation can\nbe implemented simply by examining the first (remaining) element in each of\nthe B-1 input buffers. In practice, for large values of B, more sophisticated\ntechniques can be used, although we do not discuss them here. Further, as we\nwill see shortly, there are other ways to utilize buffer pages to reduce I/0 costs;\nthese techniques involve allocating additional pages to each input (and output)\nrun, thereby making the number of runs me,rged in each pass considerably\nsmaller than the number of buffer pages B.\n13.3.1\nMinimizing the Number of Runs\nIn Pass 0 we read in B pages at a time and sort them internally to produce\nIN/Bl runs of B pages each (except for the la..'3t run, which may contain fewer\npages). With a more aggressive implementation, called replacement sort, we\ncan write out runs of approximately 2 . B internally sorted pages on average.\nThis improvement is achieved as follows. We begin by reading in pages of the\nfile of tuples to be sorted, say R, until the buffer is full, reserving (say) one\npage for use a..'3 an input buffer and one page for use a.s an output buffer. vVe\nrefer to the B ~ 2 pages of R tuples that are not in the input or output buffer\nas the CUT'TCnt set. Suppose that the file is to be sorted in ascending order on\nsome search key k. Tuples are appended to the output in ctscending order by k\nvalue.\nThe idea is to repeatedly pick the tuple in the current set with the smallest\nk value that is still greater than the largest k value in the output buffer and\nappend it to the output buffer.\nl:<cx the output buffer to remain sorted, the\nchosen tuple must satisfy the condition that its k value be greater than or\n\nEl:teT'nal Sorting\n4f9\nequal to the largest k value currently in the output buffer; of all tuples in\nthe current set that satisfy this condition, we pick the one with the smallest\nk value and append it to the output buffer. Moving this tuple to the output\nbuffer creates some space in the current set, which 've use to add the next input\ntuple to the current set. (\\Ve assume for simplicity that all tuples are the same\nsize.) This process is illustrated in Figure 13.8. The tuple in the current set\nthat is going to be appended to the output next is highlighted, as is the most\nrecently appended output tuple.\n11±-\nL\n..\n.•\n....\n..\nINPUT\nCURRENT SET\n~\nL~_II\nOUTPUT\nFigure 13.8\nGenerating Longer Runs\nWhen all tuples in the input buffer have been consumed in this manner, the\nnext page of the file is read in.\nOf course, the output buffer is written out\nwhen it is full, thereby extending the current run (which is gradually built up\non disk).\nThe important question is this: When do we have to terminate the current run\nand start a new run? As long as some tuple t in the current set has a bigger k:\nvalue than the most recently appended output tuple, we can append t to the\noutput buffer and the current run can be extended.2 In Figure 13.8, although\na tuple (k = 2) in the current set has a smaller k value than the largest output\ntuple (k = 5), the current run can be extended because the current set also has\na tuple (k = 8) that is larger than the largest output tuple.\nWhen every tuple in the current set is smaller than the largest tuple in the\noutput buffer, the output buffer is written out and becomes the last page in\nthe current run.\n\\Ve then start a new l'lm and continue the cycle of writing\ntuples from the input buffer to the current set to the output buffer. It is known\nthat this algorithm produces runs that are about 2· B pages long, on average.\nThis refinement has not been implemented in commercial database systenls\nbecause managing the main memory ava.ilable for sorting becOlnes difficult with\n2If B is large, the CPU cost of finding such a tuple t can be significant unless appropriate in·\nmemory data structures are used to organize the tuples in the buffer pool.\n\\lVe will not discuss this\nissue further.\n\n430\nCHAPTER 13\nreplacement sort, especially in the presence of variable length records. Recent\nwork on this issue, however, shows promise and it could lead to the use of\nreplacement sort in commercial systems.\n13.4\nMINIMIZING I/O COST VERSUS NUMBER OF I/OS\nWe have thus far used the number of page 1/Os as a cost metric. This metric is\nonly an approximation of true I/O costs because it ignores the effect of blocked\nI/O--issuing a single request to read (or write) several consecutive pages can\nbe much cheaper than reading (or writing) the same number of pages through\nindependent I/O requests, as discussed in Chapter 8. This difference turns out\nto have some very important consequences for our external sorting algorithm.\nFurther, the time taken to perform I/O is only part of the time taken by the\nalgorithm; we must consider CPU costs as well. Even if the time taken to do\nI/O accounts for most of the total time, the time taken for processing records is\nnontrivial and definitely worth reducing. In particular, we can use a technique\ncalled double buffeTing to keep the CPU busy while an I/O operation is in\nprogress.\nIn this section, we consider how the external sorting algorithm can be refined\nusing blocked I/O and double buffering.\nThe motivation for these optimiza-\ntions requires us to look beyond the number of I/Os as a cost metric. These\noptimizations can also be applied to other I/O intensive operations such as\njoins, which we study in Chapter 14.\n13.4.1\nBlocked I/O\nIf the number of page I/Os is taken to be the cost metric, the goal is clearly to\nminimize the number of passes in the sorting algorithm because each page in\nthe file is read and written in each pa..ss. It therefore makes sense to maximize\nthe fan-in during merging by allocating just one buffer pool page per run (which\nis to be merged) and one buffer page for the output of the merge. Thus, we\ncan merge B-1 runs, where B is the number of pages in the buffer pool. If we\ntake into account the effect of blocked access, which reduces the average cost\nto read or write a .single page, we are led to consider whether it might be better\nto read and write in units of more than one page.\nSuppose we decide to read and write in units, which we call buffer blocks,\nof b pages.\nWe must now set aside one buffer block per input run and one\nbufler block for the output of the merge, which means that we can merge at\nmost l B;)-bJ runs in each pass.\n}-<or example, if we have 10 buffer pages, we\ncan either merge nine runs at a time with one-page input and output buffer\n\nKrtcrnal SoTting\n431\nblocks, or we can merge four runs at a time with two-page input and output\nbuffer blocks. If we choose larger buffer blocks, however, the number of passes\nincreases, while we continue to read and write every page in the file in each\npass! In the example, each merging pass reduces the number of runs by a factor\nof 4, rather than a factor of 9. Therefore, the number of page I/Os increa.'3es.\nThis is the price we pay for decreasing the per-page I/O cost and is a trade-off\nwe must take into account when designing an external sorting algorithm.\nIn practice, however, current main memory sizes are large enough that all\nbut the largest files can be sorted in just two passes, even using blocked I/O.\nSuppose we have B buffer pages and choose to use a blocking factor of b pages.\nThat is, we read and write b pages at a time, and all our input and output\nbuffer blocks are b pages long. The first pass produces about N2 =\nIN/2Bl\nsorted runs, each of length 2B pages, if we use the optimization described in\nSection 13.3.1, and about N1 =\nIN/ Bl sorted runs, each of length B pages,\notherwise. For the purposes of this section, we assume that the optimization is\nused.\nIn subsequent pa.'3ses we can merge F\n=\nLB /bJ- 1 runs at a time.\nThe\nnumber of pa.'3ses is therefore 1+ lZo9pN21, and in each pass we read and write\nall pages in the file. Figure 13.9 shows the number of passes needed to sort files\nof various sizes N, given B buffer pages, using a blocking factor b of 32 pages.\nIt is quite reasonable to expect 5000 pages to be available for sorting purposes;\nwith 4KB pages, 5000 pages is only 20MB. (With 50,000 buffer pages, we can\ndo 1561-way merges; with 10,000 buffer pages, we can do 311-way merges; with\n5000 buffer pages, we can do 155-way merges; and with 1000 buffer pages, we\ncan do 30-way merges.)\nIN\nI B = 1000 1.8=50001 E± 10,000 >1 B = 50,0001\n100\n1\n1\n1\n1\n1000\n1\n1\n1\n1\n10,000\n2\n2\n1\n1\n100,000\n3\n2\n2\n2\n1,000,000\n~)\n2\n2\n2\n10,000,000\n4\n3\n~3\n2\n100,000,000\n5\n3\n3\n2\n1,000,000,000\n5\n4\n3\n3\nFigure 13.9\nNumber of Passes of External Merge Sort with Block Size b = 32\nTo compute the I/O cost, we need to calculate the number of 32-page blocks\nread or written and multiply this number by the cost of doing a 32-page block\nI/O. To find the number of block I/Os, we can find the total number of page\n\n432\nCHAPTER 13\nl/Os (number of passes rnultiplied by the number of pages in the file) and\ndivide by the block size, 32. The cost of a 32-page block I/O is the seek time\nand rotational delay for the first page, plus transfer time for all\n~~2 pages, as\ndiscussed in Chapter 8. The reader is invited to calculate the total I/O cost\nof sorting files of the sizes mentioned in Figure 13.9 with 5000 buffer pages for\ndifferent block sizes (say, b =\n1, 32, and 64) to get a feel for the benefits of\nusing blocked I/O.\n13.4.2\nDouble Buffering\nConsider what happens in the external sorting algorithm when all the tuples\nin an input block have been consumed: An I/O request is issued for the next\nblock of tuples in the corresponding input run, and the execution is forced to\nsuspend until the I/O is complete. That is, for the duration of the time taken\nfor reading in one block, the CPU remains idle (assuming that no other jobs are\nrunning). The overall time taken by an algorithm can be increased considerably\nbecause the CPU is repeatedly forced to wait for an I/O operation to complete.\nThis effect becomes more and more important as CPU speeds increase relative\nto I/O speeds, which is a long-standing trend in relative speeds. It is therefore\ndesirable to keep the CPU busy while an I/O request is being carried out;\nthat is, to overlap CPU and I/O processing. Current hardware supports such\noverlapped computation, and it is therefore desirable to design algorithms to\ntake advantage of this capability.\nIn the context of external sorting, we can achieve this overlap by allocating\nextra pages to each input buffer. Suppose a block size of b = 32 is chosen. The\nidea is to allocate an additional 32-page block to every input (and the output)\nbuffer. Now, when all the tuples in a 32-page block have been consumed, the\nCPU can process the next 32 pages of the run by switching to the second,\n'double,' block for this run.\nMeanwhile, an I/O request is issued to fill the\nempty block.\nThus, assmning that the tirne to consume a block is greater\nthan the time to read in a block, the CPU is never idle! On the other hand,\nthe number of pages allocated to a buffer is doubled (for a given block size,\nwhich means the total I/O cost stays the same). This technique, ca.lled double\nbuffering, ca.n considerably reduce the total time taken to sort a file. The use\nof buffer pages is illustrated in Figure 1:3.10.\nNote that although double buffering can considerably reduce the response tiule\nfor a given query, it may not have a significant impact on throughput, because\nthe CPU can be kept busy by working on other queries while waiting for one\nquery's I/O operation to complete.\n\nE:rteTnal Sorting\nFigure 13.10\nDouble Buffering\n13.5\nUSING B+ TREES FOR SORTING\n433\nSuppose that we have a B+ tree index on the (search) key to be used for sorting\na file of records. Instead of using an external sorting algorithm, we could use\nthe B+ tree index to retrieve the records in search key order by traversing the\nsequence set (i.e., the sequence of leaf pages). Whether this is a good strategy\ndepends on the nature of the index.\n13.5.1\nClustered Index\nIf the B+ tree index is clustered, then the traversal of the sequence set is\nvery efficient.\nThe search key order corresponds to the order in which the\ndata records are stored, and for each page of data records we retrieve, we can\nread all the records on it in sequence.\nThis correspondence between search\nkey ordering and data record ordering is illustrated in Figure 13.11, with the\na.ssumption that data entries are (key, ric!; pairs (i.e., Alternative (2) is used\nfor data entries).\nThe cost of using the clustered B+ tree index to retrieve the data records in\nsearch key order is the cost to traverse the tree from root to the left-most leaf\n(which is usually less than four IIOs) plus the cost of retrieving the pages in\nthe sequence set, plus the cost of retrieving the (say, N) pages containing the\ndata records. Note that no data page is retrieved twice, thanks to the ordering\nof data entries being the same 1:18 the ordering of data records. The number of\npages in the sequence set is likely to be much smaller than the number of data\npages because data entries are likely to be smaller than typical data records.\nThus, the strategy of using a dusterecl B+ tree inclex to retrieve the records\nin sorted order is a good one and should be used whenever such an index is\n'::lilable.\n\n434\nIndex entries\n(Direct search for\ndata entries)\nCHAPTER 13\nData\nrecords\nIndex file\n]\nD... \"\"\nFigure 13.11\nClustered B+ Tree for Sorting\nWhat if Alternative (1) is used for data entries? Then, the leaf pages would\ncontain the actual data records, and retrieving the pages in the sequence set\n(a total of N pages) would be the only cost. (Note that the space utilization is\nabout 67% in a B+ tree; the number of leaf pages is greater than the number\nof pages needed to hold the data records in a sorted file, where, in principle,\n100% space utilization can be achieved.) In this case, the choice of the B+ tree\nfor sorting is excellent!\n13.5.2\nUnclustered Index\nWhat if the B+ tree index on the key to be used for sorting is unclustered?\nThis is illustrated in Figure 13.12, with the assumption that data entries are\n(key, rid).\nIn this case each rid in a leaf page could point to a different data page. Should\nthis happen, the cost (in disk l/Os) of retrieving all data records could equal\nthe number of data records. That is, the worst~casecost is equal to the number\nof data records, because fetching each record could require a disk I/O. This\ncost is in addition to the cost of retrieving leaf pages of the B+ tree to get the\ndata entries (which point to the data records).\nIf p is the average number of records per data page and there are N data pages,\nthe number of data records is p . N. If we take f to be the ratio of the size of a\ndata entry to the size of a data record, we can approximate the number of leaf\npages in the tree by f . N. The total cost of retrieving records in sorted order\n\nE:c:temal So-rt'ing\n435\n/\nIndex entries\n(Direct search for\ndata entries)\nIndex file\nData\nrecords J D.....,\nFigure 13.12\nUnclustered B+ Tree for Sorting\nusing an unclustered B+ tree is therefore (J + p) . N. Since f is usually 0.1 or\nsmaller and p is typically much larger than 10, p . N is a good approximation.\nIn practice, the cost may be somewhat less because some rids in a leaf page\nlead to the same data page, and further, some pages are found in the buffer\npool, thereby avoiding an I/O. Nonetheless, the usefulness of an unclustered\nB+ tree index for sorted retrieval highly depends on the extent to which the\norder of data entries corresponds and-·~this is just a matter of chance-to the\nphysical ordering of data records.\nWe illustrate the cost of sorting a file of records using external sorting and un-\nclustered B+ tree indexes in Figure 13.13. The costs shown for the unclustered\nindex are worst-case numbers, based on the approximate formula p . N.\nFor\ncomparison, note that the cost for a clustered index is approximately equal to\nN, the number of pages of data records.\nI Sorting\nI p=l\nIp= 10\nr p= foo\n100\n200\n100\n1000\n10,000\n1000\n2000\n1000\n10,000\n100,000\n10,000\n40,000\n10,000\n100,000\n1,000,000\n100,000\n600,000\n100,000\n1,000,000\n10,000,000\n1,000,000\n8,000,000\n1,000,000\n10,000,000\n100,000,000\n10,000,000\n80,000,000\n10,000,000\n100,000,000\n1,000,000,000\nFigure 13.13\nCost of External Sorting (13 = 1000, b = :32) versus Unclustered Index\n\n436\nCHAPTER 13\nKeep in mind that p is likely to be doser to 100 and B is likely to be higher\nthan 1,000 in practice. The ratio of the cost of sorting versus the cost of using\nan unclustered index is likely to be even 10\\ver than indicated by Figure 13.13\nbecause the I/O for sorting is in 32-page buffer blocks, whereas the I/O for the\nundustered indexes is one page at a time. The value of p is determined by the\npage size and the size of a data record; for p to be 10, with 4KB pages, the\naverage data record size must be about 400 bytes. In practice, p is likely to be\ngreater than 10.\nFor even modest file sizes, therefore, sorting by using an unclustered index is\nclearly inferior to external sorting.\nIndeed, even if we want to retrieve only\nabout 10--20% of the data records, for example, in response to a range query\nsuch as \"Find all sailors whose rating is greater than 7,\" sorting the file may\nprove to be more efficient than using an unclustered index!\n13.6\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nII\nWhat database operations utilize sorting? (Section 13.1)\nII\nDescribe how the two-way merge sort algorithm can sort a file of arbitrary\nlength using only three main-memory pages at any time.\nExplain what\na run is and how runs are created and merged.\nDiscuss the cost of the\nalgorithm in terms of the number of passes and the I/O cost per pass.\n(Section 13.2)\nII\nHow does the general external merge SOT,t algorithm improve upon the two-\nway merge sort?\nDiscuss the length of initial runs, and how memory is\nutilized in subsequent merging passes. Discuss the cost of the algorithm in\nterms of the number of pa.'3ses and the I/O cost per pa.ss. (Section 13.3)\nII\nDiscuss the use of r'cplacement sort to increase the average length of initial\nruns and thereby reduce the number of runs to be merged. How does this\naffect the cost of external sorting? (Section 13.3.1)\nIII\n\\\\That is blocked I/O? Why is it cheaper to read a sequence of pages using\nblocked I/O than to read them through several independent requests? How\ndoes the use of blocking affect the external sorting algorithm, and how does\nit change the cost formula'? (Section 13.4.1)\n..\nWhat is double buffering?\n\\\\That is the motivation for using it?\n(Sec-\ntion 13.4.2)\nII\nIf we want to sort a file and there is a B-1- tree with the same search key, we\nhave the option of retrieving records in order through the index. Compa.re\n\nE:r:tcmal SOTt'ing\n437\nthe cost of this approach to retrieving the records in random order and then\nsorting them.\nConsider both clustered and unclustered B+ trees.\n~What\nconclusions can you draw from your comparison? (Section 13.5)\nEXERCISES\nExercise 13.1 Suppose you have a file with 10,000 pages and you have three buffer pages.\nAnswer the following questions for each of these scenarios, assuming that our most general\nexternal sorting algorithm is used:\n(a) A file with 10,000 pages and three available buffer pages.\n(b) A file with 20,000 pages and five available buffer pages.\n(c) A file with 2,000,000 pages and 17 available buffer pages.\n1. How many runs will you produce in the first pass?\n2. How many passes will it take to sort the file completely?\n3. What is the total I/O cost of sorting the file?\n4. How many buffer pages do you need to sort the file completely in just two passes?\nExercise 13.2 Answer Exercise 13.1 assuming that a two-way external sort is used.\nExercise 13.3 Suppose that you just finished inserting several records into a heap file and\nnow want to sort those records. Assume that the DBMS uses external sort and makes efficient\nuse of the available buffer space when it sorts a file. Here is some potentially useful information\nabout the newly loaded file and the DBMS software available to operate on it:\nThe number of records in the file is 4500. The sort key for the file is 4 bytes long.\nYou can assume that rids are 8 bytes long and page ids are 4 bytes long.\nEach\nrecord is a total of 48 bytes long. The page size is 512 bytes.\nEach page has 12\nbytes of control information on it. Four buffer pages are available.\n1. How many sorted subfiles will there be after the initial pass of the sort, and how long\nwill each subtile be?\n2. How many passes (including the initial pass just considered) are required to sort this\nfile?\n:3. What is the total I/O cost for sorting this file?\n4. What is the largest file, in terms of the number of records, you can sort with just four\nbuffer pages in two passes? How would your answer change if you had 257 buffer pages?\n5. Suppose that you have a B+ tree index with the search key being the same as the desired\nsort key. Find the cost of USiIlg the index to retrieve the records in sorted order for each\nof the following cases:\nlllI\nThe index uses Alternative (1) for data entries.\nlllI\nThe index uses Alternative (2) and is unclustered. (You can compute the worst-case\ncost in this case.)\n\n438\n•\nCHAPTER 1\\3\nHow would the costs of using the index change if the file is the largest that you\ncan sort in two passes of external sort with 257 buffer pages? Give your answer for\nboth clustered and unclustered indexes.\nExercise 13.4 Consider a disk with an average seek time of lOms, average rotational delay\nof 5ms, and a transfer time of 1ms for a 41< page. Assume that the cost of reading/writing\na page is the sum of these values (i.e., 16ms) unless a sequence of pages is read/written. In\nthis case, the cost is the average seek time plus the average rotational delay (to find the first\npage in the sequence) plus 1ms per page (to transfer data). You are given 320 buffer pages\nand asked to sort a file with 10,000,000 pages.\n1. Why is it a bad idea to use the 320 pages to support virtual memory, that is, to 'new'\n10,000,000\n41< bytes of memory, and to use an in-memory sorting algorithm such as\nQuicksort?\n2. Assume that you begin by creating sorted runs of 320 pages each in the first pass.\nEvaluate the cost of the following approaches for the subsequent merging passes:\n(a) Do 31g-way merges.\n(b) Create 256 'input' buffers of 1 page each, create an 'output' buffer of 64 pages, and\ndo 256-way merges.\n(c) Create 16 'input' buffers of 16 pages each, create an 'output' buffer of 64 pages,\nand do 16-way merges.\n(d) Create eight 'input' buffers of 32 pages each, create an 'output' buffer of 64 pages,\nand do eight-way merges.\n(e) Create four 'input' buffers of 64 pages each, create an 'output' buffer of 64 pages,\nand do four-way merges.\nExercise 13.5 Consider the refinement to the external sort algorithm that produces runs of\nlength 2B on average, where B is the number of buffer pages. This refinement was described\nin Section 11.2.1 under the assumption that all records are the same size. Explain why this\nassumption is required and extend the idea to cover the case of variable-length records.\nPROJECT-BASED EXERCISES\nExercise 13.6 (Note to i,nstnu:t01'S:\nAdd~t'ional deta'ils must be pmvided if this exercise is\nassigned; see Appendi:r: SO.) Implement external sorting in Minibase.\nBIBLIOGRAPHIC NOTES\nKnuth's text [442] is the classic reference for sorting algorithms.\nI\\Jemory management for\nreplacement sort is discussed in [471]. A number of papers discuss parallel external sorting\nalgorithms, including [66, 71, 223,494, 566, 647].\n\n14\nEVALUATING RELATIONAL\nOPERATORS\n...\nWhat are the alternative algorithms for selection?\nWhich alterna-\ntives are best under different conditions? How are complex selection\nconditions handled?\n...\nHow can we eliminate duplicates in projection? How do sorting and\nhashing approaches -compare?\n...\nWhat are the alternative join evaluation algorithms? Which alterna-\ntives are best under different conditions?\n...\nHow are the set operations (union, inter;section, set-difference, cross-\nproduct) implemented?\n...\nHow are aggregate operations and grouping handled?\n...\nHow does the size of the buffer pool and the buffer replacement policy\naffect algorithms for evaluating relational operators?\n..\nKey concepts:\nselections, CNF; projections, sorting versus hash-\ning; joins, block nested loops, index nested loops, sort-merge, hash;\nunion, set-difference, duplicate elimination; aggregate operations, run-\nning information, partitioning into groups, using indexes; buffer man-\nagement, concurrent execution, repeated access patterns\nNow, 'here, you see, it takes all the running you can do, to keep in the same\nplace. If you want to get somewhere else, you must run at least twice\n3..<; fast as\nthat!\n-----Lewis Carroll, Throngh the Looking Glass\n439\n\n440\nCHAPTER 14\nIn this chapter, we consider the implementation of individual relational op-\nerators in sufficient detail to understand how DBMSs are implemented. The\ndiscussion builds on the foundation laid in Chapter 12. vVe present implemen-\ntation alternatives for the selection operator in Sections 14.1 and 14.2. It is\ninstructive to see the variety of alternatives and the wide variation in per'for-\nmanee of these alternatives, for even such a simple operator. In Section 14.:3,\nwe consider the other unary operator in relational algebra, projection.\n\\iVe then discuss the implementation of binary operators, beginning with joins\nin Section 14.4. Joins are among the most expensive operators in a relational\ndatabase system, and their implementation has a big impact on performance.\nAfter discussing the join operator, we consider implementation of the binary\noperators cross-product, intersection, union, and set-difference in Section 14.5.\nWe discuss the implementation of grouping and aggregate operators, which are\nextensions of relational algebra, in Section 14.6. We conclude with a discussion\nof how buffer management affects operator evaluation costs in Section 14.7.\nThe discussion of each operator is largely independent of the discussion of other\noperators. Several alternative implementation techniques are presented for each\noperator; the reader who wishes to cover this material ill less depth can skip\nsome of these alternatives without loss of continuity.\nPreliminaries: Examples and Cost Calculations\nWe present a number of example queries using the same schema as in Chapter\n12:\nSailors(sid: integer, sname: string, rating: integer, age: real)\nReservesC'iid:\n~_nteger, bid: integer, day: dates, rname: string)\nThis schema is a variant of the one that we used in Chapter 5; we added a\nstring field rname to Reserves. Intuitively, this field is the name of the person\nwho made the reservation (and may be different from the name of the sailor .sid\nfor whom the reservation wa.\" made; a reservation may be made by a person\nwho is not a sailor on behalf of a sailor).\nThe addition of this field gives us\nmore flexibility in choosing illustrative examples. We assume that each tuple\nof Reserves is 40 bytes lOllg, that a page can hold 100 Reserves tuples, alld\nthat we have 1000 pages of such tuples. Similarly, we assume that each tuple\nof Sailors is 50 bytes long, that a page can hold 80 Sailors tuples, and that we\nhave 500 pages of such tuples.\nTwo points must be kept in Inind to understancl our discussion of costs:\n\nEval1wting Relational Operator\"\n441\n•\nAs discussed in Chapter 8, we consider only I/O costs and measure I/O\ncost in terms of the number of page l/Os. vVe also use big-O notation to\nexpress the complexity of an algorithm in terms of an input parameter mId\nassume that the reader is familiar with this notation.\nFor example, the\ncost of a file scan is O(Jlv1), where Ai is the size of the file.\n•\nvVe discuss several alternate algorithms for each operation.\nSince each\nalternative incurs the same cost in writing out the result, should this be\nnecessary, we uniformly ignore this cost in comparing alternatives.\n14.1\nTHE SELECTION OPERATION\nIn this section, we describe various algorithms to evaluate the selection opera-\ntor. To motivate the discussion, consider the selection query shown in Figure\n14.1, which has the selection condition rno:me='Joe'.\nSELECT *\nFROM\nReserves R\nWHERE\nR.rname='Joe'\nFigure 14.1\nSimple Selection Query\nWe can evaluate this query by scanning the entire relation, checking the condi-\ntion on each tuple, and adding the tuple to the result if the condition is satisfied.\nThe cost of this approach is 1000 l/Os, since Reserves contains 1000 pages. If\nonly a few tuples have rnarne= 'Joe', this approach is expensive because it does\nnot utilize the selection to reduce the number of tuples retrieved in any way.\nHow can we improve on this approach? The key is to utilize information in the\nselection condition and use an index if a suitable index is available. For exam-\nple, a B+ tree index on 'marne could be used to answer this query considerably\nfaster, but an index on bid would not be useful.\nIn the rest of this section. we consider various situations with respect to the file\norganization used for the relation and the availability of indexes and discuss\nappropriate algorithms for the selection operation.\nWe discuss only simple\nselection operations of the form eJR.attr op lw!ue(R) until Section 14.2, where\nwe consider general selections.\nIn terms of the general techniques listed in\nSection\n12.2~ the algorithms for selection use either iteration or indexing.\n14.1.1\nNo Index, Unsorted Data\nGiven a selection of the form eJRattT op value (R), if there is no index on R. attT\nand R is not sorted on R. aUT, we have to scan the entire relation. Therefore,\n\n442\nCHAPTER 14,\nthe most selective access path is a file scan. For each tuple, we must test the\ncondition R.attr op vaz'ue and add the tuple to the result if the condition is\nsatisfied.\n14.1.2\nNo Index, Sorted Data\nGiven a selection of the form O'R.attr op value(R), if there is no index on R.attr,\nbut R is physically sorted on R.attr, we can utilize the sort order by doing\na binary search to locate the first tuple that satisfies the selection condition.\nFurther, we can then retrieve all tuples that satisfy the selection condition\nby starting at this location and scanning R until the selection condition is\nno longer satisfied. The access method in this case is a sorted-file scan with\nselection condition O'R.attr op value(R).\nFor example, suppose that the selection condition is R.aUr! > 5, and that R is\nsorted on attr1 in ascending order. After a binary search to locate the position\nin R corresponding to 5, we simply scan all remaining records.\nThe cost of the binary search is O(l092M). In addition, we have the cost of the\nscan to retrieve qualifying tuples. The cost of the scan depends on the number\nof such tuples and can vary from zero to M. In our selection from Reserves\n(Figure 14.1), the cost of the binary search is [0921000 ~ 10 I/Os.\nIn practice, it is unlikely that a relation will be kept sorted if the DBMS sup-\nports Alternative (1) for index data entries; that is, allows data records to be\nstored as index data entries. If the ordering of data records is important, a\nbetter way to maintain it is through a B+ tree index that uses Alternative (1).\n14.1.3\nB+ Tree Index\nIf a clustereel B+ tree index is available on R.attr, the best strategy for selection\nconditions O'R.attr op value(R) in which op is not equality is to use the index.\nThis strategy is also a good access path for equality selections, although a hash\nindex on R.attr would be a little better. If the B+ tree index is not clustered,\nthe cost of using the index depends on the number of tuples that satisfy the\nselection, as discussed later.\nWe can use the index as follows:\nWe search the tree to find the first index\nentry that points to a qualifying tuple of R. Then we scan the leaf pages of the\nindex to retrieve all entries in which the key value satisfies the selection condi-\ntion. For each of these entries, we retrieve the corresponding tuple of R. (for\nconcreteness in this discussion, we a.<;sume that data entries use Alternatives\n(2) or (3); if Alternative (1) is used, the data entry contains the actual tuple\n\nEvaluating Relational OpemtoTs\n443\nand there is no additional cost~beyondthe cost of retrieving data entries-cfor\nretrieving tuples.)\nThe cost of identifying the starting leaf page for the scan is typically two or\nthree I/Os. The cost of scanning the leaf level page for qualifying data entries\ndepends on the number of such entries. The cost of retrieving qualifying tuples\nfrom R depends on two factors:\n•\nThe number of qualifying tuples.\n•\nWhether the index is clustered. (Clustered and unclustered B+ tree indexes\nare illustrated in Figures 13.11 and 13.12.\nThe figures should give the\nreader a feel for the impact of clustering, regardless of the type of index\ninvolved.)\nIf the index is clustered, the cost of retrieving qualifying tuples is probably\njust one page I/O (since it is likely that all such tuples are contained in a\nsingle page). If the index is not clustered, each index entry could point to a\nqualifying tuple on a different page, and the cost of retrieving qualifying tuples\nin a straightforward way could be one page I/O per qualifying tuple (unless we\nget lucky with buffering). We can significantly reduce the number of I/Os to\nretrieve qualifying tuples from R by first sorting the rids (in the index's data\nentries) by their page-id component. This sort ensures that, when we bring in\na page of R, all qualifying tuples on this page are retrieved one after the other.\nThe cost of retrieving qualifying tuples is now the number of pages of R that\ncontain qualifying tuples.\nConsider a selection of the form rnarne < 'C%' on the Reserves relation. As-\nsuming that names are uniformly distributed with respect to the initial letter,\nfor simplicity, we estimate that roughly 10% of Reserves tuples are in the result.\nThis is a total of 10,000 tuples, or 100 pages. If we have a clustered B+ tree\nindex on the marne field of Reserves, we can retrieve the qualifying tuples with\n100 I/Os (plus a few I/Os to traverse from the root to the appropriate leaf page\nto start the scan). However, if the index is unclustered, we could have up to\n10,000 I/Os in the worst case, since each tuple could cause us to read a page. If\nwe sort the rids of Reserves tuples by the page number and then retrieve pages\nof Reserves, we avoid retrieving the same page multiple times; nonetheless, the\ntuples to be retrieved are likely to be scattered across many more than 100\npages. Therefc)re, the use of an unclusterecl index for a range selection could\nbe expensive; it might be cheaper to simply scan the entire relation (which is\nlOOn pages in our example).\n\n444\nCHAPTER 14\n14.1.4\nHash Index, Equality Selection\nIf a hash index is available on R.attr and op is equality, the best way to imple-\nment the selection CTR.attr opualue(R) is obviously to use the index to retrieve\nqualifying tuples.\nThe cost includes a few (typically one or two) l/Os to retrieve the appropriate\nbucket page in the index, plus the cost of retrieving qualifying tuples from\nR. The cost of retrieving qualifying tuples from R depends on the number of\nsuch tuples and on whether the index is clustered. Since op is equality, there\nis exactly one qualifying tuple if R.attr is a (candidate) key for the relation.\nOtherwise, we could have several tuples with the same value in this attribute.\nConsider the selection in Figure 14.1.\nSuppose that there is an unclustered\nhash index on the marne attribute, that we have 10 buffer pages, and that\n100 reservations were made by people named Joe. The cost of retrieving the\nindex page containing the rids of such reservations is one or two l/Os. The cost\nof retrieving the 100 Reserves tuples can vary between 1 and 100, depending\non how these records are distributed across pages of Reserves and the order\nin which we retrieve these records. If these 100 records are contained in, say,\nsome five pages of Reserves, we have just five additional l/Os if we sort the\nrids by their page component. Otherwise, it is possible that we bring in one of\nthese five pages, then look at some of the other pages, and find that the first\npage has been paged out when we need it again. (Remember that several users\nand DBMS operations share the buffer pool.) This situation could cause us to\nretrieve the same page several times.\n14.2\nGENERAL SELECTION CONDITIONS\nIn our discussion of the selection operation thus far, we have considered selec-\ntion conditions of the form CTR.attr op vall1e (R). In general, a selection condition\nis a Boolean combination (Le., an expression using the logical connectives 1\\\nand V) of terms that have the form attribute op constant or attributel op\nattrilmte2. For example, if the WHERE clause in the query shown in Figure 14.1\ncontained the condition R.rnarne='Joe' AND R.bid=r, the equivalent algebra\nexpression would be CTR.rname='Joe'l\\R.bid=r(R).\nIn Section 14.2.1, we provide a more rigorous definition of CNF, which we\nintroduced in Section 12.2.2.\nWe consider algorithms for applying selection\nconditions without disjunction in Section 14.2.2 and then discuss conditions\nwith disjunction in Section 14.2.3.\n\nEvaluating Relaf'ional OpemtoT8\n14.2.1\nCNF and Index Matching\n445\nTo process a selection operation with a general selection condition, we first\nexpress the condition in conjunctive normal form (CNF), that is,\n&9 a\ncollection of conjunets that are connected through the use of the 1\\ operator.\nEach conjunct consists of one or more terms (of the form described previously)\nconnected by V. 1 Conjuncts that contain V are said to be disjunctive or to\ncontain disjunction.\nAs an example, suppose that we have a selection on Reserves with the condition\n(day < 8/9/02 1\\ r-rwme =\n'Joe ') V bid=5 V sid=3.\n\",re can rewrite this in\nconjunctive normal form as (day < 8/9/02 V bid=5 V s'id=3) 1\\ (marne =\n'Joe'V bid=5 V sid=3).\nVve discussed when an index matches a CNF selection in Section 12.2.2 and in-\ntroduced selectivity of access paths. The reader is urged to review that mate~ial\nnow.\n14.2.2\nEvaluating Selections without Disjunction\nWhen the selection does not contain disjunction, that is, it is a conjunction of\nterms, we have two evaluation options to consider:\n11II\n\\iVe can retrieve tuples using a file scan or a single index that matches\nsome conjuncts (and which we estimate to be the most selective access\npath) and apply all nonprimary conjuncts in the selection to each retrieved\ntuple.\nThis approach is very similar to how we use indexes for simple\nselection conditions, and we do not discuss it further. (We emphasize that\nthe number of tuples retrieved depends on the selectivity of the primary\nconjuncts in the selection, and the remaining conjuncts only reduce the\ncardinality of the result of the selection.)\nII\nWe can try to utilize several indexes. vVe examine this approach in the rest\nof this section.\nIf several indexes containing data entries with rids (i.e., Alternatives (2) or (3))\nmatch conjuncts in the selection, we can use these indexes to compute sets of\nrids of candidate tuples. \"1e can then intersect these sets of rids, typically by\nfirst sorting them, then retrieving those records whose rids are in the intersec-\ntion. If additional conjuncts are present in the selection, we can apply these\nconjuncts to discard some of the candidate tuples from the result.\n1 Every selection conditioll Olfl be expressed in CNF. V·/e refer the reader to any standard text on\nmathematical logic for the details.\n\n446\nCHAPTER 14\n~\nI--~-\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"'-\"-\"'-\n•••••- ••~~~..-\n.•........•...•..• _\n.... ---_.•--S-.-, .....·····T·-:-.·I---__._---········.············.·.-..··.···--\"'---T-\nI\nIntersecting rid Sets: Oracle 8 uses several techniques to do rid set in-\ntersection for selections with .AND. One is to ANDbitIl1aps.Another is to\ndo a hash join of.indexes. For example, gi,,811sal <5/\\ If'ice > 30 and\nindexes on sal and price, we can join the indexes on the ri(1 column1 con-\nsidering only entries that satisfy the given selection conditions. Microsoft\nSQL ServerimPlements rid set intersection through index joins. IBN!.~p2\nimplements intersection of rid sets using Bloom filters (\\I,'hjch are disy~§§~d\nin Section 22.10.2). Sybase ASE does not do rid set intersection for AND\nselections; Sybase ASIQ does it using bitmap operations.\nInformix also\ndoes rid set intersection.\nAs an example, given the condition day < 8/9/02 A bid=5 A sid=,'J, we can\nretrieve the rids of records that meet the condition day < 8/9/02 by using a\nB+ tree index on day, retrieve the rids of records that meet the condition sid=,'J\nby using a hash index on sid, and intersect these two sets of rids. (If we sort\nthese sets by the page id component to do the intersection, a side benefit is\nthat the rids in the intersection are obtained in sorted order by the pages that\ncontain the corresponding tuples, which ensures that we do not fetch the same\npage twice while retrieving tuples using their rids.) We can now retrieve the\nnecessary pages of Reserves to retrieve tuples and check bid=5 to obtain tuples\nthat meet the condition day < 8/9/02 A bid=5/\\ sid=,'J.\n14.2.3\nSelections with Disjunction\nNow let us consider that one of the conjuncts in the selection condition is a\ndisjunction of terms.\nIf even one of these terms requires a file scan because\nsuitable indexes or sort orders are unavailable, testing this conjunct by itself\n(I.e., without taking advantage of other conjuncts) requires a file scan.\nFor\nexample, suppose that the only available indexes are a hash index on marne\nand a hash index on sid, and that the selection condition contains just the\n(disjunctive) conjunct (day < 8/9/02 V rnarne='Joe'). We can retrieve tuples\nsatisfying the condition marne= 'Joe' by using the index on rnarne. However,\nday < 8/9/02 requires a file scan.\nSo we might as well do a file scan and\ncheck the condition marne= 'Joe' for each retrieved tuple. Therefore, the most\nselective access path in this example is a file scan.\nOn the other hand, if the selection condition is (day < 8/9/02 V mame='Joe')\nA sid=,'J, the index on sid matches the conjunct sid=S. We can use this index\nto find qualifying tuples and apply day < 8/9/02 V marne='Joe' to just these\ntuples.\nThe best access path in this example is the index on sid with the\nprimary conjunct sid=S.\n\nEvaluating Relational Operators\nDisjunctions:\nMicrosoft SQL Server considers the use of unions and\nbitmaps for dealing withdisjunctive conditions.\nOracle.8 considers four\nways to handle disjunctive conditions: (1) Convert the query into a union\nof queries without OR. (2) If the cOllditions involve the same attribute, such\nas sal < 5 V sal > 30, use a nested query with an IN list and an index on\nthe attribute to retrieve tuples matching a valUe in the list. (3) Use bitmap\noperations, e.g., evaluate sal <5 V sal> 30 by generating bitmaps for the\nvalues 5. and 30 and OR the bitmaps to find the tuples that satisfy one of\nthe conditions. (We discuss bitmaps in Chapter 25.) (4) Simply applythe\ndisjunctive condition as a filter on the set of retrieved tuples. Syba.'3e ASE\nconsiders the use of unions for dealing with disjunctive queries and Sybase\nASIQ uses bitmap operations.\nFinally, if every term in a disjunction has a matching index, we can retrieve\ncandidate tuples using the indexes and then take the union. For example, if the\nselection condition is the conjunct (day < 8/9/02 V rname='Joe') and we have\nB+ tree indexes on day and rname, we can retrieve all tuples such that day <\n8/9/02 using the index on day, retrieve all tuples such that rname= 'Joe' using\nthe index on rname, and then take the union of the retrieved tuples. If all the\nmatching indexes use Alternative (2) or (3) for data entries, a better approach\nis to take the union of rids and sort them before retrieving the qualifying data\nrecords.\nThus, in the example, we can find rids of tuples such that day <\n8/9/02 using the index on day, find rids of tuples such that rname= 'Joe' using\nthe index on rname, take the union of these sets of rids and sort them by page\nnumber, and then retrieve the actual tuples from Reserves. This strategy can\nbe thought of as a (complex) access path that matches the selection condition\n(day < 8/9/02 V rname='Joe').\nMost current systems do not handle selection conditions with disjunction effi-\nciently and concentrate on optimizing selections without disjunction.\n14.3\nTHE PROJECTION OPERATION\nConsider the query shown in Figure 14.2. The optimizer translates this query\ninto the relational algebra expression 7rsid,bidReserves. In general the projection\noperator is of the form 7rattTl,attr2, ... ,attrm(R). To implement projection, we have\nSELECT DISTINCT R.sid, R.bid\nFROM\nReserves R\nFigure 14.2\nSimple Projection Query\n\n448\nto do the following:\nCHAPTER l4\n1. Remove unwanted attributes (i.e., those not specified in the projection).\n2. Eliminate any duplicate tuples produced.\nThe second step is the difficult one. There are two basic algorithms, one based\non sorting and one based on hashing. In terms of the general techniques listed in\nSection 12.2, both algorithms are instances of partitioning. While the technique\nof using an index to identify a subset of useful tuples is not applicable for\nprojection, the sorting or hashing algorithms can be applied to data entries\nin an index, instead of to data records, under certain conditions described in\nSection 14.3.4.\n14.3.1\nProjection Based on Sorting\nThe algorithm based on sorting has the following steps (at least conceptually):\n1. Scan R and produce a set of tuples that contain only the desired attributes.\n2. Sort this set of tuples using the combination of all its attributes as the key\nfor sorting.\n3. Scan the sorted result, comparing adjacent tuples, and discard duplicates.\nIf we use temporary relations at each step, the first step costs IvI l/Os to scan\nR, where 111 is the number of pages of R, and T l/Os to write the temporary\nrelation, where T is the number of pages of the temporary; T is a (J'v1). (The\nexact value of T depends on the number of fields retained and the sizes of these\nfields.) The second step costs O(TlogT) (which is also 0(MlogA1), of course).\nThe final step costs T. The total cost is O(II;flogIvI). The first and third steps\nare straightforward and relatively inexpensive.\n(As noted in the chapter on\nsorting, the cost of sorting grows linearly with data..<;et size in practice, given\ntypical data\"iet sizes and main memory sizes.)\nConsider the projection on Reserves shown in Figure 14.2.\n\\iVe can scall Re-\nserves at a cos,t of 1000 I/Os. If we assume that each tuple in the temporary\nrelation created in the first step is 10 bytes long, the cost of writing this tem-\nporary relation is 250 l/Os. Suppose we have 20 buffer pages. \\Ve um sort the\ntemporary relation in two pa\"ises at a cost of 2 . 2 . 250 = 1000 l/Os. The scan\nrequired in the third step costs an additional 250 I/Os. The total cost is 2500\nl/Os.\n\nEvaluat'ing Relational Operator's\n449\nThis approach can be improved on by modifying the sorting algorithm to do\nprojection with duplicate elimination. Recall the structure of the external sort-\ning algorithm presented in Chapter 13. The very first pass (Pass 0) involves\na scan of the records that are to be sorted to produce the initial set of (in-\nternally) sorted runs.\nSubsequently, one or more passes merge runs.\nTwo\nimportant modifications to the sorting algorithm adapt it for projection:\n•\n'We can project out unwanted attributes during the first pass (Pass 0) of\nsorting. If B buffer pages are available, we can read in B pages of Rand\nwrite out (T/!vI) . B internally sorted pages of the temporary relation. In\nfact, with a more aggressive implementation, we can write out approxi-\nmately 2 . B internally sorted pages of the temporary relation on average.\n(The idea is similar to the refinement of external sorting discussed in Sec-\ntion 13.3.1.)\n•\nWe can eliminate duplicates during the merging passes. In fact, this modifi-\ncation reduces the cost of the merging passes since fewer tuples are written\nout in each pa.'3S. (Most of the duplicates are eliminated in the very first\nmerging pass.)\nLet us consider our example again. In the first pass we scan Reserves, at a cost\nof 1000 I/Os and write out 250 pages.\nWith 20 buffer pages, the 250 pages\nare written out as seven internally sorted runs, each (except the last) about 40\npages long. In the second pass we read the runs, at a cost of 250 I/Os, and\nmerge them. The total cost is 1,500 I/Os, which is much lower than the cost\nof the first approach used to implement projection.\n14.3.2\nProjection Based on Hashing\nIf we have a fairly large number (say, B) of buffer pages relative to the number\nof pages of R, a hash-based approach is worth considering.\nThere are two\nphases: partitioning and duplicate elimination.\nIn the partitioning phase, we have one input buffer page and B-1 output buffer\npages. The relation R is read into the input buffer page, one page at a time.\nThe input page is processed a.'3 follows:\nFor each tuple, we project out the\nunwanted attributes and then apply a hash function h to the combination of\nall remaining.attributes. The function h is chosen so that tuples are distributed\nuniformly to one of B-1 partitions; there is one output page per partition.\nAfter the projection the tuple is written to the output buffer page that it is\nhashed to by h.\nAt the end of the partitioning phase, we have B-1 partitions, each of which\ncontains a collection of tuples that share a common hash value (computed by\n\n450\nCHAPTER 14,\napplying h to all fields), and have only the desired fields.\nThe partitioning\nphase is illustrated in Figure 14.3.\nOriginal relation\nDisk\nB main memory buffers\nPartitions\nDisk\n2\nB-1\nFigure 14.3\nPartitioning Phase of Hash-Based Projection\nTwo tuples that belong to different partitions are guaranteed not to be dupli-\ncates because they have different hash values. Thus, if two tuples are duplicates,\nthey are in the same partition. In the duplicate elimination phase, we read in\nthe B-1 partitions one at a time to eliminate duplicates.\nThe basic idea\nis to build an in-memory hash table as we process tuples in order to detect\nduplicates.\nFor each partition produced in the first phase:\n1. Read in the partition one page at a time.\nHash each tuple by applying\nhash function h2 (1= h) to the combination of all fields and then insert it\ninto an in-memory hash table. If a new tuple hashes to the same value as\nsome existing tuple, compare the two to check whether the new tuple is a\nduplicate. Discard duplicates as they are detected.\n2. After the entire partition has been read in, write the tuples in the hash table\n(which is free of duplicates) to the result file. Then clear the in-memory\nhash table to prepare for the next partition.\nNote that h2 is intended to distribute the tuples in a partition across many\nbuckets to minimize collisions (two tuples having the same h2 values). Since\nall tuples in a given partition have the same h value, h2 cannot be the same as\nh!\nThis ha.'3h-based projection strategy will not work well if the size of the ha.'3h\ntable for a partition (produced in the partitioning phase) is greater than the\nnumber of available buffer pages B.\nOne way to handle this paTtit'ion oveT-\nflow problem is to recursively apply the hash-based projection technique to\neliminate the duplicates in each partition that overflows.\nThat is, we divide\n\nEvaluating Relational Operators\n451\nan overflowing partition into subpartitions, then read each subpartition into\nmemory to eliminate duplicates.\nIf we assume that h distributes the tuples with perfect uniformity and that the\nnumber of pages of tuples after the projection (but before duplicate elimination)\nis T, each partition contains\nB~l pages. (Note that the number of partitions\nis B-1 because one of the buffer pages is used to read in the relation during\nthe partitioning phase.) The size of a partition is therefore\nB~l' and the size\nof a hash table for a partition is\nB~l . f; where f is a fudge factor used to\ncapture the (small) increase in size between the partition and a hash table for\nthe partition. The number of buffer pages B must be greater than the partition\nsize B~l .f to avoid partition overflow. This observation implies that we require\napproximately B > j\"J-:r buffer pages.\nNow let us consider the cost of hash-based projection.\nIn the partitioning\nphase, we read R, at a cost of M I/Os. We also write out the projected tuples,\na total of T pages, where T is some fraction of M, depending on the fields that\nare projected out. The cost of this phase is therefore M + T l/Os; the cost of\nhashing is a CPU cost, and we do not take it into account. In the duplicate\nelimination phase, we have to read in every partition.\nThe total number of\npages in all partitions is T. We also write out the in-memory hash table for\neach partition after duplicate elimination; this hash table is part of the result\nof the projection, and we ignore the cost of writing out result tuples, as usual.\nThus, the total cost of both phases is M + 2T. In our projection on Reserves\n(Figure 14.2), this cost is 1000 + 2 . 250 = 1500 l/Os.\n14.3.3\nSorting Versus Hashing for Projections\nThe sorting-based approach is superior to hashing if we have many duplicates\nor if the distribution of (hash) values is very nonuniform. In this case, some\npartitions could be much larger than average, and a hash table for such a par-\ntition would not fit in memory during the duplicate elimination phase. Also,\na useful side effect of using the sorting-based approach is that the result is\nsorted. Further, since external sorting is required for a variety of reasons, most\ndatabase systems have a sorting utility, which can be used to implement pro-\njection relatively easily.\nFor these rea..-;ons, sorting is the standard approach\nfor projection. And perhaps due to a simplistic use of the sorting utility, un-\nwanted attribute removal and duplicate elimination are separate steps in many\nsystems (i.e., the basic sorting algorithm is often used without the refinements\nwe outlined).\nWe observe that, if we have B > IT buffer pages, where T is the size of\nthe projected relation before duplicate elimination, both approaches have the\n\n452\nI--~--~\"-\nCHAPTER\n1~\nProjection in Commercial Systems: InfotmLxuses hashing. IBMDB2,\nOracle 8, and Sybase ASE use sorting. Microsoft SQL Server and Sybase\nASIQ implement both hash-based and sort-based algorithms.\nsame I/O cost. Sorting takes two passes. In the first pass, we read AI pages\nof the original relation and write out T pages.\nIn the second pa<;s, we read\nthe T pages and output the result of the projection.\nUsing hashing, in the\npartitioning pha<;e, we read M pages and write T pages' worth of partitions.\nIn the second phase, we read T pages and output the result of the projection.\nThus, considerations such as CPU costs, desirability of sorted order in the\nresult, and skew in the distribution of values drive the choice of projection\nmethod.\n14.3.4\nUse of Indexes for Projections\nNeither the hashing nor the sorting approach utilizes any existing indexes.\nAn existing index is useful if the key includes all the attributes we wish to\nretain in the projection.\nIn this case, we can simply retrieve the key values\nfrom the index-without ever accessing the actual relation-and apply our\nprojection techniques to this (much smaller) set of pages.\nThis technique,\ncalled an index-only scan, and wa<; discussed in Sections 8.5.2 and 12.3.2. If\nwe have an ordered (i.e., a tree) index whose search key includes the wanted\nattributes as a prefix, we can do even better: Just retrieve the data entries\nin order, discarding unwanted fields, and compare adjacent entries to check\nfor duplicates. The index-only scan technique is discussed further in Section\n15.4.1.\n14.4\nTHE JOIN OPERATION\nConsider the following query:\nSELECT *\nFROM\nReserves R, Sailors S\nWHERE\nR.sid = S.sid\nThis query can be expressed in relational algebra using the join operation:\nR\n[Xl S.\nThe join operation, one of the most useful operations in relational\nalgebra, is the primary means of combining information from two or more\nrelations.\n\nEvaluating Relational OpemtoTs\n453\n,------------------_._.__._.._._..__\n__ _ _ _-_._----_._._--_ _-----------,\nJoins in Commercial Systems: Syba..<;eASE suppodsindex nested loop\nand sort-merge join. Sybase ASIQ supports page-oriented nested loop, in-\ndex nested loop, simple hash, and sort-merge join, in addition to join in-\ndexes (which we discuss in Chapter 25). Ol'acle8stippoitspage-oriented\nnested loops join, sort-merge join, and a variant of hybrid hash join. IBM\nDB2 supports block nested loop, sort-merge, and hybrid hash join.\nMi-\ncrosoft SQL Server supports block nested loops, index' nested loops, 80rt-\nmerge, hash join, and a technique called ha.sh team.s.\nInformix supports\nblock nested loops, index nested loops, and hybrid hash join.\nAlthough a join can be defined as a cross-product followed by selections and pro-\njections, joins arise much more frequently in practice than plain cross-products.\nFurther, the result of a cross-product is typically much larger than the result of\na join, so it is very important to recognize joins and implement them without\nmaterializing the underlying cross-product. Joins have therefore received a lot\nof attention.\nWe now consider several alternative techniques for implementing joins.\nWe\nbegin by discussing two algorithms (simple nested loops and block nested loops)\nthat essentially enumerate all tuples in the cross-product and discard tuples\nthat do not meet the join conditions.\nThese algorithms are instances of the\nsimple iteration technique mentioned in Section 12.2.\nThe remaining join algorithms avoid enumerating the cross-product.\nThey\nare instances of the indexing and partitioning techniques mentioned in Section\n12.2. Intuitively, if the join condition consists of equalities, tuples in the two\nrelations can be thought of &'3 belonging to partitions, such that only tuples in\nthe same partition can join with each other; the tuples in a partition contain\nthe same values in the join columns. Index nested loops join scans one of the\nrelations and, for each tuple in it, uses an index on the (join columns of the)\nsecond relation to locate tuples in the same partition. Thus, only a subset of\nthe second relation is compared with a given tuple of the first relation, and the\nentire cross-product is not enumerated. The last two algorithms (sort-merge\njoin and hash join) also take advantage of join conditions to partition tuples in\nthe relations to be joined and compare only tuples in the same partition while\ncomputing the join, but they do not rely on a pre-existing index. Instead, they\neither sort or hash the relations to be joined to achieve the partitioning.\nWe discuss the join of two relations Rand S, with the join condition Ri = Sj,\nusing positional notation. (If we have more complex join conditions, the basic\nidea behind each algorithm remains essentially the same. \\Ve discuss the details\nin Section 14.4.4.) vVe assmne AI pages in R with PI? tuples per page and N\n\n454\nCHAPTER 14\npages in S with PS tuples per page. \\;Ve use R and S in our presentation of the\nalgorithms, and the Reserves and Sailors relations for specific examples.\n14.4.1\nNested Loops Join\nThe simplest join algorithm is a tuple-at-a-time nested loops evaluation. We\nscan the outer relation R, and for each tuple r E R, we scan the entire inner\nrelation S. The cost of scanning R is M l/Os. We scan S a total of PR . Iv!\ntimes, and each scan costs N l/Os. Thus, the total cost is M + PR . Iv! . N.\nf oreach tuple r E R do\nforeach tuple s E S do\nif ri==Sj then add (r, s) to result\nFigure 14.4\nSimple Nested Loops Join\nSuppose we choose R to be Reserves and S to be Sailors.\nThe value of M\nis then 1,000, PR is 100, and N is 500. The cost of simple nested loops join\nis 1000 + 100 . 1000 . 500 page l/Os (plus the cost of writing out the result;\nwe remind the reader again that we uniformly ignore this component of the\ncost). The cost is staggering: 1000 + (5· 107 ) I/Os. Note that each I/O costs\nabout lams on current hardware, which means that this join will take about\n140 hours!\nA simple refinement is to do this join page-at-a-time: For each page of R, we\ncan retrieve each page of S and write out tuples (r, s) for all qualifying tuples\nr E R-page and\nS E S-page.\nThis way, the cost is M to scan R, as before.\nHowever, S is scanned only M times, and so the total cost is M + Iv! . N.\nThus, the page-at-a-time refinement gives us an improvement of a factor of PRo\nIn the example join of the Reserves and Sailors relations, the cost is reduced\nto 1000 + 1000 . 500 = 501, 000 I/Os and would take about 1.4 hours.\nThis\ndramatic improvement underscores the importance of page-oriented operations\nfor minimizing disk I/O.\nFrom these cost formulas a straightforward observation is that we should choose\nthe outer relation R to be the smaller of the two relations (R [XJ B = B [XJ R,\nas long as we keep track of field names). This choice does not change the costs\nsignificantly, however. If we choose the smaller relation, Sailors, as the outer\nrelation, the cost of the page-at-a-time algorithm is 500 + 500 ·1000 = 500,500\nI/Os, which is only marginally better than the cost of page-oriented simple\nnested loops join with Reserves as the outer relation.\n\nEvahLating Relat1:onal OperatoTs\nBlock Nested Loops Join\n455\nThe simple nested loops join algorithm does not effectively utilize buffer pages.\nSuppose we have enough memory to hold the smaller relation, say, R, with\nat least two extra buffer pages left over. \\Ve can read in the smaller relation\nand use one of the extra buffer pages to scan the larger relation S. For each\ntuple S E 5, we check R and output a tuple (1', s) for qualifying tuples s (i.e.,\nri =\nSj).\nThe second extra buffer page)s used as an output buffer.\nEach\nrelation is scanned just once, for a total I/O cost of 1\\1 + N, which is optimal.\nIf enough memory is available, an important refinement is to build an in-\nmemory hash table for the smaller relation R. The I/O cost is still M + N, but\nthe CPU cost is typically much lower with the hash table refinement.\nWhat if we have too little memory to hold the entire smaller relation? We can\ngeneralize the preceding idea by breaking the relation R into blocks that can\nfit into the available buffer pages and scanning all of 5 for each block of R. R\nis the outer relation, since it is scanned only once, and S is the inner relation,\nsince it is scanned multiple times. If we have B buffer pages, we can read in\nB-2 pages of the outer relation R and scan the inner relation S using one of\nthe two remaining pages.\nWe can write out tuples (1', s), where r E R-block,\nS E S-page, and ri = Sj, using the last buffer page for output.\nAn efficient way to find matching pairs of tuples (i.e., tuples satisfying the\njoin condition ri = Sj) is to build a main-memory hash table for the block of R.\nBecause a hash table for a set of tuples takes a little more space than just the\ntuples themselves, building a hash table involves a trade-off: The effective block\nsize of R, in terms of the number of tuples per block, ~s reduced. Building a hash\ntable is well worth the effort. The block nested loops algorithm is described in\nFigure 14.5. Buffer usage in this algorithm is illustrated in Figl.lre 14.6.\nforeach block of B-2 pages of R do\nforeach page of 5 do {\nfor all matching in-memory tuples T E R-block and s E S-page,\nadd (1', s) to result\n}\nFigure 14.5\nBlock Nested Loops Join\nThe cost of this strategy is !vI 1/0s for reading in R (which is scanned only\nonce). 5 is scanned a total of r:~21times-ignoring the extra space required\nper page due to the in-memory hash table--·and each scan costs N l/Os. The\ntotal cost is thus I\\1J + N . rrf!2 1·\n\n456\nCHAPTER lA\nJoin result\n~-\n=:::>\n~\n~\n~\nr=...\n.\"\n···-+iQD\n~~-~~\nI\nl,-~Sh table for block R I\nJ\n(k < B-1 pages)\nf-------j-,=O\nD~\nInput buffer\nOutput buffer\n(to scan all of S)\n~\n~\n~\nRelations Rand S\nr==--~:)'\nI\nDisk\nB main memory buffers\nDisk\nFigure 14.6\nBuffer Usage in Block Nested Loops Join\nConsider the join of the Reserves and Sailors relations. Let us choose Reserves\nto be the outer relation R and assume we have enough buffers to hold an in-\nmemory hash table for 100 pages of Reserves (with at least two additional\nbuffers, of course). We have to scan Reserves, at a cost of 1000 l/Os. For each\nlOa-page block of Reserves, we have to scan Sailors.\nTherefore, we perform\n10 scans of Sailors, each costing 500 l/Os. The total cost is 1000 + 10 . 500 =\n6000 l/Os. If we had only enough buffers to hold 90 pages of Reserves, we\nwould have to scan Sailors flOOO/90l = 12 times, and the total cost would be\n1000 + 12·500 = 7000 l/Os,\nSuppose we choose Sailors to be the outer relation R instead. Scanning Sailors\ncosts 500 l/Os. We would scan Reserves f500/100l = 5 times. The total cost\nis 500 + 5 . 1,000 = 5500 l/Os. If instead we have only enough buffers for 90\npages of Sailors, we would scan Reserves a total of f500/901 = 6 times. The\ntotal cost in this case is 500 + 6 . 1000 = 6500 l/Os. We note that the block\nnested loops join algorithm takes a little over a minute on our running example,\na.ssuming lOms per I/O as before.\nImpact of Blocked Access\nIf we consider the effect of blocked access to several pages, there is a funda-\nmental change in the way we allocate buffers for block nested loops. Rather\nthan using just one buffer page for the inner relation, the best approach is to\nsplit the buffer pool evenly between the two relations. This allocation results\nin more pa.sses over the inner relation, leading to more page fetches. However,\nthe time spent on seeking for pages is dramatically reduced.\nThe technique of double buffering (discussed in Chapter 13 in the context of\nsorting) can also be used, but we do not discuss it further.\n\nEvaluating Relational Operators\nIndex Nested Loops Join\n4.57\nIf there is an index on one of the relations on the join attribute(s), we can take\nadvantage of the index by making the indexed relation be the inner relation.\nSuppose we have a suitable index on S; Figure 14.7 describes the index nested\nloops join algorithm.\nforeach tuple r E R do\nforeach tuple s E S where ri == Sj\nadd (1', s) to result\nFigure 14.7\nIndex Nested Loops Join\nFor each tuple\nT\nE R, we use the index to retrieve matching tuples of S.\nIntuitively, we compare r only with tuples of S that are in the same partition,\nin that they have the same value in the join column. Unlike the other nested\nloops join algorithms, therefore, the index nested loops join algorithm does not\nenumerate the cross-product of Rand S.\nThe cost of scanning R is M, as\nbefore. The cost of retrieving matching S tuples depends on the kind of index\nand the number of matching tuples; for each R tuple, the cost is a..s follows:\n1. If the index on S is a B+ tree index, the cost to find the appropriate leaf\nis typically 2---4 l/Os. If the index is a hash index, the cost to find the\nappropriate bucket is 1-2 1/Os.\n2. Once we find the appropriate leaf or bucket, the cost of retrieving matching\nS tuples depends on whether the index is clustered. If it is, the cost per\nouter tuple r E R is typically just one more I/O. If it is not clustered, the\ncost could be one I/O per matching S-tuple (since each of these could be\non a different page in the worst case).\nAs an example, suppose that we have a hash-based index using Alternative (2)\non the sid attribute of Sailors and that it takes about 1.2 l/Os on average2\nto retrieve the appropriate page of the index.\nSince sid is a key for Sailors,\nwe have at most one matching tuple. Indeed, .sid in Reserves is a foreign key\nreferring to Sailors, and therefore we have exactly one matching Sailors tuple\nfor each Reserves tuple.\nLet us consider the cost of scanning Reserves and\nusing the index to retrieve the matching Sailors tuple for each Reserves tuple.\nThe cost of scanning Reserves is 1000. There are 100 . 1000 tuples in Reserves.\nFor each of these tuples, retrieving the index page containing the rid of the\nmatching Sailors tuple costs 1.2 l/Os (on average); in addition, we have to\nretrieve the Sailors page containing the qualifying tuple. Therefore, we have\n2This is a typical cost for hash-ba.~ed indexes,\n\n458\nCHAPTER\nl~\n100,000 ·(1 + 1.2) l/Os to retrieve matching Sailors tuples. The total cost is\n221,000 l/Os.\nAs another example, suppose that we have a hash-based index using Alternative\n(2) on the sid attribute of Reserves.\nNow we can scan Sailors (500 l/Os),\nand for each tuple, use the index to retrieve matching Reserves tuples.\nWe\nhave a total of 80 . 500 Sailors tuples, and each tuple could match with either\nzero or more Reserves tuples; a sailor may have no reservations or several.\nFor each Sailors tuple, we can retrieve the index page containing the rids of\nmatching Reserves tuples (assuming that we have at most one such index page,\nwhich is a reasonable guess) in 1.2 l/Os on average. The total cost thus far is\n500 + 40,000 . 1.2 = 48,500 l/Os.\nIn addition, we have the cost of retrieving matching Reserves tuples. Since we\nhave 100,000 reservations for 40,000 Sailors, assuming a uniform distribution\nwe can estimate that each Sailors tuple matches with 2.5 Reserves tuples on\naverage. If the index on Reserves is clustered, and these matching tuples are\ntypically on the same page of Reserves for a given sailor, the cost of retrieving\nthem is just one I/O per Sailor tuple, which adds up to 40,000 extra l/Os.\nIf the index is not clustered, each matching Reserves tuple may well be on\na different page, leading to a total of 2.5 . 40,000 l/Os for retrieving qualify-\ning tuples. Therefore, the total cost can vary from 48,500+40,000=88,500 to\n48,500+100,000=148,500 l/Os. Assuming 10ms per I/O, this would take about\n15 to 25 minutes.\nSo, even with an unclustered index, if the number of matching inner tuples for\neach outer tuple is small (on average), the cost of the index nested loops join\nalgorithm is likely to be much less than the cost of a simple nested loops join.\n14.4.2\nSort-Merge Join\nThe basic idea behind the sort-merge join algorithm is to SOTt both relations\non the join attribute and then look for qualifying tuples T E Rand s E S\nby essentially TneTging the two relations.\nThe sorting step groups all tuples\nwith the same value in the join column and thus makes it easy to identify\npartitions, or groups of tuples with the same value, in the join column. We\nexploit this partitioning by comparing the R tuples in a partition with only the\nS tuples in the same partition (rather than with all S tuples), thereby avoiding\nenumeration of the cross-product of Rand S. (This partition-ba.<-;ed approach\nworks only for equality join conditions.)\nThe external sorting algorithm discussed in Chapter 13 can be used to do the\nsorting, and of course, if a relation is already sorted on the join attribute, we\n\nEvaluating Relational OperatoTs\n4~9\nneed not sort it again. vVe now consider the merging step in detail: vVe scan\nthe relations Rand S) looking for qualifying tuples (i.e., tuples Tr in Rand\nTs in S such that Tri = Tsj ). The two scans start at the first tuple in each\nrelation. vVe advance the scan of R as long as the current R tuple is less than\nthe current S tuple (with respect to the values in the join attribute). Similarly,\nwe advance the scan of S as long as the current S tuple is less than the current\nR tuple. \\Ve alternate between such advances until we find an R tuple Tr and\na S tuple Ts with Tri = TSj'\nWhen we find tuples Tr and Ts such that Tri = Tsj , we need to output the\njoined tuple. In fact, we could have several R tuples and several S tuples with\nthe same value in the join attributes as the current tuples Tr and Ts.\nWe\nrefer to these tuples as the current R partition and the current S partition. For\neach tuple r in the current R partition, we scan all tuples s in the current S\npartition and output the joined tuple (r, s). We then resume scanning Rand\nS, beginning with the first tuples that follow the partitions of tuples that we\njust processed.\nThe sort-merge join algorithm is shown in Figure 14.8. We assign only tuple\nvalues to the variables Tr, Ts, and Gs and use the special value eof to denote\nthat there are no more tuples in the relation being scanned. Subscripts identify\nfields, for example, Tri denotes the ith field of tuple Tr. If Tr has the value\neof, any comparison involving Tri is defined to evaluate to false.\nWe illustrate sort-merge join on the Sailors and Reserves instances shown in\nFigures 14.9 and 14.10, with the join condition being equality on the sid at-\ntributes.\nThese two relations are already sorted on sid, and the merging phase of the\nsort-merge join algorithm begins with the scans positioned at the first tuple of\neach relation instance. We advance the scan of Sailors, since its sid value, now\n22, is less than the sid value of Reserves, which is now 28. The second Sailors\ntuple h&<; sid = 28, which is equal to the sid value of the current Reserves tuple.\nTherefore, we now output a result tuple for each pair of tuples, one from Sailors\nand one from Reserves, in the current partition (i.e., with sid = 28). Since we\nhave just one Sailors tuple with sid = 28 and two such Reserves tuples, we\nwrite two result tuples. After this step, we position the scan of Sailors at the\nfirst tuple after the partition with sid = 28, which ha.<; sid = 31. Similarly, we\nposition the scan of Reserves at the first tuple with sid = 31. Since these two\ntuples have the same sid values, we have found the next matching partition,\nand we must write out the result tuples generated from this partition (there\nare three such tuples). After this, the Sailors scan is positioned at the tuple\nwith sid = 36, and the Reserves scan is positioned at the tuple with sid = 58.\nThe rest of the merge phase proceeds similarly.\n\n460\nCHAPTER 14\nproc smjoin(R, B,'Hi = By)\nif R not sorted on attribute i, sort it;\nif B not sorted on attribute j, sort it;\nTr = first tuple in R;\nTs = first tuple in B;\nGs = first tuple in S;\nwhile Tr i= eo! and Gs -I=- eo! do {\nwhile Tri < GSj do\nTr = next tuple in Rafter Tr;\nwhile Tri > GSj do\nGs = next tuple in S after Gs\n/ / ranges over R\n/ / ranges over S\n/ / start of current S-partition\n/ / continue scan of R\n/ / continue scan of B\nTs = Gs;\n/ / Needed in case Tri i= GSj\nwhile Tri == GSj do {\n/ / process current R partition\nTs = Gs;\n/ / reset S partition scan\nwhile TSj == Tri do {\n/ / process current R tuple\nadd (Tr, Ts) to result;\n/ / output joined tuples\nTs = next tuple in S after Ts;} / / advance S partition scan\nTr = next tuple in Rafter Tr;\n/ / advance scan of R\n}\n/ / done with current R partition\nGs = Ts;\n}\n/ / initialize search for next S partition\nFigure 14.8\nSort-Merge Join\n~ snarrw\nmUng~\n~day\n22\ndustin\n7\n45.0\n28\nyuppy\n9\n35.0\n31\nlubber\n8\n55.5\n36\nlubber\n6\n36.0\n44\nguppy\n5\n35.0\n58\nrusty\n10\n35.0\nFigure 14.9\nAn Instance of Sailors\n28\n103\n12/04/96\nguppy\n28\n103\n11/03/96\n}'uppy\n31\n101\n10/10/96\ndustin\n31\n102\n10/12/96\nlubber\n31\n101\n10/11/96\nlubber\n58\n103\n11/12/96\ndustin\nFigure 14.10\nAn Instance of Reserves\n\nEvaluating Relational Operators\n·161\n~\nIn general, we have to scan a partition of tuples in the second relation as often\nas the number of tuples in the corresponding partition in the first relation.\nThe first relation in the example, Sailors, ha.', just one tuple in each partition.\n(This is not happenstance but a consequence of the fact that sid is a\nkey~~~\nthis example is a key-foreign key join.)\nIn contra.\"t, suppose that the join\ncondition is changed to be sname=7'name. Now, both relations contain more\nthan one tuple in the partition with sname=mame='lubber'. The tuples with\nrname= 'lubber' in Reserves have to be scanned for each Sailors tuple with\nsname='lubber'.\nCost of Sort~Merge Join\nThe cost of sorting R is O(/vlloglv1) ancl the cost of sorting S is O(NlogN).\nThe cost of the merging phase is /vI + N if no S partition is scanned multiple\ntimes (or the necessary pages are found in the buffer after the first pass). This\napproach is especially attractive if at least one relation is already sorted on the\njoin attribute or has a clustered index on the join attribute.\nConsider the join of the relations Reserves and Sailors. Assuming that we have\n100 buffer pages (roughly the same number that we assumed were available\nin our discussion of block nested loops join), we can sort Reserves in just two\npasses.\nThe first pass produces 10 internally sorted runs of 100 pages each.\nThe second pass merges these 10 runs to produce the sorted relation. Because\nwe read and write Reserves in each pass, the sorting cost is 2·2 . 1000 = 4000\nl/Os. Similarly, we can sort Sailors in two passes, at a cost of 2 . 2 . 500 = 2000\nl/Os. In addition, the seconcl phase of the sort-merge join algorithm requires\nan additional scan of both relations.\nThus the total cost is 4000 + 2000 +\n1000 + 500 = 7500 l/Os, which is similar to the cost of the block nested loops\nalgorithm.\nSuppose that we have only 35 buffer pages. \\Ve can still sort both Reserves and\nSailors in two passes, and the cost of the sort-merge join algorithm remains at\n7500 l/Os. However, the cost of the block nested loops join algorithm is more\nthan 15,000 l/Os.\nOn the other hand, if we have\n~300 buffer pages, the cost\nof the\nsort~merge join remains at 7500 I/Os, whereas the cost of the block\nnested loops join drops to 2500 l/Os. (We leave it to the reader to verify these\nnumbers. )\n\\Ve note that multiple scans of a partition of the second relation are potentially\nexpensive. In our example, if the number of Reserves tuples in a repeatedly\nscanned partition is small (say, just a few pages), the likelihood of finding the\nentire partitiOli in the buffer pool on repeated scans is very high, and the I/O\ncost remains essentially the same as for a single scan. However, if many pages\n\n462\nCHAPTER 14\nof Reserves tuples are in a given partition, the first page of such a partition\nmay no longer be in the buffer pool when we request it a second time (after\nfirst scanning all pages in the partition; remember that each page is unpinned\na'3 the scan moves past it). In this ca.se, the I/O cost could be as high as the\nnumber of pages in the Reserves partition times the number of tuples in the\ncorresponding Sailors partition!\nIn the worst-case scenario, the merging phase could require us to read the\ncomplete second relation for each tuple in the first relation, and the number of\nl/Os is O(M . N) l/Os! (This scenario occurs when all tuples in both relations\ncontain the same value in the join attribute; it is extremely unlikely.)\nIn practice, the I/O cost of the merge phase is typically just a single scan of\neach relation. A single scan can be guaranteed if at least one of the relations\ninvolved has no duplicates in the join attribute; this is the case, fortunately,\nfor key~foreign key joins, which are very common.\nA Refinement\nWe assumed that the two relations are sorted first and then merged in a distinct\npass. It is possible to improve the sort-merge join algorithm by combining the\nmerging phase of sorting with the merging phase of the join. First, we produce\nsorted runs of size B for both Rand 5. If B > VI, where L is the size of the\nlarger relation, the number of runs per relation is less than VI. Suppose that\nthe number of buffers available for the merging pha.<;e is at lea'3t 2 VI; that\nis, more than the total number of runs for Rand 5. We allocate one buffer\npage for each run of R and one for each run of 5. We then merge the runs of\nR (to generate the sorted version of R), merge the runs of 5, and merge the\nresulting Rand 5 streams a'3 they are generated; we apply the join condition\nas we merge the Rand S streams and discard tuples in the cross--product that\ndo not meet the join condition.\nUnfortunately, this idea increa<;es the number of buffers required to 2JI. How-\never, by using the technique discussed in Section 13.3.1 we can produce sorted\nruns of size approximately 2· B for both Rand 5. Consequently, we have fewer\nthan VI/2 runs of each relation, given the assumption that B > VI. Thus,\nthe total number of runs is less than VI, that is, less than B, and we can\ncombine the merging pha.ses with no need for additional buffers.\nThis approach allows us to perform a sort-merge join at the cost of reading and\nwriting Rand S in the first pa.ss and reading Rand 5 in the second pass. The\ntotal cost is thus\n~1 . (At + N). In our example, the cost goes down from 7500\nto 4500 l/Os.\n\nEvaluating Relatio'nal Operators\n463\n}\nBlocked Access and Double-Buffering\nThe blocked I/O and double-buffering optimizations, discussed in Chapter 13\nin the context of sorting, can be used to speed up the merging pass as well as\nthe sorting of the relations to be joined; we do not discuss these refinements.\n14.4.3\nHash Join\nThe hash join algorithm, like the sort-merge join algorithm, identifies par-\ntitions in Rand S in a partitioning phase and, in a subsequent probing\nphase, compares tuples in an R partition only with tuples in the correspond-\ning 5 partition for testing equality join conditions. Unlike sort-merge join, hash\njoin uses hashing to identify partitions rather than sorting. The partitioning\n(also called building) pha..\"Je of hash join is similar to the partitioning in hash-\nbased projection and is illustrated in Figure 14.3.\nThe probing (sometimes\ncalled matching) phase is illustrated in Figure 14.11.\nDisk\no 0\n0\nOutput buffer\nHash table for partition Ri\n(k < B-1 pages)\nB main memory bulTers\nDisk\nPartitions of Rand S\nJoin result\n~\nFigure 14.11\nProbing Phase of Hash Join\nThe idea is to hash both relations on the join attribute, using the same hash\nfunction h. If we ha..'3h each relation (ideally uniformly) into k partitions, we\nare assured that R tuples in partition i can join only with S tuples in the same\npartition i.\nThis observation can be used to good effect:\nWe can read in a\n(complete) partition of the smaller relation R and scan just the corresponding\npartition of S for matches.\n\\iVe never need to consider these Rand S tuples\nagain. Thus, once Rand S are partitioned, we can perform the join by reading\nin Rand 5 just once, provided enough memory is available to hold all the\ntuples in any given partition of R.\nIn practice we build an in-memory hash table for the R partition, using a ha..'3h\nfunction h2 that is different from h (since h2 is intended to distribute tuples\nin a partition based on h), to reduce CPU costs. \\Ne need enough memory to\nhold this hash table, which is a little larger than the R partition itself.\n\n464\nCHAPTER\n~4\nThe hash join algorithm is presented in Figure 14.12. (There are several variants\non this idea; this version is called Grace hash join in the literature.) Consider\nthe cost of the hash join algorithm.\nIn the partitioning phase, we have to\nscan both Rand S once and write them out once.\nThe cost of this phase\nis therefore 2(l'vi + N).\nIn the second phase, we scan each partition once,\nassuming no partition overflows, at a cost of .M + N I/Os. The total cost is\ntherefore 3(AI + N), given our assumption that each partition fits into memory\nin the second phase. On our example join of Reserves and Sailors, the total\ncost is 3 . (500 + 1000) = 4500 I/Os, and assuming lOms per I/O, hash join\ntakes under a minute. Compare this with simple nested loops join, which took\nabout 140 houTs--this difference underscores the importance of using a good\njoin algorithm.\n/ / Partition R into k partitions\nforeach tuple r E R do\nread T and add it to buffer page h(ri);\n/ / Partition S into k partitions\nforeach tuple s E S do\nread s and add it to buffer page h(sj);\n/ / Probing phase\nfor I = 1, ... ,k do {\n/ / flushed as page fills\n/ / flushed as page fills\n/ / Build in-memory hash table for Rz, using h2\nforeach tuple T E partition Rz do\nread r and insert into hash table using h2(ri) ;\n/ / Scan Sz and probe for matching Rz tuples\nforeach tuple s E partition Sz do {\nread s and probe table using h2(sj);\nfor matching R tuples T, output (7', s) };\nclear hash table to prepare for next partition;\n}\nFigure 14.12\nHash Join\nMemory Requirements and Overflow Handling\nTo increase the chances of a given partition fitting into available memory in\nthe probing phase, we must minimize the size of a partition by maximizing\nthe number of partitions. In the partitioning phase, to partition R (similarly,\n\nEvaluating Relational OperatOTS\n465\n8) into k partitions, we need at least k output buffers and one input buffer.\nTherefore, given B buffer pages, the maximum number of partitions is k =\nB - 1. Assuming that partitions are equal in size, this means that the size of\neach R partition is t!l (a'3 usual, Ai is the number of pages of R). The number\nof pages in the (in-memory) hash table built during the probing phase for a\npartition is thus\n~'~'i, where f is a fudge factor used to capture the (small)\nincrease in size between the partition and a hash table for the partition.\nDuring the probing phase, in addition to the hash table for the R partition,\nwe require a buffer page for scanning the 8 partition and an output buffer.\nTherefore, we require B >\n-k'~~ + 2.\nWe need approximately B > Jf . AI for\nthe hash join algorithm to perform well.\nSince the partitions of R are likely to be close in size but not identical, the\nlargest partition is somewhat larger than t!l' and the number of buffer pages\nrequired is a little more than B > Jf . AI. There is also the risk that, if the\nhash function h does not partition R uniformly, the hash table for one or more\nR partitions may not fit in memory during the probing phase. This situation\ncan significantly degrade performance.\nAs we observed in the context of hash-based projection, one way to handle this\npartition overflow problem is to recursively apply the hash join technique to the\njoin of the overflowing R partition with the corresponding 8 partition. That\nis, we first divide the Rand 8 partitions into subpartitions. Then, we join the\nsubpartitions pairwise. All subpartitions of R probably fit into memory; if not,\nwe apply the hash join technique recursively.\nUtilizing Extra Memory: Hybrid Hash Join\nThe minimum amount of memory required for ha.'3h join is B > Jf . AI. If\nmore memory is available, a variant of ha.'3h join called hybrid hash join\noHers better performance. Suppose that B > f· (lYI/k) , for some integer k.\nThis means that, if we divide R into k partitions of size AI/k, an in-memory\nhash table can be built for each partition. To partition R (similarly, 5) into k\npartitions, we need k output buHers and one input buHer: that is, k + 1 pages.\nThis leaves us with B - (k + 1) extra pages during the partitioning pha.<;e.\nSuppose that B - (k + 1) > f . (1'.,1/k). That is, we have enough extra memory\nduring the partitioning phase to hold an in-memory hash table for a partition\nof R. The idea behind hybrid hash join is to build an in-memory ha.<;h table\nfor the first partition of R during the partitioning pha.se, which means that\nwe do not write this partition to disk. Similarly, while partitioning 8, rather\nthan write out the tuples in the first partition of 5, we can directly probe the\n\n466\nCHAPTER }4\nin-memory table for the first R partition and write out the results. At the end\nof the partitioning phase, we have completed the join of the first partitions of\nRand S, in addition to partitioning the two relations; in the probing phase,\nwe join the remaining partitions as in hash join.\nThe savings realized through hybrid hash join is that we avoid writing the first\npartitions of Rand S to disk during the partitioning phase and reading them\nin again during the probing phase. Consider our example, with 500 pages in\nthe smaller relation Rand 1000 pages in S. 3 If we have B = 300 pages, we can\neasily build an in-memory hash table for the first R partition while partitioning\nR into two partitions. During the partitioning phase of R, we scan R and write\nout one partition; the cost is 500 + 250 if we assume that the partitions are of\nequal size. We then scan S and write out one partition; the cost is 1000 + 500.\nIn the probing phase, we scan the second partition of R and of S; the cost is\n250 + 500. The total cost is 750 + 1500 + 750 = 3000. In contrast, the cost of\nhash join is 4500.\nIf we have enough memory to hold an in-memory hash table for all of R, the\nsavings are even greater. For example, if B > f . N + 2, that is, k = 1, we can\nbuild an in-memory hash table for all of R. This llleans that we read R only\nonce, to build this hash table, and read S once, to probe the R hash table. The\ncost is 500 + 1000 = 1500.\nHash Join Versus Block Nested Loops Join\nWhile presenting the block nested loops join algorithm, we briefly discussed\nthe idea of building an in-memory hash table for the inner relation. We now\ncompare this (more CPU-efficient) version of block nested loops join with hybrid\nhash join.\nIf a hash table for the entire smaller relation fits in memory, the two algorithms\nare identical. If both relations are large relative to the available buffer size, we\nrequire several passes over one of the relations in block nested loops join; hash\njoin is a more effective application of hashing techniques in this case. The I/O\nsaved in this case by using the hash join algorithm in comparison to a block\nnested loops join is illustrated in Figure 14.13. In the latter, we read in all of\nS for each block of R; the I/O cost corresponds to the whole rectangle. In the\nhash join algorithm, for each block of R, we read only the corresponding block\nof S; the I/0 cost corresponds to the shaded areas in the figure. This difference\nin I/O due to scans of S is highlighted in the figure.\n3It is unfortunate, that in our running example, the smaller relation, which we denoted by the\nvariable R in our discussion of hash join, is in fact the Sailors relation, which is more naturally\ndenoted by 8!\n\nEval1tat'ing Relational OpemtoT8\n81\n82\nS3\n54\nS5\nFigure 14.13\nHash Join Vs. Block Nested Loops for Large Relations\n467.\nWe note that this picture is rather simplistic. It does not capture the costs\nof scanning R in the block nested loops join and the partitioning phase in the\nhash join, and it focuses on the cost of the probing phase..\nHash Join Versus Sort-Merge Join\nLet us compare hash join with sort-merge join. If we have B > VM buffer\npages, where M is the number of pages in the smaller relation and we assume\nuniform partitioning, the cost of hash join is 3(M + N) l/Os.\nIf we have\nB > VN buffer pages, where N is the number of pages in the larger relation,\nthe cost of sort-merge join is also 3(NI + N), as discussed in Section 14.4.2. A\nchoice between these techniques is therefore governed by other factors, notably:\nII\nIf the partitions in hash join are not uniformly sized, hash join could cost\nmore. Sort-merge join is less sensitive to such data skew.\nII\nIf the available number of buffers falls between -1M andVN, hash join\ncosts less than sort-merge join, since we need only enough memory to hold\npartitions of the smaller relation, wherea'3 in sort-merge join the memory\nrequirements depend on the size of the larger relation.\nThe larger the\ndifference in size between the two relations, the more important this factor\nbecomes.\nII\nAdditional considerations include the fact that the result is sorted in sort-\nmerge join.\n14.4.4\nGeneral Join Conditions\nWe have discussed several join algorithms for the case of a simple equality\njoin condition.\nOther important cases include a join condition that involves\nequalities over several attributes and inequality conditions.\nTo illustrate the\nca.'3C of several equalities, we consider the join of Reserves R and Sailors 8 with\nthe join condition R.sid=S.s'id 1\\ R.rname=S.sname:\n\n468\nCHAPTER 1.4\n•\nFor index nested loops join, we can build an index on Reserves on the\ncombination of fields (R.sid, R.rname) and treat Reserves as the inner\nrelation. vVe can also use an existing index on this combination of fields,\nor on R.s'id, or on R.marne. (Similar remarks hold for the choice of Sailors\nas the inner relation, of course.)\n•\nFor sort-merge join, we sort Reserves on the combination of fields (sid,\nmarne) and Sailors on the combination of fields (sid, snarne).\nSimilarly,\nfor hash join, we partition on these combinations of fields.\n•\nThe other join algorithms we discussed are essentially unaffected.\nIf we have an {nequality comparison, for example, a join of Reserves Rand\nSailors 5 with the join condition R.rnarne < S.sname:\n•\nWe require a B+ tree index for index nested loops join.\n•\nHash join and sort-merge join are not applicable.\n•\nThe other join algorithms we discussed are essentially unaffected.\nOf course, regardless of the algorithm, the number of qualifying tuples in an\ninequality join is likely to be much higher than in an equality join.\nWe conclude our presentation of joins with the observation that no one join\nalgorithm is uniformly superior to the others. The choice of a good algorithm\ndepends on the sizes of the relations being joined, available access methods,\nand the size of the buffer pool. This choice can have a considerable impact on\nperformance because the difference between a good and a bad algorithm for a\ngiven join can be enormous.\n14.5\nTHE SET OPERATIONS\nWe now briefly consider the implementation of the set operations R n 5, R x S,\nR u 5, and R - S. From an implementation standpoint, intersection and cr08S-\nproduct can be seen as special cases of join (with equality on all fields &'S the\njoin condition for intersection, and with no join condition for cross-product).\nTherefore, we will not discuss them further.\nThe main point to acldress in the implementation of union is the elimination\nof duplicates. Set-difference can also be implemented using a variation of the\ntechniques for duplicate elimination.\n(Union and difference queries on a sin-\ngle relation can be thought of as a selection query with a complex selection\ncondition.\nThe techniques discussecl in Section 14.2 are applicable for such\nqueries.)\n\nEval'uating Relational Operators\n469\n~\nThere are two implementation algorithms for union and set-difference, again\nbased 011 sorting and hashing. Both algorithms are instances of the partitioning\ntechnique mentioned ill Section 12.2.\n14.5.1\nSorting for Union and Difference\nTo implement R uS:\n1. Sort R using the combination of all fields; similarly, sort S.\n2. Scan the sorted Rand S in parallel and merge them, eliminating duplicates.\nAs a refinement, we can produce sorted runs of Rand S and merge these\nruns in parallel. (This refinement is similar to the one discussed in detail for\nprojection.) The implementation of R- S is similar. During the merging pass,\nwe write only tuples of R to the result, after checking that they do not appear\nin S.\n14.5.2\nHashing for Union and Difference\nTo implement R U S:\n1. Partition Rand S using a hash function h.\n2. Process each partition I as follows:\n•\nBuild an in-memory hash table (using hash function h2 i= h) for SI.\n•\nScan RI. For each tuple, probe the hash table for SI. If the tuple is in\nthe ha.,,>h table, discard it; otherwise, add it to the table.\n•\nWrite out the ha.'3h table and then dear it to prepare for the next\npartition.\nTo implement R - S, we proceed similarly. The difference is in the processing\nof a partition. After building an in-memory ha.,,>h table for SI, we scan Rz. For\neach Rz tuple, we probe the hcl.,')h table; if the tuple is not in the table, we write\nit to the result.\n14.6\nAGGREGATE OPERATIONS\nThe SQL query shown in Figure 14.14 involves an aggregate opemtion, AVG.\nThe other aggregate operations supported in SQL-92 are MIN, MAX, SUM, and\nCOUNT.\n\n470\nSELECT AVG(S.age)\nFROM\nSailors S\nFigure 14.14\nSimple Aggregation Query\nCHAPTER 14\nThe basic algorithm for aggregate operators consists of scanning the entire\nSailors relation and maintaining some running information about the scanned\ntuples; the details are straightforward. The running information for each ag-\ngregate operation is shown in Figure 14.15. The cost of this operation is the\ncost of scanning all Sailors tuples.\nI Aggregate Operation I Running Inforrniation\nSUM\nTotal of the values retrieved\nAVG\n(Total, Count) of the values retrieved\nCOUNT\nCount of values retrieved.\nMIN\nSmallest value retrieved\nMAX\nLargest value retrieved\nFigure 14.15\nRunning Information for Aggregate Operations\nAggregate operators can also be used in combination with a GROUP BY clause.\nIf we add GROUP BY rating to the query in Figure 14.14, we would have to\ncompute the average age of sailors for each rating group.\nFor queries with\ngrouping, there are two good evaluation algorithms that do not rely on an\nexisting index: One algorithm is based on sorting and the other is based on\nhashing. Both algorithms are instances of the partitioning technique mentioned\nin Section 12.2.\nThe sorting approach is simple-we sort the relation on the grouping attribute\n(rating) and then scan it again to compute the result of the aggregate operation\nfor each group. The second step is similar to the way we implement aggregate\noperations without grouping, with the only additional point being that we have\nto watch for group boundaries. (It is possible to refine the approach by doing\naggregation as part of the sorting step; we leave this as an exercise for the\nreader.) The I/O cost of this approach is just the cost of the sorting algorithm.\nIn the hashing approach we build a hash table (in main memory, if possible)\non the grouping attribute. The entries have the form (gTOuping-value, running-\ninfo). The running information depends on the aggregate operation, as per the\ndiscussion of aggregate operations without grouping. As we scan the relation,\nfor each tuple, we probe the hash table to find the entry for the group to which\nthe tuple belongs and update the running information. 'When the h&'3h table\nis cOlnplete, the entry for a grouping value can be used to compute the answer\ntuple for the corresponding group in the obvious way. If the hash table fits in\n\nEvaluating Relational OpemtoTs\n471\nmemory, which is likely because each entry is quite small and there is only one\nentry per grouping value, the cost of the hashing approach is O(.iV1), where 1V!\nis the size of the relation.\nIf the relation is so large that the hash table does not fit in memory, we can\npartition the relation using a hash function h on gTOuping-value. Since all tuples\nwith a given grouping value are in the same partition, we can then process each\npartition independently by building an in-memory hash table for the tuples in\nit.\n14.6.1\nImplementing Aggregation by Using an Index\nThe technique of using an index to select a subset of useful tuples is not ap-\nplicable for aggregation.\nHowever, under certain conditions, we can evaluate\naggregate operations efficiently by using the data entries in an index instead of\nthe data records:\n•\nIf the search key for the index includes all the attributes needed for the\naggregation query, we can apply the techniques described earlier in this\nsection to the set of data entries in the index, rather than to the collection\nof data records and thereby avoid fetching data records.\n•\nIf the GROUP BY clause attribute list forms a prefix of the index search\nkey and the index is a tree index, we can retrieve data entries (and data\nrecords, if necessary) in the order required for the grouping operation and\nthereby avoid a sorting step.\nA given index may support one or both of these techniques; both are examples\nof index-only plans. We discuss the use of indexes for queries with grouping and\naggregation in the context of queries that also include selections and projections\nin Section 15.4.1.\n14.7\nTHE IMPACT OF BUFFERING\nIn implementations of relational operators, effective use of the buffer pool is\nvery important, and we explicitly considered the size of the buffer pool in de-\ntermining algorithm parameters for several of the algorithms discussed. There\nare three main points to note:\n1. If several operations execute concurrently, they share the buffer pool. This\neffectively reduces the number of buffer pages available for each operation.\n2. If tuples are accessed using an index, especially an unclustered index, the\nlikelihood of finding a page in the buffer pool if it is requested multiple\n\n472\nCHAPTER 14\ntimes depends (in a rather unpredictable way, unfortunately) on the size of\nthe buffer pool and the replacement policy. Further, if tuples are accessed\nusing an unclustered index, each tuple retrieved is likely to require us to\nbring in a new page; therefore, the buffer pool fills up quickly, leading to a\nhigh level of paging activity.\n3. If an operation has a pattern of repeated page accesses, we can increa..<;e\nthe likelihood of finding a page in memory by a good choice of replacement\npolicy or by reseTving a sufficient number of buffers for the operation (if the\nbuffer manager provides this capability). Several examples of such patterns\nof repeated access follow:\n•\nConsider a simple nested loops join. :For each tuple of the outer re-\nlation, we repeatedly scan all pages in the inner relation. If we have\nenough buffer pages to hold the entire inner relation, the replacement\npolicy is irrelevant. Otherwise, the replacement policy becomes criti-\ncal. With LRU, we will never find a page when it is requested, because\nit is paged out. This is the sequential flooding problem discussed in\nSection 9.4.1. With MRU, we obtain the best buffer utilization~the\nfirst B-2 pages of the inner relation always remain in the buffer pool.\n(B is the number of buffer pages; we use one page for scanning the\nouter relation4 and always replace the la..'3t page used for scanning the\ninner relation.)\n•\nIn a block nested loops join, for each block of the outer relation, we\nscan the entire inner relation. However, since only one unpinned page\nis available for the scan of the inner rel~tion, the replacement policy\nmakes no difference.\n11III\nIn an index nested loops join, for each tuple of the outer relation, we\nuse the index to find matching inner tuples. If several tuples of the\nouter relation have the same value in the join attribute, there is a\nrepeated pattern of access on the inner relation; we can maximize the\nrepetition by sorting the outer relation on the join attributes.\n14.8\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIIIIJ\nConsider a simple selection query of the form (JR.attr op 1)aluAR). What\nare the alternative access paths in each of these cases:\n(i) there is no\nindex and the file is not sorted, (ii) there is no index but the file is sorted.\n(Section 14.1)\n4Think about the sequence of pins and unpins used to achieve this.\n\nEvalllat'ing Relational Operators\n473\n•\nIf a B+ tree index matches the selection condition, how does clustering\naffect the cost? Discuss this in terms of the selectivity of the condition.\n(Section 14.1)\n•\nDescribe conjunctive normal form for general selections. Define the terms\nconjunct and disfunct.\nUnder what conditions does a general selection\ncondition match an index? (Section 14.2)\n•\nDescribe the various implementation options for general selections. (Sec-\ntion 14.2)\n•\nDiscuss the use of sorting versus hashing to eliminate duplicates during\nprojection. (Section 14.3)\n•\nWhen can an index be used to implement projections, without retrieving\nactual data records?\n~Then does the index additionally allow us to elimi-\nnate duplicates without sorting or hashing? (Section 14.3)\n•\nConsider the join of relations Rand 5. Describe simple nested loops join\nand block nested loops join. What are the similarities and differences? How\ndoes the latter reduce I/O costs? Discuss how you would utilize buffers in\nblock nested loops. (Section 14.4.1)\n•\nDescribe index nested loops join. How does it differ from block nested loops\njoin? (Section 14.4.1)\n•\nDescribe sort-merge join of Rand 5. What join conditions are supported?\nWhat optimizations are possible beyond sorting both Rand 5 on the join\nattributes and then doing a merge of the two? In particular, discuss how\nsteps in sorting can be combined with the merge P&<;s. (Section 14.4.2)\n•\nWhat is the idea behind hash join? What is the additional optimization in\nhybrid hash join? (Section 14.4.3)\n•\nDiscuss how the choice of join algorithm depends on the number of buffer\npages available, the sizes of Rand 5, and the indexes available. Be spe-\ncific in your discussion and refer to cost formulas for the I/O cost of eae:h\nalgorithm. (Sections 14.12 Section 14.13)\n•\nHow are general join conditions handled? (Section 14.4.4)\n•\n\\Vhy are the set operations R n 5 and R x S special cases of joins? What is\nthe similarity between the set operations Ru 5 and R - 5? (Section 14.5)\n•\nDiscuss the use of sorting versus hashing in implementing Ru 5 and R - S.\nCompare this with the ilnplementation of projection. (Section 14.5)\n•\nDiscuss the use of running injomwtion in implementing aggregate opera-\ntions. Discuss the use of sorting versus hashing for dealing with grouping.\n(Section 14.6)\n\n474\nCHAPTER l4\n•\nUnder what conditions can we use an index to implement aggregate oper-\nations without retrieving data records? Under what conditions do indexes\nallow us to avoid sorting or ha.~hing? (Section 14.6)\n•\nUsing the cost formulas for the various relational operator evaluation algo-\nrithms, discuss which operators are most sensitive to the number of avail-\nable buffer pool pages. How is this number influenced by the number of\noperators being evaluated concurrently? (Section 14.7)\n•\nExplain how the choice of a good buffer pool replacement policy can in-\nfluence overall performance. Identify the patterns of access in typical rela-\ntional operator evaluation and how they influence the choice of replacement\npolicy. (Section 14.7)\nEXERCISES\nExercise 14.1 Briefly answer the following questions:\n1. Consider the three basic techniques, iteration, indexing, and partitioning, and the rela-\ntional algebra operators selection, projection, and join. For each technique-operator pair,\ndescribe an algorithm based on the technique for evaluating the operator.\n2. Define the term most selective access path for a query.\n3. Describe conjunctive normal form, and explain why it is important in the context of\nrelational query evaluation.\n4. When does a general selection condition match an index? What is a primary term in a\nselection condition with respect to a given index?\n5. How does hybrid hash join improve on the basic hash join algorithm?\n6. Discuss the pros and cons of hash join, sort-merge join, and block nested loops join.\n7. If the join condition is not equality, can you use sort-merge join? Can you use hash join?\nCan you use index nested loops join? Can you use block nested loops join?\n8. Describe how to evaluate a grouping query with aggregation operator MAX using a sorting-\nbased approach.\n9. Suppose that you are building a DBMS and want to add a new aggregate operator called\nSECOND LARGEST, which is a variation of the MAX operator.\nDescribe how you would\nimplement it.\n10. Give an example of how buffer replacement policies can affect the performance of a join\nalgorithm.\nExercise 14.2 Consider a relation R( a, b, c, d, e) containing 5,000,000 records, where each data\npage of the relation holds 10 records. R is organized as a sorted file with secondary indexes.\nAssume that R.a is a candidate key for R, with values lying in the range 0 to 4,999,999, and\nthat R is stored in R.a order. For each of the following relational algebra queries, state which\nof the following approaches (or combination thereof) is most likely to be the cheapest:\n•\nAccess the sorted file for R directly.\n\nEvaluating Relational Operators\n•\nUse a clustered B+ tree index on attribute R.a.\n•\nUse a linear hashed index on attribute R.a.\n•\nUse a clustered B+ tree index on attributes (R.a, R.b).\n•\nUse a linear hashed index on attributes (R.a, R.b).\n•\nUse an unclustered B+ tree index on attribute R.b.\n1.\nO\"u<50,OOOAb<50,ooo(R)\n2.\n0\"u=50,OOOAb<50,OOO (R)\n3.\nO\"u>50,OOOAb=50,ooo(R)\n4.\n0\"u=50,OOOi\\a=50,OlO (R)\n5.\nO\"a#50,OOOi\\b=50,Ooo(R)\n6.\n0\"a<50,OOOvb=50,OOO (R)\nExercise 14.3 Consider processing the following SQL projection query:\nSELECT DISTINCT E.title, E.ename FROM Executives E\nYou are given the following information:\nExecutives has attributes ename, title, dname, and address; all are string fields of\nthe same length.\nThe ename attribute is a candidate key.\nThe relation contains 10,000 pages,\nThere are 10 buffer pages.\n475\nConsider the optimized version of the sorting-based projection algorithm: The initial sorting\npass reads the input relation and creates sorted runs of tuples containing only attributes ename\nand title. Subsequent merging passes eliminate duplicates while merging the initial runs to\nobtain a single sorted result (as opposed to doing a separate pass to eliminate duplicates from\na sorted result containing duplicates).\n1. How many sorted runs are produced in the first pass? What is the average length of\nthese runs?\n(Assume that memory is utilized well and any available optimization to\nincrease run size is used.) What is the I/O cost of this sorting pass?\n2. How many additional merge passes are required to compute the final result of the pro-\njection query? What is the I/O cost of these additional passes?\n3.\n(a) Suppose that a clustered B+ tree index on tWe is available. Is this index likely to\noffer a cheaper alternative to sorting? Would your answer change if the index were\nunclustered? Would your answer change if the index were a hash index?\n(b) Suppose that a clustered B+ tree index on ename is available. Is this index likely\nto offer a cheaper alternative to sorting? Would your answer change if the index\nwere unclustered? Would your answer change if the index were a hash index?\n(c) Suppose that a clustered B+ tree index on (ename, title) is available. Is this index\nlikely to offer a cheaper alternative to sorting? Would your answer change if the\nindex were unclustered? Would your answer change if the index were a hash index?\n4. Suppose that the query is as follows:\nSELECT E.title, E.ename FROM Executives E\n\n476\nCHAPTER\n~4\nThat is, you are not required to do duplicate elimination. How would your answers to\nthe previous questions change?\nExercise 14.4 Consider the join RiXJR.a=SbS, given the following information about the\nrelations to be joined. The cost metric is the number of page l/Os unless otherwise noted,\nand the cost of writing out the result should be uniformly ignored.\nRelation R contains 10,000 tuples and has 10 tuples per page.\nRelation S contains 2000 tuples and also has 10 tuples per page.\nAttribute b of relation S is the primary key for S.\nBoth relations are stored as simple heap files.\nNeither relation has any indexes built on it.\n52 buffer pages are available.\n1. What is the cost of joining Rand S using a page-oriented simple nested loops join? What\nis the minimum number of buffer pages required for this cost to remain unchanged?\n2. What is the cost of joining Rand S using a block nested loops join? What is the minimum\nnumber of buffer pages required for this cost to remain unchanged?\n3. What is the cost of joining Rand S using a sort-merge join? What is the minimum\nnumber of buffer pages required for this cost to remain unchanged?\n4. What is the cost of joining Rand S using a hash join? What is the minimum number of\nbuffer pages required for this cost to remain unchanged?\n5. What would be the lowest possible I/O cost for joining Rand S using any join algorithm,\nand how much buffer space would be needed to achieve this cost? Explain briefly.\n6. How many tuples does the join of R. and S produce, at most, and how many pages are\nrequired to store the result of the join back on disk?\n7. Would your answers to any of the previous questions in this exercise change if you were\ntold that R.a is a foreign key that refers to S.b?\nExercise 14.5 Consider the join of R. and S described in Exercise 14.1.\n1. With 52 buffer pages, if unclustered B+ indexes existed on R.a and S.b, would either\nprovide a cheaper alternative for performing the join (using an index nested loops join)\nthan a block nested loops join? Explain.\n(a) Would your answer change if only five buffer pages were available?\n(b) vVould your answer change if S contained only 10 tuples instead of 2000 tuples?\n2. vVith 52 buffer pages, if clustered B+ indexes existed on R.a and S.b, would either provide\na cheaper alternative for performing the join (using the index nested loops algorithm)\nthan a block nested loops join? Explain.\n(a) Would your answer change if only five buffer pages were available?\n(b) Would your answer change if S contained only 10 tuples instead of 2000 tuples?\n3. If only 15 buffers were available, what would be the cost of a sort-merge join? What\nwould be the cost of a hash join?\n4. If the size of S were increased to also be 10,000 tuples, but only 15 buffer pages were\navailable, what would be the cost of a sort-merge join? What would be the cost of a\nhash join?\n\nEvaluating Relational Operators\n477\n5. If the size of S \\vere increased to also be 10,000 tuples, and 52 buffer pages were available,\nwhat would be the cost of sort-merge join? \\Vhat would be the cost of hash join?\nExercise 14.6 Answer each of the questions~ifsome question is inapplicable, explain why--\nin Exercise 14.1 again but using the following information about Rand S:\nRelation R contains 200,000 tuples and has 20 tuples per page.\nRelation S contains 4,000,000 tuples and also ha.\" 20 tuples per page.\nAttribute a of relation R is the primary key for R.\nEach tuple of R joins with exactly 20 tuples of S.\n1,002 buffer pages are available.\nExercise 14.7 We described variations of the join operation called olLter joinB in Section 5.6.4\n. One approach to implementing an outer join operation is to first evaluate the corresponding\n(inner) join and then add additional tuples padded with null values to the result in accordance\nwith the semantics of the given outer join operator.. However, this requires us to compare\nthe result of the inner join with the input relations to determine the additional tuples to be\nadded. The cost of this comparison can be avoided by modifying the join algorithm to add\nthese extra tuples to the result while input tuples are processed during the join. Consider the\nfollowing join algorithms: block nested loops join, index nested loops join, sort-merge join, and\nhash join. Describe how you would modify each of these algorithms to compute the following\noperations on the Sailors and Reserves tables discussed in this chapter:\n1. Sailors NATURAL LEFT OUTER JOIN Reserves\n2. Sailors NATURAL RIGHT OUTER JOIN Reserves\n3. Sailors NATURAL FULL OUTER JOIN Reserves\nPROJECT-BASED EXERCISES\nExercise 14.8 (Note to instructors: Additional details m71st be provided if this exen:ise is\nassigned; see AppendiJ: SO.) Implement the various join algorithms described in this chapter\nin Minibase. (As additional exercises, you Inay want to implement selected algorithms for the\nother operators as well.)\nBIBLIOGRAPHIC NOTES\nThe implementation techniques used for relational operators in System R are discussed in\n[101]. The implementation techniques used in PRTV, which utilized relational algebra trans-\nformations and a form of multiple-query optimization, are discussed in [358]. The techniques\nused for aggregate operations in Ingres are described in [246]. [324] is an excellent survey of\nalgorithms for implementing relational operators and is recommended for further reading.\nHash-based techniques are investigated (and compared with sort-based techniques) in [1 10],\n[222], [:325], and [677].\nDuplicate elimination is discussed in [99].\n[277] discusses secondary\nstorage access patterns arising in join implementations. Parallel algorithms for implementing\nrelational operations are discussed in [99, 168,220.224,2:3:3, 293,534].\n\n15\nA TYPICAL RELATIONAL\nQUERY OPTIMIZER\n..\nHow are SQL queries translated into relational algebra? As a conse-\nquence, what class of relation algebra queries does a query optimizer\nconcentrate on?\n..\nWhat information is stored in the system catalog of a DBMS and how\nis it used in query optimization?\n..\nHow does an optimizer estimate the cost of a query evaluation plan?\n..\nHow does an optimizer generate alternative plans for a query? What\nis the space of plans considered? What is the role of relational algebra\nequivalences in generating plans?\n..\nHow are nested SQL queries optimized?\n..\nKey concepts: SQL to algebra, query block; system catalog, data\ndictionary, metadata, system statistics, relational representation of\ncatalogs;\ncost estimation, size estimation, reduction factors;\nhis-\ntograms, equiwidth, equidepth, compressed;\nalgebra equivalences,\npushing selections, join ordering; plan space, single-relation plans,\nmulti-relation left-deep plans; enumerating plans, dynamic program-\nming approach, alternative approaches\nLife is what happens while you're busy making other plam.\n-John Lennon\nIn this chapter, we present a typical relational query optimizer in detail. We\nbegin by discussing how SQL queries are converted into units called blocks\n478\n\nA Typical Q'uery Optirn:izeT\n479\ni\n~\nand how blocks are translated into (extended) relational algebra expressions\n(Section 15.1).\nThe central task of an optimizer is to find a good plan for\nevaluating such expressions. Optimizing a relational algebra expression involves\ntwo basic steps:\n•\nEnumerating alternative plans for evaluating the expression. Typically, an\noptimizer considers a subset of all possible plans because the number of\npossible plans is very large.\n•\nEstimating the cost of each enumerated plan and choosing the plan with\nthe lowest estimated cost.\nWe discuss how to use system statistics to estimate the properties of the result\nof a relational operation, in particular result sizes, in Section 15.2. After dis-\ncussing how to estimate the cost of a given plan, we describe the space of plans\nconsidered by a typical relational query optimizer in Sections 15.3 and 15.4.\nWe discuss how nested SQL queries are handled in Section 15.5.\nWe briefly\ndiscuss some of the influential choices made in the System R query optimizer\nin Section 15.6. We conclude with a short discussion of other approaches to\nquery optimization in Section 15.7.\nWe consider a number of example queries using the following schema:\nSailors(sid: integer, sname: string, rating: integer, age: real)\nBoats( bid: integer, bname: string, color: string)\nReserves(sid: integer, bid: integer, day: dates, mame: string)\nAs in Chapter 14, we assume that each tuple of Reserves is 40 bytes long, that a\npage can hold 100 Reserves tuples, and that we have 1000 pages of such tuples.\nSimilarly, we assume that each tuple of Sailors is 50 bytes long, that a page\ncan hold 80 Sailors tuples, and that we have 500 pages of such tuples.\n15.1\nTRANSLATING SQL QUERIES INTO ALGEBRA\nSQL queries are optimized by decomposing them into a collection of smaller\nunits, called blocks. A typical relational query optimizer concentrates on\nop~\ntimizing a single block at a time.\nIn this section, we describe how a query\nis decomposed into blocks and how the optimization of a single block can be\nunderstood in tenus of plans composed of relational algebra operators.\n15.1.1\nDecomposition of a Query into Blocks\nvVhen a user submits an SQL query, the query is parsed into a collection of\nquery blocks and then passed on to the query optimizer.\nA query block\n\n480\nSELECT\nFROM\nWHERE\nCHAPTER 15\nS.siel, MIN (Relay)\nSailors S, Reserves R, Boats B\nS.siel = Rsiel AND Rbid = B.bid AND Rcolor = 'red' AND\nS.rating = ( SELECT MAX (S2.rating)\nFROM\nSailors S2 )\nGROUP BY S.sid\nHAVING\nCOUNT (*) > 1\nFigure 15.1\nSailors Reserving Red Boats\n(or simply block) is an SQL query with no nesting and exactly one SELECT\nclause and one FROM clause and at most one WHERE clause, GROUP BY clause,\nand HAVING clause. The WHERE clause is assumed to be in conjunctive normal\nform, as per the discussion in Section 14.2. We use the following query 8.'3 a\nrunning example:\nFaT each 8ailor' with the highe8t mting (oveT all sailors) and at least two Teser-\nvat'lons faT Ted boats, find the sailoT id and the earliest date on which the sailor\nIULS a TeseTvat:ion faT a Ted boat.\nThe SQL version of this query is shown in Figure 15.1.\nThis query has two\nquery blocks. The nested block is:\nSELECT MAX (S2.rating)\nFROM\nSailors S2\nThe nested block computes the highest sailor rating. The outer block is shown\nin Figure 15.2. Every SQL query can be decomposed into a collection of query\nblocks without nesting.\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.sid, MIN (Rday)\nSailors S, Reserves R, Boats B\nS.sid = Rsiel AND Rbicl = B.bid AND\nS.rating = RefeTence to nested block\nS.sid\nCOUNT (*) > 1\nRcolor = 'red' AND\nFigure 15.2\nOuter Block of Red Boats Query\nThe optimizer examines the system catalogs to retrieve information about the\ntypes and lengths of fields, statistics about the referenced relations, and the\naccess paths (indexes) available for them. The optimizer then considers each\nquery block and chooses a query evaluation plan for that block. \\Ve focus 1nostly\non optimizing a single query block and defer a discussion of nested queries to\nSection 15.5.\n\nA Typical Query Opt'imizer\n15.1.2\nA Query Block as a Relational Algebra Expression\n481\nThe first step in optimizing a query block is to express it as a relational algebra\nexpression. For uniformity, let us a.<;sume that GROUP BY and HAVING are also\noperators in the extended algebra used for plans and that aggregate operations\nare allowed to appear in the argument list of the projection operator.\nThe\nmeaning of the operators should be clear from our discussion of SQL. The SQL\nquery of Figure 15.2 can be expressed in the extended algebra as:\n1rS.sid,M I N(R.day) (\nHAVINGcoUNT(*»2(\nGROUP BYs.sid (\n0\"S. sid= R.sidAR.bid=B .bidAB.coloT='Ted'AS.Tating=valuc_Irom_nested_block (\nSailors x Reserves x Boats))))\nFor brevity, we used S, R, and B (rather than Sailors, Reserves, and Boats)\nto prefix attributes. Intuitively, the selection is applied to the cross-product of\nthe three relations. Then the qualifying tuples are grouped by S.sid, and the\nHAVING clause condition is used to discard some groups. For each remaining\ngroup, a result tuple containing the attributes (and count) mentioned in the\nprojection list is generated. This algebra expression is a faithful summary of\nthe semantics of an SQL query, which we discussed in Chapter 5.\nEvery SQL query block can be expressed as an extended algebra expression\nhaving this form. The SELECT clause corresponds to the projection operator,\nthe WHERE clause corresponds to the selection operator, the FROM clause corre-\nsponds to the cross-product of relations, and the remaining clauses are mapped\nto corresponding operators in a straightforward manner.\nThe alternative plans examined by a typical relational query optimizer can be\nunderstood by recognizing that a query is essentially tT'cated as a 0\"1r x algebm\ne;cpression, with the remaining operations (if any, in a given query) carried\nout on the result of the 0'1r x expression. The enr x expression for the query in\nFigure 15.2 is:\n1rS. sid,R.day(\na S.sid=R.sidAR.b'id= B .bidAB.coloT='n:d'AS'.1'atlTl.g=value_fTO/lLTlcste(Lblock (\nSailors x Reserves x Boats))\nTo make sure that the GROUP BY and HAVING operations in the query can be\ncarried out, the attributes mentioned in these clauses are added to the projec-\ntion list. Further, since aggregate operations in the SELECT cla.use, such a.s the\nMIN (R.day) operation in our example, are computed after first computing the\nCJ1r x part of the query, aggregate expressions in the projectioll list are replaced\n\n482\nCHAPTER 15\nby the names of the attributes to which they refer. Thus, the optimization of\nthe a1r x part of the query essentially ignores these aggregate operations.\nThe optimizer finds the best plan for the a1r x expression obtained in this\nmanner from a query.\nThis plan is evaluated and the resulting tuples are\nthen sorted (alternatively, hashed) to implement the GROUP BY clause.\nThe\nHAVING clause is applied to eliminate some groups, and aggregate expressions\nin the SELECT clause are computed for each remaining group. This procedure\nis summarized in the following extended algebra expression:\n1rS.sid,MI N(R.day) (\nH AVI NGcouNT(*»2(\nGROUP BYS.sid(\n1rS.sid,R.day (\na S.sid=R. sid/\\R.bid=B.bid/\\B .color='red' /\\S.rating=value_fro1n_nested_block (\nSailors x Reserves x Boats)))))\nSome optimizations are possible if the FROM clause contains just one relation\nand the relation has some indexes that can be used to carry out the grouping\noperation. We discuss this situation further in Section 15.4.1.\nTo a first approximation therefore, the alternative plans examined by a typical\noptimizer can be understood in terms of the plans considered for a1r x queries.\nAn optimizer enumerates plans by applying several equivalences between rela-\ntional algebra expressions, which we present in Section 15.3. We discuss the\nspace of plans enumerated by an optimizer in Section 15.4.\n15.2\nESTIMATING THE COST OF A PLAN\nFor each enumerated plan, we have to estimate its cost. There are two parts\nto estimating the cost of an evaluation plan for a query block:\n1. For each node in the tree, we must estimate the cost of performing the corre-\nsponding operation. Costs are affected significantly by whether pipelining\nis used or temporary relations are created to pass the output of an operator\nto its parent.\n2. For each node in the tree, we must estimate the size of the result and\nwhether it is sorted. This result is the input for the operation that corre-\nsponds to the parent of the current node, and the size and sort order in\nturn affect the estimation of size, cost, and sort order for the panmt.\n\"\\Te discussed the cost of implementation techniques for relational operators in\nChapter 14. As we saw there, estimating costs requires knowledge of various\n\nA Typical Q71.er'1I OptimizeT\n483\nparameters of the input relations, such as the number of pages and available\nindexes. Such statistics are maintained in the DBMS's system catalogs. In this\nsection, we describe the statistics maintained by a typical DBMS and discuss\nhow result sizes are estimated. As in Chapter 14, we use the number of page\nl/Os as the metric of cost and ignore issues such as blocked access, for the sake\nof simplicity.\nThe estimates used by a DBMS for result sizes and costs are at best approx-\nimations to actual sizes and costs. It is unrealistic to expect an optimizer to\nfind the very best plan; it is more important to avoid the worst plans and find\na good plan.\n15.2.1\nEstimating Result Sizes\nWe now discuss how a typical optimizer estimates the size of the result com-\nputed by an operator on given inputs. Size estimation plays an important role\nin cost estimation as well because the output of one operator can be the input\nto another operator, and the cost of an operator depends on the size of its\ninputs.\nConsider a query block of the form:\nSELECT attTibute list\nFROM\nTelation list\nWHERE\nteTml 1\\ teTm2 1\\ .. . 1\\ teTmn\nThe maximum number of tuples in the result of this query (without duplicate\nelimination) is the product of the cardinalities of the relations in the FROM\nclause. Every term in the WHERE clause, however, eliminates some of these po-\ntential result tuples. We can model the effect of the WHERE clause on the result\nsize by associating a reduction factor with each term, which is the ratio of the\n(expected) result size to the input size considering only the selection represented\nby the term. The actual size of the result can be estimated as the maximum size\ntimes the product of the reduction factors for the terms in the WHERE clause.\nOf course, this estimate reflects the\nunrealistic but simplifying\na.ssurnption\nthat the conditions tested by each term are statistically independent.\n\\Ve now consider how reduction factors can be computed for different kinds of\nterms in the WHERE clause by using the statistics available in the catalogs:\nIII\ncolumn = value:\nFor a term of this form. the reduction factor can be\napproximated by Nf{ C~1js(I) if there is H11 ind~x I on column for the relation\nin question. This formula assumes uniform distribution of tuples among the\n\n484\n•\n•\n•\nCHAPTER \\5\nindex key values; this uniform distribution assumption is frequently made\nin arriving at cost estimates in a typical relational query optimizer. If there\nis no index on col'umn, the System R optimizer arbitrarily assumes that the\nreduction factor is rlJ. Of course, it is possible to maintain statistics such\nas the number of distinct values present for any attribute whether or not\nthere is an index on that attribute. If such statistics are maintained, we\ncan do better than the arbitrary choice of It.\ncolumni = column2: In this case the reduction factor can be approximated\nby MAX\n(NKeys(~1),NKeys(I2))\nif there are indexes Il and 12 on colmnnl and\ncolumn2, respectively.\nThis formula assumes that each key value in the\nsmaller index, say, Il, has a matching value in the other index.\nGiven\na value for columnl, we assume that each of the N J(eys(I2) values for\ncol'U'mn2 is equally likely.\nTherefore, the number of tuples that have the\nsame value in column2 as a given value in columni is\nN K ets(I2)' If only\none of the two columns has an index I, we take the reduction factor to\nbe\nNKe~1Js(I); if neither column has an index, we approximate it by the\nubiquitous rlJ. These formulas are used whether or not the two columns\nappear in the same relation.\n, l\nI\no' TI\n. ) 1\n.\nf 't'\n.\nd b\nHigh (I) -\nvalue\nCO umn > va,ue.\n1e Iec uctlOn ac or IS apprOXImate\ny High(I)\n~ Low(I)\nif there is an index 1 on column. If the column is not of an arithmetic type\nor there is no index, a fraction less than half is arbitrarily chosen. Similar\nformulas for the reduction factor can be derived for other range selections.\ncolumn IN (list of values): The reduction factor is taken to be the reduction\nfactor for column = value multiplied by the number of items in the list.\nHowever, it is allowed to be at most half, reflecting the heuristic belief that\neach selection eliminates at least half the candidate tuples.\nThese estimates for reduction factors are at best approximations that rely on <l..'3-\nsumptions such as uniform distribution of values and independent distribution\nof values in different columns. In recent years more sophisticated techniques\nba\"sed on storing more detailed statistics (e.g., histograms of the values in a\ncolumn, which we consider later in this section) have been proposed and are\nfinding their way into commercial systems.\nReduction factors can also be approximated for terms of the form column IN\nsuhquery (ratio of the estimated size of the subquery result to the number\nof distinct values in column in the outer relation); NOT condition (I-reduction\nfactor for condition); valuei<column< value2; the disjunction of two conditions;\nand so on, but \\ve will not discuss such reduction factors.\nTo summarize, regardless of the plan chosen, we can estimate the size of the\nfinal result by taking the product of the sizes of the relations in the FROM clause\n\nA T:yp'lcal query Opt'irnizer\n485\n$\nEstimating Query Characteristics:\nIBM\nDB2~ Informix, Nlicrosoft\nSQL Server, Oracle 8, and Sybase ASE all usehistograms to estimate query\ncharacteristics such as result size and cost.\nAs an example, Sybase ASE\nuses one-dimensional, equidepth histograms with some special attention\npaid to high frequency values, so that their count is estimated accurately.\nASE also keeps the average count of duplicates for each prefix of an index\nto estimate correlations between histograms for composite keys (although\nit does not maintain such histograms).\nASE also maintains estimates of\nthe degree of clustering in tables and indexes. IBM DB2, Informix, and Or-\nacle also use one-dimensional equidepth histograms; Oracle automatically\nswitches to maintaining a count of duplicates for each value when there\nare few values in a column.\nMicrosoft SQL Server uses one-dimensional\nequiarea histograms with some optimizations (adjacent buckets with sim-\nilar distributions are sometimes combined to compress the histogram). In\nSQL Server, the creation and maintenance of histograms is done automat-\nically with no need for user input.\nAlthough sampling techniques have been studied for estimating result sizes\nand costs, in current systems, sampling is used only by system utilities to\nestimate statistics or build histograms but not directly by the optimizer\nto estimate query characteristics. Sometimes, sampling is used to do load\nbalancing in parallel implementations.\nand the reduction factors for the terms in the WHERE clause. We can similarly\nestimate the size of the result of each operator in a plan tree by using reduction\nfactors, since the subtree rooted at that operator's node is itself a query block.\nNote that the number of tuples in the result is not affected by projections if du-\nplicate elimination is not performed. However, projections reduce the number\nof pages in the result because tuples in the result of a projection are smaller\nthan the original tuples; the ratio of tuple sizes can be used as a reduction\nfactor for projection to estimate the result size in pages, given the size of\nthe input relation.\nImproved Statistics: Histograms\nConsider a relation with N tuples and a selection of the form colu:rnn > value\non a column with\n,,),II index I.\nThe reduction factor\nT is approximated by\nI~:.~·~I~W_~ I:~:;)l(;), and the size of the result is estimated a\"s TN. This estimate\nrelies on the assumption that the distribution of values is uniform.\n\n486\nCHAPTER\n~5\nEstimates can be improved considerably by maintaining more detailed statistics\nthan just the low and high values in the index I.\nIntuitively, we want to\napproximate the distribution of key values I as accurately as possible. Consider\nthe two distributions of values shown in Figure 15.3. The first is a nonuniform\ndistribution D of values (say, for an attribute called age). The frequency of a\nvalue is the number of tuples with that age value; a distribution is represented\nby showing the frequency for each possible age value. In our example, the lowest\nage value is 0, the highest is 14, and all recorded age values are integers in the\nrange 0 to 14. The second distribution approximates D by assuming that each\nage value in the range ato 14 appears equally often in the underlying collection\nof tuples.\nThis approximation can be stored compactly because we need to\nrecord only the low and high values for the age range (0 and 14 respectively)\nand the total count of all frequencies (which is 45 in our example).\nDistribution D\n3\n3\nUnifonn distribution approximating D\n333333333333333\no\nI\n3\n4\n5\n6\n7\n8\n9\n10\nII\n12\n13\n14\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nFigure 15.3\nUniform vs. Nonuniform Distributions\nConsider the selection age> 13. Fl'om the distribution D in Figure 15.3, we\nsee that the result has 9 tuples. Using the uniform distribution approximation,\non the other hand, we estimate the result size as fs ·45 = 3 tuples. Clearly,\nthe estimate is quite inaccurate.\nA histogram is a data structure maintained by a DBMS to approximate a data\ndistribution. In Figure 15.4, we show how the data distribution from Figure\n15.3 can be approximated by dividing the range of age values into subranges\ncalled buckets, and for each bucket, counting the number of tuples with age\nvalues within that bucket. Figure 15.4 shows two different kinds of histograms,\ncalled equiwidth and equidepth, respectively.\nConsider the\ns~lection query age > 13 again and the first (equiwidth) his-\ntogram.\nWe can estimate the size of the result to be 5 because the selected\nrange includes a third of the range for Bucket 5. Since Bucket 5 represents a\ntotal of 15 tuples, the selected range corresponds to ~ . 15\n=\n5 tuples. As this\nexample shows, we a..ssume that the distribution within a histogram bucket is\nuniform. Therefore, when we simply maintain the high and low values for index\n\nA Typical Query Optimizer\n9.0\n487\nEquiwidlh\n50\nI I\n5.0\nEquideplh\n2.25\n2.5\nlilli/II\n5.0\nBuckel I\nBucket 2\nBuckel 3\nBucket 4\nBuckel 5\nCount;::):\\\nC()unt~\nCount:::::15\nCOllnC;:::)\nCounl-::::: 15\nBucket 1\nCoullt=9\nBucket 2\nBucket 3\nBucket 4\nBucket 5\nCounl:::::10\nCount-\",IO\nCounl=7\nCounl'\"l9\nFigure 15.4\nHistograms Approximating Distribution D\nI, we effectively use a 'histogram' with a single bucket. Using histograms with\na small number of buckets instead leads to much more accurate estimates, at\nthe cost of a few hundred bytes per histogram. (Like all statistics in a DBMS,\nhistograms are updated periodically rather than whenever the data is changed.)\nOne important question is how to divide the value range into buckets. In an\nequiwidth histogram, we divide the range into subranges of equal size (in\nterms of the age value range). We could also choose subranges such that the\nnumber of tuples within each subrange (i.e., bucket) is equal. Such a histogram,\ncalled an equidepth histogram, is also illustrated in Figure 15.4.\nConsider\nthe selection age > 13 again.\nUsing the equidepth histogram, we are led to\nBucket 5, which contains only the age value 15, and thus we arrive at the exact\nanswer, 9.\nWhile the relevant bucket (or buckets) generally contains more\nthan one tuple, equidepth histograms provide better estimates than equiwidth\nhistograms. Intuitively, buckets with very frequently occurring values contain\nfewer values, and thus the uniform distribution &'isumption is applied to a\nsmaller range of values, leading to better approximations. Conversely, buckets\nwith mostly infrequent values are approximated less accurately in an equidepth\nhistogram, but for good estimation, the frequent values are important.\nProceeding further with the intuition about the importance of frequent values,\nanother alternative is to maintain separate counts for a small number of very\nfrequent values, say the age values 7 and 14 in our example, and maintain an\nequidepth (or other) histogram to cover the remaining values. Such a histogram\nis called a compressed histogram.\nMost commercial DB1\\1Ss currently use\nequidepth histograms, and some use compressed histograms.\n\n488\nCHAPTER\n~5\n15.3\nRELATIONAL ALGEBRA EQUIVALENCES\nIn this section, we present several equivalences among relational algebra expres-\nsions; and in Section 15.4, we discuss the space of alternative plans considered\nby a optimizer.\nOur discussion of equivalences is aimed at explaining the role that such equiva-\nlences play in a System R style optimizer. In essence, a basic SQL query block\ncan be thought of as an algebra expression consisting of the cross-product of\nall relations in the FROM clause, the selections in the WHERE clause, and the\nprojections in the SELECT clause.\nThe optimizer can choose to evaluate any\nequivalent expression and still obtain the same result.\nAlgebra equivalences\nallow us to convert cross-products to joins, choose different join orders, and\npush selections and projections ahead of joins. For simplicity, we assume that\nnaming conflicts never arise and we need not consider the renaming operator\np.\n15.3.1\nSelections\nTwo important equivalences involve the selection operation. The first one in-\nvolves cascading of selections:\nGoing from the right side to the left, this equivalence allows us to combine sev-\neral selections into one selection. Intuitively, we can test whether a tuple meets\neach of the conditions C1 ... Cn. at the same time. In the other direction, this\nequivalence allows us to take a selection condition involving several conjuncts\nand replace it with several smaller selection operations. Replacing a selection\nwith several smaller selections turns out to be very useful in combination with\nother equivalences, especially commutation of selections with joins or cross-\nproducts, which we discuss shortly. Intuitively, such a replacement is useful in\ncases where only part of a complex selection condition can be pushed.\nThe second equivalence states that selections are commutative:\nIn other words,' we can test the conditions C1 and C2 in either order.\n15.3.2\nProjections\nThe rule for cascading projections says that successively elilninating columns\nfrom a relation is equivalent to sirnply eliminating all but the columns retained\n\nA Typical qu.ery OptimizeT\nby the final projection:\n489\nEach ai is a set of attributes of relation R, and ai\n~ aHl for i\n= 1 ... n -\n1.\nThis equivalence is useful in conjunction with other equivalences such as\ncommutation of projections with joins.\n15.3.3\nCross-Products and Joins\nTwo important equivalences involving cross-products and joins.\n~re present\nthem in terms of natural joins for simplicity, but they hold for general joins as\nwell.\nFirst, assuming that fields are identified by name rather than position, these\noperations are commutative:\nRx8\nRN8\n8xR\nThis property is very important. It allows us to choose which relation is to be\nthe inner and which the outer in a join of two relations.\nThe second equivalence states that joins and cross-products are associative:\nR x (8 x T)\nRN (8NT)\n(R x 8) x T\n(R N 8) NT\nThus we can either join Rand 8 first and then join T to the result, or join 8\nand T first and then join R to the result. The intuition behind associativity\nof cross-products is that, regardless of the order in which the three relations\nare considered, the final result contains the same columns. Join associativity is\nbased on the same intuition, with the additional observation that the selections\nspecifying the join conditions can be cascaded. Thus the same rows appear in\nthe final result, regardless of the order in which the relations are joined.\nTogether with commutativity, associativity essentially says that we can choose\nto join any p<l:ir of these relations, then join the result with the third relation,\nand always obtain the same final result. For example, let us verify that\nR N (8 N T)\n~\n(T,C><] R) N 8\nFrom commutativity, we have:\nRN (8NT)\nRN (TN 8)\n\n490\nFrom associativity, we have:\nRM (TM S)\nUsing commutativity again, we have:\n(R fxJ T) M S\nCHAPTER .15\nIn other words, when joining several relations, we are free to join the relations\nin any order we choose. This order-independence is fundamental to how a query\noptimizer generates alternative query evaluation plans.\n15.3.4\nSelects, Projects, and Joins\nSome important equivalences involve two or more operators.\nWe can commute a selection with a projection if the selection operation in-\nvolves only attributes retained by the projection:\nEvery attribute mentioned in the selection condition c must be included in the\nset of attributes a.\nWe can combine a selection with a cross-product to form a join, as per the\ndefinition of join:\nWe can commute a selection with a cross-product or a join if the selection\ncondition involves only attributes of one of the arguments to the cross-product\nor join:\noAR x S)\nac(R fxJ S)\nac(R) x S\nac(R) fxJ S\nThe attributes mentioned in c must appear only in R and not in S.\nSimilar\nequivalences hold if c involves only attributes of S and not R, of course.\nIn general, a selection a c on R x S can be replaced by a ca<;cade of selections\nac], aC2 , and aC;J such that Cl involves attributes of both Rand S, C2 involves\nonly attributes of R, and C;:l involves only attributes of S:\nUsing the ca...<;cading rule for selections, this expression is equivalent to\n\nA Typical Query Optimizer\n491\n:;.\nUsing the rule for commuting selections and cross-products, this expression is\nequivalent to\nCTC1 (CTC2 (R) x\nCTC3 (S))\nThus we can push part of the selection condition c ahead of the cross-product.\nThis observation also holds for selections in combination with joins. of course.\n\\Ve can commute a projection with a cross-product:\nwhere al is the subset of attributes in a that appear in R, and a2 is the subset\nof attributes in a that appear in S. We can also commute a projection with\na join if the join condition involves only attributes retained by the projection:\nwhere al is the subset of attributes in a that appear in R, and a2 is the subset\nof attributes in a that appear in S. Further, every attribute mentioned in the\njoin condition c must appear in a.\nIntuitively, we need to retain only those attributes of Rand S that are either\nmentioned in the join condition c or included in the set of attributes a retained\nby the projection.\nClearly, if a includes all attributes mentioned in c, the\nprevious commutation rules hold. If a does not include all attributes mentioned\nin C, we can generalize the commutation rules by first projecting out attributes\nthat are not mentioned in c or a, performing the join, and then projecting out\nall attributes that are not in a:\nNow,\n(Ll is the subset of attributes of R that appear in either a or c, and a2 is\nthe subset of attributes of S that appear in either a or c.\nWe can in fact derive the more general commutation rule by using the rule for\ncascading projections and the simple commutation rule, and we leave this a.s\nan exercise for the reader.\n15.3.5\nOth,er Equivalences\nAdditional equivalences hold when we consider operations such as set-difference,\nunion, and intersection. Union and intersection are associative and commuta-\ntive. Selections and projections can be commuted with each of the set opera-\ntions (set-difference, union, and intersection). \\Ve do not discuss these equiva-\nlences further.\n\n492\nSELECT\nFROM\nWHERE\nGROUP BY\nHAVING\nS.rating, COUNT (*)\nSailors S\nS.rating > 5 AND S.age = 20\nS.rating\nCOUNT DISTINCT (S.sname) > 2\nFigure 15.5\nA Single-Relation Query\nCHAPTER 15\n15.4\nENUMERATION OF ALTERNATIVE PLANS\nWe now come to an issue that is at the heart of an optimizer, namely, the space\nof alternative plans considered for a given query. Given a query, an optimizer\nessentially enumerates a certain set of plans and chooses the plan with the\nleast estimated cost; the discussion in Section 12.1.1 indicated how the cost\nof a plan is estimated.\nThe algebraic equivalences discussed in Section 15.3\nform the basis for generating alternative plans, in conjunction with the choice\nof implementation technique for the relational operators (e.g., joins) present\nin the query.\nHowever, not all algebraically equivalent plans are considered,\nbecause doing so would make the cost of optimization prohibitively expensive\nfor all but the simplest queries.\nThis section describes the subset of plans\nconsidered by a typical optimizer.\nThere are two important cases to consider: queries in which the FROM clause\ncontains a single relation and queries in which the FROM clause contains two or\nmore relations.\n15.4.1\nSingle-Relation Queries\nIf the query contains a single relation in the FROM clause, only selection, pro-\njection, grouping, and aggregate operations are involved; there are no joins. If\nwe have just one selection or projection or aggregate operation applied to a re-\nlation, the alternative implementation techniques and cost estimates discussed\nin Chapter 14 cover all the plans that must be considered. We now consider\nhow to optimize queries that involve a combination of several such operations,\nusing the following query as an example:\nFor each rating greater than 5, print the rating and the nurnber' of 20-year'-old\nsailors with that rating, provided that there are at least two such sailors with\ndifferent names.\nThe SQL version of this query is shown in Figure 15.5.\nUsing the extended\nalgebra notation introduced in Section 15.1.2, we can write this query as:\n7fS. m ling,C'OUNT(*) (\n\nA T,ypical QueI7/ Optim'izcr\nH AVINGCOUNTDISTINCT(8.snume»2(\nGROUP\nBYS.rating(\n7iS.rating .5'.sname (\n(lS.raling>5AS.age=20 (\nSailors)))))\n493\nNotice that S.sname is added to the projection list, even though it is not in the\nSELECT clause, because it is required to test the HAVING clause condition.\nWe are now ready to discuss the plans that an optimizer would consider. The\nmain decision to be made is which access path to use in retrieving Sailors\ntuples. If we considered only the selections, we would simply choose the most\nselective access path, based on which available indexes match the conditions in\nthe WHERE clause (as per the definition in Section 14.2.1). Given the additional\noperators in this query, we must also take into account the cost of subsequent\nsorting steps and consider whether these operations can be performed without\nsorting by exploiting some index.\nWe first discuss the plans generated when\nthere are no suitable indexes and then examine plans that utilize some index.\nPlans without Indexes\nThe basic approach in the absence of a suitable index is to scan the Sailors\nrelation and apply the selection and projection (without duplicate elimination)\noperations to each retrieved tuple, as indicated by the following algebra expres-\nsion:\n7iS.1'ating,S.8'l!ame (\n(lS.Ta[ing>5AS.age=20 (\nSailors))\nThe resulting tuplE~s are then sorted according to the GROUP BY clause (in the\nexample query, on mting), and one answer tuple is generated for each group that\nmeets the condition in the HAVING clause. The computation of the aggregate\nfunctions in the SELECT and HAVING clauses is done for each group, using one\nof the techniques described in Section 14.6.\nThe cost of this approach consists of the costs of each of these steps:\n1. Perfonning a file scan to retrieve tuples and apply the selections and pro-·\njections.\n2. 'Writing out tuples after the selections and projectiolls.\n3. Sorting these tuples to implement the GROUP BY clause.\n\n494\nCHAPTER t5\nNote that the HAVING clause does not cause additional I/O. The aggregate\ncomputations can be done on-the-fiy (with respect to I/O) as we generate the\ntuples in each group at the end of the sorting step for the GROUP BY clause.\nIn the example query the cost includes the cost of a file scan on Sailors plus\nthe cost of writing out (S. rating, S.sname) pairs plus the cost of sorting as per\nthe GROUP BY clause. The cost of the file scan is NPages(Sailors), which is 500\nI/Os, and the cost of writing out (S. rating, S.sname) pairs is NPages(Sailors)\ntimes the ratio of the size of such a pair to the size of a Sailors tuple times the\nreduction factors of the two selection conditiolls.\nIn our example, the result\ntuple size ratio is about 0.8, the mting selection has a reduction factor of 0.5,\nand we use the default factor of 0.1 for the age selection. Therefore, the cost\nof this step is 20 l/Os. The cost of sorting this intermediate relation (which\nwe call Temp) can be estimated as 3*NPages(Temp), which is 60 I/Os, if we\nassume that enough pages are available in the buffer pool to sort it in two\npasses.\n(Relational optimizers often a.'3sume that a relation can be sorted in\ntwo passes, to simplify the estimation of sorting costs. If this assumption is not\nmet at run-time, the actual cost of sorting may be higher than the estimate.)\nThe total cost of the example query is therefore 500 + 20 + 60 = 580 l/Os.\nPlans Utilizing an Index\nIndexes can be utilized in several ways and can lead to plans that are signifi-\ncantly faster than any plan that does not utilize indexes:\n1. Single-Index Access Path: If several indexes match the selection condi-\ntions in the WHERE clause, each matching index offers an alternative access\npath. An optimizer can choose the access path that it estimates will result\nin retrieving the fewest pages, apply any projections and nonprimary se-\nlection terms (i.e., parts of the selection condition that do not match the\nindex), and proceed to compute the grouping and aggregation operations\n(by sorting on the GROUP BY attributes).\n2. Multiple-Index Access Path: If several indexes using Alternatives (2)\nor (3) for data entries match the selection condition, each such index can\nbe used to retrieve a set of rids. vVe can intersect these sets of rids, then\nsort the result by page id (a.\"lsuming that the rid representation includes\nthe page id) and retrieve tuples that satisfy the primary selection terms of\nall the matching indexes. Any projections and nonprimary selection terms\ncan then be applied, followed by gTC)l1ping and aggregation operations.\n3. Sorted Index Access Path: If the list of grouping attributes is a prefix\nof a trec index, the index can be used to retrieve tuples in the order required\nby the GROUP BY clause. All selection conditions can be applied on each\n\nA Typical Qu,ery Optimizer\nretrieved tuple, unwanted fields can be removed, and aggregate operations\ncomputed for each gTOUp. This strategy works well for clustered indexes.\n4. Index-Only Access Path: If all the attributes mentioned in the query\n(in the SELECT, WHERE, GROUP BY, or HAVING clauses) are included in the\nsearch key for some dense index on the relation in the FROM clause, an\nindex-only scan can be used to compute answers.\nBecause the data\nentries in the index contain all the attributes of a tuple needed for this\nquery and there is one index entry per tuple, we never neep to retrieve\nactual tuples from the relation. Using just the data entries from the index,\nwe can carry out the following steps as needed in a given query: Apply\nselection conditions, remove unwanted attributes, sort the result to achieve\ngrouping, and compute aggregate functions within each group. This index-\nonly approach works even if the index does not match the selections in the\nWHERE clause. If the index matches the selection, we need examine only\na subset of the index entries; otherwise, we must scan all index entries.\nIn either case, we can avoid retrieving actual data records; therefore, the\ncost of this strategy does not depend on whether the index is clustered. In\naddition, if the index is a tree index and the list of attributes in the GROUP\nBY clause forms a prefix of the index key, we can retrieve data entries in\nthe order needed for the GROUP BY clause and thereby avoid sorting!\nWe now illustrate each of these four cases, using the query shown in Figure\n15.5 as a running example.\nWe assume that the following indexes, all using\nAlternative (2) for data entries, are available:\na B+ tree index on rating, a\nhash index on age, and a B+ tree index on (rating. sname, age). For brevity,\nwe do not present detailed cost calculations, but the reader should be able to\ncalculate the cost of each plan. The steps in these plans are scans (a file scan,\na scan retrieving tuples by using an index, or a scan of only index entries),\nsorting, and writing temporary relations; and we have already discussed how\nto estimate the costs of these operations.\nAs an example of the first C<1se, we could choose to retrieve Sailors tuples such\nthat S. age=20 using the hash index on age. The cost of this step is the cost\nof retrieving the index entries plus the cost of retrieving the corresponding\nSailors tuples, which depends on whether the index is clustered. vVe can then\napply the condition S.mting > 5 to each retrieved tuple; project out fields not\nmentioned in ~he SELECT, GROUP BY, and HAVING clauses; and write the result\nto a temporary relation. In the example, only the rating and sname fields need\nto be retained.\nThe temporary relation is then sorted on the rating field to\nidentify the groups, and some groups are eliminated by applying the HAVING\nconclitioIl.\n\n496\nCHAPTER 15\n.\n.._._-\n--\n-~--l\nUtilizing Indexes: All of the main RDBMSs recognize the importance\nof index-only plans and look for such plans whenever possible.\nIn IBM\nDD2, when creating an index a user can specify ia set of 'include' \"alumns\nthat are to be kept in the index but are not part of the index key. This\nallows a richer set of index-only queries to be handled, because columns\nfrequently a.ccessed are included in the index even if they are ;notpart of\nthe key. In Microsoft SQL Server, an interesting class of index-only plans\nis considered: Consider a query that selects attributes sal and~age from a\ntable, given an index on sal and another index on age. SQL Server uses\nthe indexes by joining the entries on the rid of data records to identify\n(sal, age) pairs that appear in the table.\nAs an example of the second case, we can retrieve rids of tuples satisfying\nmting>5 using the index on rating, retrieve rids of tuples satisfying age=20 us-\ning the index on age, sort the retrieved rids by page number, and then retrieve\nthe corresponding Sailors tuples. We can retain just the rating and name fields\nand write the result to a temporary relation, which we can sort on mting to\nimplement the GROUP BY clause.\n(A good optimizer might pipeline the pro-\njected tuples to the sort operator without creating a temporary relation.) The\nHAVING clause is handled as before.\nAs an example of the third case, we can retrieve Sailors tuples in which S. mting\n> 5, ordered by rating, using the B+ tree index on rating. We can compute\nthe aggregate functions in the HAVING and SELECT clauses on-the-fly because\ntuples are retrieved in rating order.\nAs an example of the fourth case, we can retrieve data entT'ies from the (mting,\nsname, age) index in which mting > 5. These entries are sorted by rating (and\nthen by snarne CLnJ age, although this additional ordering is not relevant for\nthis query).\nvVe can choose entries with age=20 and compute the aggregate\nfunctions in the HAVING and SELECT clauses on-the-fly because the data entries\nare retrieved in rating order. In this case, in contrast to the previous case, we\ndo not retrieve any Sailors tuples. This property of not retrieving data records\nmakes the index-only strategy especially valuable with unclusterecl indexes.\n15.4.2\nMultiple-Relation Queries\nQuery blocks that contain two or more relations in the FROM clause require joins\n(or cross-products).\nFinding a good plan for such queries is very important\nbecause these queries can be quite expensive. Regardless of the plan chosen,\nthe size of the final result can be estimated by taking the product of the sizes\n\nA Typical Q'lLeTy OptimizeT\n497\nof the relations in the FROM clause and the reduction factors for the terms in\nthe WHERE clause. But, depending on the order in which relations are joined,\nintermediate relations of widely varying sizes can be created, leading to plans\nwith very different costs.\nEnumeration of Left-Deep Plans\nAs we saw in Chapter 12, current relational systems, following the lead of the\nSystem R optimizer, only consider left-deep plans.\n\\;Ye now discuss how this\ndass of plans is efficiently searched using dynamic programming.\nConsider a query block of the form:\nSELECT attribute list\nFROM\nrelation list\nWHERE\nteT1nl 1\\ term2 1\\ ... 1\\ ter1nn\nA System R style query optimizer enumerates all left-deep plans, with selections\nand projections considered (but not necessarily applied!) as early as possible.\nThe enumeration of plans can be understood &'3 a multiple-pass algorithm in\nwhich we proceed as follows:\nPass 1:\nWe enumerate all single-relation plans (over some relation in the\nFROM clause). Intuitively, each single-relation plan is a partial left-deep plan\nfor evaluating the query in which the given relation is the first (in the linear\njoin order for the left-deep plan of which it is a part).\nWhen considering\nplans involving a relation A, we identify those selection terms in the WHERE\nclause that mention only attributes of A.\nThese are the selections that can\nbe performed when first accessing A, before any joins that involve A. We also\nidentify those attributes of A not mentioned in the SELECT clause or in terms\nin the WHERE clause involving attributes of other relations.\nThese attributes\ncan be projected out when first accessing A, before any joins that involve A.\nWe choose the best access method for A to carry out these selections and\nprojections, &'3 per the discussion in Section 15.4.1.\nFor each relation, if we find plans that produce tuples in different orders, we\nretain the cheapest plan for each such ordering of tuples. An ordering of tuples\ncould prove useful at a subsequent step, say, for a sort-merge join or imple-\nmenting a GROUP BY or ORDER BY clause. Hence, for a single relation, we may\nretain a file scan (&'3 the cheapest overall plan for fetching all tuples) and a B+\ntree index (I:LS the cheapest plan for fetching all tuples in the search key order).\nPass 2: We generate all two-relation plans by considering each single-relation\nplan retained after Pass 1 &'3 the outer relation and (successively) every other\n\n498\nCHAPTER Jf5\nrelation as the inner relation.\nSuppose that A is the outer relation and B\nthe inner relation for a particular two-relation plan.\nWe examine the list of\nselections in the WHERE clause and identify:\n1. Selections that involve only attributes of B and can be applied before the\njoin.\n2. Selections that define the join (i.e., are conditions involving attributes of\nboth A and B and no other relation).\n3. Selections that involve attributes of other relations and can be applied only\nafter the join.\nThe first two groups of selections can be considered while choosing an access\npath for the inner relation B. We also identify the attributes of B that do not\nappear in the SELECT clause or in any selection conditions in the second or\nthird group and can therefore be projected out before the join.\nNote that our identification of attributes that can be projected out before the\njoin and selections that can be applied before the join is based on the relational\nalgebra equivalences discussed earlier. In particular, we rely on the equivalences\nthat allow us to push selections and projections ahead of joins. As we will see,\nwhether we actually perform these selections and projections ahead of a given\njoin depends on cost considerations. The only selections that are really applied\nbefor\"e the join are those that match the chosen access paths for A and B. The\nremaining selections and projections are done on-the-fly as part of the join.\nAn important point to note is that tuples generated by the outer plan are as-\nsumed to be pipelined into the join. That is, we avoid having the outer plan\nwrite its result to a file that is subsequently read by the join (to obtain outer\ntuples). For SOlne join methods, the join operator rnight require materializing\nthe outer tuples. For example, a hash join would partition the incoming tuples,\nand a sort-merge join would sort them if they are not already in the appropri-\nate sort order. Nested loops joins, however, can use outer tuples H,\"i they are\ngenerated and avoid materializing them.\nSimilarly, sort-merge joins can use\nouter tuples as they are generated if they are generated in the sorted order\nrequired for the join. We include the cost of materializing the outer relation,\nshould this be necessary, in the cost of the join. The adjustments to the join\ncosts discussed in Chapter 14 to reflect the use of pipelining or materialization\nof the outer are straightforward.\nFor each single-relation plan for A retained after Pa.\"iS 1, for each join method\nthat we consider, we must determine the best access lnethod to llse for B. The\naccess method chosen for B retrieves, in general, a subset of the tuples in B,\npossibly with some fields eliminated, as discllssed later. Consider relation B.\n\nA T:lJpical\nqlleT~1J Optim'iztT\n4~9\n\\Ve have a collection of selections (some of which are the join conditions) and\nprojections on a single relation, and the choice of the best access method is\nmade a<; per the discussion in Section 15.4.1. The only additional consideration\nis that the join method might require tuples to be retrieved in some order. For\nexample, in a sort-merge join, we want the inner tuples in sorted order on the\njoin column(s). If a given access method does not retrieve inner tuples in this\norder, we must add the cost of an additional sorting step to the cost of the\naccess method.\nPass 3: We generate all three-relation plans. We proceed as in Pass 2, except\nthat we now consider plans retained after Pass 2 as outer relations, instead of\nplans retained after Pass 1.\nAdditional Passes: This process is repeated with additional passes until we\nproduce plans that contain all the relations in the query.\nWe now have the\ncheapest overall plan for the query as well as the cheapest plan for producing\nthe answers in some interesting order.\nIf a multiple-relation query contains a GROUP BY clause and aggregate functions\nsuch as MIN, MAX, and SUM in the SELECT clause, these are dealt with at the\nvery end. If the query block includes a GROUP BY clause, a set of tuples is\ncomputed based on the rest of the query, as described above, and this set is\nsorted as per the GROUP BY clause. Of course, if there is a plan according to\nwhich the set of tuples is produced in the desired order, the cost of this plan\nis compared with the cost of the cheapest plan (a<;smning that the two are\ndifferent) plus the sorting cost. Given the sorted set of tuples, partitions are\nidentified and any aggregate functions in the SELECT clause are applied on a\nper-partition basis, as per the discussion in Chapter 14.\nExamples of Multiple-Relation Query Optimization\nConsider the query tree shown in Figure\n12.~~.\nFigure 15.6 shows the same\nquery, taking into account how selections and projections are considered early.\nIn looking at this figure, it is worth ernphc1...sizing that the selections shown on\nthe leaves are not necessarily done in a distinct step that precedes the .ioin~H\nrather,\n(:1...<; we have seen, they are considered as potential matching predicates\nwhen considerIng the available access paths on the relations.\nSuppose that we have the following indexes, all unclustered and using Alter-\nnative (2) for data entries: a B+ tree index on the rating field of Sailors, a\nhash index on the sid field of Sailors, and a B+ tree index on the bid field of\n\n500\nCHAPTER 10\nOptimization in Commercial Syst~ms: IBM DB2, Informix, Microsoft\nSQL Server, Oracle 8, and Sybase ASE all search for left-deep trees using\ndynamic programming, as described here, with several variations. For ex-\nample, Oracle always considers interchanging the two relations in a hash\njoin, which could lead to right-deep trees or hybrids. DB2 gene'rates some\nbushy trees as well. Systems often use a variety of strategies for generating\nplans, going beyond the systematic bottom-up enumeration that we de-\nscribed, in conjunction with a dynamic programming strategy for costing\nplans and remembering interesting plans (to avoid repeated analysis of the\nsame plan).\nSystems also vary in the degree of control they give users.\nSybase ASE and Oracle 8 allow users to force the choice of join orders\nand indexes--Sybase ASE even allows users to explicitly edit the execu-\ntion plan-whereas IBM DB2 does not allow users to direct the optimizer\nother than by setting an 'optimization level,' which influences how many\nalternative plans the optimizer considers.\nII sname\nI\nReserves\nUrating > 5\nI\nSailors\nFigure 15.6\nA Query Tree\n\nA Typical Qv,cry Optinl'izcr\n5Q1\nReserves.\nIn addition, we a'Ssume that we can do a sequential scan of both\nReserves and Sailors. Let us consider how the optimizer proceeds.\nIn Pch<;S 1, we consider three access methods for Sailors (B+ tree, hash index,\nand sequential scan), taking into account the selection IJrating>5' This selection\nmatches the B+ tree on rating and therefore reduces the cost for retrieving\ntuples that satisfy this selection. The cost of retrieving tuples using the hash\nindex and the sequential scan is likely to be much higher than the cost of using\nthe B+ tree. So the plan retained for Sailors is access via the B+ tree index, and\nit retrieves tuples in sorted order by rating. Similarly, we consider two access\nmethods for Reserves taking into account the selection IJbid=100. This selection\nmatches the B+ tree index on Reserves, and the cost of retrieving matching\ntuples via this index is likely to be much lower than the cost of retrieving tuples\nusing a sequential scan; access through the B+ tree index is therefore the only\nplan retained for Reserves after Pass 1.\nIn Pass 2, we consider taking the (relation computed by the) plan for Reserves\nand joining it (as the outer) with Sailors. In doing so, we recognize that now,\nwe need only Sailors tuples that satisfy crrating>5 and IJsid=value, where value\nis some value from an outer tuple. The selection IJsid=value matches the hash\nindex on the sid field of Sailors, and the selection crrating>5 matches the B+\ntree index on the rating field.\nSince the equality selection has a much lower\nreduction factor, the hash index is likely to be the cheaper access method.\nIn addition to the preceding consideration of alternative access methods, we\nconsider alternative join methods. All available join methods are considered.\nFor example, consider a sort-merge join.\nThe inputs must be sorted by sid;\nsince neither input is sorted by sid or has an access method that can return\ntuples in this order, the cost of the sort-merge join in this case must include\nthe cost of storing the two inputs in tempora.ry relations and sorting them. A\nsort-merge join provides results in sorted order by sid, but this is not a useful\nordering in this example because the projection 7fsname is applied (on-the-fly)\nto the result of the join, thereby eliminating the sid field from the answer.\nTherefore, the plan using sort-merge join is retained after Pch<;S 2 only if it is\nthe least expensive plan involving Reserves and Sailors.\nSimilarly, we also consider taking the plan for Sailors retained after Pass 1 and\njoining it (as the outer relation) with Reserves. Now we recognize that we need\nonly Reserves tuples that satisfy IJhid=100 and\nIJsid=val'lU~' where value is some\nvalue from an outer tuple. Again, we consider all available join methods.\nvVe finally retain the cheapest plan overall.\nAs another example, illustrating the ca<;e when more than two relations are\njoined, consider the following query:\n\n502\nCHAPTER 15\nSELECT\nS.sid, COUNT(*) AS numres\nFROM\nBoats B, Reserves R, Sailors S\nWHERE\nR.sid = S.sid AND B.bid=R.bid AND Rcolor = 'red'\nGROUP BY S.sid\nThis query finds the number of red boats reserved by each sailor. This query\nis shown in the form of a tree in Figure 15.7.\nITsid. COUNT(') AS numras\nI\nGROUPB)' ~id\nI\n\"'<I\nSailors\nbid~bld\n(Jcolor';:::. 'red'\nReserves\nBoats\nFigure 15.7\nA Query Tree\nSuppose that the following indexes are available: for Reserves, a B+ tree on the\nsid field and a clustered B+ tree on the bid field; for Sailors, a B+ tree index on\nthe sid field and a hash index on the sid field; and for Boats, a B+ tree index\non the color field and a ha'3h index on the color field.\n(The list of available\nindexes is contrived to create a relatively simple, illustrative example.) Let us\nconsider how this query is optimized. The initial focus is on the SELECT, FROM,\nand WHERE clauses.\nIn Pass 1, the best plan is found for accessing each relation, regarded as the\nfirst relation in an execution plan. :For Reserves and Sailors, the best plan is\nobviously a. file scan because no selections match an available index. The best\nplan for Boats is to use the hash index on color, which matches the selection\nB. coloT =\n'T'(~d '. The B+ tree on color also matches this selection and is retained\neven though the hash index is cheaper, because it returns tuples in sorted order\nby color.\nIn Pass 2, for each of the plans generated in Pass 1, taken as the outer relation,\nwe consider joining another rela.tion a'3 the inner one. Hence, we consider each\nof the following joins: file scan of Reserves (outer) with Boats (inner), file scan\nof lleserves (outer) with Sailors (inner), file scan of Sailors (outer) with Boats\n(inner), file scan of Sailors (outer) with Reserves (inner), Boats accessed via\nB+ tree index on color (outer) with Sailors (inner) 1 Boats accessed via ha'3h\n\nA T.ypical\nq'lleT~1J Opt'im'izer\nS(}3\nindex on color (outer) with Sailors (inner), Boats accessed via B+ tree index\non color (outer) with Reserves (inner), and Boats accessed via hash index on\ncolor (outer) with RE'.3erves (inner).\nFor each such pair, we consider every join method, and for each join method,\nwe consider every available access path for the inner relation.\nFor each pair\nof relations, we retain the cheapest of the plans considered for every sorted\norder in which the tuples are generated.\nFor example, with Boats accessed\nvia the hash index on coloT as the outer relation, an index nested loops join\naccessing Reserves via the B+ tree index on bid is likely to be a good plan;\nobserve that there is no ha.\"h index on this field of Reserves. Another plan for\njoining Reserves and Boats is to access Boats using the hash index on coloT,\naccess Reserves using the B+ tree on bid, and use a sort-merge join; this plan,\nin contrast to the previous one, generates tuples in sorted order by bid.\nIt\nis retained even if the previous plan is cheaper, unless an even cheaper plan\nproduces the tuples in sorted order by bid. However, the previous plan, which\nproduces tuples in no particular order, would not be retained if this plan is\ncheaper.\nA good heuristic is to avoid considering cross-products if possible. If we apply\nthis heuristic, we would not consider the following 'joins' in Pass 2 of this\nexample: file scan of Sailors (outer) with Boats (inner), Boats accessed via B+\ntree index on color (outer) with Sailors (inner), and Boats accessed via hash\nindex on color (outer) with Sailors (inner).\nIn Pass 3, for each plan retained in Pass 2, taken as the outer relation, we\nconsider how to join the remaining relation as the inner one. An example of a\nplan generated at this step is the following: Access Boats via the hash index\non coloT, access Reserves via the B+ tree index on bid, and join them using\na sort-merge join, then take the result of this join as the outer and join with\nSailors using a sort-merge join, accessing Sailors via the B+ tree index on the\nsid field. Note that, since the result of the first join is produced in sorted order\nby bid, wherea.\" the second join requires its inputs to be sorted by s'id, the result\nof the first join must be sorted by sid before being used in the second join. The\ntuples in the result of the second join are generated in sorted order by sid.\nThe GROUP BY clause is considered after all joins, and it requires sorting on\nthe sid field.\nFor each plan retained in Pass 3, if the result is not sorted on\nsid, we add the cost of sorting on the sid field. The sample plan generated in\nPass 3 produces tuples in sid order; therefore, it may be the cheapest plan for\nthe query even if a cheaper plan joins all three relations but does not produce\ntuples in sid order.\n\n504\nCHAPTER 15\n15.5\nNESTED SUBQUERIES\nThe unit of optimization in a typical system is a query block, and nested queries\nare dealt with using some form of nested loops evaluation.\nConsider the fol-\nlowing nested query in SQL: Find the names of sailors with the highest rating:\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.rating = ( SELECT MAX (S2.rating)\nFROM\nSailors S2 )\nIn this simple query, the nested subquery can be evaluated just once, yielding\na single value. This value is incorporated into the top-level query as if it had\nbeen part of the original statement of the query. For example, if the highest\nrated sailor has a rating of 8, the WHERE clause is effectively modified to WHERE\nS. rating = 8.\nHowever, the subquery sometimes returns a relation, or more precisely, a table\nin the SQL sense (i.e., possibly with duplicate rows).\nConsider the following\nquery: Find the names of sailors who have Teserved boat number 103:\nSELECT\nFROM\nWHERE\nS.sname\nSailors S\nS.sid IN ( SELECT\nFROM\nWHERE\nRsid\nReserves R\nRbid = 103 )\nAgain, the nested subquery can be evaluated just once, yielding a collection\nof sids.\nFor each tuple of Sailors, we must now check whether the sid value\nis in the computed collection of sids; this check entails a join of Sailors and\nthe computed collection of sids, and in principle we have the full range of join\nmethods to choose from.\nFor example, if there is an index on the sid field\nof Sailors, an index nested loops join with the computed collection of sid\", as\nthe outer relation and Sailors as the inner one might be the most efficient join\nmethod. However, in many systems, the query optimizer is not smart enough\nto find this strategy\na common approach is to always do a nested loops join\nin which the inner relation is the collection of sid\" computed from the subquery\n(and this colle(~tion may not be indexed).\nThe motivation for this approach is that it is a simple variant of the technique\nused to deal with condated ([neTics such as the following version of the previous\nquery:\nSELECT S.snallle\n\nA TYVical Q'lteTy Optim'izer\n5()5\nFROM\n~lHERE\nSailors S\nEXISTS ( SELECT *\nFROM\nReserves R\nWHERE\nR.bid = 103\nAND S.sid = R.sid )\nThis query is correlated-\"the tuple variable S from the top-level query appears\nin the nested subquery. Therefore, we cannot evaluate the subquery just once.\nIn this case the typical evaluation strategy is to evaluate the nested subquery\nfor each tuple of Sailors.\nAn important point to note about nested queries is that a typical optimizer\nis likely to do a poor job, because of the limited approach to nested query\noptimization. This is highlighted next:\n•\nIn a nested query with correlation, the join method is effectively index\nnested loops, with the inner relation typically a subquery (and therefore\npotentially expensive to compute).\nThis approach creates two distinct\nproblems.\nFirst, the nested subquery is evaluated once per outer tuple;\nif the same value appears in the correlation field (S.sid in our example) of\nseveral outer tuples, the same subquery is evaluated many times. The sec-\nond problem is that the approach to nested subqueries is not set-oriented.\nIn effect, a join is seen as a scan of the outer relation with a selection on\nthe inner subquery for each outer tuple. This precludes consideration of\nalternative join methods, such as a sort-merge join or a hash join, that\ncould lead to superior plans.\n•\nEven if index nested loops is the appropriate join method, nested query\nevaluation may be inefficient. For example, if there is an index on the sid\nfield of Reserves, a good strategy might be to do an index nested loops join\nwith Sailors as the outer relation and Reserves &'3 the inner relation and\napply the selection on bid on-the-fly. However, this option is not considered\nwhen optimizing the version of the query that uses IN, because the nested\nsubquery is fully evaluated as a first step; that is, Reserves tuples that\nmeet the bid selection are retrieved first.\n•\nOpportunities for finding a good evaluation plan may also be missed be-\ncause of the implicit ordering imposed by the nesting. For example, if there\nis an index. on the sid field of Sailors, an index nested loops join with Re-\nserves a,s the outer relation and Sailors as the inner one might be the most\nefficient plan for our example correla,ted query. However, this join ordering\nis never considered by an optimizer.\nA nested query often has an equivalent query without nesting, and a correlated\nquery often he1.<; an equivalent query without correlation. vVe already saw cor-\n\n506\nCHAPTER\n~5\nNested Queries: IBM DB2, Informix, Microsoft SQL Server, Orade 8,\nand Sybase ASE all use some version of correlated evaluation to handle\nnested queries, which are an important part qf tbe TPC-D benchmark;\nIBM and Informix support a version in which the results of subqueries are\nstored in a 'memo' table and the same subquery is not executed multiple\ntimes. All these RDBMSs consider decqrrelation and \"flattening\" of nested\nqueries as an option. Microsoft SQL Server, Oracle 8 and IBM DB2 also\nuse rewriting techniques, e.g., Magic Sets (see Chapter 24) or variants, in\nconjunction with decorrelation.\nrelated and uncorrelated versions of the example nested query. There is also\nan equivalent query without nesting:\nSELECT S.sname\nFROM\nSailors S, Reserves R\nWHERE\nS.sid = R.sid AND R.bid=103\nA typical SQL optimizer is likely to find a much better evaluation strategy if it is\ngiven the unnested or 'decOlTelated' version of the example query than if it were\ngiven either of the nested versions of the query. Many current optimizers cannot\nrecognize the equivalence of these queries and transform one of the nested\nversions to the nonnested form. This is, unfortunately, up to the educated user.\nFrom an efficiency standpoint, users are advised to consider such alternative\nformulations of a query.\nWe conclude our discussion of nested queries by observing that there could be\nseveral levels of nesting. In general, the approach we sketched is extended by\nevaluating such queries from the innermost to the outermost levels, in order, in\nthe absence of correlation. A correlated subquery must be evaluated for each\ncandidate tuple of the higher-level (sub)query that refers to it. The basic idea\nis therefore similar to the case of one-level nested queries; we omit the details.\n15.6\nTHE SYSTEM R OPTIMIZER\nCurrent relational query optimizers have been greatly influenced by choices\nmade in the qesign of IBM's System R query optimizer.\nImportant design\nchoices in the System R optimizer include:\n1. The use of statistics about the databa'3e instance to estiInate the cost of a\nquery evaluation plan.\n2. A decision to consider only plans with binary joins in which the inner\nrelation is a base relation (i.e., not a telnporary relation). This heuristic\n\nA Typical Que7'y Optimizer'\n507\nreduces the (potentially very large) number of alternative plans that must\nbe considered.\n3. A decision to focus optimization on the class of SQL queries without nesting\nand treat nested queries in a relatively ad hoc way.\n4. A decision not to perform duplicate elimination for projections (except as\na final step in the query evaluation when required by a DISTINCT clause).\n5. A model of cost that accounted for CPU costs as well as I/O costs.\nOur discussion of optimization reflects these design choices, except for the last\npoint in the preceding list, which we ignore to retain our simple cost model\nbased on the number of page l/Os.\n15.7\nOTHER APPROACHES TO QUERY OPTIMIZATION\nWe have described query optimization based on an exhaustive search of a large\nspace of plans for a given query. The space of all possible plans grows rapidly\nwith the size of the query expression, in particular with respect to the number\nof joins, because join-order optimization is a central issue. Therefore, heuristics\nare used to limit the space of plans considered by an optimizer. A widely used\nheuristic is that only left-deep plans are considered, which works well for most\nqueries.\nHowever, once the number of joins becomes greater than about 15,\nthe cost of optimization using this exhaustive approach becomes prohibitively\nhigh, even if we consider only left-deep plans.\nSuch complex queries are becoming important in decision-support environ-\nments, and other approaches to query optimization have been proposed. These\ninclude rule-based optimizers, which use a set of rules to guide the gen-\neration of candidate plans, and randomized plan generation, which uses\nprobabilistic algorithms such as simulated annealing to explore a large space of\nplans quickly, with a reasonable likelihood of finding a good plan.\nCurrent research in this area also involves techniques for estimating the size\nof intermediate relations more accurately; parametric query optimization,\nwhich seeks to find good plans for a given query for each of several different\nconditions that might be encountered at run-time; and multiple-query opti-\nmization, in which the optimizer takes concurrent execution of several queries\ninto account.\n15.8\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n\n508\nCHAPTER 15,\n•\n\\Vhat is an SQL qlleTJJ block? \\Vhy is it important in the context of query\noptimization? (Section 15.1)\n•\nDescribe how a query block is translated into extended relational algebra.\nDescribe and motivate the extensions to relational algebra. VVhy are a'Tr x\nexpressions the focus of an optimizer? (Section 15.1)\n•\n\\Vhat are the two parts to estimating the cost of a query plan?\n(Sec-\ntion 15.2)\n•\nHow is the result size estimated for a (nrx expression? Describe the use of\nreduction factors, and explain how they are calculated for different kinds\nof selections? (Section 15.2.1)\n•\n~What are histograms?\nHow do they help in cost estimation?\nExplain\nthe differences between the different kinds of histograms, with particular\nattention to the role of frequent data values. (Section 15.2.1)\n•\nVVhen are two relational algebra expressions considered equivalent? How is\nequivalence used in query optimization? What algebra equivalences that\njustify the common optimizations of pushing selections ahead of joins and\nre-ordering join expressions? (Section 15.3)\n•\nDescribe left-deep plans and explain why optimizers typically consider only\nsuch plans. (Section 15.4)\n•\nWhat plans are considered for (sub)queries with a single relation?\nOf\nthese, which plans are retained in the dynamic programming approach to\nenumerating left-deep plans?\nDiscuss access methods and output order\nin your answer. In particular, explain index-only plans and why they are\nattractive. (Section 15.4)\n•\nExplain how query plans are generated for queries with multiple relations.\nDiscuss the space and time complexity of the dynamic programming ap-\nproach, and how the plan generation process incorporates heuristics like\npushing selections and join ordering. How are index-only plans for multiple-\nrelation queries identified?\nHow are pipelining opportunities identified?\n(Section 15.4)\n•\nHow are nested subqueries optimized and evaluated?\nDiscuss correlated\nqueries and the additional optimization challenges they present. \\Vhy are\nplans produced for nested queries typically of poor quality? VVhat is the\nlesson for application programmers? (Section 15.5)\n•\nDiscuss some of the influential design choices made in the System R opti-\nmizer. (Section 15.6)\n•\nBriefly survey optimization techniques that go beyond the dynamic pro-\ngramming framework discussed in this chapter. (Section 15.7)\n\nA Typical Q'ueTy Ophm'izeT\nEXERCISES\nExercise 15.1 Briefly answer the following questions:\n5q9\n1. In the context of query optimization, what is an SQL query block?\n2. Define the term redw:t'i.on factor.\n3. Describe a situation in which projection should precede selection in processing a project-\nselect query, and describe a situation where the opposite processing order is better.\n(Assume that duplicate elimination for projection is done via sorting.)\n4. If there are unclustered (secondary) B+ tree indexes on both R.a and S.b, the join\nR [Xla=bS could be processed by doing a sort-merge type of join-without doing any\nsorting-by using these indexes.\n(a) Would this be a good idea if Rand S each has only one tuple per page or would it\nbe better to ignore the indexes and sort Rand S? Explain.\n(b) What if Rand S each have many tuples per page? Again, explain.\n5. Explain the role of interesting orders in the System R optimizer.\nExercise 15.2 Consider a relation with this schema:\nErnployees(eid: integer, ename: string, sal: integer, title: string, age: integer)\nSuppose that the following indexes, all using Alternative (2) for data entries, exist: a hash\nindex on eid, a B+ tree index on sal, a hash index on age, and a clustered B+ tree index\non (age, sal). Each Employees record is 100 bytes long, and you can assume that each index\ndata entry is 20 bytes long. The Employees relation contains 10,000 pages.\n1. Consider each of the following selection conditions and, assuming that the reduction\nfactor (RF) for each term that matches an index is 0.1, compute the cost of the most\nselective access path for retrieving all Employees tuples that satisfy the condition:\n(a) sol> 100\n(b) age. = 25\n(c) age> 20\n(d) eid = 1, 000\n(e) sal> 200 1\\ age> 30\n(f) sal> 2001\\ age = 20\n(g) sal> 2001\\ title ='CFO'\n(h) sal> 200 II age> 301\\ hUe ='CFO'\n2. Suppose that, for each of the preceding selection conditions, you want to retrieve the\naverage salary of qualifying tuples. For each selection condition, describe the least ex-\npensive evalwltion method and state its cost.\n:3. Suppose that, for each of the preceding selection conditions, you want to compute the av--\nerage salary for each age group. For each selection condition, describe the least expensive\nevaluation method and state its cost.\n4. Suppose that, for each of the preceding selection conditions, you want to compute the\naverage age for each sa/level (Le.) group by sal). For each selection condition, describe\nthe least expensive evaluation method and state its cost.\n\n510\nCHAPTER\n~5\n5. For each of the following selection conditions, describe the best evaluation method:\n(a) sal> 200 V age = 20\n(b) sal> 200 V title ='CFO'\n(c) title ='CFO' 1\\ ename ='Joe'\nExercise 15.3 For each of the following SQL queries, for each relation involved, list the\nattributes that must be examined to compute the answer. All queries refer to the following\nrelations:\nEmp(eid: integer, did: integer, sal: integer, hobby: char(20))\nDept( did: integer, dname: char(20), floor: integer, budget: real)\n1. SELECT COUNT(*) FROM Emp E, Dept D WHERE E.did = D.did\n2. SELECT MAX(E.sal) FROM Emp E, Dept D WHERE E.did = D.did\n3. SELECT MAX(E.sal) FROM Emp E, Dept D WHERE E.did = D.did AND D.floor = 5\n4. SELECT E.did, COUNT(*) FROM Emp E, Dept D WHERE E.did = D.did GROUP BY D.did\n5. SELECT D.floor, AVG(D.budget) FROM Dept D GROUP BY D.tloor HAVING COUNT(*) > 2\n6. SELECT D.tloor, AVG(D.budget) FROM Dept D GROUP BY D.floor ORDER BY D.floor\nExercise 15.4 You are given the following information:\nExecutives has attributes ename, title, dname, and address; all are string fields of\nthe same length.\nThe ename attribute is a candidate key.\nThe relation contains 10,000 pages.\nThere are 10 buffer pages.\n1. Consider the following query:\nSELECT E.title, E.ename FROM Executives E WHERE E.title='CFO'\nAssume that only 10% of Executives tuples meet the selection condition.\n(a) Suppose that a clustered B+ tree index on title is (the only index) available. What\nis the cost of the best plan? (In this and subsequent questions, be sure to describe\nthe plan you have in mind.)\n(b) Suppose that an unclustered B+ tree index on title is (the only index) available.\nWhat is the cost of the best plan?\n(c) Suppose that a clustered B+ tree index on enarne is (the only index) available.\nWhat is the cost of the best plan?\n(d) Suppo$e that a clustered B+ tree index on address is (the only index) available.\nWhat is the cost of the best pian?\n(e) Suppose that a clustered B+ tree index on (ename, title) is (the only index) avail-\nable. What is the cost of the best plan?\n2. Suppose that the query is as follows:\nSELECT E.ename FROM Executives E WHERE E.title='CFO' AND E.dname='Toy'\n\nA T..7Jpical Query Opl'irnizer\n51J\nAssume that only 10% of Executives tuples IIleet the condition E.title ='C FO', only\n10% meet E.dname ='Toy', and that only 5% meet both conditions.\n(a) Suppose that a clustered B+ tree index on title is (the only index) available. What\nis the cost of the best plan?\n(b) Suppose that a clustered B+ tree index on dname is (the only index) available.\nWhat is the cost of the best plan?\n(c) Suppose that a clustered B+ tree index on (title, dname) is (the only index) avail-\nable. What is the cost of the best plan?\n(d) Suppose that a clustered B+ tree index on (title, ename) is (the only index) avail-\nable. What is the cost of the best plan?\n(e) Suppose that a clustered B+ tree index on (dname, title, ename) is(the only index)\navailable. What is the cost of the best plan?\n(f) Suppose that a clustered B+ tree index on (ename, title, dname) is (the only index)\navailable. What is the cost of the best plan?\n3. Suppose that the query is as follows:\nSELECT E.title, COUNT(*) FROM Executives E GROUP BY E.title\n(a) Suppose that a clustered B+ tree index on title is (the only index) available. What\nis the cost of the best plan?\n(b) Suppose that an unclustered B+ tree index on t'ltle is (the only index) available.\nWhat is the cost of the best plan?\n(c) Suppose that a clustered B+ tree index on ename is (the only index) available.\nWhat is the cost of the best plan?\n(d) Suppose that a clustered B+ tree index on (ename, title) is (the only index) avail-\nable. What is the cost of the best plan?\n(e) Suppose that a clustered B+ tree index on (title, ename) is (the only index) avail-\nable. What is the cost of the best plan?\n4. Suppose that the query is as follows:\nSELECT E.title, COUNT(*) FROM Executives E\nWHERE E.dname > 'W%' GROUP BY E.title\nAssume that only 10% of Executives tuples meet the selection condition.\n(a) Suppose that a clustered B+ tree index on title is (the only index) available. What\nis the cost of the best plan? If an additional index (on any search key you want) is\navailable, would it help produce a better plan?\n(b) Suppose that an unclustered B+ tree index OIl title is (the only index) available.\nWhat is the cost of the best plan?\n(c) Suppose. that a clustered B+ tree index on dname is (the only index) available.\nWhat is the cost of the best plan? If an additional index (on any search key you\nwant) is available, would it help to produce a better plan'?\n(d) Suppose that a clustered B+ tree index on (dname, title) is (the only index) avail-\nable. What is the cost of the best plan?\n(e) Suppose that a clustered B+ tree index on (title,dname) is (the only index) avail-\nable. What is the cost of the best plan?\n\n,512\nCHAPTER\n~5\nExercise 15.5 Consider the query 7rA.B,C,D(R CXlA=CS). Suppose that the projection routine\nis based on sorting and is smart enough to eliminate all but the desired attributes during the\ninitial pass of the sort and also to toss out duplicate tuples on-the-fly while sorting, thus\neliminating two potential extra pa.'ises. Finally, assume that you know the following:\nR is 10 pages long, and R tuples are aoo bytes long.\nS is 100 pages long, and S tuples are 500 bytes long.\nC is a key for S, and A is a key for R.\nThe page size is 1024 bytes.\nEach S tuple joins with exactly one R tuple.\nThe combined size of attributes A, B, G, and D is 450 bytes.\nA and B are in R and have a combined size of 200 bytes; C and D are in S.\nL\n'''''hat is the cost of writing out the final result? (As usual, you should ignore this cost\nin answering subsequent questions.)\n2. Suppose that three buffer pages are available, and the only join method that is imple-\nmented is simple (page-oriented) nested loops.\n(a) Compute the cost of doing the projection followed by the join.\n(b) Compute the cost of doing the join followed by the projection.\n(c) Compute the cost of doing the join first and then the projection on-the-fly.\n(d) Would your answers change if 11 buffer pages were available?\nExercise 15.6 Briefly answer the following questions:\nL Explain the role of relational algebra equivalences in the System R optimizer.\n2. Consider a relational algebra expression of the form Uc(-lq (R X S)).\nSuppose that the\nequivalent expression with selections and projections pushed as much as possible, taking\ninto accollnt only relational algebra equivalences, is in one of the following forms.\nIn\neach case give an illustrative example of the selection conditions and the projection Iitits\n(c, I, el, 11, etc.).\n(a) Equivalent mm:imally pushed form: rrl1(u,dR) X S),\n(b) Equivalent mal:imally pushed form: rrll(ucl(R) x U c2(S)).\n(c)\nEquivalent maximally Tn/shed f07'771:\nCTe (rrll (n12 (R) x 8)).\n(d)\nEquivalent maximally pushed fONT!:\nUc1 (rrll (u,drrdR)) x 8)).\n(e) Equivalent ma:rimally pushed fO'l'17L' Ucl (nil (rr12 (CTC2(R)) x S)).\n(f)\nEqui~}(tlent rnaximally pushed fOT1n:\n71\"1 (0'\" 1(71\"1l (71\"12 (Uc2(R)) x 8))).\nExercise 15.7 Consider the following relational schema and SQL query. The schema cap-\ntureti information about employees, departments, and company finances (organized on a per\ndepartment basis).\nEmp(eid: integer, did: integer, sal: integer, hobby: char(20))\nDept(did: integer, dn07ne: char(20), floor: integer, phone: char(10))\nFinance( did: integer, budget: real, sales: real, e:r:penses: real)\nConsider t.he following query:\n\nA Typical Q'llcry Opi'irnizer\n51:?\nSELECT\nFRON\nWHERE\nD.dname, F.budget\nEmp E, Dept D, Finance F\nE.did=D.did AND D.did=F.did AND D.floor=l\nAND E.sal ~ 59000 AND E.hobby = 'yodeling'\n1. Identify a relational algebra tree (or a relational algebra expression if you prefer) that\nreflects the order of operations a decent query optimizer would choose.\n2. List the join orders (i.e., orders in which pairs of relations can be joined to compute the\nquery result) that a relational query optimizer will consider. (Assume that the optimizer\nfollows the heuristic of never considering plans that require the computation of cross-\nproducts.) Briefly explain how you arrived at your list.\n3. Suppose that the following aclditional information is available:\nlJnclustered B+ tree\nindexes exist on Ernp.did, Ernp.sal, Dept.floor, Dept. did, and Finance. did. The system's\nstatistics indicate that employee salaries range from 10,000 to 60,000, employees enjoy\n200 different hobbies, and the company owns two floors in the building.\nThere are\na total of 50,000 employees and 5,000 departments (each with corresponding financial\ninformation) in the database. The DBMS used by the company has just one join method\navailable, index nested loops.\n(a) For each of the query's base relations (Emp, Dept, and Finance) estimate the\nnumber of tuples that would be initially selected from that relation if all of the\nnon-join predicates on that relation were applied to it before any join processing\nbegins.\n(b) Given your answer to the preceding question, which of the join orders considered\nby the optimizer has the lowest estimated cost?\nExercise 15.8 Consider the following relational schema and SQL query:\nSuppliers(sid: integer, snarne: char(20), city: char(20»)\nSupply(sid: integer, pid: integer)\nParts(pid: integer, pnarne: char (20), price: real)\nSELECT\nFROM\nWHERE\nS.sname, P.pname\nSuppliers S, Parts P, Supply Y\nS.sid = Y.sid AND Y.pid = P.pid AND\nS.city = 'Madison' AND P.price :s: 1,000\n1. What information abollt these relations does the query optimizer need to select a good\nquery execution plan for the given query?\n2. How many different join orders, assuming that cross-products are disallowed, does a\nSystem R. style query optimizer consider whcn deciding how to process the given query?\nList each of thcse join orders.\n3. \\\\-That indexes' might be of help in processing this query? Explain briefly.\n4. How does adding DISTINCT to the SELECT clause affect the plans produced?\n5. How does adding ORDER BY sname to the query affect the plans produced?\nG. How does adding GROUP BY .marne to the query affect the plans produced?\nExercise 15.9 Consider the following scenario:\n\n514\nEmp( eid: integer, sal: integer, age: real, did: integer)\nDept( did: integer, pTOJid: integer, budget: real, status: char (10))\nProj(Tlf\"ojid: integer, code: integer, report: varchar)\nCHAPTER 15\nAssume that each Emp record is 20 bytes long, each Dept record is 40 bytes long, and each\nProj record is 2000 bytes long on average. There are 20,000 tuples in Emp, 5000 tuples in\nDept (note that did is not a key), and 1000 tuples in Proj. Each department, identified by\ndid, has 10 projects on average.\nThe file system supports 4000 byte pages, and 12 buffer\npages are available. All following questions are based on this information. You can assume\nuniform distribution of values. State any additional assumptions. The cost metric to use is\nthe number of page 1/005. Ignore the cost of writing out the final result.\n1. Consider the following two queries: \"Find all employees with age = 30\" and \"Find all\nprojects with code = 20,\"\nAssume that the number of qualifying tuples is the same\nin each case. If you are building indexes on the selected attributes to speed up these\nqueries, for which query is a clustered index (in comparison to an unclustered index) more\nimportant?\n2. Consider the following query: \"Find all employees with age> 30.\" Assume that there is\nan unclustered index on age. Let the number of qualifying tuples be N. For what values\nof N is a sequential scan cheaper than using the index?\n3. Consider the following query:\nSELECT *\nFROM\nEmp E, Dept D\nWHERE\nE.did=D.did\n(a) Suppose that there is a clustered hash index on did on Emp. List all the plans that\nare considered and identify the plan with the lowest estimated cost.\n(b) Assume that both relations are sorted on the join column. Lis.t all the plans that\nare considered and show the plan with the lowest estimated cost.\n(c) Suppose that there is a clustered B+ tree index on did on Emp and Dept is sorted\non did. List all the plans that are considered and identify the plan with the lowest\nestimated cost.\n4. Consider the following query:\nSELECT\nFROM\nWHERE\nGROUP BY\nD.dicl, COUNT(*)\nDept D, Proj P\nD.projid=P.projid\nD.clid\n(a) Suppose that no indexes are available.\nShow the plan with the lowest estimated\ncost.\n(b) If there is a hash index OIl P.projid what is the plan with lowest estimated cost?\n(c:) If there is a hash index on D.pmjid what is the plan with lowest estimated cost?\n(d) If there is a hash index on D-JiTojid and P.projid what is the plan with lowest\nestimated cost?\n(e) Suppose that there is a clustered B+ tree index on D.did\nand a hash index on\nP.]Jmjid. Show the plan with the lowest estimated cost.\n(f) Suppose that there is a clustered B+ tree index on D.did, a lUh<:h index OIl D.]JT'O)id,\nand a hash index on P.pmjid. Show the plan with the lowest estimated cost.\n\nA Typical\nQ'UeT~lJ Opti'rnizcT\n51i)\n(g) Suppose that there is a clustered B+ tree index on (D. did, D.pmjidj and a ha..,h\nindex on P.pmjid. Show the plan with the lowest estimated cost.\n(h) Suppose that there is a clustered B+ tree index on (D.pmjid, D.did> and a h&<;h\nindex on P.pmjid. Show the plan with the lowest estimated cost.\n5. Consider the following query:\nSELECT\nFROM\nWHERE\nGROUP BY\nD.did, COUNT(*)\nDept D, Proj P\nD.projid=P.projid AND D.budget>99000\nD.did\nAssume that department budgets are uniformly distributed in the range 0 to 100,000.\n(a) Show the plan with lowest estimated cost if no indexes are available.\n(b) If there is a hash index on P.pmjid show the plan with lowest estimated cost.\n(c) If there is a hash index on D. budget show the plan with lowest estimated cost.\n(d) If there is a hash index on D.pmjid and D.budget show the plan with lowest\nesti~\nmated cost.\n(e) Suppose that there is a clustered B+ tree index on (D.did,D.budget) and a hash\nindex on P.projid. Show the plan with the lowest estimated cost.\n(f) Suppose there is a clustered B+ tree index on D.did, a hash index on D.b1ldget,\nand a hash index on P.projid. Show the plan with the lowest estimated cost.\n(g) Suppose there is a clustered B+ tree index on (D. did, D.budgct, D.projid> and a\nhash index on P.pmjid. Show the plan with the lowest estimated cost.\n(h) Suppose there is a clustered B+ tree index on (D. did, D.projid, D.budget) and a\nhash index on P.pmjid. Show the plan with the lowest estimated cost.\n6. Consider the following query:\nSELECT\nFROM\nWHERE\nE.eid, D.did, P.projid\nEmp E, Dept D, Proj P\nE.sal=50,000 AND D.budget>20,000\nE.did=D.did AND D.projid=P.projid\nAssume that employee salaries are uniformly distributed in the range 10,009 to 110,008\nand that project budgets are uniformly distributed in the range 10,000 to 30,000. There\nis a clustered index on sal for Emp, a clustered index on did for Dept, and a clustered\nindex on pmjid for Proj.\n(a) List all the one-relation, two--relation, and\nthree~relation subplans considered in\noptimizing this query.\n(b) Show the plan with the lowest estimated cost for this query.\n(c) If the index on Proj wel'(\" unclustered, would the cost of the preceding plan change\nsubstant:ially? What if the index on Emp or on Dept were unclllstered?\n\n516\nBIBLIOGRAPHIC NOTES\nCHAPTER 15\nQuery optimization is critical in a relational DBMS, and it has therefore been extensivElly\nstudied.\n'Ve concentrate in this chapter on the approach taken in System R, as described\nin [668], although our discussion incorporates subsequent refinements to the approach. [78,1]\ndescribes query optimization in Ingres. Good surveys can be found in [41OJ and [399J. [434]\ncontains several articles on query processing and optimization.\nFrom a theoretical standpoint, [155] shows that determining whether two conjunctive q'ueT'ies\n(queries involving only selections, projections, and cross-products) are equivalent is an NP-\ncomplete problem; if relations are mv,ltisets, rather than sets of tuples, it is not known whether\nthe problem is decidable, although it is IT:zP hard. The equivalence problem is shown to be\ndecidable for queries involving selections, projections, cross-products, and unions in [643];\nsurprisingly, this problem is undecidable if relations are multisets [404]. Equivalence of con-\njunctive queries in the presence of integrity constraints is studied in [30], and equivalence of\nconjunctive queries with inequality selections is studied in [440].\nAn important problem in query optimization is estimating the size of the result of a query\nexpression.\nApproaches based on sampling are explored in [352, 353, 384, 481, 569].\nThe\nuse of detailed statistics, in the form of histograms, to estimate size is studied in [405, 558,\n598]. Unless care is exercised, errors in size estimation can quickly propagate and make cost\nestimates worthless for expressions with several operators, This problem is examined in [400].\n[512] surveys several techniques for estimating result sizes and correlations between values in\nrelations. There are a number of other papers in this area; for example, [26, 170, 594, 725],\nand our list is far from complete,\nSemantic qnery optimization is based on transformations that preserve equivalence only when\ncertain integrity constraints hold. The idea was introduced in [437] and developed further in\n[148,682, 688].\nIn recent years, there has been increasing interest in complex queries for decision support\napplications.\nOptimization of nested SQL queries is discussed in [298, 426, /130, 557, 760].\nThe use of the Magic Sets technique for optimizing SQL queries is studied in [55:3, 554, 555,\n670, 67:3].\nRule-based query optimizers are studiecl in [287, 326, 490, 539, 596].\nFinding it\ngood join order for queries with it large number of joins is studied in [401, 402, 453, 726].\nOptimization of multiple queries for simultaneous execution is considerecl in [585, 633, 669].\nDetermining query plans at run-time is discussed in [327, 403]. Re-optimization of running\nqueries based on statistics gathered during query execution is considered by Kabra and DeWitt\n[413]. Probabilistic optimization of queries is proposed in [183, 229].\n\nPART V\nTRANSACTION MANAGEMENT\n\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n16\nOVERVIEW OF TRANSACTION\nMANAGEMENT\n...\nWhat four properties of transactions does a DBMS guarantee?\n...\nWhy does a DBMS interleave transactions?\n...\nWhat is the correctness criterion for interleaved execution?\n...\nWhat kinds of anomalies can interleaving transactions cause?\n...\nHow does a DBMS use locks to ensure correct interleavings?\n...\nWhat is the impact of locking on performance?\n...\nWhat SQL commands allow programmers to select transaction char-\nacteristics and reduce locking overhead?\n...\nHow does a DBMS guarantee transaction atomicity and recoveryfrom\nsystem crashes?\n..\nKey concepts: ACID properties, atomicity, consistency, isolation,\ndurability; schedules, serializability, recoverability, avoiding cascading\naborts; anomalies, dirty reads, unrepeatable reads, lost updates; lock-\ning protocols, exclusive and shared locks, Strict Two-Phase Locking;\nlocking performance, thrashing, hot spots; SQL transaction charac-\nteristics, savepoints, rollbacks, phantoms, access mode, isolation level;\ntransaction manager, recovery manager, log, system crash, media fail-\nure; stealing frames, forcing pages; recovery phases, analysis, redo and\nundo.\n---~~--_....~------_._~---._._._..•_----_....\nI always say, keep a diary and someday it'11 keep you.\n·fvlae West\n519\n\n520\nCHAPTER 16,\nIn this chapter, we cover the concept of a lm'nsacl£on, 'iNhich is the\nfounda~\ntion for concurrent execution and recovery from system failure in a DBMS. A\ntransaction is defined as anyone e;recut£on of a user program in a DBMS and\ndiffers from an execution of a program outside the DBMS (e.g., a C program\nexecuting on Unix) in important ways. (Executing the same program several\ntimes generates several transactions.)\nFor performance reasons, a DBJ'vlS lul.'> to interleave the actions of several trans-\nactions. (vVe motivate interleaving of transactions in detail in Section 16.3.1.)\nHowever, to give users a simple way to understand the effect of running their\nprograms, the interleaving is done carefully to ensure that the result of a con-\ncurrent execution of transactions is nonetheless equivalent (in its effect on the\ndatabase) to some serial, or one-at-a-time, execution of the same set of transac-\ntions, How the DBMS handles concurrent executions is an important a\"spect of\ntransaction management and the subject of concurrency control. A closely r&-\nlated issue is how the DBMS handles partial transactions, or transactions that\nare interrupted before they run to normal completion, The DBMS ensures that\nthe changes made by such partial transactions are not seen by other transac-\ntions.\nHow this is achieved is the subject of crash r'ecovery. In this chapter,\nwe provide a broad introduction to concurrency control and crash recovery in\na. DBMS, The details are developed further in the next two chapters.\nIn Section 16.1, we discuss four fundamental properties of database transactions\nand how the DBMS ensures these properties. In Section 16.2, we present an ab-\nstract way of describing an interleaved execution of several transactions, called\na schedule. In Section 16,3, we discuss various problems that can arise due to\ninterleaved execution, \\Ve introduce lock-based concurrency control, the most\nwidely used approach, in Section 16.4. We discuss performance issues associ-\nated with lock-ba'ied concurrency control in Section 16.5. vVe consider locking\nand transaction properties in the context of SQL in Section 16.6, Finally, in\nSection 16.7, we present an overview of how a clatabase system recovers from\ncrashes and what steps are taken during normal execution to support crash\nrecovery.\n16.1\nTHE ACID PROPERTIES\nvVe introduced the concept of database trans;:Lctions in Section 1.7, To reca-\npitulate briefly, a transaction is an execution of a user program, seen by the\nDBMS as a series of read and write operations.\nA DBJ\\iIS must ensure four important properties of transactions to maintain\ndata in the face of concurrent a.ccess and system failures:\n\nOverview of Transaction Alanagernent\n521\n1. Users should be able to regard the execution of each transaction as atomic:\nEither all actions are carried out or none are.\nUsers should not have to\nworry about the effect of incomplete transactions (say, when a system crash\noccurs).\n2. Each transaction, run by itself with no concurrent execution of other trans-\nactions, lnust preserve the consistency of the datab&c;e. The DBMS as-\nsumes that consistency holds for each transaction. Ensuring this property\nof a transaction is the responsibility of the user.\n3. Users should be able to understand a transaction without considering the\neffect of other concurrently executing transactions, even if the DBMS in-\nterleaves the actions of several transactions for performance reasons. This\nproperty is sometimes referred to &'3 isolation: Transactions are isolated,\nor protected, from the effects of concurrently scheduling other transactions.\n4. Once the DBMS informs the user that a transaction has been successfully\ncompleted, its effects should persist even if the system crashes before all\nits changes are reflected on disk. This property is called durability.\nThe acronym ACID is sometimes used to refer to these four properties of trans-\nactions: atomicity, consistency, isolation and durability. We now consider how\neach of these properties is ensured in a DBMS.\n16.1.1\nConsistency and Isolation\nUsers are responsible for ensuring transaction consistency.\nThat is, the user\nwho submits a transaction must ensure that, when run to completion by itself\nagainst a 'consistent' database instance, the transaction will leave the databa.,se\nin a 'consistent' state. For example, the user may (naturally) have the consis-\ntency criterion that fund transfers between bank accounts should not change\nthe total amount of money in the accounts.\nTo transfer money from one ac-\ncount to another, a transaction must debit one account, temporarily leaving the\ndatabase inconsistent in a global sense, even though the new account balance\nmay satisfy any integrity constraints with respect to the range of acceptable\naccount balances. The user's notion of a consistent database is preserved when\nthe second account is credited with the transferred amount. If a faulty trans-\nfer program always credits the second account with one dollar less than the\nalllount debited frOlll the first account, the DBMS cannot be expected to de-\ntect inconsistencies due to such errors in the user program's logic.\nThe isolation property is ensured by guaranteeing that, even though actions\nof several transactions rnight be interleaved, the net effect is identical to ex-\necuting all transactions one after the other in sorne serial order.\n(vVe discuss\n\n522\nCHAPTER\n16~\nhm'll the DBMS implements this guarantee in Section 16.4.)\nFor example, if\ntwo transactions T1 and T2 are executed concurrently, the net effect is guar-\nanteed to be equivalent to executing (all of) T1 followed by executing T2 or\nexecuting T2 followed by executing Tl.\n(The DBIvIS provides no guarantees\nabout which of these orders is effectively chosen.) If each transaction maps a\nconsistent database instance to another consistent database instance, execut-\ning several transactions one after the other (on a consistent initial database\ninstance) results in a consistent final database instance.\nDatabase consistency is the property that every transaction sees a consistent\ndatabase instance.\nDatabase consistency follows from transaction atomicity,\nisolation, and transaction consistency.\nNext, we discuss how atomicity and\ndurability are guaranteed in a DBMS.\n16.1.2\nAtomicity and Durability\nTransactions can be incomplete for three kinds of reasons. First, a transaction\ncan be aborted, or terminated unsuccessfully, by the DBMS because some\nanomaly arises during execution. If a transaction is aborted by the DBMS for\nSOlne internal reason, it is automatically restarted and executed anew. Second,\nthe system may crash (e.g., because the power supply is interrupted) while one\nor more transactions are in progress. Third, a transaction may encounter an\nunexpected situation (for example, read an unexpected data value or be unable\nto access some disk) and decide to abort (i.e., terminate itself).\nOf course, since users think of transactions &<; being atomic, a transaction that\nis interrupted in the middle may leave the database in an inconsistent state.\nTherefore, a DBMS must find a way to remove the effects of partial transactions\nfrom the database. That is, it must ensure transaction atomicity: Either all of a\ntransaction's actions are carried out or none are. A DBMS ensures transaction\natomicity by vindoing the actions of incomplete transactions. This means that\nusers can ignore incomplete transactions in thinking about how the database is\nmodified by transactions over time. To be able to do this, the DBMS maintains\na record, called the log. of all writes to the database. The log is also used to\nensure durability: If the system crashes before the changes made by a completed\ntransaction are written to disk, the log is used to remember and restore these\nchanges when th~ systenl restarts.\nThe DBMS component that ensures atomicity and durability, called the r'ec;ov-\ncry rnanagcr', is discussed further in Section 16.7.\n\nOverview of Transaction, A1anagement\n16.2\nTRANSACTIONS AND SCHEDULES\nA transaction is seen by the DBMS a'l a series, or list, of actions. The actions\nthat can be executed by a transaction include reads and writes of database\nobjects.\nTo keep our notation simple, we a'Jsume that an object 0 is always\nread into a program variable that is also named O. 'Ne can therefore denote\nthe action of a transaction T reading an object 0 as RT(O); similarly, we can\ndenote writing as HTT(O). When the transaction T is clear from the context,\nwe omit the subscript.\nIn addition to reading and writing, each transaction must specify as its final\naction either commit (i.e., complete successfully) or abort (i.e., terminate\nand undo all the actions carried out thus far). AbortT denotes the action of T\naborting, and CommitT denotes T committing.\nWe make two important assumptions:\n1. Transactions interact with each other only via databa'Je read and write\noperations; for example, they are not allowed to exchange messages.\n2. A database is a fiJ;ed collection of independent objects. When objects are\nadded to or deleted from a database or there are relationships between\ndatabase objects that we want to exploit for performance, some additional\nissues arise.\nIf the first assumption is violated, the DBMS has no way to detect or prevent\ninconsistencies cause by such external interactions between transactions, and it\nis upto the writer of the application to ensure that the program is well-behaved.\nWe relax the second assumption in Section 16.6.2.\nA schedule is a list of actions (reading, writing, aborting, or committing)\nfrom a set of transactions, and the order in which two actions of a transaction\nT appear in a schedule must be the same as the order in which they appear in T.\nIntuitively, a schedule represents an actual or potential execution sequence. For\nexample, the schedule in Figure 16.1 shows an execution order for actions of two\ntransactions T1 and T2. \\eVe move forward in time as we go down from one row\nto the next. \\Ve emphasize that a schedule describes the actions of transactions\nas seen by the DBMS. In addition to these actions, a transaction rnay carry out\nother actions, such as reading or writing from operating system files, evaluating\narithmetic expressions, and so on; however, we a:ssume that these actions do\nnot affect other transactions; that is, the effect of a transaction on another\ntransaction can be understood solely in terms of the cornmon database objects\nthat they read and write.\n\n524\nT1\nT2\nR(A)\nHl(A)\nR(B)\nIV(B)\nR(C)\nW\"(C)\nFigure 16.1\nA Schedule Involving Two Transactions\nCHAPTER 16\nNote that the schedule in Figure 16.1 does not contain an abort or commit ac-\ntion for either transaction. A schedule that contains either an abort or a commit\nfor each transaction whose actions are listed in it is called a complete sched-\nule.\nA complete schedule must contain all the actions of every transaction\nthat appears in it. If the actions of different transactions are not interleaved-\nthat is, transactions are executed from start to finish, one by one-we call the\nschedule a serial schedule.\n16.3\nCONCURRENT EXECUTION OF TRANSACTIONS\nNow that we have introduced the concept of a schedule, we have a convenient\nway to describe interleaved executions of transactions. The DBMS interleaves\nthe actions of different transactions to improve performance, but not all inter-\nleavings should be allowed. In this section, we consider what interleavings, or\nschedules, a DBMS should allow.\n16.3.1\nMotivation for Concurrent Execution\nThe schedule shown in Figure 16.1 represents an interleaved execution of the\ntwo transactions. Ensuring transaction isolation while permitting such concur··\nrent execution is difficult lnlt necessary for performance reasons. First, while\none transa.etion is waiting for a page to be read in from disk, the CPU can\nprocess another transaction. This is because I/O activity can be done in par-\nallel with CPU activity in a computer.\nOverlapping I/O and CPU activity\nreduces the amount of time disks and processors are idle and increases system\nthroughput (the average number of transactions completed in a given time).\nSecond, interleaved execution of a short transaction with a long transaction\nusually allows the short transaction to complete quickly.\nIn serial execution,\na short transaction could get stuck behind a long transaction, leading to un-\npredictable delays in response time, or average time taken to complete a\ntransaction.\n\n01)eruiew of Transaction A!anagf'rnent\n16.3.2\nSerializabHity\nA serializable schedule over a set S of cormnitted transactions is a schedule\nwhose effect on any consistent database instance is guaranteed to be identical\nto that of some complete serial schedule over S. That is, the databa..<;e instance\nthat results from executing the given schedule is identical to the database in-\nstance that results frOlIl executing the transactions in some serial order. 1\nAs an example, the schedule shown in Figure 16.2 is serializable. Even though\nthe actions of T1 and T2 are interleaved, the result of this schedule is equivalent\nto running T1 (in its entirety) and then running T2. Intuitively, T1 's read and\nwrite of B is not influenced by T2's actions on A, and the net effect is the same\nif these actiolls are 'swapped' to obtain the serial schedule Tl; T2.\nTl\nT2\nR(A)\nvV(A)\nR(A)\nW(A)\nR(B)\nvV(B)\nR(B)\nW(B)\nCommit\nCommit\nFigure 16.2\nA Serializable Schedule\nExecuting transactions serially in different orders may produce different results,\nbut all are presumed to be acceptable: the DBMS makes no guarantees ahout\nwhich of them will be the outcome of an interleaved execution.\nTo see this,\nnote that the two example transactions from Figure 16.2 can be interleaved a.s\nshown in Figure 16.:3. This schedule, also serializable, is equivalent to the serial\nschedule T2; Tl. If T1 and T2 are submitted concurrently to a DBMS, either\nof these schedules (among others) could be chosen.\nThe preceding definition of a serializable schedule does not cover the case of\nschedules containing aborted transactions. We extend the definition of serial-\nizable schedules to cover aborted transactions in Section 16.3.4.\nllf a transaction prints a value to the screen, this 'effed' is not directly captured in the database.\nFor simplicity, we assume that such values are abo written into the database.\n\n526\nTl\nT2\nR(A)\nvV(A)\nR(A)\nR(B)\nvV(B)\nvV(A)\nR(B)\nvV(B)\nCommit\nCommit\nFigure 16.3\nAnother Serializable Schedule\nCHAPTER 16\nFinally, we note that a DBMS might sometimes execute transactions in a way\nthat is not equivalent to any serial execution; that is, using a schedule that is\nnot serializable. This can happen for two reasons. First, the DBMS might use\na concurrency control method that ensures the executed schedule, though not\nitself serializable, is equivalent to some serializable schedule (e.g., see Section\n17.6.2). Second, SQL gives application programmers the ability to instruct the\nDBMS to choose non-serializable schedules (see Section 16.6).\n16.3.3\nAnomalies Due to Interleaved Execution\nWe now illustrate three main ways in which a schedule involving two consistency\npreserving, committed transactions could run against a consistent database and\nleave it in an inconsistent state. Two actions on the same data object conflict if\nat least one of them is a write. The three anomcllous situations can be described\nin terms of when the actions of two transactions Tl and T2 conflict with each\nother: In a write-read (WR) conflict, T2 reads a data object previously\nwritten by Tl; we define read-write (RW) and write-write (WW) conflicts\nsimilarly.\nReading Uncommitted Data (WR Conflicts)\nThe first source of anomalies is that a transaction T2 could read a database\nobject A that has been modified by another transaction Tl, which ha\"i not yet\ncommitted. Such a read is called a dirty read. A simple example illustrates\nhow such a schedule could lead to an inconsistent database state.\nConsider\ntwo transactions Tl and T2. each of which, run alone, preserves datal)a\"ie\nconsistency: Tl transfers 8100 from A to B, and T2 increments both A and\nB by G% (e.g., annual interest is deposited into these two accounts). Suppose\n\nOverview of Transaction A1anagement\n527\nthat the actions are interleaved so that (1) the account transfer program Tl\ndeducts $100 from account A, then (2) the interest deposit program T2 reads\nthe current values of accounts A and B and adds 6% interest to each, and then\n(3) the account transfer program credits $100 to account B. The corresponding\nschedule, which is the view the DBMS has of this series of events, is illustrated\nin Figure 16.4. The result of this schedule is different from any result that we\nwould get by running one of the two transactions first and then the other. The\nproblem can be traced to the fact that the value of A written by TI is read by\nT2 before TI has completed all its changes.\nTl\nT2\nR(A)\nvV(A)\nR(A)\nW(A)\nR(B)\nliV(B)\nCommit\nR(B)\nW(B)\nCommit\nFigure 16.4\nReading Uncommitted Data\nThe general problem illustrated here is that Tl may write some value into A\nthat makes the databa..':le inconsistent. As long as TI overwrites this value with\na 'correct' value of A before committing, no harm is done if TI and T2 run in\nsome serial order, because T2 would then not see the (temporary) inconsistency.\nOn the other hetnel, interleaved execution can expose this inconsistency and lead\nto an inconsistent final database state.\nNote that although a transaction must leave a database in a consistent state\nafter it completes, it is not required to keep the database consistent while it is\nstill in progress. Such a requirement would be too restrictive: To transfer money\nfrom one account to another, a transaction rn1l8t debit one account, temporarily\nleaving the database inconsistent, and then credit the second account, restoring\nconsistency.\n\n528\nUnrepeatable Reads (RW Conflicts)\nCHAPTERd.6\nThe second way in which anomalous behavior could result is that a transaction\nT2 could change the value of an object A that has been read by\n1:1, transaction\nTl, while Tl is still in progress.\nIfTl tries to read the value of A again, it will get a different result, even though\nit has not modified A in the meantime. This situation could not arise in a serial\nexecution of two transactions; it is called an unrepeatable read.\nTo see why this can cause problems, consider the following example. Suppose\nthat A is the number of available copies for a book. A transaction that places\nan order first reads A, checks that it is greater tha,n 0, and then decrements it.\nTransaction Tl reads A and sees the value 1. Transaction T2 also reads A and\nsees the value 1, decrements A to 0 and commits. Transaction Tl then tries to\ndecrement A and gets an error (if there is an integrity constraint that prevents\nA from becoming negative).\nThis situation can never arise in a serial execution of Tl and T2; the second\ntransaction would read A and see 0 and therefore not proceed with the order\n(and so would not attempt to decrement A).\nOverwriting Uncommitted Data (WW Conflicts)\nThe third source of anomalous behavior is that a transaction T2 could overwrite\nthe value of an object A, which has already been modified by a transaction Tl,\nwhile Tl is still in progress. Even if T2 does not read the value of A written\nby Tl, a potential problem exists as the following example illustrates.\nSuppose that Harry and Larry are two employees, and their salaries must be\nkept equal. Transaction Tl sets their salaries to $2000 and transaction T2 sets\ntheir salaries to $1000. If we execute these in the serial order Tl followed by\nT2, both receive the salary $1000: the serial order T2 followed by Tl gives each\nthe salary $2000. Either of these is acceptable from a consistency standpoint\n(although Harry and Larry may prefer a higher salary!).\nNote that neither\ntransaction reads a salary value before writing it----such a write is called a\nblind write, t:or obvious rea.sons.\nNow, consider the following interleaving of the actions of 1'1 and T2: T2 sets\nHarry's salary to $1000, Tl sets Larry's salary to $2000, T2 sets La.rry's salary\nto $1000 and commits, and finally Tl sets Harry's salary to $2000 and connnits.\nThe result is not identical to the result of either of the two possible serial\n\nOuenJiew of Transaction ~Management\nexecutions, and the interleaved schedule is therefore not serializable. It violates\nthe desired consistency criterion that the two salaries must be equal.\nThe problem is that we have a lost update. The first transaction to commit,\nT2, overwrote Larry's salary as set by Tl. In the serial order T2 followed by\nT1, Larry's salary should reflect Tl's update rather than T2's, but Tl's update\nis 'lost'.\n16.3.4\nSchedules Involving Aborted Transactions\nWe now extend our definition of serializability to include aborted trallsactions. 2\nIntuitively, all actions of aborted transactions are to be undone, and we can\ntherefore imagine that they were never carried out to begin with. Using this\nintuition, we extend the definition of a serializable schedule as follows: A se-\nrializable schedule over a set S of transactions is a schedule whose effect on\nany consistent database instance is guaranteed to be identical to that of some\ncomplete serial schedule over the set of committed transactions in S.\nThis definition of serializability relies on the actions of aborted transactions\nbeing undone completely, which may be impossible in some situations.\nFor\nexample, suppose that (1) an account transfer program T1 deducts $100 from\naccount A, then (2) an interest deposit program T2 reads the current values of\naccounts A and B and adds 6% interest to each, then commits, and then (3)\nT1 is aborted. The corresponding schedule is shown in Figure 16..5.\nTl\nT2\nR(A)\nW(A)\nR(A)\nvV(A)\nR(B)\nHl(B)\nCommit\nAbort\nFigure 16.5\nAn Unrecoverable Schedule\n2 Vie must also consider incomplete transactions for a rigorous discussion of system failures, because\ntransactions that are active when the system fails are neither aborted nor committed. However, system\nrecovery usually begins by aborting all active transactions. and for our informal discussion, considering\nschedules involving committed and aborted transactions is sufficient.\n\n530\nCHAPTER$16\nNow, T2 has read a value for A that should never have been there.\n(Recall\nthat aborted transactions' effects are not supposed to be visible to other trans-\nactions.) If T2 had not yet committed, we could deal with the situation by\ncascading the abort of TI and also aborting T2; this process recursively aborts\nany transaction that read data written by T2, and so on. But T2 has already\ncommitted, and so we cannot undo its actions. \\Ve say that such a schedule\nis unrecoverable. In a recoverable schedule, transactions commit only after\n(and if!) all transactions whose changes they read commit. If transactions read\nonly the changes of committed transactions, not only is the schedule recover-\nable, but also aborting a transaction can be accomplished without cascading\nthe abort to other transactions. Such a schedule is said to avoid cascading\naborts.\nThere is another potential problem in undoing the actions of a transaction.\nSuppose that a transaction T2 overwrites the value of an object A that has been\nmodified by a transaction TI, while TI is still in progress, and Tl subsequently\naborts.\nAll of Tl's changes to database objects are undone by restoring the\nvalue of any object that it modified to the value of the object before Tl's\nchanges.\n(We look at the details of how a transaction abort is handled in\nChapter 18.) When Tl is aborted and its changes are undone in this manner,\nT2's changes are lost as well, even if T2 decides to commit. So, for example, if\nA originally had the value 5, then WetS changed by T1 to 6, and by T2 to 7, if\nT1 now aborts, the value of A becomes 5 again. Even if T2 commits, its change\nto A is inadvertently lost. A concurrency control technique called Strict 2PL,\nintroduced in Section 16.4, can prevent this problem (as discussed in Section\n17.1) .\n16.4\nLOCK-BASED CONCURRENCY CONTROL\nA DBMS must be able to ensure that only serializable, recoverable schedules\nare allowed and that no actions of committed transactions are lost while undo-\ning aborted transactions. A DBl'vlS typically uses a locking protocol to achieve\nthis. A lock is a small bookkeeping object CL.ssociated with a database object.\nA locking protocol is a set of rules to be followed by each transaction (and en-\nforced by the DBlVIS) to ensure that, even though actions of several transactions\nmight be interleaved, the net effect is identical to executing all transactions in\nsOlne serial or~ler. Different locking protocols use different types of locks, such\nas shared locks or exclusive locks, as we see next, when we discuss the Strict\n2PL protocol.\n\nOve-nJiew of Transaction jvlanagement\n16.4.1\nStrict Two-Phase Locking (Strict 2PL)\n531\nThe most widely used locking protocol, called Strict Two-Phase Locking, or\nStrict 2PL, has two rules. The first rule is\n1. If a transaction T wants to read (respectively, rnodify) an object, it\nfirst requests a shared (respectively, exclusive) lock on the object.\nOf course, a transaction that has an exclusive lock can also read the object;\nan additional shared lock is not required. A transaction that requests a lock is\nsuspended until the DBMS is able to grant it the requested lock. The DBMS\nkeeps track of the locks it has granted and ensures that if a transaction holds\nan exclusive lock on an object, no other transaction holds a shared or exclusive\nlock on the same object. The second rule in Strict 2PL is\n2. All locks held by a transaction are relea.'3ed when the transaction is\ncompleted.\nRequests to acquire and release locks can be automatically inserted into trans-\nactions by the DBMS; users need not worry about these details.\neWe discuss\nhow application programmers can select properties of transactions and control\nlocking overhead in Section 16.6.3.)\nIn effect, the locking protocol allows only 'safe' interleavings of transactions.\nIf two transactions access completely independent parts of the database, they\nconcurrently obtain the locks they need and proceed merrily on their ways. On\nthe other band, if two transactions access the same object, and one wants to\nmodify it, their actions are effectively ordered serially·all actions of one of\nthese transactions (the one that gets the lock on the common object first) are\ncompleted before (this lock is released and) the other transaction can proceed.\nWe denote the action of a transaction T requesting a shared (respectively, exclu-\nsive) lock on object 0 as 5T(0) (respectively, XT(O)) and omit the subscript\ndenoting the tn1l1saction when it is clear from the context.\nAs an example,\nconsider the schedule shown in Figure 16.4. This interleaving could result in a\nstate that cannot result from any serial execution of the three transactions. For\ninstance, T1 could change A from 10 to 20, then T2 (which reads the value 20\nfor A) could change B from 100 to 200, and then T1 would read the value 200\nfor B. If run serially, either Tl or T2 would execute first, and read the values\n10 for A and 100 for B: Clearly, the interleaved execution is not equivalent to\neither serial execution.\nIf the Strict 2PL protocol is used, such interleaving is disallowed. Let us see\nwhy.\nAssuming that the transactions proceed <tt the same relative speed as\n\n532\nCHAPTER*16\nbefore, T1 would obtain an exclusive lock on A first and then read and write\nA (Figure 16.6). Then, 1'2 would request a lock on A. However, this request\n1'1\nT2\nX(A)\nR(A)\nlV(A)\nFigure 16.6\nSchedule Illustrating Strict 2PL\ncannot be granted until 1'1 releases its exclusive lock on A, and the DBMS\ntherefore suspends 1'2.\n1'1 now proceeds to obtain an exclusive lock on B,\nreads and writes B, then finally commits, at which time its locks are released.\nT2's lock request is now granted, and it proceeds. In this example the locking\nprotocol results in a serial execution of the two transactions, shown in Figure\n16.7.\nT1\nT2\nX(A)\nR(A)\nW(A)\nX(B)\nR(B)\nW(B)\nCommit\nX(A)\nR(A)\nW(A)\nX(B)\nR(B)\nH'(B)\nCommit\nFigure 16.7\nSchedule Illustrating Strict 2PL with Serial Execution\nIn general, however, the actions of different transactions could be interleaved.\nAs an example, consider the interleaving of two transactions shown in Figure\n16.8, which is permitted by the Strict 2PL protocol.\nIt can be shown that the Strict 2PL algorithm allows only serializable sched-\nules. None of the anomalies discussed in Section 16.3.:3 can arise if the DBMS\nimplements Strict 2PL.\n\nOvenriew of Tran,.'wdion Alanagement\nTl\nT2\n8(A)\nR(A)\n8(A)\nR(A)\nX(B)\nR(B)\nvV(B)\nConllnit\nX(C)\nR(C)\nW(C)\nCommit\nFigure 16.8\nSchedule Following Strict 2PL with Interleaved Actions\n16.4.2\nDeadlocks\n533\nConsider the following example. Transaction T1 sets an exclusive lock on object\nA, T2 sets an exclusive lock on B, T1 requests an exclusive lock on B and is\nqueued, and T2 requests an exclusive lock on A and is queued.\nNow, T1 is\nwaiting for T2 to release its lock and T2 is waiting for T1 to release its lock.\nSuch a cycle of transactions waiting for locks to be released is called a deadlock.\nClearly, these two transactions will make no further progress.\nWorse, they\nhold locks that may be required by other transactions. The DBMS must either\nprevent or detect (and resolve) such deadlock situations; the common approach\nis to detect and resolve deadlocks.\nA simple way to identify deadlocks is to use a timeout mechanism. If a trans-\naction has been waiting too long for a lock, we can a'3sume (pessimistically)\nthat it is in a deadlock cycle and abort it. We discuss deadlocks in more detail\nin Section 17.2.\n16.5\nPERFORMANCE OF LOCKING\nLock-b\"l.'sed schqmes are designed to resolve conflicts between transactions and\nuse two ba'3ic mechanisms:\nblocking and aborting.\nBoth mechanisrns involve\na performance penalty: Blocked transactions may hold locks that force other\ntransactions to wait, and aborting and restarting a transaction obviously wa..'3tes\nthe work done thus far by that transaction. A deadlock represents an extreme\ninstance of blocking in which a set of transactions is forever blocked unless one\nof the deadlocked transactions is aborted by the DBMS.\n\nOverview of TmnsCLction lHanagement\n535\n•\nBy reducing hot spots. A hot spot is a databa.ge object that is frequently\naccessed and modified, and causes a lot of blocking delays. Hot spots can\nsignificantly affect performance.\nThe granularity of locking is largely determined by the databa..<;;e system's im-\nplementation of locking, and application programmers and the DBA have little\ncontrol over it.\nWe discuss how to improve performance by minimizing the\nduration locks are held and using techniques to deal with hot spots in Section\n20.10.\n16.6\nTRANSACTION SUPPORT IN SQL\nWe have thus far studied transactions and transaction management using an\nabstract model of a transaction as a sequence of read, write, and abort/commit\nactions.\nWe now consider what support SQL provides for users to specify\ntransaction-level behavior.\n16.6.1\nCreating and Terminating Transactions\nA transaction is automatically started when a user executes a statement that\naccesses either the database or the catalogs, such as a SELECT query, an UPDATE\ncommand, or a CREATE TABLE statement.4\nOnce a transaction is started, other statements can be executed as part of this\ntransaction until the transaction is terminated by either a COMMIT command\nor a ROLLBACK (the SQL keyword for abort) command.\nIn SQL:1999, two new features are provided to support applications that involve\nlong-running transactions, or that must run several transactions one after the\nother.\nTo understand these extensions, recall that all the actions of a given\ntransaction are executed in order, regardless of how the actions of different\ntransactions are interleaved. We can think of each transaction as a sequence of\nsteps.\nThe first feature, called a savepoint, allows us to identify a point in a trans-\naction and selectively roll back operations carried out after this point. This\nis especially useful if the transaction carries out what-if kinds of operations,\nand wishes to undo or keep the changes based on the results.\nThis can be\naccomplished by defining savepoints.\n4Some SQL statements·..·····e.g., the CONNECT statement, which connects an application program to a\ndatabase server\ndo not require the creation of a transaction.\n\n536\nCHAPTEH 16\nr-·······~~~~\n.---.-----\n----\n\"\"\"'-<··---------1\nI\nSQL:1999 Nested Transactions: The concept of atn,msactioll as an\nI\nI,\natomic sequence of actions has been extended in SQL:1999 thrQugh the i\n•\nintroduction of the savepo'int feature. This allows parts of a transaction to\nbe selectively rolled back. The introduction of savepoints represents the\nfirst SQL support for the concept of nested transactions, which have\nbeen extensively studied in the research community.\nThe idea is that a\nI\ntransaction can have several nested subtransactions, each of which can\nI\nbe selectively rolled back. Savepoints snpport a simple form of one-level\ni\nnesting.\n'--------------,\n..\n\"\n, -\nIn a long-running transaction, we may want to define a series of savepoints.\nThe savepoint command allows us to give each savepoint a name:\nSAVEPDINT (savepoint name)\nA subsequent rollback command can specify the savepoint to roll back to\nROLLBACK TO SAVEPDINT (savepoint name)\nIf we define three savepoints A, B, and C in that order, and then rollback to\nA, all operations since A are undone, including the creation of savepoints B\nand C. Indeed, the savepoint A is itself undone when we roll hack to it, and\nwe must re-establish it (through another savepoint conunand) if we wish to be\nable to roll back to it again. From a locking standpoint, locks obtained after\nsavepoint A can be released when we roll back to A.\nIt is instructive to compare the use of savepoints with the alternative of execut-\ning a series of transactions (i.e., treat all operations in between two consecutive\nsavepoints as a new transaction).\nThe savepoint mechanism offers two ad-\nvantages.\nFirst, we can roll back over several savepoints.\nIn the alternative\napproach, we can roll back only the most recent transaction, which is equiv-\nalent to rolling back to the most recent savepoint,\nSecond, the overhead of\ninitiating several transactions is avoided.\nEven with the use of savepoints, certain applications might require us to run\nseveral transactions one after the other.\nTo minimize the overhead in such\nsituations, SQL:1999 introduces another feature, called chained transactions,\n\\Ve can cornmit or roll back a transaction and immediately initiate another\ntransaction,\nThis is done by using the optional keywords AND CHAIN in the\nCOMMIT and ROLLBACK statements.\n\nOverview of Transaction AIanagement\n16.6.2\nWhat Should We Lock?\n537\nUntil now, we have discussed transactions and concurrency control in tenus of\nan abstract model in which a database contains a fixed collection of objects, and\neach transaction is a series of read and write operations on individual objects.\nAn important question to consider in the context of SQL is what the DBMS\nshould treat as an object when setting locks for a given SQL statement (that is\npart of a transaction).\nConsider the following query:\nSELECT S.rating, MIN (S.age)\nFROM\nSailors S\nWHERE\nS.rating = 8\nSuppose that this query runs as part of transaction T1 and an SQL statement\nthat modifies the age of a given sailor, say Joe, with rating=8 runs a-s part of\ntransaction T2. What 'objects' should the DBMS lock when executing these\ntransactions? Intuitively, we must detect a conflict between these transactions.\nThe DBMS could set a shared lock on the entire Sailors table for T1 and set\nan exclusive lock on Sailors for T2, which would ensure that the two transac-\ntions are executed in a serializable manner. However, this approach yields low\nconcurrency, and we can do better by locking smaller objects, reflecting what\neach transaction actually accesses.\nThus, the DBMS could set a shared lock\non every row with mting=8 for transaction T1 and set an exclusive lock on\njust the row for the modified tuple for transaction T2. Now, other read-only\ntransactions that do not involve nding=8 rows can proceed without waiting for\nT1 or T2.\nAs this example illustrates, the DBMS can lock objects at different granular-\nities:\n\\~re can lock entire tables or set row-level locks. The latter approach is\ntaken in current systems because it offers much better performance. In practice,\nwhile row-level locking is generally better, the choice of locking granularity is\ncomplicated. For example, a transaction that examines several rows and mod-\nifies those tha1 satisfy some condition might be best served by setting shared\nlocks on the entire table and setting exclusive locks on those rows it wants to\nlllodify. vVe diseuss this issue further in Section 17.5.3.\nA second point to note is that SQL statements conceptually access a collection\nof rows described by a .selection predicate. In the prf~cedingexample, transaction\nT1 accesses all rows with mting=8. vVe suggested that this could be dealt with\nby setting shared locks on all rows in Sailors that had rating=8. Unfortunately,\nthis is a little too silnplistic. To sec why, consider an SQL statelnent that inserts\n\n538\nCHAPTER 1.6\na new sailor with mting=8 and runs as transaction T3.\n(Observe that this\nexample violates our assumption of a fixed number of objects in the database,\nbut we must obviously deal with such situations in practice.)\nSuppose that the DBJ\\iIS sets shared locks on every existing Sailors row with\nmting=8 for Tl. This does not prevent transaction T3 from creating a brand\nnew row with mting=8 and setting an exclusive lock on this row. If this new row\nhas a smaller age value than existing rows, Tl returns an answer that depends\non when it executed relative to T2. However, our locking scheme imposes no\nrelative order on these two transactions.\nThis phenomenon is called the phantom problem:\nA transaction retrieves\na collection of objects (in SQL terms, a collection of tuples) twice and sees\ndifferent results, even though it does not modify any of these tuples itself. To\nprevent phantoms, the DBMS must conceptually lock all possible rows with\nmting=8 on behalf of Tl.\nOne way to do this is to lock the entire table, at\nthe cost of low concurrency. It is possible to take advantage of indexes to do\nbetter, as we will see in Section 17.5.1, but in general preventing phantoms can\nhave a significant impact on concurrency.\nIt may well be that the application invoking T1 can accept the potential inac-\ncuracy due to phantoms. If so, the approach of setting shared locks on existing\ntuples for Tl is adequate, and offers better performance.\nSQL allows a pro-\ngrammer to make this choice---and other similar choices'--explicitly, as we see\nnext.\n16.6.3\nTransaction Characteristics in SQL\nIn order to give programmers control over the locking overhead incurred by\ntheir transactions, SQL allows them to specify three characteristics of a trans-\naction:\naccess mode, diagnostics size, and isolation level.\nThe diagnostics\nsize determines the number of error conditions that can be recorded; we will\nnot discuss this feature further.\nIf the access mode is READ ONLY, the transaction is not allowed to modify\nthe databclse. Thus, INSERT, DELETE, UPDATE, and CREATE comlnands cannot\nbe executed. If we have to execute one of these commands, the access mode\nshould be set to READ WRITE. 1<or transactions with READ ONLY access mode.\nonly shared locks need to be obtained, thereby increasing concurrency.\nThe isolation level controls the extent to which a given transaction is ex-\nposed to the actions of other transactions executing concurrently. By choosing\none of four possible isolation level settings, a user can obtain greater concur-\n\nOvenrie11J of Tnmsaction l\"tfanagernent\n53Q\nrencyat the cost of increasing the transaction's exposure to other transactions'\nuncommitted changes.\nIsolation level choices are READ UNCOMMITTED, READ COMMITTED, REPEATABLE\nREAD, and SERIALIZABLE. The effect of these levels is summarized in Figure\n16.10. In this context, dirty read and unrepeatable read are defined as usuaL\nLevel\nREAD UNCOMMITTED\nREAD COMMITTED\nREPEATABLE READ\nSERIALIZABLE\nDirty Read\nMaybe\nNo\nNo\nNo\nUnrepeatable Read\nMaybe\nMaybe\nNo\nNo\nMaybe\nMaybe\nMaybe\nNo\nFigure 16.10\nTransaction Isolation Levels in SQL-92\nThe highest degree of isolation from the effects of other transactions is achieved\nby setting the isolation level for a transaction T to SERIALIZABLE. This isolation\nlevel ensures that T reads only the changes made by committed transactions,\nthat no value read or written by T is changed by any other transaction until T\nis complete, and that if T reads a set of values based on some search condition,\nthis set is not changed by other transactions until T is complete (i.e., T avoids\nthe phantom phenomenon).\nIn terms of a lock-based implementation, a SERIALIZABLE transaction obtains\nlocks before reading or writing objects, including locks on sets of objects that\nit requires to be unchanged (see Section 17.5.1) and holds them until the end,\naccording to Strict 2PL.\nREPEATABLE READ ensures that T reads only the changes made by commit-\nted transactions and no value read or written by T is changed by any other\ntransaction until T is complete.\nHowever, T could experience the phantom\nphenomenon; for example, while T examines all Sailors records with rating=1,\nanother transaction might add a new such Sailors record, which is missed by\nT.\nA REPEATABLE READ transaction sets the same locks a'S a SERIALIZABLE trans-\naction, except that it does not do index locking; that is, it locks only individual\nobjects, not sets of objects. vVe discuss index locking in detail in Section 17.5.1.\nREAD COMMITTED ensures that T reads only the changes made by committed\ntransactions, and that no value written by T is changed by any other transaction\nuntil T is complete.\nHowever, a value read by T may well be modified by\n\n540\nCHAPTER 1!6\nanother transaction while T is still in progress, and T is exposed to the phantom\nproblem.\nA READ COMMITTED transaction obtains exclusive locks before writing objects\nand holds these locks until the end. It also obtains shared locks before read-\ning objects, but these locks are released immediately; their only effect is to\nguarantee that the transaction that last modified the object is complete. (This\nguarantee relies on the fact that every SQL transaction obtains exclusive locks\nbefore writing objects and holds exclusive locks until the end.)\nA READ UNCOMMITTED transaction T can read changes made to an object by an\nongoing transaction; obviously, the object can be changed further while T is in\nprogress, and T is also vulnerable to the phantom problem.\nA READ UNCOMMITTED transaction does not obtain shared locks before reading\nobjects. This mode represents the greatest exposure to uncommitted changes\nof other transactions; so much so that SQL prohibits such a transaction from\nmaking any changes itself-a READ UNCOMMITTED transaction is required to have\nan access mode of READ ONLY. Since such a transaction obtains no locks for\nreading objects and it is not allowed to write objects (and therefore never\nrequests exclusive locks), it never makes any lock requests.\nThe SERIALIZABLE isolation level is generally the safest and is recommended for\nmost transactions. Some transactions, however, can run with a lower isolation\nlevel, and the smaller number of locks requested can contribute to improved sys-\ntem performance. For example, a statistical query that finds the average sailor\nage can be run at the READ COMMITTED level or even the READ UNCOMMITTED\nlevel, because a few incorrect or missing values do not significantly affect the\nresult if the number of sailors is large.\nThe isolation level and access mode can be set using the SET TRANSACTION com~\nmand. For example, the following command declares the current transaction\nto be SERIALIZABLE and READ ONLY:\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE READ ONLY\nWhen a transaction is started, the default is SERIALIZABLE and READ WRITE.\n16.7\nINTRODUCTION TO CRASH RECOVERY\nThe recovery manager of a DBMS is responsible for ensuring transaction\natomicity and durability. It ensures atomicity by undoing the actions of trans-\nactions that do not commit, and durability by making sure that all actions of\n\nOverview of 7'ransaction lvlanagernent\n541\ncommitted transactions survive systenl crashes, (e.g., a core dump caused by\na bus error) and media failures (e.g., a disk is corrupted).\n\\\\Then a DB]\\,IS is restarted after crashes. the recovery manager is given control\nand must bring the databa.'le to a consistent state. The recovery manager is\nalso responsible for undoing the actions of an aborted transaction. To see what\nit takes to implement a recovery manager, it is necessary to understand what\nhappens during normal execution.\nThe transaction manager of a DBMS controls the execution of transactions.\nBefore reading and writing objects during normal execution, locks must be ac-\nquired (and released at some later time) according to a chosen locking protocol.5\nFor simplicity of exposition, we make the following assumption:\nAtomic Writes: Writing a page to disk is an atomic action.\nThis implies that the system does not crash while a write is in progress and is\nunrealistic. In practice, disk writes do not have this property, and steps must\nbe taken during restart after a crash (Section 18.6) to verify that the most\nrecent write to a given page was completed successfully, and to deal with the\nconsequences if not.\n16.7.1\nStealing Frames and Forcing Pages\n\\Vith respect to writing objects, two additional questions arise:\n1. Can the changes made to an object 0 in the buffer pool by a transaction T\nbe written to disk before T commits? Such writes are executed when an-\nother transaction wants to bring in a page and the buffer manager chooses\nto replace the frame containing 0; of course, this page must have been\nunpinned by T. If such writes are allowed, we say that a steal approach\nis used. (Informally, the second transaction 'steals' a frame from T.)\n2. \\\\\"hen a transaction cOl1units, must we ensure that all the changes it has\nmade to objects in the buffer pool are immediately forced to disk? If so.\nwe say that a force approach is used.\nFrom the standpoint of implementing a recovery manager, it is simplest to use\na buffer manager with a no-steaL force approach. If a no-steal approach is used,\nwe do not have to undo the changes of an aborted transaction (because these\ndumges have not been written to disk) l and if a force approach is used, we do\nSA concurrency control technique that does not involve locking could be used instead, but we\na.ssul1lc t hat locking is llsed.\n\n542\nCHAPTER.16\nnot have to redo the changes of a committed transaction if there is a subsequent\ncrash (because all these changes are guaranteed to have been written to disk\nat commit time).\nHowever, these policies have important drawbacks. The no-steal approach as-\nsumes that all pages modified by ongoing transactions can be accommodated\nin the buffer pool, and in the presence of large transactions (typically run in\nbatch mode, e.g., payroll processing), this assumption is unrealistic. The force\napproach results in excessive page I/O costs. If a highly used page is updated\nin succession by 20 transactions, it would be written to disk 20 times. With a\nno-force approach, on the other hand, the in-memory copy of the page would\nbe successively modified and written to disk just once, reflecting the effects\nof all 20 updates, when the page is eventually replaced in the buffer pool (in\naccordance with the buffer manager's page replacement policy).\nFor these reasons, most systems use a steal, no-force approach.\nThus, if a\nframe is dirty and chosen for replacement, the page it contains is written to\ndisk even if the modifying transaction is still active (steal); in addition, pages in\nthe buffer pool that are modified by a transaction are not forced to disk when\nthe transaction commits (no-force).\n16.7.2\nRecovery-Related Steps during Normal Execution\nThe recovery manager of a DBMS maintains some information during normal\nexecution of transactions to enable it to perform its task in the event of a\nfailure.\nIn particular, a log of all modifications to the database is saved on\nstable storage, which is guaranteed6 to survive crashes and media failures.\nStable storage is implemented by maintaining multiple copies of information\n(perhaps in different locations) on nonvolatile storage devices such as disks or\ntapes.\nAs discussed earlier in Section 16.7, it is important to ensure that the log\nentries describing a change to the database are written to stable storage before\nthe change is made; otherwise, the system might crash just after the change,\nleaving us without a record of the change. (Recall that this is the Write-Ahead\nLog, or WAL, property.)\nThe log enables the recovery manager to undo the actions of aborted and\nincomplete transactions and redo the actions of committed transactions. For\nexample, a transaction that committed before the crash may have made updates\n(jNothing in life is really guaranteed except death and taxes. However, we can reduce the chance\nof log failure to be vanishingly small by taking steps such as duplexing the log and storing the copies\nin different secure locations.\n\nOverview of Transaction A1anagement\n5il3\nTuning the Recovery Subsystem: DBMS performance can be greatly\naffected by the overhead imposed by the recoverysubsystem. A DBA can\ntake several steps to tune this subsystem1 such at> correctlysizing the log\nand how it is managed on diskl controlling the rate at which buffer pages\nare forced to disk, choosing a good frequency for checkpointing, and so\nforth.\nto a copy (of a database object) in the buffer pool, and this change may not have\nbeen written to disk before the crash, because of a no-force approach.\nSuch\nchanges must be identified using the log and written to disk. Further, changes\nof transactions that did not commit prior to the crash might have been written\nto disk because of a steal approach. Such changes must be identified using the\nlog and then undone.\nThe amount of work involved during recovery is proportional to the changes\nmade by committed transactions that have not been written to disk at the time\nof the crash. To reduce the time to recover from a crash, the DBMS period-\nically forces buffer pages to disk during normal execution using a background\nprocess (while making sure that any log entries that describe changes these\npages are written to disk first, i.e., following the WAL protocol).\nA process\ncalled checkpointing, which saves information about active transactions and\ndirty buffer pool pages, also helps reduce the time taken to recover from a\ncrash. Checkpoints are discussed in Section 18.5.\n16.7.3\nOverview of ARIES\nARIES is a recovery algorithm that is designed to work with a steal, no-force\napproach. When the recovery manager is invoked after a crash, restart proceeds\nin three pha.'Ses. In the Analysis phase, it identifies dirty pages in the buffer\npool (i.e., changes that have not been written to disk) and active transactions\nat the time of the cra.'Sh. In the Redo pha.'Se, it repeats all actions, starting\nfrom an appropriate point in the log, and restores the databa.'Se state to what it\nwa.'S at the time of the crash. Finally, in the Undo phase, it undoes the actions\nof transactions that did not commit, so that the database reflects only the\nactions of committed transactions. The ARIES algorithm is discussed further\nin Chapter 18.,\n16.7.4\nAtomicity: Implementing Rollback\nIt is important to recognize that the recovery subsystem is also responsible for\nexecuting the ROLLBACK command, which aborts a single transaction. Indeed,\n\n544\nCHAPTER 1,6\nthe logic (and code) involved in undoing a single transaction is identical to that\nused during the Undo phase in recovering from a system crash. All log records\nfor a given transctction are organized in a linked list and can be efficiently\naccessed in reverse order to facilitate transaction rollback.\n16.8\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\nWhat are the ACID properties? Define atomicity, consistency, isolation,\nand durability and illustrate them through examples, (Section 16.1)\nIII\nDefine the terms transaction, schedule, complete schedule, and seTial sched-\nule. (Section 16.2)\nIII\nWhy does a DBMS interleave concurrent transactions? (Section 16.3)\nIII\nWhen do two actions on the same data object conflict? Define the anoma-\nlies that can be caused by conflicting actions (dirty reads, unrepeatable\nreads, lost updates). (Section 16.3)\nIII\nWhat is a serializable schedule?\nWhat is a Tecoverable schedule?\nWhat\nis a schedule that avoids cascading abor'ts?\nWhat is a strict schedule?\n(Section 16.3)\nIII\nWhat is a locking pmtocol:? Describe the Strict Two-Phase Locking (StTict\n2PL) protocol.\nWhat can you say about the schedules allowed by this\nprotocol? (Section 16.4)\nIII\nWhat overheads are associated with lock-based concurrency control? Dis-\ncuss blocking and abo7,ting overheads specifically and explain which is more\nimportant in practiee. (Section 16.5)\nIII\nWhat is thrashing? What should a DBA do if the system thrashes? (Sec-\ntion 16.5)\nIII\nHow can throughput be increased? (Section 16.5)\nIII\nHow are transactions created and terminated in SQL? What are save-\npoints?\nWhat are chained transactions?\nExplain why savepoints and\nchained tninsactions are useful. (Section 16.6)\nIII\nWhat are the considerations in determining the locking granularity when\nexecuting SQL statements? \\Nhat is the phantom problem? \\\\That irnpact\ndoes it have on performance? (Section 16.6.2)\n\nOverview of Transaction !vIanagement\n•\nvVhat transaction characteristics can a programmer control in SQL? Dis-\ncuss the different access modes and isolat'ion levels in particular.\nvVhat\nissues should be considered in selecting an access mode and an isolation\nlevel for a transaction? (Section 16.6.3)\n•\nDescribe how different isolation levels are implemented in terms of the locks\nthat are set. 'What can you say about the corresponding locking overheads?\n(Section 16.6.3)\n•\nvVhat functionality does the recovery manager of a DBMS provide? What\ndoes the transaction manager do? (Section 16.7)\n•\nDescribe the steal and force policies in the context of a buffer manager.\nWhat policies are used in practice and how does this affect recovery? (Sec-\ntion 16.7.1)\n•\nWhat recovery-related steps are taken during normal execution?\nWhat\ncan a DBA control to reduce the time to recover from a crash?\n(Sec-\ntion 16.7.2)\n•\nHow is the log used in transaction rollback and crash recovery?\n(Sec-\ntions 16.7.2, 16.7.3, and 16.7.4)\nEXERCISES\nExercise 16.1 Give brief answers to the following questions:\n1. What is a transaction?\nIn what ways is it different from an ordinary program (in a\nlanguage such as C)?\n2. Define these terms:\natomicity, consistency, isolation, durability, schedule, blind write,\ndirty read, unrepeatable read, serializable schedule, recoverable schedule, avoidsvcascading-\naborts schedule.\n3. Describe Strict 2PL.\n4. What is the phantom problem? Can it occur in a database where the set of database\nobjects is fixed and only the values of objects can be changed?\nExercise 16.2 Consider the following actions taken by transaction 1'1 on database objects\nX and Y:\nR(X), W(X),R(Y), W(Y)\n1. Give an example of another transaction 1'2 that, if run concurrently to transaction l'\nwithout some form of concurrency control, could interfere with 1'1.\n2. Explain how the use of Strict 2PL would prevent interference between the two transac-\ntions.\n:1. Strict 2PL is lIsed in many database systems. Give two reasons for its popularity.\n\n546\nCHAPTER l6\nExercise 16.3 Consider a database with objects X and Y and assume that there are two\ntransactions Tl and T2. Transaction T1 reads objects X and Y and then writes object X.\nTransaction T2 reads objects X and Y and then writes objects X and Y.\n1. Give an example schedule with actions of transactions T1 and T2 on objects X and Y\nthat results in a write-read conflict.\n2. Give an example schedule with actions of transactions T1 and T2 on objects X and Y\nthat results in a read-write conflict.\n3. Give an example schedule with actions of transactions T1 and T2 on objects X and Y\nthat results in a write-write conflict.\n4. For each of the three schedules, show that Strict 2PL disallows the schedule.\nExercise 16.4 We call a transaction that only reads database object a read-only transac-\ntion, otherwise the transaction is called a read-write transaction. Give brief answers to the\nfollowing questions:\n1. What is lock thrashing and when does it occur?\n2. What happens to the database system throughput if the number of read-write transac-\ntions is increased?\n3. What happens to the datbase system throughput if the number of read-only transactions\nis increased?\n4. Describe three ways of tuning your system to increase transaction throughput.\nExercise 16.5 Suppose that a DBMS recognizes increment, which increments an integer-\nvalued object by 1, and decrement as actions, in addition to reads and writes. A transaction\nthat increments an object need not know the value of the object; increment and decrement\nare versions of blind writes. In addition to shared and exclusive locks, two special locks are\nsupported: An object must be locked in I mode before incrementing it and locked in D mode\nbefore decrementing it. An I lock is compatible with another I or D lock on the same object,\nbut not with 5 and X locks.\n1. Illustrate how the use of I and D locks can increase concurrency.\n(Show a schedule\nallowed by Strict 2PL that only uses 5 and X locks. Explain how the use of I and D\nlocks can allow more actions to be interleaved, while continuing to follow Strict 2PL.)\n2. Informally explain how Strict 2PL guarantees serializability even in the presence of I\nand D locks.\n(Identify which pairs of actions conflict, in the sense that their relative\norder can affect the result, and show that the use of 5, X, I, and D locks according\nto Strict 2PL orders all conflicting pairs of actions to be the same as the order in some\nserial schedule.)\nExercise 16.6 Answer the following questions: SQL supports four isolation-levels and t.wo\naccess-modes, for a total of eight combinations of isolation-level and access-mode.\nEach\ncombination impiicitly defines a class of transactions; the following questions refer to these\neight classes:\n1. Consider the four SQL isolation levels. Describe which of the plHmomena can occur at\neach of these isolation levels: dirty read, unrepeatable read, phantom problem.\n2. For each of the four isolation levels, give examples of transactions that could be run\nsafely at that level.\n:.3. Why does the access mode of a transaction matter?\n\nOverview of Transaction Manage'm,ent\nExercise 16.7 Consider the university enrollment database schema:\n547\nStudent(snurn: integer, snarne: string, majoT: string, level: string, age: integer)\nClass(name: string, meets_at: time, Toom: string, fid\"' integer)\nEnrolled(snum: integer, cname: string)\nFaculty(fid: integer, fname: string, deptid: integer)\nThe meaning of these relations is straightforward; for example, Enrolled has one record per\nstudent-class pair such that the student is enrolled in the class.\nFor each of the following transactions, state the SQL isolation level you would use and explain\nwhy you chose it.\n1. Enroll a student identified by her snum into the class named 'Introduction to Database\nSystems'.\n2. Change enrollment for a student identified by her snum from one class to another class,\n3. Assign a new faculty member identified by his fid to the class with the least number of\nstudents.\n4. For each class, show the number of students enrolled in the class.\nExercise 16.8 Consider the following schema:\nSuppliers(sid: integer, sname: string, addTess: string)\nParts(pid: integer, pname: string, coloT: string)\nCatalog(sid: integer, pid: integer, cost: real)\nThe Catalog relation lists the prices charged for parts by Suppliers.\nFor each of the following transactions, state the SQL isolation level that you would use and\nexplain why you chose it.\n1. A transaction that adds a new part to a supplier's catalog.\n2. A transaction that increases the price that a supplier charges for a part.\n3. A transaction that determines the total number of items for a given supplier.\n4. A transaction that shows, for each part, the supplier that supplies the part at the lowest\nprice.\nExercise 16.9 Consider a database with the following schema:\nSuppliers(sid: integer, sname: string, addTess: string)\nParts(pid: integer, pname: string, coloT: string)\nCatalog(sid: integer, pid: integer, cost: real)\nThe Catalog relation lists the prices charged for parts by Suppliers.\nConsider three transactions 1'1,1'2, and 1'3; 1'1 always h8.o.'3 SQL isolation level SERIALIZABLE.\nWe first run 1'1 concurrently with 1'2 and then we run 1'1 concurrently with 1'2 but we change\nthe isolation level of 1'2 as specified below. Give a database instance and SQL statements for\n1'1 and 1'2 such that result of running 1'2 with the first SQL isolation level is different from\nrunning 1'2 with the second SQL isolation level. Also specify the common schedule of 1'1 and\n1'2 and explain why the results are different.\n\n548\n1. SERIALIZABLE versus REPEATABLE READ.\n2. REPEATABLE READ versus READ COHMITTED.\n3. READ COMHITTED versus READ UNCOMHITTED.\nBIBLIOGRAPHIC NOTES\nCHAPTER 16\nThe transaction concept and some of its limitations are discussed in [332J. A formal transac-\ntion model that generalizes several earlier transaction models is proposed in [182].\nTwo-phase locking was introduced in [252], a fundamental paper that also discusses the con-\ncepts of transactions, phantoms, and predicate locks.\nFormal treatments of serializability\nappear in [92, 581].\nExcellent in-depth presentations of transaction processing can be found in [90] and [770]. [338]\nis a classic, encyclopedic treatment of the subject.\n\n17\nCONCURRENCY CONTROL\n..\nHow does Strict 2PL ensure serializability and recoverability?\n..\nHow are locks implemented in a DBMS?\n..\nWhat are lock conversions and why are they important?\n..\nHow does a DBMSresolve deadlocks?\n..\nHow do current systerns deal with the phantom problerrl?\n..\nWhy are specialized locking techniques used on tree indexes?\n..\nHow does multiple-granularity locking work?\n..\nWhat is Optimistic concurrency control?\n..\nWhat is Timestarrlp-Ba..')ed concurrency control?\n..\nWhat is Multiversion concurrency control?\n..\nKey concepts: Two-phase locking (2PL), serializability, recoverabil-\nity, precedence graph, strict schedule, view equivalence, view seri-\nalizable, lock nlanager, lock table, transaction table, latch, convoy,\nlock upgrade, deadlock, waits-for graph, conservative 2PL, index lock-\ning, predicate locking, multiple-granularity locking, lock escalation,\nSQL isolation level, phantom problerrl, optirnistic concurrency con-\ntrol, Thornas Write Rule, recoverability\nPooh was sitting in his house one day, counting his pots of honey,\nwhen there carne a knock on the door.\n\"!'''ourteen,'' said Pooh. \"Corne in. Fourteen. Or 'wa.c:; it fifteen? Bother.\nT'hat's rnuddled rnc.\"\n549\n\n550\nCHAPTER 17\n'~Hallo, Pooh/' said Rabbit.\n\"Halla, R,abbit.\nFourteen, wasn't it?\"\n\"What was?\" \"lVly pots of honey what I was counting.\"\n\"Fourteen, that's right.\"\n\"Are you sure?\"\n\"No,\" said Rabbit. \"Does it matter?\"\n\"--\",,---A.A. Milne, The House at Pooh Comer\nIn this chapter, we look at concurrency control in more detail. We begin by\nlooking at locking protocols and how they guarantee various irnportant proper-\nties of schedules in Section 17.1. Section 17.2 is an introduction to how locking\nprotocols are implemented in a DBMS. Section 17.3 discusses the issue of lock\nconversions, and Section 17.4 covers deadlock handling. Section 17.5 discusses\nthree specialized locking protocols---for locking sets of objects identified by some\npredicate, for locking nodes in tree-structured indexes, and for locking collec-\ntions of related objects. Section 17.6 examines some alternatives to the locking\napproach.\n17.1\n2PL, SERIALIZABILITY, AND RECOVERABILITY\nIn this section, we consider how locking protocols guarantee some important\nproperties of schedules; namely, serializability and recoverability. Two sched-\nules are said to be conflict equivalent if they involve the (sarne set of) actions\nof the same transactions and they order every pair of conflicting actions of two\ncommitted transactions in the sanle way.\nAs we saw in Section 16.3.3, two actions conflict if they operate on the same\ndata object and at least one of them is a write.\nThe outcome of a schedule\ndepends only on the order of conflicting operations; we can interchange any\npair of nonconflicting operations without altering the effect of the schedule on\nthe database.\nIf two schedules are conflict equivalent, it is easy to see that\nthey have the same effect on a database. Indeed, because they order all pairs\nof conflicting operations in the same way, we can obtain one of thern frorn\nthe other by repeatedly swapping pairs of nonconflicting actions, that is, by\nswapping pairs of actions whose relative order does not alter the outcome.\nA schedule is conflict serializable if it is conflict equivalent to some serial\nschedule. Every conflict serializable schedule is serializable, if we assurne that\nthe set of items in the databa\"se does not grow or shrink; that is, values can\nbe nlodified but items are not added or deleted. We lllake this assurnption for\nnow and consider its consequences in Section 17.5.1. However, sonle serializ-\nable schedules are not conflict serializable, as illustrated in Figure 17.1. This\nschedule is equivalent to executing the transactions serially in the order TI,\n~r2,\n\nCOnC'lLrrency Control\nT1\nT2\nT3\nR(A)\nW(A)\nCOllirnit\nVV(A)\nCOllirnit\nW(A)\nCommit\nFigure 1.7.1\nSerializable Schedule That Is Not Conflict Serializable\nT3, but it is not conflict equivalent to this serial schedule because the writes of\nT1 and T2 are ordered differently.\nIt is useful to capture all potential conflicts between the transactions in a sched-\nule in a precedence graph, also called a serializability graph. The prece-\ndence graph for a schedule S contains:\n•\nA node for each comnlitted transaction in S.\n•\nAn arc franl Ti to Tj if an action of Ti precedes and conflicts with one of\nTj's actions.\nThe precedence graphs for the schedules shown in Figures 16.7, 16.8, and 17.1\nare shown in Figure 17.2 (parts a, b, and c, respectively).\n(a)\n(b)\nFigure 17.2\nExamples of Precedence Graphs\n'I'he Strict 2PL protocol (introduced in Section 16.4) allows only conflict seri-\nalizable schedules, as is seen frcHu the following two results:\n\n.... -2\n~)b .\nCHAPTER Iv\n1. A schedule S' is conflict serializable if and only if its precedence graph is\nacyclic. (An equivalent serial schedule in this C'k'3e is given by any topolog-\nical sort over fhe precedence graph.)\n2. Strict 2PL ensures t.hat the precedence graph for any schedule that it a11o\\vs\nis acyclic.\nA widely studied variant of Strict 2PL, called Two-Phase Locking (2PL),\nrelaxes the second rule of Strict 2PL to allow transactions to release locks before\nthe end, that is, before the comnlit or abort action. For 2PL, the second rule\nis replaced by the following rule:\n(2PL) (2) A transaction cannot request additional locks once it re-\nleases any lock.\nThus, every transaction h3.-\" a 'growing' phase in which it acquires locks, fol-\nlowed by a 'shrinking' phase in which it releases locks.\nIt can be shown that even nonstrict 2PL ensures acyclicity of the precedence\ngraph and therefore allows only conflict serializable schedules. Intuitively, an\nequivalent serial order of transactions is given by the order in which transactions\nenter their shrinking phase: If T2 reads or writes an object written by Tl, Tl\nIllUSt have released its lock on the object before 7\n12 requested a lock on this\nobject.\n~rhus, Tl precedes T2. (A sirnilar argulnent shows that Tl precedes\nT2 if 7'2 writes an object previously read by Tl. A forIllal proof of the claim\nwould have to show that there is no cycle of transactions that 'precede' each\nother by this argurnent.)\nA schedule is said to be strict if a value written by a transaction T is not\nread or overwritten by other transactions until T either aborts or eOlnrnits.\nStrict schedules are recoverable, do not require cascading aborts, and actions of\naborted transactions can be undone by restoring the original values of lnodified\nobjects.\n(See the last exaInple in Section\n16.::~.4.)\nStrict 2PL irnproves on\n2PL by guaranteeing that every allowed schedule is strict in addition to being\nconflict serializable. The reason is that when a transaction 'T writes an object\nunder Strict 2PL, it holds the (exclusive) lock until it conunits or aborts. Thus,\nno other transaction can see or rnodify this object until T is cornplete.\n]'he reader is invited to revisit the exarnples in Section\n16.:~.3 to see how the\ncorresponding schedules are disallowed by Strict 2PL and 2PL. Sirnilarly, it\nwould be instructive to \\vork out how the schedules for the exarnples in Section\n16.~3.4 are disallc)\\ved by Strict 2PL but not by 2PL.\n\n17.1.1\nView Serializability\nConflict serializability is sufficient but not necessary for serializability. A 1n01'e\ngeneral sufficient condition is vievv serializability. Two schedules 81 and 82 over\nthe saIne set of transactions\"------any transaction that appears in either 81 or 82\nrnust also appear in the other-----------are view equivalent under these conditions:\n1. If fTi reads the initial value of object A in 81, it Blust also read the initial\nvalue of A in 82.\n2. If Ti reads a value of A written by Tj in 81, it IIlUst also read the value of\nA written by Tj in 82.\n3. For each data object A, the transaction (if any) that perforlns the final\nwrite on A in 81 must also perform the final write on A in 82.\nA schedule is view serializable if it is view equivalent to SaIne serial schedule.\nEvery conflict serializable schedule is view serializable, although the converse\nis not true.\n~or example, the schedule shown in Figure 17.1 is view serializable,\nalthough it is not conflict serializable.\nIncidentally, note that this exalnple\ncontains blind writes. This is not a coincidence; it can be shown that any view\nserializable schedule that is not conflict serializable contains a blind write.\nAs we saw in Section 17.1, efficient locking protocols allow us to ensure that\nonly conflict serializable schedules are allowed. Enforcing or testing vie\\v seri-\nalizability turns out to be lIluch lnore expensive, and the concept therefore has\nlittle practical use, although it increases our understanding of serializability.\n17.2\nINTRODUCTION TO LOCK MANAGEMENT\nThe part of the I)Bl\\1S that keeps track of the locks issued to transactions is\ncalled the lock manager. The lock lnanager rnaintains a lock table, which\nis a ha\"sh table with the data object identifier (4S the key.\nThe DBNIS also\nInaintains a descriptive entry for each transaction in a transaction table,\nand alIlong other things, the entry contains a pointer to a list of locks held b:y\nthe transaction. This list is checked before requesting a lock, to ensure that a\ntransaction does not request the saIne lock twice.\nA lock table entry for an object-u--which can be a page, a record, and so\non, depending on the DBMS---contains the following inforrnation: the nurnber\nof transactions currently holding a. lock on the object (this can be rnore than\none if the object is locked in shared rnode), the nature of the lock (shared or\nexclusive), and a pointer to\n(1, queue of lock requests.\n\n554\nCHAPTER 17\n17.2.1\nImplementing Lock and Unlock Requests\nAccording to the Strict 2PL protocol, before a transaction T reads or writes a\ndatabase object 0, it must obtain a shared or exclusive lock on 0 and Inust\nhold on to the lock until it commits or aborts. When a transaction needs a\nlock on an object, it issues a lock request to the lock manager:\n1. If a shared lock is requested, the queue of requests is ernpty, and the object\nis not currently locked in exclusive mode, the lock manager grants the lock\nand updates the lock table entry for the object (indicating that the object\nis locked in shared mode, and incrernenting the number of transactions\nholding a lock by one).\n2. If an exclusive lock is requested and no transaction currently holds a lock\non the object (which also implies the queue of requests is empty), the lock\nrnanager grants the lock and updates the lock table entry.\n3. Otherwise, the requested lock cannot be immediately granted, and the\nlock request is added to the queue of lock requests for this object.\nThe\ntransaction requesting the lock is suspended.\nWhen a transaction aborts or comrnits, it releases all its locks. When a lock\non an object is released, the lock manager updates the lock table entry for the\nobject and exarnines the lock request at the head of the queue for this object.\nIf this request can now be granted, the transaction that made the request is\nwoken up and given the lock. Indeed, if several requests for a shared lock on the\nobject are at the front of the queue, all of these requests can now be granted\ntogether.\nNote that if TI has a shared lock on 0 and 1'2 requests an exclusive lock,\nT2's request is queued. Now, if T3 requests a shared lock, its request enters\nthe queue behind that of T2, even though the requested lock is cornpatible\nwith the lock held by TI. This rule ensures that T2 does not starve, that is,\nwait indefinitely while a stream of other transactions acquire shared locks and\nthereby prevent T2 frorn getting the exclusive lock for which it is waiting.\nAtomicity of Locking and Unlocking\nThe irnplernentation of lock and l1nlock cornrnands rnust ensure that these are\natomic operations.\nTo ensure atornicity of these operations when several in-\nstances of the lock rnanager code can exccute concurrently, access to the lock\ntable has to be guarded by an operating systern synchronization rnechanisrn\nsuch a..s a sernaphore.\n\nCo'nc'UT'rency (}onlToI\n5~5\nTo understand why, suppose that a transaction requests an exclusive lock.\nThe lock manager checks and finds that no other transaction holds a lock on\nthe object and therefore decides to grant the request. But, in the 11leantirne,\nanother transaction rnight have requested and received a conflicting lock. To\nprevent this, the entire sequence of actions .in a lock request call (checking\nto see if the request can be granted, updating the lock table, etc.)\nmust be\nirnplernented as an atornic operation.\nOther Issues: Latches, Convoys\nIn addition to locks, which are held over a long duration, a DBMS also supports\nshort-duration latches. Setting a latch before reading or writing a page ensures\nthat the physical read or write operation is atomic; otherwise, two read/write\noperations rnight conflict if the objects being locked do not correspond to disk\npages (the units of I/O). Latches are unset immediately after the physical read\nor write operation is cOlnpleted.\nWe concentrated thus far on how the DBMS schedules transactions based on\ntheir requests for locks. This interleaving interacts with the operating system's\nscheduling of processes' access to the CPU and can lead to a situation called\na convoy, where most of the CPU cycles are spent on process switching. The\nproblem is that a transaction T holding a heavily used lock may be suspended\nby the operating system.\nUntH T is resurned, every other transaction that\nneeds this lock is queued. Such queues, called convoys, can quickly become\nvery long; a convoy, once forrned, tends to be stable. Convoys are one of the\ndrawbacks of building a DB~lS on top of a general-purpose operating system\nwith preeruptive scheduling.\n17.3\nLOCK CONVERSIONS\nA transaction rnay need to acquire an exclusive lock on an object for which it\nalready holds a shared lock. For exarnple, a SQL update statenlent could result\nin shared locks being set on each row in a table. If a row satisfies the condition\n(in the WHERE clause) for being updated, an exclusive lock must be obtained\nfor that row.\nSuch a lock upgrade request lnust be handled specially by granting the exclu-\nsive lock illunediately if no other transaction holds a shared lock on the object\nand inserting the request at the front of the queue other\\vise. The rationale\nfor favoring the transaction thus is that it already 1101ds a shared lock on the\nobject and queuing it behind. another tretnsaction that wants an exclusive lock\non thf~ SeHne object causes both a deadlock. UnfortunatelY,while favoring lock\nupgrades helps, it does not prevent deadlocks caused by two conflicting upgrade\n\n556\nCHAPTER 17\nrequests. For exalnplc, if two transactions that hold a shared lock on an object\nboth request an upgrade to an exclusive lock, this leads to a deadlock.\nA better approach is to avoid the need for lock upgrades altogether by obtaining\nexclusive locks initially, and downgrading to a shared lock once it is clear that\nthis is sufficient. In our exalnple of an SQL update statelnent, rows in a table\nare locked in exclusive rnode first. If a row does not satisfy the condition for\nbeing updated, the lock on the row is dnwngraded to a shared lock. Does the\ndovvngrade approach violate the 2PL requirernent?\nOn the surface, it does,\nbecause downgrading reduces the locking privileges held by a transaction, and\nthe transaction Illay go on to acquire other locks. However, this is a special case,\nbecause the transaction did nothing but read the object that it downgraded,\neven though it conservatively obtained an exclusive lock. We can safely expand\nour definition of 2PL from Section 17.1 to allow lock downgrades in the growing\nphase, provided that the transaction has not lnodified the object.\nThe downgrade approach reduces concurrency by obtaining write locks in some\ncases where they are not required. On the whole, however, it irnproves through-\nput by reducing deadlocks. This approach is therefore widely used in current\ncommercial systems. Concurrency can be increased by introducing a new kind\nof lock, called an update lock, that is cornpatible with shared locks but not\nother update and exclusive locks.\nBy setting an update lock initially, rather\nthan exclusive locks, we prevent conflicts with other read operations. Once we\nare sure we need not update the object, we can downgrade to a shared lock. If\nwe need to update the object, we rnust first upgrade to an exclusive lock. This\nupgrade does not lead to a deadlock because no other transaction can have an\nupgrade or exclusive lock on the object.\n17.4\nDEALING WITH DEADLOCKS\nDeadlocks tend to be rare and typically involve very few transactions. In prac-\ntice, therefore, databa.'3c systerns periodically check for deadlocks.\n'Vhen a\ntransaction Ti is suspended because a lock that it requests cannot be granted,\nit rnust wa,it until all transactions Tj that currently hold conflicting locks re-\nlea,'3e thcrn. The lock rnanager rnaintains a structure called a waits-for graph\nto detect deadlock cycles.\nThe nodes\ncorr(~spond to active transactions, and\nthere is an arc frolnTi to 'Tj if (and only if)Ti is \\vaiting for 1) to release a\nlode The lock rnanagcr adds edges to this graph when it queues lock requests\nand rernoves edges \\vhcn it gra,nts lock requests.\nConsider the schedule shown in F'igure 17.:3, The last step, sho\\vn belovv the\nline, creates a cycle in the \\vaits-for graph.\nFigure 17.4 shovvs the ·waits-for\ngraph before and after this step.\n\nCOnC1tTTency C'ontTol\nTl\nT2\nT3\nT4\nS(A)\nJl(A)\nX(B)\nvV(B)\n8(B)\n8(C)\nR(C)\nX(C)\nX(B)\nX(A)\nFigure 17.3\nSchedule Illustrating Deadlock\n597\n(a)\nFigure 17.4\n\\\\!aits-for Graph Before and After Deadlock\n\n558\nCHAPTER lw7\nObserve that the \\vaits-for graph describes all active transactions, some of which\neventually abort. If there is an edge froIn Ti to T'j in the 'N~aits-for graph, and\nboth Ti and Tj eventually commit, there is an edge in the opposite direc-\ntion (froIn l'j to Ti) in the precedence graph (which involves only cOlluuitted\ntransactions) .\nThe waits-for graph is periodically checked for cycles, which indicate deadlock.\nA deadlock is resolved by aborting a transaction that is on a cycle and releasing\nits locks; this action allows SOlne of the waiting transactions to proceed. The\nchoice of which transaction to abort can be made using several criteria: the\none with the fewest locks, the one that has done the least work, the one that is\nfarthest from completion, and so all. FUrther, a transaction might have been\nrepeatedly restarted; if so, it should eventually be favored during deadlock\ndetection and allowed to complete.\nA silnple alternative to maintaining a waits-for graph is to identify deadlocks\nthrough a timeout mechanism: If a transaction has been waiting too long for\na lock, we assume (pessiruistically) that it is in a deadlock cycle and abort it.\n17.4.1\nDeadlock Prevention\nElnpirical results indicate that deadlocks are relatively infrequent, and detection-\nbased schemes work well in practice. However, if there is a high level of con-\ntention for locks and therefore an increased likelihood of deadlocks, prevention-\nbased schelnes could perform better. We can prevent deadlocks by giving each\ntransaction a priority and ensuring that lower-priority transactions are not\nallowed to wait for higher-priority transactions (or vice versa).\nOne way to\nassign priorities is to give each transaction a timestamp when it starts up.\nThe lower the timestamp, the higher is the transaction's priority; that is, the\noldest transaction has the highest priority.\nIf a transaction Ti requests a lock and transaction Tj holds a conflicting lock,\nthe lock lnanager can use one of the following two policies:\nII\nWait-die: If Ti has higher priority, it is allowed to wait; otherwise, it is\naborted.\nII\nWound-wait: If Ti has higher priority, abort 7); otherwise, l\"1i waits.\nIn the \\vait-die scherne, lower-priority transactions can never wait for higher-\npriority transactions. In the wound-wait scherne, higher-priority transactions\nnever wait for lower-priority transactions.\nIn either ease, no deadlock cyc.le\ndevelops.\n\nConcurrency Control\n539\nA subtle point is that we nlust also ensure that no transaction is perennially\naborted because it never has a sufficiently high priority.\n(Note that, in both\nschernes, the higher-priority transaction is never aborted.)\nWhen a transac-\ntion is aborted and restarted, it should be given the same timestamp it had\noriginally.\nReissuing timestarnps in this way ensures that each transaction\nwill eventually becorne the oldest transaction, and therefore the one with the\nhighest priority, and will get all the locks it requires.\nThe wait-die scheme is nonpreemptive; only a transaction requesting a lock can\nbe aborted. As a transaction grows older (and its priority increases), it tends\nto wait for more and rnore younger transactions. A younger transaction that\nconflicts with an older transaction may be repeatedly aborted (a disadvantage\nwith respect to wound-wait), but on the other hand, a transaction that has\nall the locks it needs is never aborted for deadlock reasons (an advantage with\nrespect to wound-wait, which is preemptive).\nA variant of 2PL, called Conservative 2PL, can also prevent deadlocks. Un-\nder Conservative 2PL, a transaction obtains all the locks it will ever need when\nit begins, or blocks waiting for these locks to become available. This scheme\nensures that there will be no deadlocks, and, perhaps lllore important, that a\ntransaction that already holds some locks will not block waiting for other locks.\nIf lock contention is heavy, Conservative 2PL can reduce the time that locks\nare held on average, because transactions that hold locks are never blocked.\nThe trade-off is that a transaction acquires locks earlier, and if lock contention\nis low, locks are held longer under Conservative 2PL. From a practical per-\nspective, it is hard to know exactly what locks are needed ahead of time, and\nthis approach leads to setting more locks than necessary.\nIt also has higher\noverhead for setting locks because a transaction has to release all locks and try\nto obtain thern all over if it fails to obtain even one lock that it needs. This\napproach is therefore not used in practice.\n17.5\nSPECIAI-JIZED LOCKING TECHNIQUES\nThus far we have treated a database as a fixed collection of independent data\nobjects in our presentation of locking protocols.\nWe now relax each of these\nrestrictions and discuss the consequences.\nIf the collection of databa.se objects is not fixed, but can grow and shrink\nthrough the insertion and deletion of objects, we must deal with a subtle cOlnpli-\ncation known a'3 the phantom, problem, which was illustrated in Section 16.6.2.\nWe discuss this problern in Section 17.5.1.\n\n560\nCHAPTER 17\nAlthough treating a database\nH\",S an independent collection of objects is ade-\nquate for a discussion of serializability and recoverability, luuch better perfc)l'-\nrnance can sOlnetilnes be obtained using protocols that recognize and exploit\nthe relationships between objects. vVe discuss two such C<l\"ses, nalnely, locking\nin tree-structured indexes (Section 17.5.2) and locking a collection of objects\nwith contairnnent relationships between the1n (Section 17.5.3).\n17.5.1\nDynamic Databases and the Phantom Problem\nConsider the following exarnple: rrransaction Tl scans the Sailors relation to\nfind the oldest sailor for each of the rating levels 1 and 2. First, Tl identifies\nand locks all pages (assurning that page-level locks are set) containing sailors\nvvith rating 1 and then finds the age of the oldest sailor, which is, say, 71.\nNext, transaction T2 inserts a new sailor with rating 1 and age 96.\nObserve\nthat this new Sailors record can be inserted onto a page that does not contain\nother sailors with rating 1; thus, an exclusive lock on this page does not conflict\nwith any of the locks held by Tl. T2 also locks the page containing the oldest\nsailor with rating 2 and deletes this sailor (whose age is, say, 80).\nT2 then\ncomrnits and releases its locks.\nFinally, transaction T1 identifies and locks\npages containing (all remaining) sailors with rating 2 and finds the age of the\noldest such sailor, which is, say, 63.\nThe result of the interleaved execution is that ages 71 and 63 are printed in\nresponse to the query. If T1 had run first, then T2, we would have gotten the\nages 71 and 80; if T2 had run first, then T1, we would have gotten the ages\n96 and 63. Thus, the result of the interleaved execution is not identical to any\nserial exection of Tl and 1'2, even though both transactions follow Strict 2PL\nand cOlnmit.\nThe problem is that T1 assurnes that the pages it has locked\ninclude all pages containing Sailors records with rating 1, and this assurnption\nis violated when rT2 inserts a new such sailor on a different page.\n'rhe Haw is not in the Strict 2PL protocol.\nR,ather, it is in T1 's irnplicit as-\nsurnption that it has locked the set of all Sailors n~cordswith rating value 1.\nT1 's sernantics requires it to identify all such records, but locking pages that\ncontain such records at a given tirne does not prevent new \"phantorn\" records\nfrorn being added on other pages. ]\"'1 has th~refore not locked the set of desired\nSailors records,\nStrict 2PL guarantees conflict serializability; indeed, there are no cycles in the\nprecedence graph for this exarnple because conflicts are defined with respect\nto objects (in this excunple, pages) read/written by the traJlsactions. However,\nbecause the set of objects that shrndd have been locked by Tl was altered by\nthe actions ofT2, the olltcorne of the schedule differed frolll the outcorne of any\n\nCfoncuTTency Control\n561\nserial execution.\n1~his exalllple brings out an irnportant point about conflict\nserializability: If new itenls are added to the databa'ie, conflict serializability\ndoes not guarantee serializability.\n.i\\ closer look at how a transaction identifies pages containing Sailors records\nvvith rating 1 suggests hovv the problenl can be handled:\n•\nIf there is no index and all pages in the file rnust be scanned, T1 IUUSt\nsomeho\\v\nensurf~ that no new pages are added to the file, in addition to\nlocking all existing pages.\n•\nIf there is an index on the rating field, T1 can obtain a lock on the index\npage~again, assurning that physical locking is done at the page level-that\ncontains a data entry with rating= 1. If there are no such data entries, that\nis, no records with this rating value, the page that would contain a data\nentry for rating=l is locked to prevent such a record from being inserted.\nAny transaction that tries to insert a record with rating=-l into the Sailors\nrelation 11lUSt insert a data entry pointing to the new record into this index\npage and is blocked until T1 releases its locks.\nThis technique is called\nindex locking.\nBoth techniques effectively give T1 a lock on the set of Sailors records with rat-\ning=l: Each existing record with rating=l is protected frolll changes by other\ntransactions, and additionally, new records with rating=l cannot be inserted.\nAn independent issue is how transaction 7'1 can efficiently identify and lock\nthe index page containing rating=1. We discuss this issue for the case of tree-\nstructured indexes in Section 17.5.2.\n\\Ve note that index locking is a special ca..se of a luore general concept called\npredicate locking.\nIn our exalnple, the lock on the index page irnplieitly\nlocked all Sailors records that satisfy the logical predicate rating= 1.\n1V101'e\ngenerally, we cn]} support irnplicit locking of all records that rnatch an arbitra,ry\npredicate. General predicate locking is expensive to irnplenlent and therefore\nnot cOllullonly used.\n17.5.2\nConcurrency Control in B+ Trees\nA straightforward approach to concurrency control for B+ trees and\nISA~iI\nindexes is to ignore the index structure, treat each page as a data object, and\nuse senne version of 2PL. This siInplistic locking strategy vvould lead to very high\nlock contention in the higher levels of the tree~ because every tree search begins\nat the root and proceeds along sorne path to a leaf node. Fortunately, Innch\nInore efficient locking protocols that exploit the hierarchical structure of a tree\n\n562\nCHAPTER J7\nindex are known to reduce the locking overhead while ensuring seriaIizability\nand recoverability.We discuss sorne of these approaches briefly, concentrating\non the search and insert operations.\nTwo observations provide the necessary insight:\n1. The higher levels of the tree only direct searches.\nAll the 'real' data is\nin the leaf levels (in the forrnat of one of the three alternatives for data\nentries).\n2. For inserts, a node must be locked (in exclusive rnode, of course) only if a\nsplit can propagate up to it frorn the modified leaf.\nSearches should obtain shared locks on nodes, starting at the root and pro-\nceeding along a path to the desired leaf. The first observation suggests that a\nlock on a node can be released as soon as a lock on a child node is obtained,\nbecause searches never go back up the tree.\nA conservative locking strategy for inserts would be to obtain exclusive locks on\nall nodes as we go down from the root to the leaf node to be modified, because\nsplits can propagate all the way from a leaf to the root. However, once we lock\nthe child of a node, the lock on the node is required only in the event that a\nsplit propagates back to it. In particular, if the child of this node (on the path\nto the modified leaf) is not full when it is locked, any split that propagates up\nto the child can be resolved at the child, and does not propagate further to the\ncurrent node. Therefore, when we lock a child node, we can release the lock on\nthe parent if the child is not full. The locks held thus by an insert force any\nother transaction following the sarne path to wait at the earliest point (i.e., the\nnode nearest the root) that rnight be affected by the insert. The technique of\nlocking a child node and (if possible) releasing the lock on the parent is called\nlock-coupling, or crabbing (think of how a crab walks, and cornpare it to\nhow we proceed down a tree, alternately releasing a lock on a parent and setting\na lock on a child).\nWe illustrate B-t- tree locking using the tree in Figure 17.5. To search for data\nentry 38*, a transaction T'i rnust obtain an S lock on node A, read the contents\nand deterrnine that it needs to examine node B, obtain an S lock on node B\nand release the lock on A, then obtain an S lock on node C\n1 and relea.'3e the\nlock on B, then obtain an S lock on nodeD and release the lock on ().\nTi always rnaintains a lock all one node in the path, to force new transactions\nthat want to read or nlodify nodes on the sarne path to wait until the current\ntransaction is done. If transaction 7) wants to delete 38*, for exarnple, it rnust\nalso traverse the path fro111 the root to node D and is forced to wait until 1\n1i\n\nCO'TLc1trrency ContTol\nA\nFigure 17.5\nB+ 'Thee Locking Example\nis done. Of course, if SOl11e transaction Tk holds a lock on, say, node C before\nTi reaches this node, Ti is similarly forced to wait for Tk to complete.\nTo insert data entry 45*, a transaction 111USt obtain an S lock on node A, obtain\nan 8 lock on node B and release the lock on A, then obtain an S lock on node\nC (observe that the lock on B is not released, because C is full), then obtain\nan X lock on node E and release the locks on C and then B. Because node E\nhas space for the new entry, the insert is accomplished by modifying this node.\nIn contrast, consider the insertion of data entry 25*.\nProceeding as for the\ninsert of 45*, we obtain an X lock on node H. Unfortunately, this node is full\nand must be split. Splitting H requires that we also rnodify the parent, node F,\nbut the transaction ha..., only an S lock on F. Thus, it must request an upgrade\nof this lock to an X lock. If no other tra,nsaction holds an S lock on F, the\nupgrade is granted, and since F has space, the split does not propagate further\nand the insertion of 25* can proceed (by splitting If and locking G to modify\nthe sibling pointer in I to point to the newly created node). However, if another\ntransaction holds an 8 lock on node F, the first transaction is suspended until\nthis transaction relea.ses its Slack.\nObserve that if another transaction holds an 8 lock on }' and also wants to\naccess node H, we have a deadlock because the first transaction has an X lock\non If. The preceding exarnple also illustrates an interesting point about sibling\npointers: When \\ve split leaf node If, the new node rn'Ust be added to the left\nof Ii, since otherwise the node whose sibling pointer is to be changed would\nbe node 1, which ha\"-,, a different parent. To rnodify a sibling pointer on I, we\n\n564\nC~HAPTER 1t{\n'would have to lock its parent, node C: (and possibly ancestors of C:, in order to\nlock en.\nExcept for the locks on int(~rrnediatenodes that we indicated could be released\nearly, senne variant of 2PL HUlst be used to govern when locks can be released,\nto ensure serializability and recoverability.\nT'his approach irllproves considerably on the naive use of 2PL, but several ex-\nclusive locks are still set unnecessarily and, although they are quickly released,\naffect perfon.nance substantially. One way to iInprove perforlllance is for inserts\nto obtain shared locks instead of exclusive locks, except for the leaf, which is\nlocked in exclusive 11lode. In the vast rnajority of cases, a split is not required\nand this approach works very well. If the leaf is full, however, we Blust upgrade\nfrom shared locks to exclusive locks for all nodes to which the split propagates.\nNote that such lock upgrade requests can also lead to deadlocks.\nThe tree locking idea'3 that we describe illustrate the potential for efficient\nlocking protocols in this very important special case, but they are not the\ncurrent state of the art. The interested reader should pursue the leads in the\nbibliography.\n17.5.3\nMultiple-Granularity Locking\nAnother specialized locking strategy, called multiple-granularity locking,\nallows us to efficiently set locks on objects that contain other objects.\nFor instance, a database contains several files, a file is a collection of pages,\nand a page is a collection of records. A transaction that expects to access rnost\nof the pages in a file should probably set a lock on the entire file, rather than\nlocking individual pages (or reeords) when it needs thern.\nDoing so reduces\nthe locking overhead considerably. On the other hand, other tra,nsactions that\nrequire access to parts of the file....·....·-even parts not needed by this transaction··-·..·-\nare blocked. If a transaction accesses relatively few pages of the file, it is better\nto lock only those pages. Sirnilarly, if a transaction accesses several records on\na page, it should lock the entire page, and if it accesses just a few records, it\nshould lock just those records.\nThe question to be addressed is h(nv a lock rnanager can efficiently ensure that\na page, for exaruple, is not locked by a transaction while another transaction\nholds a conflicting lock on the file containing the page (a.nd therefore, irnplicitly,\non the page).\n\nConC7lTTency COintTol\n5jj5\nThe idea is to exploit the hierarchical nature of the\n'contains~ relationship. A\ndat.abase contains a set of files, each file contains a set of pages, and each page\ncontains a set of records.\nThis contairunent hierarchy can be thought of as\na tree of objects, \\vhere each node contains all its children.\n(The approach\ncan easily be extended to cover hierarchies that are not trees, but we do not\ndiscuss this extension.) A lock on a node locks that node and, irnplicitly, all its\ndescendants. (Note that this interpretation of a lock is very different fron1 B+\ntree locking, where locking a node does not lock any descendants ilnplicitly.)\nIn addition to shared (8) and exclusive (XO) locks, rnultiple-granularity locking\nprotocols also use two new kinds of locks, called intention shared (18) and\nintention exclusive (IX) locks.\n18 locks conflict only with X locks.\nIX\nlocks conflict with 8 and X locks. To lock a node in S (respectively, X) luode,\na transaction must first lock all its ancestors in 18 (respectively, 1X) rllode.\nThus, if a transaction locks a node in 8 rIlode, no other transaction can have\nlocked any ancestor in X rnode; siInilarly, if a transaction locks a node in X\nmode, no other transaction can have locked any ancestor in 8 or X mode. This\nensures that no other transaction holds a lock on an ancestor that conflicts\nwith the requested 8 or X lock on the node.\nA common situation is that a transaction needs to read an entire file and modify\na few of the records in it; that is, it needs an 8 lock on the file and an 1X lock\nso that it can subsequently lock sorne of the contained objects in X mode. It\nis useful to define a new kind of lock, called an 81X lock, that is logically\nequivalent to holding an 8 lock and an I X lock. A transaction can obtain a\nsingle 81X lock (which conflicts with any lock that conflicts with either S or\nI X) instead of an 8 lock and an I X lock.\nA subtle point is that locks rnust be relea..sed in leaf-to-root order for this proto-\ncol to work correctly. Tb see this, consider what happens when a transaction Ti\nlocks all nodes on a path frolH the root (corresponding to the entire database)\nto the node corresponding to sorne page p in 18 rnode, locks p in 8 rHode, and\nthen relea..ses the lock on the root node.\nAnother transaction T j could now\nobtain an X lock on the root. This lock iInplicitly gives Tj an .£Y lock on page\np, which conflicts with the 8 lock currently held by Ti.\nlIIultiple-granularity locking lllust be used with 2PL to ensure serializability.\nThe 2PL protocol dictates when locks can be rele(ksed. At that tirne, locks ob-\ntained using rIlultiple-granularity locking can be released and IIlUSt be relcclsed\nin leaf-to-root order.\nFinally, there is the question of hO\\\\I to decide what granularity of locking is\nappropriate for a given transaction. One approach is to begin by obtaining fine\ngranularity locks (e.g., at the record level) and, after the transaction requests\n\n566\nCHAPTER 107\n-~\",,_._ ..-_\n_._.._.._.._ _--_.--_._._..--\n_\n_-~.\"\n__._-_\n_........\n..\n_-_._-~.,,\"\nLock Granularity: SOfie database systeIlls allow programmers to over-\nride the default mechanisllt for choosing a lock granularity. For exalnple,\nMicrosoft SQL Server allows users to select page locking instead of table\nlocking, using the keyword PAGLOCK. IBrvf'sDB2 UDB allows for explicit\ntable-level locking.\na certain nUlnber of locks at that granularity, to start obtaining locks at the\nnext higher granularity (e.g., at the page level). This procedure is called lock\nescalation.\n17.6\nCONCURRENCY CONTROL WITHOUT LOCKING\nLocking is the most widely used approach to concurrency control in a DBMS,\nbut it is not the only one. We now consider some alternative approaches.\n17.6.1\nOptimistic Concurrency Control\nLocking protocols take a pessimistic approach to conflicts between transactions\nand use either transaction abort or blocking to resolve conflicts. In a systenl\nwith relatively light contention for data objects, the overhead of obtaining locks\nand following a locking protocol must nonetheless be paid.\nIn optimistic concurrency control, the basic premise is that most transactions\ndo not conflict with other transactions, and the idea is to be as permissive\nas possible in allowing transactions to execute. Transactions proceed in three\nphases:\n1. Read: The transaction executes, reading values froIn the database and\nwriting to a private workspace.\n2. Validation: If the transaction decides that it wants to c0l111uit, the DBIvIS\nchecks whether the transaction could possibly have conflicted with any\nother concurrently executing transaction. If there is a possible conflict, the\ntransaction is aborted; its private workspace is cleared and it is restarted.\n:3. Write: If validation deterrnines that there are no possible confliets, the\nchanges to data objects 111ade by the transaction in its private workspace\nare copied into the databa.se.\nIf, indeed, there are few confiicts, and validation can be done efficiently, this\napproach should lead to better' performance than locking. If there are rnany\n\nConcurrency Control\n567\nconflicts, the cost of repeatedly restarting transactions (thereby wasting the\n,york they've done) hurts perfornlance significantly.\nEach transaction Ti is assigned a thnestamp TS(T'i) at the beginning of its\nvalidation pha.':ie, and the validation criterion checks whether the tiITlestalnp-\nordering of transactions is an equivalent serial order. For every pair of transac-\ntions Ti and Tj such that TS(l\"1i) < TS(Tj), one of the following validation\nconditions ITIUSt hold:\n1. Ti completes (all three phases) before Tj begins.\n2. Ti completes before Tj starts its Write phase, and Ti does not write any\ndatabase object read by Tj.\n3. Ti completes its Read phase before Tj completes its Read phase, and Ti\ndoes not write any database object that is either read or written by T j.\nTo validate T j, we must check to see that one of these conditions holds with\nrespect to each comlnitted transaction Ti such that TS(Ti) < TS(Tj). Each\nof these conditions ensures that Tj's modifications are not visible to Ti.\nFurther, the first condition allows Tj to see some of Ti's changes, but clearly,\nthey execute completely in serial order with respect to each other. The second\ncondition allows Tj to read objects while Ti is still modifying objects, but there\nis no conflict because Tj does not read any object rnodified by T'i. Although\nTj might overwrite some objects written by Ti, all of Ti's writes precede all of\nTj's writes. The third condition allows Ti and Tj to write objects at the same\ntime and thus have even IT10re overlap in time than the second condition, but\nthe sets of objects written by the two transactions cannot overlap. Thus, no\nRW, WR, or WW conflicts are possible if any of these three conditions is met.\nChecking these validation criteria requires us to maintain lists of objects read\nand written by each transaction. Further, while one transaction is being vali-\ndated, no other transaction can be allowed to commit; otherwise, the validation\nof the first transaction might miss conflicts with respect to the newly com-\nmitted transaction. The Write phase of a validated transaction rnust also be\ncompleted (so that its effects are visible outside its private workspace) before\nother transactions can be validated.\nA synchronization rnechanisrn such as a critical section can be used to ensure\nthat at most one transaction is in its (colllbined) Validation/Write phases at\nany tirne.\n(When a process is executing a critical section in its code, the\nsystern suspends all other processes.) Obviously, it is irnportant to keep these\npha~es ~lS short H.S possible in order to rniniruize the irnpact on concurrency. If\ncopies of rnodified objects have to be copied frorn the private workspace, this\n\n568\nCHAPTER 17\ncan rnake the \\Vrite phase long.\nAn alternative approach (which carries the\npenalty of poor physical locality of objects, such as B·+· tree leaf pages, that\nrnust be clustered) is to use a level of indirection. In this schernc, every object\nis accessed via a logical pointer, and in the \\Vrite phase, we sirnply switch the\nlogical pointer to point to the version of the object in the private workspace,\ninstead of copying the 0 bject.\nClearly, it is not the ca.sc that optiInistic concurrency control has no overheads;\nrather, the locking overheads of lock-based approaches are replaced with the\noverheads of recording read-lists and write-lists for transactions, checking for\nconflicts, and copying changes frorn the private workspace. Sirnilarly, the irn-\nplicit cost of blocking in a lock-based approach is replaced by the implicit cost\nof the work wasted by restarted transactions.\nImproved Conflict Resolution1\nOptirnistic Concurrency Control using the three validation conditions described\nearlier is often overly conservative and unnecessarily aborts and restarts trans-\nactions. In particular, according to the validation conditions, T'i cannot write\nany object read by Tj. IIowever, since the validation is airned at ensuring that\nTi logically executes before Tj, there is no harm if Ti writes all data items\nrequired by Tj before 7) reads theIn.\nThe problerIl arises because we have no way to tell when Ti wrote the object\n(relative to Tj's reading it) at the tirne we validate Tj, since all we have is the\nlist of objects written by T'i and the list read by T j. Such false conflicts can be\nalleviated by a finer-grain resolution of data conflicts, using rnechanisrI1s very\nsinlilar to locking.\nThe basic idea is that each transaction in the R,cacl pha.se tells the DBMS about\niteIIls it is reading, and ·when a transaction Ti is cornrnitted (and its writes are\naccepted), the DBMS checks whether any of the iterns written by Ti are being\nread by any (yet to be validated) transaction T j.\nIf so, we kno\\v thatTj 's\nvalidation rnust eventually fail. vVe can either allow T,i to discover this when\nit is validated (the die policy) or kill it and restart it innnediately (the kill\npolicy).\nrfhe details are c1.,) follo\\vs. Before reading a data iterrl,\n(1, transaction Tenters\nan access entry in a h::lSh table. The access entry contains the transact'ion\nid, a data object id, and a rn..odified flag (initially set to false), and entries are\nhashed on the data object id. A terl1porary exclusive lock is obtained on the\n1\"\\Ve thank Alexander Thoma..sian for writing this section.\n\n(}O'lLCU1\"rency Cfontrol\n569\n$\nhash bucket containing the entry, and the lock is held \\vhile the read data iteIIl\nis copied frolll the datab<:lSe bufIer into the private 'workspace of the transactioll.\nDuring validation of\n~T the hash buckets of all data objects accessed by T\nare again locked (in exclusive 11lode) to check if T has encountered any data\nconflicts.\n~r has encountered a conflict if the rnodified flag is set to true in one\nof its access entries.\n(This &')SUIIles that the 'die' policy is being used; if the\n'kill' policy is used, 'T is restarted when the flag is set to true.)\nIf T is successfully validated, we lock the hash bucket of each object lnodified\nby T, retrieve all access entries for this object, set the rnodified flag to true,\nand release the lock on the bucket. If the 'kill' policy is used, the transactions\nthat entered these access entries are restarted. We then\ncornplete~r's Write\nphase.\nIt seems that the 'kill' policy is always better than the 'die' policy, because it\nreduces the overall response time and wasted processing. However, executing\nT to the end has the advantage that all of the data items required for its\nexecution are prefetched into the database\nbuffe~r, and restarted executions of\nT will not require disk I/O for reads. This assumes that the database buffer\nis large enough that prefetched pages are not replaced, and, 1nore irnportant,\nthat access invariance prevails; that is, successive executions of T require\nthe same data for execution. When T is restarted its execution tirne is nluch\nshorter than before because no disk I/O is required, and thus its chances of\nvalidation are higher.\n(Of course, if a transaction has already completed its\nRead phase once, subsequent conflicts should be handled using the 'kill' policy\nbecause all its data objects are already in the buffer pool.)\n17.6.2\nTimestamp-Based Concurrency Control\nIn lock-ba'3ed concurrency control, conflicting actions of different transactions\nare ordered by the order in which locks are obtained, and the lock protocol ex-\ntends this ordering on actions to transactions, thereby ensuring serializability.\nIn optirrlistic concurrency control, a tiInestanlp ordering is irnposed on trans-\nactions and validation checks that all conflicting actions occurred in the saIne\norder.\nTinlcstarnps can also be used in another \\vay: Each transaction can be assigned\na tirnestanlp at startup, and we can ensure, at execution tirne, that if action\na'i of transaction T'i conflicts \\vith action aj of transaction Tj, a'i occurs before\naj if'1'8(T'i) < TS(Tj). If an action violates this ordering, the transaction is\naborted and restarted.\n\n570\nCHAPTER 17\nTo irnplernent this concurrency control schellH~l every database object 0 is given\na read tirnestampRTS (0) and a write timestamp v~lTS (0). If transaction\nT wants to read object 0, and TS(T) <\n~VTS(O), the order of this read\nwith respect to the most recent write on 0 would violate the timestamp order\nbetween this transaction and the writer. Therefore, T is aborted and restarted\nwith a new, larger timestarnp. If TS(T) > WTS(O), Treads 0, and l~TS(O)\nis set to the larger of RTS(O) and TS(T). (Note that a physical change--the\nchange to RTS(O)-is written to disk and recorded in the log for recovery\npurposes, even on reads. This write operation is a significant overhead.)\nObserve that if T is restarted with the same timestamp, it is guaranteed to be\naborted again, due to the saIne conflict. Contrast this behavior with the use of\ntimestamps in 2PL for deadlock prevention, where transactions are restarted\nwith the same timestarnp as before to avoid repeated restarts. This shows that\nthe two uses of timestamps are quite different and should not be confused.\nNext, consider what happens when transaction T wants to write object 0:\n1. If TS(T) < RTS(O), the write action conflicts with the most recent read\naction of 0, and T is therefore aborted and restarted.\n2. If TS(T) < WTS(O), a naive approach would be to abort T because\nits write action conflicts with the most recent write of 0 and is out of\ntimestamp order. However, we can safely ignore such writes and continue.\nIgnoring outdated writes is called the Thomas Write Rule.\n3. Otherwise, T writes 0 and WTS(O) is set to TS(T).\nThe Thomas Write Rule\nWe now consider the justification for the TholIlas Write Rule.\nIf TS(T) <\nWTS(O), the current write action has, in effect, been made obsolete by the\nrnost recent write of 0, which follows the current write according to the tirnes-\ntalnp ordering. We can think of T's write action as if it had occurred irnrnedi-\nately before the rnost recent write of 0 and was never read by anyone.\nIf the Thoma') vVrite Rule is not used, that is, T is aborted in case (2), the\ntirnestamp protocol, like 2PL, allows only conflict serializable schedules. If the\nTho1l1aS '\\Trite R,ule is used, S(Hne schedules are perrnitted that are not conflict\nserializable, fl.'3 illustrated by the schedule in Figure 17.6.2 Because T2's \\vrite\nfollows Tl's read and precedes Tl's write of the sanle object, this schedule is\nnot conflict serializable.\n21n the other direction, 2PL pennits some schedules that are not allowed by the timestamp algo-\nrithm with the Thomas Write Rule; see Exercise 17.7.\n\nCOnC'llr'Tency ContTol\nT1\nR(A)\nW(A)\nCOlnmit\nT2\nl{T(A)\nCornrnit\n571\nFigure 17.6\nA Serializable Schedule 'rhat Is Not Conflict Serializable\nl'he Thomas Write Rule relies on the observation that T2's write is never seen\nby any transaction and the schedule in Figure 17.6 is therefore equivalent to\nthe serializable schedule obtained by deleting this write action, which is shown\nin Figure 17.7.\nT1\nT2\nR(A)\nCommit\nW(A)\nCommit\nFigure 17.7\nA Conflict Serializable Schedule\nRecoverability\nUnfortunately, the timestamp protocol just presented permits schedules that\nare not recoverable, as illustrated by the schedule in Figure 17.8. If T S(T1) :::::: 1\nand T8(T2) = 2, this schedule is permitted by the timestalnp protocol (with\nor without the 1\"ho111as \\\\Trite Rule). The tiInestalnp protocol can be modified\nto disallow such schedules by buffering all write actions until the transaction\nCOIDlnits. In the example, when Tl wants to write A, WTS(A) is updated to\nreflect this action, but the change to A. is not carried out irrllnediately; instead,\nit is recorded in a private workspace, or buffer.\nWhen T2 wants to read A\nsubsequently, its thnestamp is cornpared with l¥TS(A), and the read is seen\nto be perrnissible. However,\n~r2 is blocked until T1 cornpletes. If T1 cornrnits,\nits change to A is copied fro111 the buffer; other\\vise, the changes in the buffer\nare discarded.\n~r2 is then allowed to read A.\nThis blocking of T2 is sinlilar to the effect of T1 obtaining an exclusive lock on\nA. Nonetheles8, even with this modification, the tirnestarnp protocol perrnits\nsorne schedules not perrnitted by 2PL; the two protocols are not quite the senne.\n(See Exercise 17.7.)\n\n1\"1\nT2\nR(A)\nl~T(B)\nCorllrnit\nFigure 17.8\nAn Unrecoverable Schedule\nBecause recoverability is essential, such a modification must be used for the\ntimestamp protocol to be practical. Given the added overhead this entails, on\ntop of the (considerable) cost of maintaining read and write tilnestamps, thnes-\ntamp concurrency control is unlikely to beat lock-based protocols in centralized\nsystems. Indeed, it has been used mainly in the context of distributed database\nsystems (Chapter 22).\n17.6.3\nMultiversion Concurrency Control\nThis protocol represents yet another way of using timestamps, assigned at\nstartup time, to achieve serializability. The goal is to ensure that a transac-\ntion never has to wait to read a database object, and the idea is to maintain\nseveral versions of each database object, each with a write timestamp, and let\ntransaction Ti read the most recent version whose timestarnp precedes TS(Ti).\nIf transaction 1'i wants to write an object, we must ensure that the object\nhas not already been read by sonle other transaction T j such that T S (Ti) <\n1'S(Tj). If we allow Ti to write such an object, its change should be seen by\nTj for serializability, but obviously Tj, which read the object at SaIne tinle in\nthe past, will not see\n~r'i's change.\nTo check this condition, every object also has an associated read timestarnp,\nand whenever a transaction reads the object, the read timestamp is set to\nthe maxhuuru of the current read tilnestarnp and the reader's tirnestarnp. If 7',t\nwants to write an object 0 and TS(Ti) < RTS(O), Ti is aborted and restarted\nwith a new, larger timestamp. Otherwise, Ti creates a new version of 0 and\nsets the read and write tirnestarnps of the new version to 7'S(Ti).\nThe drawbacks of this sehenle are similar to those of tirnestarnp concurrency\ncontrol, and in addition, there is the cost of rnaintaining versions.\nOn the\nother hand, reads are never blocked, which can be irnportant for workloads\ndorninated by transactions that only read values frorn the database.\n\nC:onC7LTr'ency C}ontTol\nr.::\"\"3,'\n;j(\nI-;~t Do Real-;~ms Do?\n~~~-~~;, Informix, Microsoft-~~\n~\nServer, and Sybase ABE use Strict 2PL or variants (if a transaction re-\nI\nquests a lower than SERIALIZABLE SQL isolation level; see Section 16.6).\nIVlicrosoft SQL Server also supports rnodifieation timestamps so that a\ntransaction can run \"\"vithout setting locks and validate itself (do-it-yourself\nOptirnisticConC1:1rrency Control!).\nOracle 8 uses a lllultiversion concur-\nrency control scherne in \"\"vhich readers never wait; in fact, readers never\nget locks and detect conflicts by checking if a block changed since they\nread it. All these systerlls support rnultiple-granu1arity locking, with sup-\nport for table, page, and row level locks.\nAll deal with deadlocks using\nwaits-for graphs. Sybase ASIQ supports only table-level locks and aborts\na transaction if a lock request fails-·--updates (and therefore conflicts) are\nrare in a data warehouse, and this simple scheme suffices.\n________.\n.\",\".1\n17.7\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nWhen are two schedules conflict equivalent? What is a conflict serializable\nschedule? What is a strict schedule? (Section 17.1)\n•\nWhat is a precedence graph or serializability graph? Ilow is it related to con-\nflict serializability? How is it related to two-phase locking? (Section 17.1)\n•\nWhat does the lock manager do? Describe the lock table and transaction\ntable data structures and their role in lock management. (Section 17.2)\nII\nDiscuss the relative\nnH.~rits of lock upgrades and lock downgrades.\n(Sec-\ntion 17.3)\nlIlI\nDescribe and cornpare deadlock detection a,nd deadlock prevention schernes.\n\\Vhy are detection schernes rnore cornrnonly used? (Section 17.4)\nII\nIf the collection of database objects is not fixed, but can gro\\v and shrink\nthrough insertion and deletion of objects, we lnust deal with a subtle corll-\nplication known as the phantorn problern. Describe this problern and the\nindex locking approach to solving the probleln. (Section 17.5.1)\nII\nIn tree index structures, locking higher levels of the tree can becorne a per-\nforrnanee bottleneck. Explain why. I)escribe specialized locking techniques\nthat address the problenl, and explain why they work correctly despite not\nlJeing two-phase. (Section 17.5.2)\nII\nAI71lt'iple-granldaT'ity locki'ng enables us to set locks on objects that contain\nother objects, thus iInplicitly locking all contained objects.\n\\Vhy is this\napproach irnportant and how does it work? (Section 17.5.3)\n\n574\nCHAPTER \\7\n•\nIn optirrtistic conClLrrency control, no locks are set and transactions read\nand rnodify data objects in a private workspace. How are conflicts between\ntransactions detected and resolved in this approach? (Section 17.6.1)\n•\nIn tirnestamp-based concurrency control, transactions are assigned a times-\ntarnp at startup; how is it used to ensure serializability?\nHow does the\nThomas Write Rule improve concurrency? (Section 17.6.2)\n•\nExplain why tinlestamp-based concurrency control allows schedules that\nare not recoverable. Describe how it can be modified through buffering to\ndisallow such schedules. (Section 17.6.2)\n•\nDescribe multiversion concurrency control. What are its benefits and dis-\nadvantages in comparison to locking? (Section 17.6.3)\nEXERCISES\nExercise 17.1 Answer the following questions:\n1. Describe how a typical lock manager is implemented.\nWhy must lock and unlock be\natomic operations? What is the difference between a lock and a latch? What are convoys\nand how should a lock manager handle them?\n2. Compare lock downgrades with upgrades.\nExplain why downgrades violate 2PL but\nare nonetheless acceptable.\nDiscuss the use of update locks in conjunction with lock\ndowngrades.\n3. Contrast the timestamps assigned to restarted transactions when tinwstanlps are used\nfor deadlock prevention versus when timestamps are used for concurrency control.\n4. State and justify the Thomas Write Rule.\n5. Show that, if two schedules are conflict equivalent, then they are view equivalent.\n6. Give an example of a serializable schedule that is not strict.\n7. Give an example of a strict schedule that is not serialiable.\n8.\n~1otivate and describe the use of locks for improved conflict resolution in Optinlistic\nConcurrency Control.\nExercise 17.2 Consider the following cla..'3ses of schedules: ser'ializable l confiict-serializable,\n1..J'iew-ser'ializable, recoverable, avoids-cascading-aborts, and strict.\nFor each of the following\nschedules, state which of the preceding cla.'3ses it belongs to. If you cannot decide whether a\nschedule belongs in a certain class ba.'3ed on the listed actions, explain briefly.\nThe actions are listed in the order they are scheduled and prefixed with the transaction name.\nIf a commit or abort is not shown, the schedule is incomplete; assurne that abort or cornrnit\nlllust follow all the listed actions.\n1. Tl:R(X), T2:R(X), Tl:W(X), T2:\\iV(X)\n2. Tl:W(X), T2:R(Y), Tl:R(Y), T2:R(X)\n\nC}oncurrency Contr'Ol\n3. Tl:R(X), T2:R(Y), T~3:\\V(X), T2:R(X), Tl:R(Y)\n4. Tl:R(X), T1:R(Y), T1:W(X), T2:R(Y), T:3:\\¥CY), Tl:W(X), T2:R(Y)\n5. Tl:R(X), T2:W(X), Tl:W(X), T2:Abort, Tl:Cmnmit\n6. Tl:R(X), T2:W(X), Tl:\\V(X), T2:Comrnit, Tl:Comm.it\n7. T1:W(X), T2:R(X), 1'l:W(X), 1'2:Abort, T.1:COllllllit\n8. Tl:W(X), T2:R(X), Tl:W(X), T2:Conunit, Tl:Col1unit\n9. Tl:W(X), T2:R(X), Tl:W(X), T2:Commit, Tl:Abort\n10. 1'2: R(X), 1'3:W(X), T3:Cmnrnit, Tl:W(Y), Tl:Commit, T2:R(Y),\nT2:W(Z), T2:Colllmit\n11. Tl:R(X), T2:W(X), T2:Cornrnit, Tl:W(X), Tl:Colllmit, T:3:R(X), T3:Collnnit\n12. Tl:R(X), T2:W(X), Tl:W(X), T3:R(X), Tl:Comlllit, T2:Corn111it, 1'3:Comlnit\n575\nExercise 17.3 Consider the following concurrency control protocols: 2PL, Strict 2PL, Con-\nservative 2PL, Optimistic, Tilnestamp without the Thomas Write Rule, 1'ilnestamp with the\nThomas Write Rule, and Multiversion. l'or each of the schedules in Exercise 17.2, state which\nof these protocols allows it, that is, allows the actions to occur in exactly the order shown.\nFor the timestamp-based protocols, assurne that the timestamp for transaction Ti is i and\nthat a version of the protocol that ensures recoverability is used.\nFurther, if the Thomas\nWrite Rule is used, show the equivalent serial schedule.\nExercise 17.4 Consider the following sequences of actions, listed in the order they are sub-\nmitted to the DBMS:\n•\nSequence 81: Tl:R(X), T2:W(X), T2:W(Y), T3:W(Y), Tl:W(Y),\nTl:Commit, T2:Commit, T3:Commit\n•\nSequence 82: Tl:R(X), T2:W(Y), T2:W(X), T3:W(Y), Tl:W(Y),\nTl:C0111mit, T2:Commit, T3:Commit\n!<or each sequence and for each of the following concurrency control rnechanislns, describe\nhow the concurrency control mechanislll handles the sequence.\nAssurne that the tirnestarnp of transaction Ti is 'i. Fbr lock-based concurrency control rnech-\naniS111S, add lock and unlock requests to the previous sequence of actions as per the locking\nprotocol. The DB\"NIS processes actions in the order shown. If a transaction is blocked, a%Ulne\nthat all its actions are queued until it is reslllned; the I)B11S continues with the next action\n(according to the listed sequence) of an unblocked transaction.\n1. Strict 2PL with tiluestamps used for deadlock prevention.\n2. Strict 2PL with deadlock detection. (Show the waits-for graph in C(l.se of deadlock.)\n~3. Conservative (and Strict, i.e., with locks held until end-of-transaction) 2PL.\n4. Optimistic concurrency control.\n5. Tiruestarup concurrency control with buffering of reads and writes (to ensure recover-\nability) and the Tholnas Write Rule.\n6. rvluitiversioll concurrency control.\n\n576\nCIIAPTER 1J\nAll Schedules\n•• _.\n\" •• ~._~\n._\",_.\n•\n•••_.__._•••_. __ ~\",_.~•••••••• ~~•• ~.~\n~~••••• u •••• ~•••• ~\n........\nView Serializable\n'1\n(.,~\n~_~_u.uu\n••• u ••_ ••••••••••••••••••••••••• \\\ni\n(---~~~t~~-t ~rft~l ~riaJ~ I-R~:able1\nI r-~ ~:t~ll:;--~--:;~i~~~rt11\nLl=='==--t~---~-=k-t-+~LLJ\n~---_._---~--~..~...~_..__.._....._._.._-_......._._-_.._--</\nFigure 17.9\nVenn Dia,gram for Classes of Schedules\nExercise 17.5 For each of the following locking protocols, assulning that every transaction\nfollows that locking protocol, state which of these desirable properties are ensured: serializ-\nability, conflict-·serializability, recoverability, avoidance of cascading aborts.\n1. Always obtain an exclusive lock before writing; hold exclusive locks until end-of-transaction.\nNo shared locks are ever obtained.\n2. In addition to (1), obtain a shared lock before reading; shared locks can be released at\nany time.\n3. As in (2), and in addition, locking is two-phase.\n4. As in (2), and in addition, all locks held until end-of-transaction.\nExercise 17.6 The Venn diagranl (frorn [76]) in Figure 17.9 shows the inclusions between\nseveral classes of schedules.\nGive one exaulple schedule for each of the regions Sl through\nS12 in the diagrarn.\nExercise 17.7 Briefly answer the following questions:\n1. Draw a Venn diagnnll that ShO\\V8 the inclusions between the classes of schedules perulit-\ntccl by the following concurrency control protocols: 2PL, 8tr'ict 2PL, Conscr1.Jauve 2PL,\nOptirni.stic, Timestamp without the\nThorna8~Vr'ite Rule, Tirne8tam.p with the ThonHt8\nWrite\nR1J,lc~ and Ah.tlt'll,)(:T.'Fion.\n2. Give one ex<:'unple schedule for each region in the diagrarIl.\n:3. Extend the Venn diagranl to include serializable and conflict-serializable schedules.\nExercise 17.8 Answer each of the follmving questions briefly. The questions are based on\nthe following relational schern.a:\nErnp( C'id: integer) cnarne: string, age: integer, salary: real, did: integer)\nDept(~~~~~_~_~:.~.~::,drunne: string, flooT: integer)\n(mel on the fc)llowing update cornrnand:\nreplace (salary = 1.1 *F]\\{P.salary) where .KtvlP.enarne = 'Santa'\n\nC:onC1LTTeTLCY C\\Jn(:Tol\n5\nr-.'.....\n\",{\n1. Give an exaruple of a query that would conflict with this comrnand (in a concurrency\ncontrol sense) if both were run at the s::.une tim.e. Explain what could go wroIlg~ and how\nlocking tuples would solve the probleIll.\n2. Give an exarnple of a query or a cOHlInand that would conflict with this cOIl1ruand, such\nthat the conflict could not be resolved by just locking individual tuples or pages but\nrequires index locking.\n~). Explain what index locking is and how it resolves the preceding conflict.\nExercise 17.9 SQL supports four isolation-levels and two access-rllodes, for a total of eight\ncornbinations of isolation-level and access-rnode. Each corubinatioll inlplicitly defines a class\nof transactions; the follO\\ving questions refer to these eight classes:\n1. For each of the eight classes, describe a locking protocol that allows only transactions in\nthis class. Does the locking protocol for a given class make any assurnptiolls about the\nlocking protocols used for other classes? Explain briefly.\n2. Consider a schedule generated by the execution of several SQL transactions. Is it guar-\nanteed to be conflict-serializable? to be serializable'? to be recoverable?\n3. Consider a schedule generated by the execution of several SQL transactions, each of\nwhich has READ ONLY access-mode.\nIs it guaranteed to be conflict-serializable?\nto be\nserializable? to be recoverable?\n4. Consider a schedule generated by the execution of several SQL transactions, each of\nwhich has SERIALIZABLE isolation-level. Is it guaranteed to be conflict-serializable? to\nbe serializable? to be recoverable?\n5. Can you think of a tinlCstarup-based concurrency control scheme that can support the\neight classes of SQL transactions?\nExercise 17.10 Consider the tree shown In Figure 19.5.\nDescribe the steps involved in\nexecuting each of the following operations according to the tree-index concurrency control\nalgorithm discussed in Section 19.3.2, in terms of the order in which nodes are locked, un-\nlocked, read, and written. Be specific about the kind of lock obtained and answer each part\nindependently of the others, always starting with the tree shown in Figure 19.5.\n1. Search for data entry 40*.\n2. Search for all data entries k* with k ~ 40.\n3. Insert data entry 62*.\n4. Insert data entry 40*.\n5. Insert data entries 62* and 75*.\nExercise 17.11 Consider a database organized in tenns of the following hierarachy of ob-\njects: The database itself is an object (D), and it contains two files (Fl (lud F'2), each of\nwhich contains 1QOO p<.Lges (PI . .. PlOOO ancl Pl()()l ... P2000, respectively). Each page con-\ntains 100 records, and records aTe identified as p : i, where]J is the page identifier and i is the\nslot of the record on that page.\nI'vlultiple-granularity locking is used, with 5', .i\\{) 15',#1 X and S'IX locks, and datah<Lse-level,\nfile-level, page-level ;111<1 record-level locking.\nFor each ()f the foll()\\ving operations, indicaJ;e\nthe sequence of lock requests that 1nust be generated by a transctction that wants to carry\nout (just) these operations:\n\n578\n1. Read record P1200 : 5.\n(~HAPTER 1;:7\n2. Reacl recorclsP1200 : 98 through P1205 : 2.\nii. Read aU (records on all) pages in file }'l.\n4. 'Read pages P500 through P520.\n5. Read pages PIO through P980.\n6. Read all pages in PI and (ba..ged on the values read) rnodify 10 pages.\n7. Delete record P1200 : 98. (This is a blind write.)\n8. Delete the first record frorn each page.\n(Again~ these are blind writes.)\n9. Delete all records.\nExercise 17.12 Suppose that we have only two types of transactions, Tl and T2. Transac-\ntions preserve database consistency when run individually. We have defined several integrity\nconstnLiTtts such that the DBNIS never executes any SQL statenwnt that brings the database\ninto an inconsistent state. Assunle that the DBlVIS does not perform any concurrency control.\nGive an exarllple schedule of two transactions Tl and T2 that satisfies all these conditions,\nyet produces a database instance that is not the result of any serial execution of 7'1 and T2.\nBIBLIOGRAPHIC NOTES\nConcurrent access to B trees is considered in several papers, including [70,456,472,505,678].\nConcurrency control techniques for Linear Hashing are presented in [240] and [543]. Multiple-\ngranularity locking is introduced in [3:36] and studied further in [127, 449].\nA concurrency control method that works with the ARIES recovery rnethod is presented in\n[545]. Another paper that considers concurrency control issues in the context of recovery is\n[492]. AlgorithrIls for building indexes without stopping the DBMS are presented in [548] and\n[9].\n1'he performance of B tree concurrency control algorithnls is studied in [704].\nPerfor-\nnlance of various concurrency control algorithms is discussed in [16, 729, 735]. A good survey\nof concurrency control rnethods and their perfornlance is [734].\n[455] is a comprehensive\ncollection of papers on this topic.\nTilYwstarIlp-based multiversion concurrency control is studied in [6201. IvIultiversion concur--\nrency control algoritllIIls are studied forrnally in [87J.\nLock-based rnultiversion techniques\nare considered in [460].\nOptirnistic concurrency control is introduced in [457].\nThe use of\naccess invariance to irnprove conflict resolution in high-contention environrnents is discussed\nin [281] and [280].\n1'rallsacti0I1 rnanagenwnt issues for real-tirne databa..'5e systerns are dis-\ncussed in [1, 15, :368, :382, :386, 448]. There is a large body of theoretical results on data.ba.se\nconcurrency control; [582, 89] offer thorough textbook presentations of this rnaterial.\n\n18\nCRASH RECOVERY\n(,...\nWhat steps are taken in the ARIES method to recover fronl a DBl\\1S\ncrash?\n...\nHow is the log rnaintained during nonnal operation?\n..\nHow is the log used to recover frorn a crash?\n(\",..\nWhat infonnation in addition to the log is used during recovery?\n\"'What is a checkpoint and why is it used?\n...\nW'hat happens if repeated crashes occur during recovery?\n...\nHow is media failure handled?\n...\nHow does the recovery algorithnl interact with concurrency control?\n..\nKey concepts:\nsteps in recovery, analysis, redo, undo; ARIES,\nrepeating history;\nlog,\nLSN,\nforcing\npages,\nWAL; types of log\nrecords, update, cornrnit, abort, end, cOlnpensation; transaction ta-·\nble, lastLSN; dirty page table, recLSN; checkpoint, fuzzy checkpoint-\ning, rnao;;;ter log record; rnedia recovery; interaction with concurrency\ncontrol; shadow paging\nHurnpty Durnpty sat on a \\vall.\nlIurnpty Durnpty h(1,(1 a great fall.\nA.ll the King's horses and all the King's tnen\nCould not put lIllrnpty together again.\n-~~~()ld nursery rhyrne\n579\n\n580\nC~HAPTER 18p\nThe recovery manager of aDB~/[S is responsible for ensuring tvvo irnportant\nproperties of transactions: Atornicity and durability. It ensures ato'Tn:icity by\nundoing the actions of transactions that do not conlIllit and durab'il'ity by rnak-\ning sure that all actions of conunitted transactions survive system crashes\n(e.g., a core durnp caused by a bus error) and Inedia failures (e.g., a disk is\ncorrupted).\n1\"'he recovery rnanager is one of the hardest cOlllponents of a DBlViS to design\nand inlplernent. It rnust deal 'with a wide va,riety of database states because\nit is called on during systenl failures. In this chapter, \"\\Ive present the ARIES\nrecovery algorithnl, which is conceptually sinlple, works well with a wide range\nof concurrency control rnechanisrns, and is being used in an incre&sing number\nof database syterns.\nWe begin with an introduction to ARIES in Section 18.1.\nWe discuss the\nlog, which a central data structure in recovery, in Section 18.2, and other\nrecovery-related data structures in Section 18.3.\nWe complete our coverage\nof recovery-related activity during normal processing by presenting the Write-\nAhead Logging protocol in Section 18.4, and checkpointing in Section 18.5.\nWe discuss recovery frorn a crash in Section 18.6. Aborting (or rolling back)\na single transaction is a special case of Undo, discussed in Section 18.6.3. We\ndiscuss media failures in Section 18.7, and conclude in Section 18.8 with a\ndiscussion of the interaction of concurrency control and recovery and other ap-\nproaches to recovery. In this chapter, we consider recovery only in a centralized\nDBMS; recovery in a distributed DBMS is discussed in Chapter 22.\n18.1\nINTRODUCTION TO ARIES\nARIES is a recovery algorithrn designed to work with a steal, no-force ap-\nproach. When the recovery rnanager is invoked after a, craBh: restart proceeds\nin thn~e phases:\n1. Analysis: Identifies dirty pages in the buffer pool (i.e., changes that have\nnot been written to disk) and active transactions at the tiITle of the crash.\n2. Redo: H,epeats all actions, starting frOID an appropriate point in the log,\nand restores the database state to what it was at the tirne of the e1'a8h.\n~). lJndo: lJndoes the actions of transactions that did not cOllunit, so tlU:l,t\nthe databa,se reflects only the actions of cornrnitted transactions.\nConsider the sirnple execution history illustrated in Figure 18.1.\n'\\\\7hen the\nsysteIIl is restarted, the A,nalysis phase identifies 'Tl H.nd\n~r:3 as transactions\n\nCrYlBh Recovery\nLSN\nL()(;\n10 -\nupdate: T1 writes P5\n20 ---\nupdate: T2 writes P3\n30 -\nT2 comnlit\n40 ---\nT2end\n50 -\nupdate: T3 writes PI\n60 -\nupdate: T3 writes P3\nX\nCRASH, RESTART\nFigure 18.1\nExecution History with a Crash\nh&l,.'\na\nactive at the time of the crash and therefore to be undone; T2 as a corrnuitted\ntransaction~ and all its actions therefore to be written to disk; and PI ~ P3, and\nP5 as potentially dirty pages. All the updates (including those of TI and T3)\nare reapplied in the order shown during the Redo phase. Finally, the actions\nof TI and T;-3 are undone in reverse order during the Undo phase; that is, T3's\nwrite of P3 is undone, 7\"3's write of PI is undone, and then TI ~s write of P5\nis undone.\nThree Inain principles lie behind the ARIES recovery algoritlun:\n!II\nWrite-Ahead Logging: Any change to a database object is first recorded\nin the log; the record in the log lllUst be written to stable storage before\nthe change to the database object is written to disk.\n!II\nRepeating History During Redo: On restart following a crash, ,AllIES\nretraces all actions of the DBlVlS before the crash and brings the systern\nback to the exact state that it wa,s in at the tilne of the crash.\nThen,\nit undoes the actions of transactions still active at the tirne of the cra..sh\n(effectively aborting theln).\n11II\nLogging Changes During Undo: Changes lnada to the databa.se '.vhile\nundoing a transaction are logged to ensure such an action is not repeated\nin the event of repeated (failures causing) restarts.\nThe second point distinguishes AllIES frorn other recovery algorithrns and is\nthe basis for rnuch of its sirnplicity and flexibility.\nIn particular, ABIES can\nsupport conCUlTcnc:Jl control protocols that involve locks of finer granularity\nthan a page (e.g., record-level loc:ks).\nT1he SeCO]lc! and third points are also\n\n582\nCIIAPTER 18\n---.....,.------_._...._..._._._-_._._---\nCrash Recovery:\nIB~1 DB2, Inforrnix, Iv1icrosoft SQL Server, Oracle 8,\nand Sybase l\\SE all use a WAL seherue for recovery. IBIvI DB2 uses ARIES,\nI!I\niI\nand the others use seherues that are actually quite sinlilar to ARIES (e.g.,\nI\nall changes are re-applied, not just the changes made by transactions that\nI\nare 'winners') although there are several variations.\n-------\n~ .. -\n...................._.\n.\n----\n~\nimportant in dealing with operations where redoing and undoing the opera-\ntion are not exact inverses of each other. We discuss the interaction between\nconcurrency control and crash recovery in Section 18.8, where we also discuss\nother approaches to recovery briefly.\n18.2\nTHELOG\nThe log, SOlnetirnes called the trail or journal, is a history of actions executed\nby the DBMS. Physically, the log is a file of records stored in stable storage,\nwhich is assumed to survive crashes; this durability can be achieved by main-\ntaining two or more copies of the log on different disks (perhaps in different\nlocations), so that the chance of all copies of the log being sinlultaneously lost\nis negligibly small.\nThe most recent portion of the log, called the log tail, is kept in nlain Inemory\nand is periodically forced to stable storage.\nThis way, log records and data\nrecords are written to disk at the same granularity (pages or sets of pages).\nEvery log record is given a unique id called the log sequence number\n(LSN). As with any record id, we can fetch a log record with one disk access\ngiven the LSN. Further, LSNs should be assigned in ruonotonically increasing\norder; this property is required for the ARIES recovery algorithrn. If the log is\na sequential file, in principle growing indefinitely, the LSN can sirllply be the\naddress of the first byte of the log record'!\nFor recovery purposes, every page in the databa':lc contains the LSN of the rnost\nrecent log record that describes a change to this page. This LSN is called the\npageLSN.\nA log record is\\vritten for each of the following actions:\n--_._--_.._...._._---------\n1In practice, various techniques are used to identify portions of the log that are 'too old' to be\nneeded again to bound the amount of stable storage used for the log. Given such a bound, the log may\nbe implemented a...<; a 'circular' file, in which case the I..ISN may be the log record id plus a \"UYm,p-count.\n\n583\n•\nUpdating a Page: After rTlodifying the page, an 'npdate type record\n(dt:~\nscribed later in this section) is appended to the log tail. Tlhe pageLSN of\nthe page is then set to the LSN of the update log record. (The page Blust\nbe pinned in the buffer pool while these actions are carried out.)\nIII\nConl1nit: vVhen a transaction decides to conunit, it force-writes a conk\n'{nit type log record containing the transaction id. That is, the log record\nis appended to the log, and the log tail is written to stable storage, up to\nand including the cOllunit record.2 The transaction is considered to have\ncOIlnnitted at the instant that its cOlnmit log record is written to stable\nstorage. (Solne additional steps rnust be taken, e.g., reilloving the transac-\ntion's entry in the transaction table; these follow the 'writing of the cOlInnit\nlog record.)\n•\nAbort: When a transaction is aborted, an abort type log record containing\nthe transaction id is appended to the log, and Undo is initiated for this\ntransaction (Section 18.fL3).\n•\nEnd: As noted above, when a transaction is aborted or comrnitted, some\nadditional actions rnust be taken beyond writing the abort or COlllIllit log\nrecord.\nAfter all these additional steps are c()lnpleted, an end type log\nrecord containing the transaction id is appended to the log.\nIII\nUndoing an update: When a transaction is rolled back (because the\ntransaction is aborted, or during recovery frorn a crash), its updates are\nundone. When the action described by an update log record is undone, a\ncornpensation log Teconl, or CLR, is written.\nEv(~ry log record has certain fields: prevLSN, transID, and type. The set of\nall log records for a given transaction is rnaintained as a linked list going back\nin tirne, using\nthE~ prevLSN field; this list HUlst be updated whenever a log\nrecord is added. The transII) field is the id of the transaction generating the\nlog record, and the type field obviously indicates the type of the log record.\nAdditional fields depend on the type of the log record. vVe already rnentioned\nthe additional contents of the various log record types, with the exception of\nthe update EtIId cornpensa>tion log r(~cord types, \\\\Thieh we describe next.\nUpdate Log Records\nThe fields in an update log record are illustrated in Figure 18.2. frhe pageID\nfield is the page iel of the Inodified page; the length in bytes and the offset of the\n'2 Note that this step requires the buffer manager to be able to selectively force pages to\nstabl(~\nstorage.\n\n584\nprevLloiN\ntraa\".iID\ntype\npageIJ.)\nlength\noffset\nC~HAPTERfl8\nbefore~image\nafter-image\nFields common to all log records\nAdditional fi.elds for update log records\nFigure 18.2\nContents of an Update Log Record\nchange are also included. The before-image is the value of the changed bytes\nbefore the change; the after-image is the value after the change. An update\nlog record that contains both before- and after-images can be used to redo\nthe change and undo it. In certain contexts, which we do not discuss further,\nwe can recognize that the change will never be undone (or, perhaps, redone).\nA redo-only update log record contains just the after-iluage; similarly an\nundo-only update record contains just the before-iluage.\nCompensation Log Records\nA compensation log record (CLR) is written just before the change recorded\nin an update log record U is undone. (Such an undo can happen during nor-\nrnal system execution when a transaction is aborted or during recovery froIn a\ncrash.) A cOlnpensation log record C describes the action taken to undo the\nactions recorded in the corresponding update log record and is appended to\nthe log tail just like any other log record. 'fhe cornpensation log record C also\ncontains a field called undoNextLSN, which is the LSN of the next log record\nthat is to be undone for the transaction that wrote 11pdate record lJ; this field\nin C is set to the value of prevLSN in [J.\nAs an exarllple, consider the fourth update log reeord shown in Figure 18.3.\nIf this update is undone, a CLIl \\vould be written, and the inforrnation in it\nwould include the transII), pageID, length, offset, and before-iInage fields froln\nthe update record. Notice that the CLH, records the (undo) action of changing\nthe affected bytes back to the before-irnage value; thus, this value and the\nlocation of the affected bytes constitute the redo infonnation for the action\ndescribed by the CLH,. r-Ihe undoNextLSN field is set to the LSN of the first\nlog record in Figure 18.:3.\nlJnlike an update log record, a CLIl describes an action that \\vill never be\ntlndone, that is, \\ve never undo an undo action. '1'he rea,son is sirnple: An update\nlog record describes a change lnade by a transaction during nonnal execution\nand the transaction rnay subsequently be aborted, whereas a (:LH, describes\nelll a,ctiol1 tak.en to rollback a transaction for \\vllich the decision to abort has\nalrea.lJy been rnade.\nTherefore, the transaction rnust be rolled back, and the\n\nC:Tash Recover~lj\n585\nundo action described by the CLIl is definitely required. This observation is\nvery useful because it bounds the a1nount of space needed for the log during\nrestart froin a crash: 1'he nUlnber of CLHs that ca,n be vvritten during LJndo is\nno 1nore than the nurnber of update log records for active transactions at the\ntirne of the crash.\nA CLR. II1ay be 'written to stable stora,ge (follo\\ving \\iVAL, of course) but the\nundo action it describes rIlay not yet been vvrittcn to disk when the systenl\ncrashes again. In this case, the undo action described in the CLR is reapplied\nduring the Itedo phase, just like the action described in update log records.\nFor these re&'3ons, a CLIl contains the infonnation needed to reapply, 01' redo,\nthe change described but not to reverse it.\n18.3\nOTHER RECOVERY..REI.JATED STRU'CTURES\nIn addition to the log, the following two tables contain important recovery-\nrelated infornlation:\nII\nTransaction Table: This table contains one entry for each active trans-\naction.\n'The entry contains (arnong other things) the transaction id, the\nstatus, and a field called lastLSN, which is the LSN of the rnost recent log\nrecord for this transaction. The status of a transaction can be that it is in\nprogress, corunlitted, or aborted. (In the latter two cases, the transaction\nwill be rernoved fro1l1 the table once certain 'clean up' steps are c(nupleted.)\nII\nDirty page table: This table contains one entry for each dirty page in\nthE:~ buffer pool, that is, each page with changes not yet reflected on disk.\nThe entry contains a field recLSN, vvhich is tlH~ LSN of the first log record\nthat caused the page to becorne dirty.\nNote that this LSN identifies the\nearliest log record that lnight have to be redone for this page during restart\nfronl a cr<:1,sh.\nI)uring norrnal operation, these cLre rnainta..ined by the transa,ction rnanager and\nthe buffer rnanager, respectively, and during restart after a crash, these ta,bles\nare reconstructed in the Analysis phase of restart.\nConsider the follc)\\ving silupic exarnple. 11:ansaction TIOOO changes the value of\nbytes 21 to 2:3 011 page P500 frorn 'ABC' to '!)EF', transaction 'T2000 changes\n'lII.r to 'I<IJ\\;1' on page P600, transaction 1~2000 changes bytes 20 through 22\nfronl 'C;rJE' to 'QRS' on page ])50(\\ then transaction T1000 changes 'TlTV'\nto '-\\,VXY' on pageP505.\nI'he dirty page table, the transaction\ntable':~ ;:lnd\n_\n_\n---------_.__.__.__.---\n3The status field is not shown in the figure for space reasons; all transactions are in progress.\n\n586\n(~HAPTERil8\npageID\nrecLSN\nP500\nlength\noffset\nhefore-image\nafter-image\nP600\nP505\nDIRTY PAGE\nTRANSACTION TABLE\nprcyl..-..\"iN\ntransID\ntype\npagelD\n'1'1000\nupdate\nP500\n'1'2000\nupdate\nP600\n'1'2000\nupdate\nPSOO\nTlOOO\nupdate\nP505\nLOG\n3\n3\n2\\\nABC\nDEF\n4\\\nHU\nKLM\n20\nCiDE\nQRS\n2\\\nTUV\nFigure 18.3\nInstance of Log and Ttansaction Table\nthe log at this instant are shown in Figure 18.3. ()bserve that the log is shown\ngrowing froni top to bottorn; older records are at the top. Although the records\nfor each transaction are linked using the prevLSN field, the log as a whole also\nhas a sequential order that is iInportant---for exarnple, T2000's change to page\nP500 follows TIOOO's change to page P500, and in the event of a crash, these\nchanges nUlst be redone in the sanle order.\n18.4\nTHE WRITE-AHEAD LOG PROTOCOI.J\nBefore writing a page to disk, every update log record that describes a change\nto this page rnust be forced to stable storage. This is accornplished by forcing\nall log records up to and including the one with LSN equal to the pageLSN to\nstable storage before \\vriting the page to disk.\nThe irnportance of the \"TAl\" protocol carulot be overerl1phasized- --\\VAL is the\nfundarnentaJ rule that ensures that a record of every change to the database\nis available while atternpting to recover froni a cra\"sh. If a.· transaction rnade (l.\nchange and corIllnitted j the no-force approa,(:h Incans that SOlne of these changes\nrnay not have been vvrittcn to disk at the tirne of a sulJsequent cTcu,h. \\\\Tithout a\nrecord of these changes, there would be no wa.y to erlsurc that the changes of a\ncornl11.itted transaction survive crashes. Note that the definition of a cortun'itted\ntTnn.sacf'ion is effectivel,Y 'a transa,ction all of whose log records j including a\nconunit record j have l)een \\vritten to stcl,ble storage'.\n\\Vhen a txctnsc.tction is cornrnitted, the log U.til is forced to\nstabl(~ storage, even\nif a no-force appro;J,ch is being used. It is '.1l1orth contrasting this operation with\nthe a,ctions taken under a\nforc(~ approach: If a., force approach is used, all the\npages rIlodified by the transaction, rather than a portion of the log that includes\nall its records, IHllS!, be forced to disk Vl1herl the transaction conllIlits. The set of\n\n(}rash Reco'LJery\n5£7\nall changed pages is typically 11luch larger than the log tajl because the size of\nan update log record is close to (tvvice) the size of the changed bytes, ·which is\nlikely to be Inuch s1na11er than the page size. Further, the log is 1naintained as a\nsequential file, and all \\\\Trites to the log are sequential \"\\Trites. Consequently, the\ncost of forcing the log tail is luuch sIllaller than the cost of \\vriting aJl changed\npages to disk.\n18.5\nCHECKPOINTIN(;\nA checkpoint is like a snapshot of the DB:NlS state, and by taking checkpoints\nperiodically, as we will see, the DBl\\1S can reduce the alnount of work to be\ndone during restart in the event of a subsequent crc1..sh.\nCheckpointing in ARIES has three steps. First, a begin_checkpoint record is\nwritten to indicate when the checkpoint starts. Second, an end __checkpoint\nrecord is constructed, including in it the current contents of the transaction\ntable and the dirty page table, and appended to the log.\nThe third step is\ncarried out after the end_checkpoint record is written to stable storage: A\nspecial master record containing the LSN of the begirLcheckpoint log record is\nwritten to a known place on stable storage. 'Vhile the end__checkpoint record\nis being constructed, the DBMS continues executing transactions and writing\nother log records; the only guarantee we have is that the transaction table and\ndirty page table are accurate as of the\nti'lY~e of the begirLcheckpoint record.\nThis kind of checkpoint, called a fuzzy checkpoint, is inexpensive because it\ndoes not require quiescing the SystCIll or writing out pages in the buffer pool\n(unlike senne other forlns of checkpointing). On the other hand, the effectiveness\nof this checkpointing technique is lirnited by the earliest recLSN of pages in the\nd.irty pages table, because during restart we Inust redo changes starting froin\nthe log record \\vhose LSN is equal to this recI.lSN. l-Iaving a background process\nthat periodically writes dirty pages to disk helps to lirnit this probleln.\nvVhen the SystCIIl cornes back up after a crash, the restart process begins by\nlocating the rnost recent checkpoint record. For uniforlnity, the systeIll al\\v::tys\nbegins no1'n1al execution by takirlg a checkpoint, in \\vhich. the transaction table\nand dirty page table are both Clnpty.\n18.6\nRECOV~:RIN'G FROM A sysrr~=M CRASH\n\\Vhen the systenl is restarted after a crash, the recovery Iuana,ger proceeds in\nthree phases, as shown in Figure 18.4.\n\n588\nUNDO\nREDO\nLO(;\nANALYSIS\nj\nOldest log rttord\n,\\\nof traa\"actious\nactive at era<;l.\nSmallest rec.LSN\nB\nin dirty page table\nat end of Analysis\nc\n['-'lost recent checkpoint\nCRASH (end of log)\nC~HAPTEIl Its\nFigure 18.4\nThree l'lhases of Restart in ARIES\nThe Analysis phase begins by examInIng the rnost recent begin_checkpoint\nrecord, whose LSN is denoted C in Figure 18.4, and proceeds forward in the\nlog until the last log record. '1'he Redo phase follows Analysis and redoes all\nchanges to any page that Illight have been dirty at the tir11e of the crash; this set\nof pages and the starting point for Redo (the srnallest recLSN of any dirty page)\nare deterrnined during Analysis. 'The Undo phase follows Redo and undoes the\nchanges of all transactions active at the tirne of the crash; again, this set of\ntransactions is identified during the Analysis phase. Note that Redo reapplies\nchanges in the order in which they were originally carried out; Undo reverses\nchanges in the opposite order, reversing the lllost recent change first.\nObserve that the relative order of the three points A, B, and C in the log rnay\ndiffer frolIl that shown in Figure 18.4. The three phases of restart are described\nin rnore detail in the following sections.\n18.6.1\nAnalysis Phase\nl'he Analysis phase perfonns three tc1...,ks:\n1. It detennines the point in the log at \\vhich to start the Redo pass.\n2. It deterrnines ((1, conservative superset of the) pages in the buffer pool that\n\\Ver8' clirty at the tirne of the crash.\n:3. It identifies'iransEtctions that \\\\rere active at the tirne of the crash and rnust\nbe undone.\nAnalysis 'begins by exEtrnining the rnost recent begirLcheckpoint log record and\ninitializing the dirty page table and transaction table to the copies of those\nstructures in the next end-e.:heckpoint record.\n~rhus, t11ese tables are initialized\nto the set of dirty pages and active transcl,c:tions at the tilne of the checkpoint.\n\nC:'1'ash\nReco'Ve'T~lJ\n5~9\n(If additional log records are between the begiILcheckpoint and encLcheckpoint\nrecords, the tables HIUst be adjusted to reflect the inforluation in these records~\nbut \\ve cnnit the details of this step. See Exercise 18.9.) A.naJysis then scans\nthe log in the for\\vard direction until it reaches the end of the log:\nIII\nIf an end log record for a transaction T is encountered,T is reIlloved fronl\nthe transaction table because it is no longer active.\nIII\nIf a log record other than an end record for a transaction T is encountered,\nan entry for T is added to the transaction table if it is not already there.\nFurther, the entry for T is rnodified:\n1. The lastLSN field is set to the LSN of this log record.\n2. If the log record is a cOllnnit record, the status is set to C, otherwise\nit is set to U (indicating that it is to be undone).\nIII\nIf a redoable log record affecting page P is encountered, and P is not in\nthe dirty page table, an entry is inserted into this table with page id P and\nrecLSN equal to the LSN of this redoable log record. This LSN identifies\nthe oldest change affecting page P that may not have been written to disk.\nAt the end of the Analysis phase, the transaction table contains an accurate\nlist of all transactions that were active at the tilue of the crash·-···_·--this is the\nset of transactions with status U. The dirty page table includes all pages that\nwere dirty at the tirne of the crash but rnay also contain SOIne pages that were\nwritten to disk. If an end_write log record were written at the cornpletion of\nea,ch write operation, the dirty page table constructed during Analysis could\nbe lnade rnore accurate, but in AHJES, the additional cost of writing eneLwrite\nlog records is not considered to be worth the gain.\nAs an exa.rnple, consider the execution illustrated in Figure\n18.~3. Let us extend\nthis execution by assurning that ]'2000 COlIllnits, then TIOnO rnodifies another\npage, say, .P700, and appends an update record to the log tail, and then the\nsystern cra\"shes (before this update log record is written to stable storage).\nThe dirty page table and the transaction table, held in rnernory, are lost in the\ncra..sh. The rnost recent checkpoint \\Vah') taken at the beginning of the execution,\n\\vith an ernpty tran.saction table and dirty page table; it is not shown in Figure\n18.;3.\nAfter excunining this log record, \\vhich \\ve assurne is just before the\nfirst log record shown in the figure, Analysis initializes the two tables to l>e\nernpty. Scanning forv:.rard in the log, T'1000 is added to the transaction table;\nin additiol1,P500 is\nad(h~d to the dirty page ta,blc\\vith recLSN equal to the\nLSN of the first sho\\vn log record. Sirnilarly, T2C)OO is added to the transaction\ntable andPGOO is added to the dirty page table. There is no change based on\nthe third log record, and the fourth record\nn:~sults in the addition of P505 to\n\n590\nCHAPTER,18\nthe dirty page table. The eOllnnit record forT2000 (not in the figure) is no\\v\nencountered, and T2000 is relIloved fro111 the transaction table.\nThe Analysis pha~e is now eornplete, and it is recognized that the only active\ntransaction at the tilne of the crash is TIOOO, \\vith lastLSN equal to the LSN\nof the fourth record in Figure 18.3. rrhe dirty page table reconstructed in the\nAnalysis pha.cse is identical to that shown in the figure. The update log record\nfor the change to P700 is lost in the crash and not seen during the Analysis\npa.'3s.\nThanks to the WAL protocol, however, all is well······--the corresponding\nchange to page P700 cannot have been written to disk either!\nSaIne of the updates rnay have been written to disk; for concreteness, let us\naSSUIne that the change to P600 (and only this update) was written to disk\nbefore the crash. ThereforeP600 is not dirty, yet it is included in the dirty\npage table. rIhe pageLSN on page P600, however, reflects the write because it\nis now equal to the LSN of the second update log record shown in Figure 18.3.\n18.6.2\nRedo Phase\nDuring the Redo phase, ARIES reapplies the updates of all transactions, COill-\nrnitted or otherwise.\nFurther, if a transaction was aborted before the crash\nand its updates were undone, as indicated by CLRs, the actions described in\nthe CLRs are also reapplied. This repeating history paradigm distinguishes\nARIES from other proposed vVAL-based recovery algoritlnIls and causes the\ndatabase to be brought to the sarne state it was in at the time of the crash.\nrrhe R,edo phase begins with the log record that has the srnallest recLSN of all\npages in the dirty page table constructed by the Analysis pass because this log\nrecord identifies the oldest update that rnay not have been written to disk prior\nto the crash. Starting frorn this log record, R,edo scans forward until the end\nof the log. For each redoable log record (update or CLR) encountered,\nRx~do\nchecks whether the logged action HUlst be redone. The action rnust be redone\nunless one of the follo\\ving conditions holds:\nIIIIl\nThe affected page is not in the dirty page table.\nII\nrrhe affected page is in the dirty page table, but the recLSN for the entry\nis gTcatcT tlU),'t1 the LSN of the log record being checked.\nII\n1'he pageLSN (stored on the page, which rnust be retrieved to check this\ncondition) is gTea!;cr than\nOT equal to the LSN of the log record being\nchecked.\nrrh(~ first condition obviously 1118a11S that all changes to this page have been\n\\vritten to disk. Because the recLSN is the first update to this pa.,ge that lnay\n\nCrush lleco1Jery\n591\nnot have been written to disk, the second condition rneans that the update\nbeing checked ,\"va.s indeed propagated to disk. The third condition, \\vhieh is\nchecked la.\",st because it requires us to retrieve the page, also ensures that the\nupdate being checked was ·written to disk, because either this update or a later\nupdate to the page wcL.'3 written. (R,ecall our a.ssurnption that a write to a page\nis atomic; this assurnption is irnportant here!)\nIf the logged action lllust be redone:\n1. The logged action is reapplied.\n2. The pageLSN on the page is set to the LSN of the redone log record. No\nadditional log record is written at this tiIne.\nLet us continue with the exarnple discussed in Section 18.6.1. FrorIl the dirty\npage table, the smallest recLSN is seen to be the LSN of the first log record\nshown in Figure 18.3.\nClearly, the changes recorded by earlier log records\n(there happen to be none in this example) have been written to disk.\nNow,\nRedo fetches the affected page, P500, and compares the LSN of this log record\nwith the pageLSN on the page and, because we assurned that this page was not\nwritten to disk before the crash, finds that the pagE~LSN is less. The update\nis therefore reapplied; bytes 21 through 23 are changed to 'DEF', and the\npageLSN is set to the LSN of this update log record.\nRedo then exarnines the second log record. Again, the affected page, P600, is\nfetched and the pageLSN is cornpared to the LSN of the update log record. In\nthis case, because we assurned thatP600 was written to disk before the crash,\nthey are equal, and the update does not have to be redone.\nThe rernaining log records are processed sirnilarly, bringing the systern back\nto the exact state it was in at the tirue of the cra,5h. Note that the first hvo\nconditions indicating that a redo is unnecessary never hold in this exaruple.\nIntuitively, they corne into play when the dirty page table contains a very old\nrecLSN, going back to before the rJlost recent checkpoint. In this case, as Iledo\nscans forwa.rd frorn the log record with this LSN, it encounters log records for\npages that were written to disk prior to the checkpoint and therefore not in\nthe dirty page table in the checkpoint.\nSorne of these pages Inay be dirtied\nagain after the checkpoint; nonetheless, the updates to these pages prior to the\ncheckpoint need not be redone. Although the third condition alone is sufficient\nto recognize that these updates need not be redone, it requires us to fetch\nthe affected page. 'The first t\\VO conditions allc)\\v us to recognize this situation\n\\vithout fetching the page.\n(The reader is encouraged to construct exaulples\nth,lt illustrate the use of each of these conditions; see Exercise 18.8.)\n\n592\n.At the end of the Iledo phase, end type records are written for an transactions\nwith status C, which are rCllloved '£1'01n the transaction table.\n18.6.3\nUndo Phase\nThe 1Jndo phase, unlike the other two pha..')cs, scans backward fronl the end\nof the log.\nThe goal of this phase is to undo the actions of all transactions\nactive at the tilne of the cra..sh, that is, to effectively abort the1n. This set of\ntransactions is identified in the transaction table constructed by the AIlalysis\nphase.\nThe Undo Algorithm\nUndo begins with the transaction table constructed by the .Analysis phase,\nwhich identifies all transactions active at the tiIne of the crash, and includes the\nLSN of the 1110st recent log record (the lastLSN field) for each such transaction.\nSuch transactions are called loser transactions. All actions of losers IllUst be\nundone, and further, these actions rnust be undone in the reverse of the order\nin which they appear in the log.\nConsider the set of lastLSN values for all loser transactions. Let us call this set\nToUndo. Undo repeatedly chooses the largest (Le., rnost recent) LSN value in\nthis set and processes it, until rro1Jndo is ernpty. To process a log record:\n1. If it is a CLR and the undoNextLSN value is not null, the undoNextLSN\nvalue is added to the set ToUndo; if the undoNextLSN is null, an end\nrecord is written for the transaction because it is cornpletely undone, and\nthe CLR, is discarded.\n2. If it is an. update record, a CLR, is written and the corresponding a,ction is\nundone, as described in Section 18.2, and the prevLSN value in the update\nlog record is added to the set ToUndo.\n\\i\\lhen the set rroUndo is ernpty, the lJndo phas(~ is cornplete. I{estart is no\\v\ncornplete, and the systenl can proceed 'with nonnal operations.\nLet us continue with the scenario discussed in Sections 18.6.1 and 18.6.2. The\nonlv active tratlsaction at the tiTne of the cra,sh\\vas detennined to be TI000.\n~J\n.\n, ' -\n-\n.\n,\n.\n..\n.,\n'-'\n-\n.,\n.\n\"\n.\n-\n••\n:F'rorn the transaction table, vve get the LSN of its Inost recent log record, \\vhich\nis the fourth update log record in Figure 18.3. '1'he npdn.te is undone, and a\nCL11 is \\vritten\\vith undoNextLSN equal to the LSN of the first log record in\nthe figure. T'he next record to be undone for transaction i\"TI000 is the first log\nrecord in the figure.\nAfter this is undone, a CLR a,nel an end log record for\n7\n11000 <trewritten, <tnd the IJndo phase is cornplete.\n\nCT'ash ReC()7.]CT'!i\n59,3\nIn this exarnple, undoing the action recorded in the first log record causes the\naction of the third log\nrecor~l, \\vhich is due to a conunitted traJlsaetioIl, to be\noverwritten and thereby lost!\nrrhis situation arises because 1\"'2000 overvvrote\na data itcrIl \\vritten by TIOOO while 1''1000 W(l..'3 still active; if Strict 2PLwere\nfollo\\ved, 1'2000 \\vould not have been allowed to overwrite this data iterH.\nAborting a Transaction\nAborting a. transaction is just a special case of the Undo phase of Restart in\n\\vhich a single transaction, rather than a set of transactions, is undone. The\nexarnple in Figure 18.5, discussed next, illustrates this point.\nCrashes during Restart\nIt is important to understand how the lTndo algorithrn presented in Section\n18.6.3 handles repeated systern crashes. Because the details of precisely how\nthe action described in an update log record is undone are straightforward,\nwe discuss Undo in the presence of systern crashes using an execution history,\nshown in Figure 18.5, that abstracts away unnecessary detail.\nThis exarnple\nillustrates how aborting a transaction is a special case of Undo and how the use\nof CLRs ensures that the Undo action for an update log record is not applied\ntwice.\nLSN\nLOG\n00, os -;-\nbegin_checkpoint, end_checkpoint\n10 -r\nupdate: Tl writes P5\nprevLSN\n20\n-F-\nupdate: T2 writes P3\n30\nTl abort\n.-\n\\\nI\n40,45\n-F-\nCLR: Undo T1 LSN 10, T1 tnd\nundonextLSN\n50\n-..,......\nupdate: T3 writes PI\ni\nI\nj\n!\nI\n60 -......\nupdate: 1'2 writes P5\n....../\n><\nCRASH, RESTART\n70\nCLR: l1ndo '1'2 LSN 60\nso, 85\nCLR: Undo '1'3 LSN 50, '1'3 end\n><\nCRASH, RR~TART\n90,95\nCLR: lJndo T2 LSN 20, T2 end\nFigure 18.5\nExC'uTlple of Undo with Repeatt!d\nC~rashes\n\n594\n(;IIAPTEH. J8\n'rhe log shcnvs the order in \\vhich theDB~IS executed various actions; note that\nthe LSNs are in ascending order ~ and that eal~h log record for a transaction ha..'3\na prevLSN' field that points to the previous log record for that transaction.\n\\~Te\nhave not shown 'n/ull prevLSNs, that is, SOIne special '\\lailleused in the prevLSN\nfield of the first log record for a, transaction to indicate tha,t there is no previous\nlog record.\nvVe also cOlnpacted the figure by occasionally displaying hvo log\nrecords (separated by a cOIlllna) on a single line.\nLog record (with LSN) 30 indicates that Tl aborts. All actions of this trans-\naction should be undone in reverse order, and the only action of ~r1, described\nby the update log record 10, is indeed undone as indicated by CLR, 40.\nAfter the first crash, Analysis identifies F)l (with recLSN 50), P~3 (with recLSN\n20), and P5 (with recLSN 10) as dirty pages. Log record 45 shows that Tl is a\ncornpleted transaction; hence, the transaction table identifies T2 (with lastLSN\n60) andT3 (with lastLSN' 50) as active at the tirne of the crash.\n'1'he Redo\nplu:1...se begins with log record 10, which is the rninirnurn recLSN in the dirty\npage table, and reapplies all actions (for the update and CLR, records), as per\nthe Redo algorithIl1 presented in Section 18.6.2.\nThe r1'olJndo set consists of LSNs 60, for 1'12, and 50, for\nT~3. The lJndo phase\nnow begins by processing the log record with LSN 60 because 60 is the largest\nLSN in the ToUndo set.\nThe update is undone, and a CLR, (with LSN 70)\nis written to the log.\n~rhis CLIl has llndoNextLSN equal to 20, which is the\nprevLSN value in log record 60; 20 is the next action to be undone for 112. Now\nthe largest rernaining LSN in the rroundo set is 50. The\n\\vritE~ corresponding\nto log record 50 is now undone, and a CLH, describing the change is 'written.\nrrhis eLH, has LSN 80, and its undoNextLSN field is null because 50 is the\nonly log record for transaction T3. Therefore T~3 is cOITlpletely undone, and an\nend record is written. Log records 70, 80, and 85 aTe written to stable storage\nbefore the systern crct.shes a second tirHe; ho\\vever, the changes described by\nthese records ITlay not have been written, to disk..\n\\\\lhen the systern is resta.rted after the S8C011(1 cra,,')}L Analysis deterrnines that\nthe only active transactioIl at the tirne of the crash \\\\1[1.'3 'T2; in addition, the dirty\npa,ge table is identicaJ to \\\\That it \\VEtS during the previous restart. Log records\n10 througll 85 are processed Etgain during Itedo. (If sorne of the changes rnade\nduring the prcv\"ious H,edo were vvritten to disk, the pageLSN's on the affected\npages are used to detect this situation and avoid writing these pages again.)\nT'he lJndo phase considers the onlyLSN in the TolJndo set, 70, and processes it\n}))\" adding tIle llndoNextLSN value (20) to the 1'olJndo set. Next, log record 20\nis processed l)y llndoing~r2's \"\\Trite of page }J:.3, a.nd a. CIJl, is VvTitten (LSN 90).\nBecause 20 is the first of 7'2's log records\nand therefore, the laAst of its records\n\nto be undone~~the undoNextLSN field in this CLR IS ntlll, an end record IS\nwritten for T2, al1d the TolJndo set is no\\v E:~rnpty.\nllecovery is no\\v cornplete, and norrnal execution can resurne vvith the ·writing\nof a checkpoint record.\nThis exarnple illustrated repeated crashes during the lJndo phage.\nIi()l' corn-\npleteness, let us consider vvhat happens if the s)'stern cra,'3hes ·while R,estart is\nin the Analysis or Iledo pha.se. If a crash occurs during the Analysis phase, all\nthe work done in this phase is lost, and on restart the Analysis phase starts\nafresh vvith the sa11le inforrnation as before. If a cr&'3h occurs during the Redo\nphase, the only effect that survives the cra...9h is that sorne of the changes rnade\nduring Redo 11U1Y have been written to disk prior to the crash. R,estart starts\nagain with the Analysis phase and then the Redo phase, and sorne update log\nrecords that were redone the first tirne around will not be redone a second tirne\nbecause the pageLSN is now equal to the update record's LSN (although the\npages have to be fetched again to detect this).\nWe can take checkpoints during llestart to rninirnize repeated work in the event\nof a crash, but we do not discuss this point.\n18.7\nMEDIA RECOVERY\nMedia recovery is based on periodically rnaking a copy of the database.\nBe-\ncause copying a large database object such as a file CeHl take a long tirHe, and\nthe I)BMS rnust be allowed to continue vvith its operations in the Ineantirne,\ncreating a copy is handled in a rnanner sirnilar to taking a fuzzy checkpoint.\n\\\\Then a databa.se object such as a file or a page is corrupted, the copy of that\nobject is brought up-to-date by using the log to identify and reapply the changes\nof cornnlitted transactions and undo the changes of uncoI1unitted transactions\n(as of the tirne of the rnedia recovery operation).\nThe begilLchecl<.point LSN of the rnost recent cOlllplete checkpoint is recorded\nalong \\vith the copy ()f the database object to luinirnize the vvork in reapplying\nchaxlges of\ncornrnitt(~d transactions.\nLet us COlnpare the sl11<:.111est recLSN of\na dirty page in the corresponding encLcheckpoint record \\vith the I;SN of the\nbegirLcheckpoint record and call the slua.ller of these tvvo LSNs I. \\Ve observe\nthat the actions recorded in all log records with LSNs less thaD I Inust be\nref1ectE~(l in the copy. Thus: 0111y log records \\vithLSNs greater than I need be\nreapplied to the copy.\n\n/\"\"()6\nD...\nC~HAPTg,R 18\nFinally, the updates of transactions that are incornplete at the tiIl1e of Inedia\nrecovery or that \\vere aborted after the fuzzy copy \\va\" corllpleted need to be\nundone to ensure that the page reflects only the actions of conunitted transac-\ntions. The set of such transactions can be identified\n(1...'3 in the Analysis pass,\nand we ornit the details.\n18.8\nOTHER APPROACHES AND INTERACTION WITH\nCONCURRENCY CONTROL\nLike ARIES, the Inost popular alternative recovery algoritlllns also rnaintain a\nlog of databa.'3e actions according to the \\VAL protocol.\nA InajaI' distinction\nbetween ARIES and these variants is that the Redo phase in ARIES repeats\nhistory, that is, redoes the actions of all transactions, not just the non-losers.\nOther algorithms redo only the non-losers, and the Redo phase follows the\nUndo phase, in which the actions of losers are rolled back.\nThanks to the repeating history paradigm and the use of CLRs, ARIES sup-\nports fine-granularity locks (record-level locks) and logging of logical operations\nrather than just byte-level rnodifications. For exalllple, consider a transaction\nT that inserts a data entry 15* into a B+ tree index. Between the tirne this\ninsert is done and the time that T is eventually aborted, other transactions Inay\nalso insert and delete entries frorn the tree. If record-level locks are set rather\nthan page-level locks, the entry 15* I11ay be on a different physical page when\nT aborts fr0111 the one that T inserted it into. In this case, the undo operation\nfor the insert of 15* lllUSt be recorded in logical tenns because the physical\n(byte-level) actions involved in undoing this operation are not the inverse of\nthe physical actions involved in inserting the entry.\nI.Jogging logical operations yields considerably higher concurrency, although the\nuse of fine-granularity locks can lead to increased locking activity (because rnore\nlocks 1nust be set).\nHence: there is a trade-off between different \\VAL-b<.k\"led\nrecovery schclnes. vVe chose to cover ARIES because it has several attractive\nproperties, in pa.rticular, its sirnplicity and its ability to support fine-granularity\nlocks and logging of logical operations.\nOne of the earliest recovery algorithrns, llsed in the Syster11 R, prototype at\nIBI\\'1, takes a\nv(~ry different approach.\n'There is no logging and, of course,\nno \\VAL protocol.\nInstead, the database is treated as a collection of pages\nand accessed thTough a page table, which IIHtpS page ids to disk addresses.\n\\Vhen a transaction Inakes changes to\n<'1 data pagel it actually Inakes a copy\nof the page, called the shadow of the page, a,nel changes the shadow page.\nThe transaction copies the appropriate part of the page table and chan,ges the\nentry for the changed page to point to the shadov,r, so that it can see the\n\nchanges; ho\\vever ~ other transactions continue to see the original page table,\nand therefore the original page, until this transaction COll1lnits.\nAborting a\ntransaction is sirnple: .Just discard its shadcnv versions of the page table and\nthe data pages. Cornrnitting a transaction involves rnaking its version of the\npage table public and discarding the original data pages that are superseded\nby shado\\v pages.\nThis schelue suffers frorn a nUlnber of problerlls.\nFirst, data becornes highly\nfragrnented clue to the replacernent of pages by shado\"v versions, \"vhich rIlay be\nlocated far fr01n the original page. This phenornenon reduces data clustering\nand rnakes good garbage collection irnperative. Second, the sche1ne does not\nyield a sufficiently high degree of concurrency.\nrrhird, there is a, substantial\nstorage overhead due to the use of shadow pages.\n~burth, the process aborting\na transaction can itself run into deadlocks, and this situation rllust be specially\nhandled because the sernantics of aborting an abort transaction gets rnurky.\nFor these reasons, even in Systern R, shadow paging was eventually superseded\nby \\VAL-based recovery techniques.\n18.9\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n..\nWhat are the advantages of the ARIES recovery algoritluu? (Section 18.1)\nII\nDescribe the three steps in crash recovery in ARIES? What is the goal of\nthe Analysis phase? The redo phase? The undo phase? (Section 18.1)\nII\n\\lVhat is the LSN of a log record? (Section 18.2)\nII\n\\Vhat are the different types of log records <:l,,11d when are they written?\n(Section 18.2)\nifill\n\"Vhat inforrnation is rnaintained in the transaction table and the dirty page\ntable? (Section 18.3)\nifill\nvVhat is\\Vrite- Ah(~ad Logging? \\Vhat is forced to disk at the tirne a trans-\naction COIlllnits? (Section 18.4)\n•\n\\~That is a fuzzy checkpoint? \\Vhy is it useful? \"\\That is a rnaBter log record?\n(Section 18.5)\nIII\nIn \\vhich direction does the .A.nalysis phase of recovery scan the log? At\n\\vhich point in the log does it begin and end the scan? (Section 18.6.1)\nII\nDescril)c \\vhat infonnation is gathered in the Analysis pha,se and ho\\v.\n(Section 18.6.1)\n\n598\nCHAPTER IS\n•\nIn \\vhich direction does the Redo phase of recovery process the log?\nAt\nwhich point in the log does it begin and end? (Section 18.6.2)\n•\nWhat is a redoable log record?\nUnder what conditions is the logged ac-\ntion redone? \\Vhat steps are carried out when a logged action is redone?\n(Section 18.6.2)\n•\nIn which direction does the Undo phase of recovery process the log? At\nwhich point in the log does it begin and end? (Section 18.6.3)\n•\nWhat are loser transactions? How are they processed in the Undo phase\nand in what order? (Section 18.6.3)\n•\nExplain what happens if there are crashes during the Undo phase of re-\ncovery. What is the role of CLR.s? What if there are ~rashes during the\nAnalysis and Redo phases? (Section 18.6.3)\nII\nHow does a DBlV1S recover from 111edia failure without reading the complete\nlog? (Section 18.7)\n•\nRecord-level logging increases concurrency. What are the potential prob-\nlems, and how does ARIES address them? (Section 18.8)\nII\nWhat is shadow paging? (Section 18.8)\nEXERCISES\nExercise 18.1 Briefly answer the following questions:\n1. How does the recovery rnanager ensure atornicity of transactions? How does it ensure\ndurability?\n2. What is the difference between stable storage and disk?\n:3. What is the difference between a systenl crash and a uledi\"l failure?\n4. Explain the vVAL protocol.\nt). Describe the steal and no-force policies.\nExercise 18.2 Briefly answer the follO\\ving questions:\n1. v\\That are the properties required of LSNs?\n2. \\\\That arc the fields in an update log record? Explain the use of each field.\n:3. VVhat are redoal)le log records?\n4. vVha.t are the differences between update log records and CLRs?\nExercise 18.3 Briefly answer the following questions:\n1. \\Vhat are the roles of the Analysis, Iledo 1 and Undo phases in AHlES?\n2. (;onsider the execution shown in Figure 18.6.\n\nLSN\nLOG\n00\nbegin__checkpoint\n10 -\nend_cbeckpoint\n20 -\nupdate: Tl writes P5\n30 ---\nupdate: T2 writes P3\n40\n.....-.-\nT2 commit\nSO\nT2end\n60\n.....-.-\nupdate: T3 writes P3\n70\nTl abort\n><\nCRASH, RESTART\nFigure 18.6\nExecution with a Crash\nLSN\nLOG\n00 -r-\nupdate: Tl writes P2\n10\n-l-\nupdate: Tl writes PI\n20 -\nupdate: T2 writes P5\n30\nupdate: T3 writes P3\n40\nT3 commit\n50 1-\nupdate: T2 writes PS\n60 i\nupdate: T2 writes P3\n70\n-r-\nT2 abort\nFigure 18.7\nAborting a Transaction\n(a) What is done during Analysis?\n(Be precise about the points at which Analysis\nbegins and ends and describe the contents of any tables constructed in this phase.)\n(b) What is done during Redo? (Be precise about the points at which Redo begins and\nends.},\n(c) 'VVhat is done during Undo?\n(Be precise about the points H.t which Undo begins\nand ends.)\nExercise 18.4 Consider the execution shown in Figllre 18.7.\n1. Extend the figure to shuw prevLSN and llndonextLSN values.\n2. Describe the actions taken to rollback transaction 7\"'2.\n\n600\nC~HAPTER 1B\nLSN\nLOG\n00 -\nbegin_checkpoint\n10\nend_checkpoint\n20\n7\nupdate: 1'1 write..~ PI\n30 ---\nupdate: 1'2 writesP2\n40 ---\nupdate: 1'3 writes P3\n50 -\n1'2 commit\n60 ---\nupdate: 1'3 writes P2\n70\n......0-\n1'2 end\n80 -\nupdate: 1'1 writes P5\n90\n-!-\n1'3 abort\n><\nCRASH,RESTART\nFigure 18.8\nExecution with Multiple Cra...,hes\n3. Show the log after T2 is rolled back, including all prevLSN and undonextLSN values in\nlog records.\nExercise 18.5 Consider the execution shown in Figure 18.8. In addition, the systerll crashes\nduring recovery after writing two log records to stable storage and again after writing another\ntwo log records.\n1. What is the value of the LSN stored in the master log record?\n2. What is done during Analysis?\n3. What is done during Redo?\n4. \\Vhat is done during Undo?\n,5. Show the log when recovery is complete, including all non-null prevLSN and unclonextLSN\nvalues in log records.\nExercise 18.6 Briefly answer the following questions:\n1. How is checkpointing done in ARIES?\n2. Checkpointing can also be done as follows: Quiesce the systerll so that only checkpointing\nactivity can be in progress, write out copies of all dirty pages, and include the dirty page\ntable and trallsaction table in the checkpoint record. \\\\lhat are the pros and cons of this\napproach versus the checkpointiug a,pproach of ARIES?\n:3. \\Vhat happens if a second begiILcheckpoint record is encountered during the Analysis\nph<-1..13e?\n4. C;an a second en(Lcheckpoint record be encountered during the AnaJysis phase?\n5.\n\\iVh,Y is the use of CLRs irnportant for the use of undo actions that are not the physical\ninverse of the original update?\n\nFigure 18.9\nLSN\nLOG\n00 -'-\nbegin_checkpoint\n10\n-!-\nupdate: Tl writes PI\n20 +\n'1'1 commit\n30 -+\nupdate: T2 writes P2\n40 T\n'1'1 end\n50 -\n'1'2 abort\n60 -r\nupdate: '1'3 write'ii P3\n70\nend_checkpoint\n80 --\n'1'3 commit\n><\nCRASH, RESTART\nLog Records between Checkpoint Records\n601\nf;\n6. Give an example that illustrates how the paradigm of repeating history and the use of\nCLRs allow ARIES to support locks of finer granularity than a page.\nExercise 18.7 Briefly answer the following questions:\n1. If the system fails repeatedly during recovery, what is the rrlaximum nunlber of log\nrecords that can be written (as a function of the number of update and other log records\nwritten before the crash) before restart cOInpletes successfully?\n2. What is the oldest log record we need to retain?\n3. If a bounded amount of stable storage is used for the log, how can we always ensure\nenough stable storage to hold all log records written during restart?\nExercise 18.8 Consider the three conditions under which a redo is unnecessary (Section\n20.2.2).\n1. \\Vhy is it cheaper to test the first two conditions?\n2. Describe an execution that illustrates the use of the first condition.\n;3. Describe an execution that illustrates the use of the second condition.\nExercise 18.9 The description in Section 18.6.1 of the Analysis pha,se rnade the sirnplifying\nassulTlptioll that no log records appeared between the begill-checkpoint and end_checkpoint\nrecords for the Inost recent cOlnplete checkpoint. The following questions explore how such\nrecords should be handled.\n1. Explain why log records could be written between the begiIl-checkpoint and eneLcheckpoint\nrecords.\n2. Describe how the Analysis phase could be Inodified to handle such records.\n;3. Consider the execution sho\\vn in\nl~\"igure 18.9. Show the contents of the encLcheckpoint\nrecord.\n4. Illustrate your rnodified Analysis pha.se on the execution shown in Figure 18.9.\n\n602\nExercise 18.10 Ans\\ver the following questions briefly:\nC~HAPTER IB\n1. Explain how 1nedin recovery is handled in ARIES.\n2. \\Vhat are the pros (I,nd cons of using fuzzy durnps for nledia recovery?\n~1. \\Vhat are the sirYlilarities and differencf~s between checkpoints and fuzzy chunps?\n4. Contrast ARIES with other vVAL-based recovery schernes.\n5. Contrast AHlES with shadcrw-page-bcLsed recovery.\nBIBLIOGRAPHIC NOTES\nOur discussion of the ARIES recovery algorithm is based on [544].\n[282] is a survey article\nthat contains a very readable, short description of ARIES. [541, 545] also discuss ARIES.\nFine,·granularity locking increases concurrency but at the cost of 11101'e locking activity; [542]\nsuggests a technique based on LSNs for alleviating this problerYl.\n[458] presents a for111al\nverification of ARIES.\n[355] is an excellent survey that provides a broader treatrnent of recovery algoritlulls than our\ncoverage, in which we chose to concentrate on one particular algorithrn. [17] considers perfor-\nrnance of concurrency control and recovery algorithrIls, taking into account their interactions.\nThe irnpact of recovery on concurrency control is also discussed in [769].\n[625] contains a\nperforrnance analysis of various recovery techniques. [236] cornpares recovery techniques for\nmain rnerllory database systeulS, which are optirnized for the case that 11l0st of the active data\nset fits in rnain H1ernory.\n[478] presents a description of a recovery algorithm based on write-ahead logging in which\n'loser' transactions cHe first undone and then (only) transactions that corl1nlitted before the\ncrash are redone. Shadow paging is described in [493, 337]. A scherne that uses a cOlnbination\nof shadow paging and in-place updating is described in [624].\n\nPART VI\nDATABASE DESIGN AND TUNING\n\n19\nSCHEMA REFINEMENT AND\nNORMAL FORMS\nWI\"\nWhat problems are caused by redundantly storing information?\n..\nWhat are functional dependencies?\n..\nWhat are nornlal forms and what is their purpose?\n...\nWhat are the benefits of BCNF and 8NF?\n...\nWhat are the considerations in decolllposing relations into appropriate\nnormal forms?\n..\nWhere does normalization fit in the process of database design?\n...\nAre luore general dependencies useful in database design?\n..\nKey concepts: redundancy, insert, delete, and update anomalies;\nfunctional dependency, Armstrong's Axioms; dependency closure, at-\ntribute closure; normal fonns, BCNF, 8NF; decOlnpositions, lossless-\njoin, dependency-preservation; multivalued dependencies, join depen-\ndencies, inclusion dependencies, 4NF, 5NF\nIt is a nlelancholy truth that even great IneIl have their poor relations.\nCharles Dickens\nConceptual database design gives us a set of relation 8chen1&') and integrity\nconstraints (ICs) that can be regarded a,s a good starting point for the final\ndat(1)ase design.\nT'his initial design IHust be refined by taking the lCg into\naccount rnore fully than is possiblc\\vith just the Ell rnodel constructs aIId also\nby considering perforrnance criteria and typical workloads.\nIn this chapter,\nwe c!iscllss how lCs can be used to refine the conceptual schenul produced by\nG05\n\n606\n(]HAprrER 19\ntranslating anER, 1Hodel design into a collection of relations.\n\\Vorkload and\nperforrnance considerations are discussed in Chapter 20.\n\\Ve concentrate on an irnportant class of constraints called flLnct'ional depen-\ndencies.\nOther kinds of les, for exarnple, m/ultival'll,ed dependencies and join\ndependencies, also provide useful inforrnation. They can sOIuetilnes reveal re-\ndundancies that cannot be detected using functional dependencies alone. We\ndiscuss these other constraints briefly.\nThis chapter is organized as follows. Section 19.1 is an overview of the schenla\nrefineInent approach discussed in this chapter. We introduce functional depen-\ndencies in Section 19.2. In Section 19.3, we show how to reason with functional\ndependency information to infer additional dependencies from a given set of\ndependencies.\nWe introduce norlnal forIns for relations in Section 19.4; the\nnormal form satisfied by a relation is a measure of the redundancy in the rela-\ntion. A relation with redundancy can be refined by decomposing it, or replacing\nit with smaller relations that contain the saIne information but without redun-\ndancy. We discuss deco1npositions and desirable properties of decompositions\nin Section 19.5, and we show how relations can be decomposed into smaller\nrelations in desirable normal forms in Section 19.6.\nIn Section 19.7, we present several examples that illustrate how relational\nschemas obtained by translating an ER model design can nonetheless suffer\nfroln redundancy, and we discuss how to refine such schemas to eliminate the\nproblems. In Section 19.8, we describe other kinds of dependencies for databa..se\ndesign. We conclude with a discussion of nornlalization for our case study, the\nInternet shop, in Section 19.9.\n19.1\nINTRODUCTION TO SCHEMA REFINEMENT\nWe now present an overview of the probleIns that schenla refinement is intended\nto address and a refinernent approach based on decolnpositions.\nIledundant\nstorage of inforrnation is the root cause of these problerns. Although decoInpo-\nsition can elirninate redundancy, it can lead to problclns of its o\\vn and should\nbe used with caution.\n19.1.1\nPro~lems Caused by Redundancy\nStoring the SeHne inforrnation redundantly, that is, in l110re than one place\n\\vithin a database, can lead to several problcll1S:\nII\nRedundant Storage: SOU1C iuforInation is stored repeatedly.\n\n607\nII\nUpdate Anomalies: If one copy of sueh repeated data is updated, an\ninconsistency is created unless all copies cu'c sirnilarly updated.\nII\nInsertion Anomalies: It IIU1Y not be possible to store certain inforlnation\nunless sorne other, unrelated, inforIIlatioIl is stored as well.\nII\nDeletion Anomalies: It rnay not be possible to delete certain inforrnation\nvvithout losing SOHle other, unrelated, infofrnation as v'lell.\nConsider a relation obtained by translating a variant of the IIourly_Emps entity\nset frorn Chapter 2:\nHourly_Ernps(:2,sn, na'lne, lot, rating, hOllrly_wages, hOllr.'LwoTked)\nIn this chapter, we ornit attribute type inforrnation for brevity, since our focus\nis on the grouping of attributes into relations. We often abbreviate an attribute\nnarne to a single letter and refer to a relation schema by a string of letters, one\nper attribute. For exarllple, we refer to the Hourly_Ernps scherna as SNLRWH\n( ~V denotes the hov,rly_wages attribute).\n1'he key for Hourly_Emps is ssn. In addition, suppose that the hourly_wages\nattribute is deterrnined by the Tating attribute.\nThat is, for a given 7'CLting\nvalue, there is only one perrllissible houTly_wages value. This IC is an exanlple\nof a functional dependency.\nIt leads to possible redundancy in the relation\nHourly_Ernps, as illustrated in Figure 19.1.\nI narne\n.,\n._ ....._..\n-, --- - ._-\n123-22-3666\nAttishoo\n48\n8\n10\n40\n._.n_··_··~·~···~·\n~..\"\"\"\".'.'.\"\"<-\",,'-\"\"\"'.'\n231-31-5368\nSruiley\n22\n8\n10\n~30\n....'~.\".li...\"'•.\n.__....~.._..._.._..-\n131-24-3650\nSrllethurst\n35\n5\n7\n~30\n-\n434-26-3751\nGuldu\n:35\n5\n7\n~)2\n---\n612-67-4134\n:NIadayan\n35\n8\n10\n40\n._........._- .\n.-\nFigure 19.1\nAn Instance of the Hourly_Emps Relation\nIf the saIne value appears in the rating colurnn of tv'lo tuples, the IC tells us\nthat the sarne value HUlst appear in the hourly_wages colurnn\nc1.'3 well.\nThis\nredundancy has the sarne negative consequences a..s before:\nII\nRedtlndant StoTage: rrhe rating value 8 corresponds to the hourly wage 10,\nand this (L.':lsociation is repeated three tirnes.\nII\n[Tpdate AnoTno1ic,,: The hO'lLTlY_'l1HLgcs in the first tuple could be updated\nwithout rnaking a sirnilar change in the second tuple.\n\n608\nCHAPT'ER 19\nt\n•\nInseT\"lion Ano'mal'ie,';: VVe cannot insert a tuple for an crnployee unless \\ve\nknow the hourly wage for the ernployee's rating value.\n•\nDelet'ion finon-LaUe..,: If \\ve delete all tuples vvith a given rating value (e.g.,\nwe delete the tuples for Snlcthurst and Guldu) \\ve lose the &t;sociation\nbet:\\veen that Tat'ing value and its houTly_wage value.\nIdeally, vve \\vant scherna.s that do not pennit redundancy, but at the very least\nwe want to be able to identify schernas that do allow redundancy. Even if we\nchoose to accept a scherna vvith sorne of these drawbacks, perhaps owing to\nperforlnance considerations, we want to rnake an infonned decision.\nNull Values\nIt is worth considering whether the use of null values can address some of these\nproblems. As we will see in the context of our exarnple, they cannot provide a\ncomplete solution, but they can provide sorne help. In this chapter, we do not\ndiscuss the use of null values beyond this one exarnple.\nConsider the example Hourly_Elnps relation. Clearly, null values cannot help\neliminate redundant storage or update anomalies.\nIt appears that they can\naddress insertion and deletion anomalies. For instance, to deal with the inser-\ntion anolnaly exarnple, we can insert an elTIplayee tuple with null values in the\nhourly wage field. However, null values cannot address all insertion anornalies.\nFor exarnple, we cannot record the hourly wage for a rating unless there is\nan ernployee with that rating, because we cannot store a null value in the ssn\nfield, which is a prirnary key field. Sinlilarly, to deal with the deletion anomaly\nexarnple, we rnight consider storing a tuple with null values in all fields except\nTat'ing and hourly_wages if the last tuple with a given rating would otherwise\nbe deleted.\nHowever, this solution does not work because it requires the\n8871,\nvalue to be null, and prirnary key fields cannot be null. Thus, rl/ull values do\nnot provide a general solution to the problerns of reclundancy, even though they\ncan help in sorne cases.\n19.1.2\nDecompositions\nIntuitively, redundancy arises \\vhen a relational schcrna forces an association\nbet\\veen attributes that is not natural. Functional dependencies (and, for that\nrnatter, other Ies) can 'be used to identify such situations and suggest re£1ne-\nrnents to the scheruEL The essential idea is that rnany problerns arising fr0111 re-\ndundancy carl be addressed by replacing a relation 'with a collection of'srnaller'\nrelations.\n\nScheTna Refine'Tnent and NOTlnal ]?o'T'rns\n609\nA. decomposition of a relation schema It consists of replacing the relation\nscherna by t\\VO (or 1no1'e) relation schcrnas that each contain a subset of the\nattributes of R and together include all attributes in R. Intuitively, \\eve \\evant\nto store the inforrnation in any given instance of R by storing projections of\nthe instance. This section exalnines the use of decornpositions through several\nexanlples.\nvVe can decornpose lIourly_Ernps into two relations:\nIfourly_Ernps2C~.:~n~naTne, lot, 'rating, hOUr\\'Lwo'rked)\n\\Vages(rating, hourly_wages)\nThe instances of these relations corresponding to the instance of Hourly_E1nps\nrelation in Figure 19.1 is shown in Figure 19.2.\n'--['-ioT] 'rating [ hours_lLJorked-\nn\n]\nI narne\n..\n._-\n.....\"\"....,,,..........\n123-22-3666\nAttishoo\n48\n8\n40\n-\n231-31-5368\nSluiley\n22\n8\n30\n-_..._......\n_...\n131-24-3650\nSmethurst\n35\n5\n30\n...-.,...-.....-...\"'....\"......,...._-\n..._._...\n434-26-3751\nGuldu\n35\n5\n32\n-_...~._--_...\n612-67-4134\nMadayan\n35\n8\n40\n[ ssn\nI rating\nl!!ourly~'u)ages\nFl_~_no\n_\nFigure 19.2\nInstances of Hourly...Emps2 and vVages\nNote that we can easily record the hourly wage for any rating sirnply by adding\na tuple to \\;Vages, even if no ernployee with that rating appears in the cur-\nrent instance of flourly_Ernps.\nChanging the wage associated \\vith a rating\ninvolves updating a single Wages tuple. This is rnore efficient than updating\nseveral tuples (as in the original design), and it elirninates the potential for\ninconsistency.\n19.1.3\nProblems Related to Decomposition\nlJnless \\ve are careful~ decornposing a relation scherna can create 1n01'e problerns\nthan it solves. rrvVO irnportant questions llHlst be asked repeatedly:\n1. 1)0 vve need to decornpose a relation?\n\n610\n2. \\\\That problerns (if any) does a given deeornposition cause?\nCHAPTER 19\nTo help \\vith the first question, several norrrtal j'O'rnl,8 have been proposed for\nrelations. If a relation scherna is ill one of these nOfrual 1'orrns, we knovv that\ncertain kinds of problerlls cannot arise. Considering the norrnal forrn of a given\nrelation scherna can help us to decide \\vhether or not to decornpose it further. If\nvve decide that a relation scherna 111USt be decornposed further, vve rnust choose\na particular dec()lnposition (I.e., a particular collection of slnaller relations to\nreplace the given relation).\nWith respect to the second question, two properties of decornpositions are\nof particular\ninter(~st.\nThe lossless-join property enables us to recover any\ninstance of the decornposed relation froln corresponding instances of the s111aller\nrelations.\nThe dependency-preservation property enables us to enforce any\nconstraint on the original relation by sinlply enforcing SaIne contraints on each\nof the srnaller relations.\nThat is, we need not perform joins of the slllaller\nrelations to check whether a constraint on the original relation is violated.\nFrom a performance standpoint, queries over the original relation may require\nus to join the decomposed relations. If such queries are common, the perfor-\nrnance penalty of decomposing the relation may not be acceptable.\nIn this\ncase, we may choose to live with some of the problems of redundancy and not\ndecompose the relation. It is important to be aware of the potential problerns\ncaused by such residual redundancy in the design and to take steps to avoid\nthern (e.g., by adding SaIne checks to application code).\nIn sonle situations,\ndecomposition could actually improve performance. This happens, for exam-\nple, if lnost queries and updates exanline only one of the decornposed relations,\nwhich is srnaller than the original relation.\nvVe do not discuss the irnpact of\ndecompositions on query perforInance in this chapter; this issue is covered in\nSection 20.8.\n()ur goal in this chapter is to explain S0111e powerful concepts and design guide-\nlines b&'3ed on the theory of functional dependencies. A good datab&'3e designer\nshould have a firm grasp of nor1nal fonns and \\vhat problerns they (do or do\nnot) alleviate, the technique of decornposition, and potential problerns vvith\ndecornpositions. For exaInple, a designer often asks questions such &'3 these: Is\na relation in a given nonnal forIn? Is a decornposition clependency-preserving?\nOur objective is to explain when to raise these questions and the significance\nof the answers.\n\nScherna Refine'rnent and NornuIl }'OT'lnS\n19.2\nFUNCTIONAL DEPENDENCIES\n611\nA functional dependency (FD) is a kind of Ie that generalizes the concept\nof a key.\nLet R be a relation scherna and let ..¥\" and Y be nonernpty sets of\nattributes in R. We say that an instance r of R satisfies the FDX ~ }i 1 if the\nfollowing holds for every pair of tuples tl and t2 in r-.\nIf t1.X = t2 ..X, then tl.}T = t2.Y'\".\nw(~ use the notation tl.X to refer to the projection of tuple t1 onto the at-\ntributes in .<\\'\", in a natural extension of our TIlC notation (see Chapter 4) t.a\nfor referring to attribute a of tuple t. An FD X\n----7 Yessentially says that if two\ntuples agree on the values in attributes X, they 111Ust also agree on the values\nin attributes Y.\nFigure 19.3 illustrates the rneaning of the FD AB ----7 C by showing an instance\nthat satisfies this dependency. The first two tuples show that an FD is not the\nsame as a key constraint: Although the FD is not violated, AB is clearly not\na key for the relation. The third and fourth tuples illustrate that if two tuples\ndiffer in either the A field or the B field, they can differ in the C field without\nviolating the FD. On the other hand, if we add a tuple (aI, bl, c2, dl) to the\ninstance shown in this figure, the resulting instance would violate the FD; to\nsee this violation, compare the first tuple in the figure with the new tuple.\n--\n...-.........................\n...\"\"\"......................\"N\na1\nb1\nc1\nd1\na1\nb1\nc1\nd2\na1\nb2\nc2\ndl\na2\nbl\nc3\nell\n...............\"\"...............,.\"'\"\n' ........ n ....' ...................-.---- -\"-\nFigure 19.3\nAn Instance that Satisfies AB -Jo C\nIlecall that a legal instance of a relation nUlst satisfy all specified les, including\nall specified FDs. As noted in Section 3.2, Ies rIlust be identified and specified\nba...sed on the sernantics of the real-world enterprise being n1odeled. By looking\nat an instance of a relation, we rnight be able to tell that a certain FD does not\nhold. I-Iowever; we C<-:l.Tl never deduce that an FD docs hold by looking at one\nor 1I10re instances of the relation, beca,use an FD, like other les, is a staternent\nabout all possible legal instances of the relation.\n1X _._, Y is re,lel\naAS X fu'nctionally deteTrninc8 Y, or simply a..s X determ'ines Y.\n\n612\nC~HAP'TgR 19\n.A prirnary key constraint is a special ease of an .1\"1). The attributes in the key\nplay the role of X, and the set of all attributes in the relation plays the role of\nY. Note, ho\\vever, that the definition of an FD does not require that the set ..Y\"\nbe 111iniInal; the additionalrninimality condition Illust be Inet for\n-'~ to be a key.\nIf ..~ ----+Y holds, \\vhere Y\" is the set of all attributes, and there is SCHne (strictly\ncontajned) subset llof .iJ( such that 1/ ----+\n},~ holds, then ..X\" is a 81LIJerkey.\nIn the rest of this chapter, ·we see several exarIlples of FDs that are not key\nconstraints.\n19.3\nREASONING ABOUT FDS\nGiven a set of FDs over a relation scheula .R, typically several additional FDs\nhold over R whenever all of the given FDs hold. As an exalnple, consider:\nWorkersC~~n, naTne, lot, did, since)\nWe know that ssn ----+ did holds, since ssn is the key, and FD did ----+ lot is given\nto hold.\nTherefore, in any legal instance of Workers, if two tuples have the\nsame ssn value, they Blust have the sarne did value (frolH the first FD), and\nbecause they have the sarrle did value, they must also have the saIne lot value\n(1'1'0111 the second FD). Therefore, the FD ssn ----+ lot also holds on Workers.\nWe say that an FD f is implied by a given set F of FDs if f holds on every\nrelation instance that satisfies all dependencies in F; that is, f holds whenever\nall FDs in F hold. Note that it is not sufficient for f to hold on SaIne instance\nthat satisfies all dependencies in F; rather, f rnust hold on every instance that\nsatisfies all dependencies in P'.\n19.3.1\nClosure of a Set of FDs\nThe set of all .FDs irnplied by a given set F of FDs is called the closllre of\nJi-', denoted as .F'+. An irnportant question is how we can infer, or cornpute,\nthe closure of a given set ]? of FDs.\n~rhe answer is sirnple and elegant.\nThe\nfolhwving three rules, called Armstrong's Axioms, can be applied repeatedly\nto infer all FI)s irnplied by a set ]? of FDs. \\lVe use ..,\"\"Y, and Z to denote scts\nof attributes over a relation scherna It:\n\\I\nReflexivity: If X ~\n}T, then X\n----+ Y.\n\\I\nAugn1.entation: If )( -'tY, then ..YZ ---t\n}TZ for any Z.\n11II\nTransitivity: If)( ----+Y (urd Y\n\nSche-rna Rejinenu:nt and lVoT'lnall?oTTns\n61~\nTheorem 1 AT1nstrong'8 A:riorns are sound J in that they gene1ute only 1t'Ds\nin F\"+- 'when apT)l'ied to a set, F of j?D8. They are also completeJ in that repeated\na]Jplicat'ion afthese T\"ules 1JJill generate all FDs in the ClOS'U7\"'e .Fl+.\nThe soundness of Arrnstrong's Axiorns is straightfor\\vard to prove. Cornplete-\nness is harder to show; see Exercise 19.17.\nIt is convenient to use SOlne additional rules while rea..soning about P+:\n•\nUnion: If X ~ Yand X ~ Z, then X ~ YZ.\n•\nDecomposition: If X -+ YZ, then X ~ y' and X -7 Z.\nThese additional rules are not essential; their soundness can be proved using\nArmstrong's AxiolllS.\nTo illustrate the use of these inference rules for FDs, consider a relation schelua\nABC with FDs A ····-*B and B -7 C. In a trivial FD, the right side contains\nonly attributes that also appear on the left side; such dependencies always hold\ndue to reflexivity. Using reflexivity, we can generate all trivial dependencies,\nwhich are of the form:\nX ~ Y, where Y ~ X, X ~ ABC, and Y ~ ABC.\nFrOHl transitivity we get A -+ C. Fronl auglnentation we get the nontrivial\ndependencies:\nAC--->- BG\nY\n, AB -7 AC', AB -7 C13.\nAs another exalnple, we use a rnore elaborate version of Contracts:\nContracts(!:..9ntractid, supplierid, pro,jectid, dept'id, partid, qty, val'ue)\n\\Ve denote the schenla for Contracts a..s CSJDPQ V. The rneaning of a tuple is\nthat the contract with contractid C is an agreelnent that supplier S (sv,pplierid)\n'will supply Q iterns of part? (par-tid) to project J (pTo,ject'id) associated with\ndepartrnent D (deptid); the value tl of this contract is equal to value.\nThe following res are known to hold:\n1. 1'he contract id Gl is a key: C -+ CSJDP(J V.\n2. A project purclHlses a given part using a single contract: .II) --+ C\n1\n•\n\n614\n(JHAPTER 19\n3. .A. departInent purcha..'3es a.t most one part froul a supplier: 8D ---+ P.\nSeveral a.dditional FDs hold in the closure of the set of given FDs:\nE'rorIl .IP -} C\\ G'1 -+ C!SJD.PCJ 'V, and transitivity, \"\"VB infer .IP -_..+ CJSJDPCJ V.\nFraIn 8D _...-7 P and augnlentation, we infer SDJ -7 JP.\nFraIn 8DJ -7 .IP, JP -7\nCSJDPQ~r, and transitivity, we infer SDJ ---+ CSJD-\nPQ V. (Incidentally, while it Illay appear tenlpting to do so, we cannot conclude\nSD -7 CSDPQ V, canceling .I on both sides. FD inference is not like aritlunetic\nIllultiplication!)\nWe can infer several additionalFDs that are in the closure by using augruen-\ntation or decomposition. For exarnple, from C----+ CSJDPQ V, using decompo-\nsition, we can infer:\nC -7 C, C -7 5, C -7 J, C -} D, and so forth\nFinally, we have a number of trivial FDs from the reflexivity rule.\n19.3.2\nAttribute Closure\nIf we just want to check whether a given dependency, say, X\n---+ Y, is in the\nclosure of a set\n~F' of FDs, we can do so efficiently without cornputing Fl+. We\nfirst cornpute the attribute closure X+with respect to F, \\vhich is the set\nof attributes A such that X -7 A can be inferred using the Arrnstrong Axioms.\nThe algorithrn for computing the attribute closure of a set X of attributes is\nshown in Figure 19.4.\nclosure = X;\nrepeat until there is no change: {\nif there is an FD U -} V in F such that U ~ closllre,\nthen set clo,sure == closure U v·\n}\nFigure 19.4\nComputing the Attribute Closure of Attribute Sct X\nTheorem 2 The algorithln .shown inF'iguTc 1.9.4 cornputes the attr'ibv,te closure\nX-+-- of the attribute set ~Y 'IDith respect to the sct of }\"1Ds Fl.\n\nScherna RejiTtCrnerd and NOTlnal ForTns\n615\n$\nThe proof of this theorern is considered in Exercise 19.15. This algoriUuIl can\nbe rl10dified to find keys by starting with set .i\\\"\" containing a, single attribute and\nstopping as soon cl,.o;;; ClOS'UTC contains all attributes in the relation scherna. By\nvarying the starting attribute and the order in \\vhich the algorithrIl considers\nFDs, \\ve can obtain all candidate keys.\n19.4\nNORMAL FORMS\nGiven a relation sche111a, we need to decide whether it is a good design or we\nneed to decornpose it into srnaller relations.\nSuch a decision llUlst be guided\nby an understanding of what problenls, if any, arise froln the current schelna.\nTo provide such guidance, several normal forms have been proposed.\nIf a\nrelation schelna is in one of these norrnal forIns, we know that certain kinds of\nproblerlls cannot arise.\nThe nonnal forrns ba...'Sed on FDs are fir-st nor-rnal forrn (1Nfj, second nor-mal\nforrn (2NJ?), thi'td norrnalfor-rn (3NF), and Boyce-Codd nor-rnal for-rn (BCN]?).\nThese fonns have increasingly restrictive requirernents: Every relation in BCNF\nis also in 3NF, every relation in 3NF is also in 2NF, and every relation in 2NF is\nin INF. A relation is in first normal fortH if every field contains only atornic\nvalues, that is, no lists or sets. This requirerllent is iInplicit in our definition\nof the relational rnode!.\nAlthough SOHle of the newer database systerlls are\nrelaxing this requirernent, in this chapter we aSSUlne that it always holds. 2NF\nis Inainly of historical interest. 3NF and BCNF are irnportant frolH a database\ndesign standpoint.\nWhile studying norrnal fonns, it is irnportant to appreciate the role played by\nFDs. Consider a relation scherna I? with f1ttributes ABC:. In the absence of any\nICs, any set of ternary tuples is a legal instance and there is no potential for\nredundancy. ()n the other hand, suppose that \\ve have the FI) A\n--,'> 13. Now if\nseveral tuples have the sarne A value, they rnust also have tllC sarneB value.\nThis potential redundanc:y can be predicted using the FD il1fonnation. If 11101'8\ndetailed 1Cs are specified, \\ve rnay be able to detect rnore subtle redundancies\nas \\vell.\n\\Ve prilnarily discuss redundancy revealed l)y PI) inforrnation. In Section 19.8,\n\\ve discuss 11lore sophisticated 1Cs ca1led rnuUivalued dependencies and join\ndependencies and norrnal forrns based on theIn.\n19.4.1\nBoyce...Codd Normal Form\nLet I? be a relation scherna, 1? be the set ofF'I)s given to hold over R, .iX\" be a\n.. -\n.\nsubset of the attributes ofR, and A be (\\,.11 attribute of I?\nl~ is in Boyce-Codd\n\n616\n(~HAPTERtg\nnormal form if, for everyFl)X -+ A in F, one of the follo\\ving statements is\ntrue:\n•\nA E .<Y; that is, it is a trivial FD, or\n•\nX is a superkey.\nIntuitively, in a BCNF relation, the only nontrivial dependencies are those\nin 'which a key detennines SaIne attribute(s).\nTherefore, each tuple can be\nthought of\nC1....':l an entity or relationship, identified by a key and described by\nthe reluaining attributes. !(ent (in [425]) puts this colorfully, if a little loosely:\n\"Each attribute nlust describe [an entity or relationship identified by] the key,\nthe \\vhole 'key, and nothing but the key.\" If we use ovals to denote attributes\nor sets of attributes and dravv arcs to indicate FDs, a relation in BCNF has\nthe structure illustrated in Figure 19.5, considering just one key for simplicity.\n(If there are several candidate keys, each candidate key can play the role of\nKEY in the figure, with the other attributes being the ones not in the chosen\ncandidate key.)\n--...-::-----\nNonkey attr2\nFigure 19.5\nFDs in a BCNF Relation\nBCNF ensures that no redundancy can be detected using FD infonnation alone.\nIt is thus the Inost desirable norrnal form (fronl the point of view of redundancy)\nif we take into account only FD information. 1'his point is illustrated in Figure\n19.6.\nFigure 19.6\nInstance Illustrating BCNF\nThis figure shc)\\vs (t\\VO tuples in) an instance of a relation with three attributes\nX,\n}T, an.d A. r:Chere a.re t\"vo tuples with the saIne value in the X colurnn. Now\nsuppose that \\ve kno\\v that this instance satisfies an FD -,y._-+ A.\n~re can see\nthat one of the tuples heLl) the value a in the A colurnn.\n\\\\lhat can \\ve infer\nal)out the value in the A colllrnn in the second tuple? 'Using the FI), \\ve can\nconclude that the second tuple also has the value a in this colurnn. (Note that\nthis is really the only kind of inference \\ve can Ina,ke about values in the fields\nof tuples by usingFDs.)\n\nSchc'lna Refinenu:;nt and N(J'r'rnal F'orrns\nBut is this situation not an exaInple of redundancy? \\Ve appear to have stored\nthe value a t\\viee. Can such a situation arise in a BCNF relation? The ans\\ver\nis No! If this relation is in BCNF, because A is distinct fronl ..x:-, it follows that\nX IllU8t be a key.\n(Otherwise, the FD X -+ A \\vould violate BC:NF.) If .IY is\na key, then Yl\n=\nY2, which Ineans that the two tuples are identical Since a\nrelation is defined to be a 8et of tuples, \\\\re cannot have two copies of the saIne\ntuple and the situation shc)\\vn in Figure 19.6 cannot arise.\nrrherefore, if a relation is in BCNF, every field of every tuple records a piece\nof inforlnation that cannot be inferred (using only FDs) frorn the values in all\nother fields in (all tuples of) the relation instance.\n19.4.2\nThird Normal Form\nLet R be a relation scherna, F be the set of FDs given to hold over R, X be a\nsubset of the attributes of R, and A be an attribute of R. R is in third normal\nforIn if, for every FD X -+ A in F, one of the following statenlents is true:\n•\nA EX; that is, it is a trivial FD, or\n•\nX is a superkey, or\n•\nA is part of sorne key for R.\nrrhe definition of 3NF is sinlilar to that of BCNF, with the only difference being\nthe third condition. Every BCNF relation is also in 3NF. To understand the\nthird condition, recall that a key for a rela,tion is a rninirnal set of attributes\nthat uniquely deterrnines all other attributes.\nA rrlllst be part of a key (any\nkey, if there are several).\nIt is not enough for A to be part of a superkey,\nbecause the latter condition is satisfied by every attribute!\nFinding all keys\nof a relation scherna is known to be an NP-cornplete problern, and so is the\nprob1ern of detennining whether a relation seherna is in 3NF.\nSuppose that a dependency X· -+ A causes a violation of 3NF. There are two\ncases:\n•\nX is a proper 8'l.lb8Ct of 80'(ne key K. Such a dependency is 801netirnes called\na partial dependency. In this Cc1se, we store (X, ./1) pairs redundantl:y.\nAs an eXEtlnple, consider the Ileserves relation \\vith attributes SBIJC\n1 frorn\nSection 19.7.4. The only key is 8El), and \\ve have the FD 8 -_.+ C/. vVe store\nthe credit ca,rd nurnber for a sailor as lnany tirnes\n<:1.'3 there are reservations\nfor that sailor.\n•\nX is not a pTOpCT snb8ct of any key.\nSuch a dependerlcy is sornetirnes\ncalled a transitive dependency, because it rneans \\ve have a chain of\n\n618\nCHAPTER 19\ndependencies !( ---+ X\n---+ A. The problem is that we cannot associate an\nX value \\vith a K value unless we also associate an A value vvith an X\nvalue. As an exanlple, consider the Hourly-Enlps relation with attributes\nSNLRWH froIn Section 19.7.1. The only key is S, but there is an FD R\n---+ 1,V, \\vhieh gives rise to the chain S ---+ R\n-~-\" W. The consequence is that\n\\ve cannot record the fact that elnployee S has rating R without knowing\nthe hourly \\vage for that rating. 'This condition leads to insertion, deletion,\nand update anoIllalies.\nPartial dependencies are illustrated in Figure 19.7, and transitive dependencies\nare illustrated in Figure 19.8. Note that in Figure 19.8, the set X of attributes\n11lay or Illay not have some attributes in conunon with KE-Y; the diagranl should\nbe interpreted as indicating only that X is not a subset of KEY.\nCase 1: A not in KEY\nFigure 19.7\nPartial Dependencies\nCase 1: A not in KEY\nCase 2: A is in KEY\nFigure 19.8\nTransitive Dependencies\nThe Inotivation for 3NF is rather technical. By Inaking an exception for certain\ndependencies involving key attributes, we can ensure that every relation schclna\ncan be decornposed into a collection of 3NF relations using only dec(nnpositions\nthat have certain desirable properties (Section 19.5). Such a guarantee does not\nexist for BCNF relations; the :3NF definition weakens the BCNF requirernents\njust enough to Inake this guarantee possible. \\Ve Inay therefore cOlnprornise by\nsettling for a :3NF design. As we see in Chapter 20, we 11lay sOllletilnes accept\nthis\ncornpr()Jni~e (or even settle for a non-:3NF scheIna) for other reasons as\nwell.\nlJnlike BCNF, however, BOlne redundancy is possible \"Vvith :~NF. The problerns\nclssoci<:tted \\vith partial and transitiv(~ dependencies persist if there is a nontriv-\nial dep(~ndencyX\n--~., A and X is not a sup(~rkey, even if the relation is in :3NF\nl)ccause A is pa,rt of a key. Th understand this point, let us revisit the R,eserves\n\nScherna Refinelnent fLTUi J.N'oT1nal Forrns\n619\nrelation with attributes SEDe a,nd the FD S ~ ['1, \\vhich states that a sailor\nuses a unique credit card to pay for reservations. S is not a key, clnd C is not\npart of a key. (In fact, the only key is SED.) Hence, this relation is not in 3NF;\n(S, CJ pairs are stored redundantly. IIowever, if we also know that credit cards\nuniquely identify the o\\vner, vve have the FD C --? 5, which rneans that GEJD\nis also a key for Reserves. Therefore, the dependency S -7 C does not violate\n3NF, and R,eserves is in 3NF. Nonetheless, in all tuples containing the saIne 5\nvalue, the saIne (8, CJ pair is redulJ.dantly recorded.\nFor cOlllpleteness, we reluark that the definition of second norrnal form is\nessentially that partial dependencies are not allowed. Thus, if a relation is in\n3NF (which precludes both partial and transitive dependencies), it is also in\n2NF.\n19.5\nPROPERTIES OF DECOMPOSITIONS\nDecoIllposition is a tool that allows us to eliminate redundancy. As noted in\nSection 19.1.3, however, it is iInportant to check that a decoInposition does not\nintroduce new problellls. In particular, we should check whether a decomposi-\ntion allows us to recover the original relation, and whether it allows us to check\nintegrity constraints efficiently. vVe discuss these properties next.\n19.5.1\nLossless-Join Decomposition\nLet R be a relation schelna and let F be H, set of FDs over R. A decolnposition\nof R into two schernas with attribute sets X andY is said to be a lossless-join\ndecomposition with respect to F if, for every instance T of R that satisfies\nthe dependencies in }?, 1Tx('r)\nN\n1T}-(r)\n=\nT. In other words, \\ve can recover\nthe original relation 1'rorn the deconlposed relations.\nThis definition can easily be extended to cover a decornposition of Ii into Inore\nthan two relations. It is ea.\"sy to see that T\n~ 1fx(r)\n[XJ\n1TyC,.,) ahvays holds.\nIII general, though, the other direction does not hold. If sve take projections\nof a relation and recornbine theln using natural joirl,\\Ve typically obta.in SOlne\ntuples that 'were 1.'1.ot in the original relation.\nThis situation is illustrated in\nFigure 19.9.\nBy replacing the instance T shown in Figure 19.9 \"Vvith the instances 1f8P(r) and\n1TPI) (r), '\\ve lose sorne inforInation. In particular, suppose that the tuples in 't\nd(-~note relationships.vVe can no longer tell that the relationships (81, PI, d:3)\nand (8:3,])1, d:d do not hold. rrhe decoluposition of schelna SPD into S.P and\nPI) is therefore loss,Y if the instance '( shown in the figure is legal, that is, if this\n\n620\nC;HAPTERt 19\n81\npI\nell\nf---- ._-\n_..-\n82\np2\nd2\n..-\n83\npI\nd3\n_.\nsl\npI\nd3\ns3\npI\nell\n[P--·-I~\n§.'~\n~~\npI\nd3\n1rI:JD(r)\ns1\npI\ns9\np2\n'....\nf---..\ns3\npI\n...-\nInstance T\n.....,.,....\n.._.\nsl\npI.\nell\ns2\np2\nd2\n.._..-\nL~.9.\npI.\nd3\nFigure 19.9\nInstances Illustrating Lossy Decompositions\ninstance could arise in the enterprise being rIlodeled. (Observe the siInilarities\nbetween this eX~Llnple and the Contracts relationship set in Section 2.5.3.)\nAll decompositions used to eli'minate redundancy must be lossless. The follow-\ning sirnple test is very useful:\nTheorern 3 Let R be a relation and F be a set of FDs that hold over Il. The\ndecomposition ofR into relations with attribute sets III and R2 is l08sless if and\nonly if p+ contains either the FD R1 n R2\n---+ R 1 or the FDR1 n R2\n---'f R2.\nIn other words, the attributes cornrIlon to Rl and R2 HUlst contain a key for\neither RIOI' R 2 .2\nIf a relation is decornposed into 1110re than two relations,\nan efficient (tiTne polynomial in the size of the dependency set) algoritllln is\navailable to test whether or not the dec(nnposition is lossless, but we will not\ndiscuss it.\nConsider the lIourly_Ernps relation again.\nIt has attributes SNLRWII, and\nthe FI) R ~ W causes a violation of 3NF. We dealt ¥lith this violation by\ndecorIlposing the relation into SNLRII and IlvV. Since R is cornrnon to both\ndecornposed relations and Ii ---+ W holds, this decornposition is lossless-join.\nThis exarnple illustrates a general observation that follows froIH Theorerll 3:\nIf an Ff) X\n---+\n}T holds over a relation ii and\n~y n }T is ernpty, the\ndecornposition ofR into .R -\ny~ and XY is lossless.\nX appears in both It\n--~~. y' (since\n~¥ (I\n}7 is ernpty) and .IYY, and it is a key for\n){}T.\n2See Exercise 19.19 for a proof of Theorern :3.\nExercise 19.11 illustrates that the 'only if\nl claim\ndepends on the IL')slIInption that only functional dependencies can be specified a..s integrity constraints.\n\nScheTna Rejinc'fnent and lVorrnal FOT1ns\n6~1\nAnother hnportant observation, '\\vhieh we state without\npr()of~ hal;) to do \\vith\nrepeated decolnpositiollS. Suppose that a relation Ii is decornposed into Rl and\nR2 through a IOBsless-join decolupositiol1, and tlUtt Rl is decolnposed intoRl.1\nand R12 through another lossless-join decolnposition. Then, the decolnposition\nof R into R.lI, R.12, and .R2 is lossless-join; by joining ftll and R12, \\ve can\nrecover R.1, and by then joining Rl and R2, we can recover flo\n19.5.2\nDependency-Preserving Decomposition\nConsider the Contracts relation with attributes C8JDPCJVfronl Section 19.3.l.\nThe given FDs are C -+ C8JDPQV, JP \"'-7 C\n1\n, and SD -+ P. Because SD is not\na key the dependency SD \".,-1- P causes a violation of BCNF.\nWe can decolnpose Contracts into two relations with schelnas CSJDQ V and\nSDP to address this violation; the decolnposition is lossless-join.\nThere is\none subtle problelll, however. We can enforce the integrity constraint JP -} C\neasily when a tuple is inserted into Contracts by ensuring that no existing tuple\nhas the same JP values (as the inserted tuple) but different C values.\nOnce\nwe decompose Contracts into CSJDQ V and SDP, enforcing this constraint\nrequires an expensive join of the two relations whenever a tuple is inserted into\nCSJDQ V. We say that this decornposition is not dependency-preserving.\nIntuitively, a dependency-preserving decornposition allows us to enforce all FDs\nby exarnining a single relation instance on each insertion or rnodification of a tu-\nple. (Note that deletions cannot cause violation of FDs.) To define dependency-\npreserving decornpositions precisely, we have to introduce the concept of a pro-\njection of FDs.\nLet R be a relation schenla that is decolnposed into two schernaswith attribute\nsets X' and }/, and let F be a set of FDs over Il. T'he projection of F on X is\nthe set of FDs in the closure l i'+ (not just .F !) that involve only attributes in X.\n\\Ve denote the projection of I? on attributes .iYa,s Fx . .Note that a dependency\nU -_...+ V in F+ is in\nl~~x; only if all the cLttributes in [/ and V are in .iY.\nThe decornposition of relation scherna Il with FI)s }' into schcrnas with attribute\nsets ..:¥ and }/is dependency-preserving if CF'x U F\\·)+\n== I?+, That is, if we\ntake the dependencies in };'( and Fv and cornpute the closure of their un.ion, vve\nget back all dependencies in the closure of F. rrherefore, \\ve need to enforce onl,y\nthe dependencies in Ji'){ and F}r: allFDs in }'+ are then sure to be satisfied.\n~ro\nenforce\n}~':( ,\\V8 need to exarnine only relation )( (on in.serts to that relation).\nTo enforce F'y,·, \\Ale need to exarnine only rela,tion Y·.\n\n622\nTo appreciate the need to consider the (:Iosure Fl+- while COIUpllting the projec-\ntion of f?, suppose that a relation R \\vith attributes ABG1 is clecornposed into\nrelations\\vith attributes AB and Be:. The set\n~F of FDs overR includes A -+\nB, B\n---+ C, and G1 -+ A. Of these, A\n----+ B is in\n1~~1B and B -+ C) is in }'\"'lBC.\nBut is this decoIIlposition dependency-preserving?\n\\~lhat about C ---+ A? This\ndependency is not irnplied by the dependencies listed (thus far) for [<'AB and\n}13c·\nThe closure of 1~1 contains all dependencies in }' plus A -+ C, B -+ A, and C ~---+ B.\nConsequently, f:1B also contains B -+ A, and .FBc contains C -+ B. Therefore,\nFAB U F}3c; contains A -+ B, B -+ C, Ii -+ A, and C -,. B. The closure of the\nd(~pendenciesin f:1.B and }'BC now includes C -). A (which follows frorn C _.....+ B,\nB·--+ A, and transitivity). l'hus, the deccHnposition preserves the dependency\nC-+ A.\nA direct application of the definition gives us a straightforward algoritlun for\ntesting whether a deconlposition is dependency-preserving.\n(This algorithrn\nis exponential in the size of the dependency set.\nA polynomial algorithnl is\navailable; see Exercise 19.9.)\nWe began this sectiol1with an exanlple of a lossless-join deC0111position that was\nnot dependency-preserving. Other decorupositions are dependency-preserving,\nbut not lossless. A silnple exalnple consists of a relation ABC' with FD\nA~---+ B\nthat is decornposed into AB and BG.\n19.6\nNORMAI-iIZATION\nHaving covered the concepts needed to understand the role of HortHa} fonns\nand decolnpositions in databa\"se design, we now consider algoritlnIls for con-\nverting relations to BCNF or :3NF. If a relation schelna, is not in BCNF, it\nis possible to obtain a lossless-join deccunpositioll into a collection of BCNF\nrelation sCherl1chs. Unfortunately, there lllay be no dependenc,y-preserving de-·\ncOlnposition into a collection of BCN.F relation schernas.\nl-Ic}\\vever, there is\nahvays\n(l, dependency-preserving, lossless-join decoruposition into a collection\nof ~3NF relation schernas.\n19.6.1\nDecomposition into BCNF\nvVo now present an <llgorithIl1 for decornposing a relation scherna Ii with a set\nof FI)sF into a collection of BCNF relation schernas:\n\nScherna ,RefinClnent and IVoTTnal l?orrns\n623\n1. Suppose that R is not in BCNF. Let .IX' C It, A be ::1, single attribute in R,\nand X --7 A be an FD that causes a violation of BCNF. DecornposeR into\nR - il and XA.\n2. If either Ii -\n\",4 or\n..YA is not in BCN.F, decornpose thern further by a\nrecursive application of this algorithrn.\nIf ..- \",4 denotes the set of attributes other than A in Il, and ./YA denotes the\nunion of attributes in -\"Yand A. Since X ----+ A violates BCNF, it is not a trivial\ndependency; further, A is a single attribute.\nTherefore, A is not in X; that\nis, ..;\\ n A is ernpty.\nTherefore, each dec()lnposition carried out in Step\n1. is\nlossless-join.\nThe set of dependencies associated vvith R .- A and XA is the projection of F\nonto their attributes. If one of the new relations is not in BCNF, w'e decornpose\nit further in Step 2.\nSince a decornposition results in relations with strictly\nfewer attributes, this process terrninates, leaving us with a collection of relation\nschernas that are all in BCNF. Further, joining instances of the (two or 1nore)\nrelations obtained through this algorithrn yields precisely the corresponding\ninstance of the original relation (i.e., the decorllposition into a collection of\nrelations each of which in BCNF is a lossless-join dec()lnposition).\nConsider the Contracts relation with attributes C3JDPQ V and key C. We are\ngiven FDs JP ---7 C and 3D -+ P. By using the dependency 3D -+ P to guide the\ndecornposition, we get the two schernas 3DP and C5JDQV. 51)P is in BCNF.\nSuppose that we also have the constraint that each project deals with a single\nsupplier: .I _.+ 5. This rneans tlutt the sche1na CSJDQ V is not in BeN}? So we\ndeccnnpose it further into J3 and C.IDC2 V. C --+ JDQ V holds over CJDQ V; the\nonly other FI)s that hold are those obtained frorll this PI) by augrnentation, and\ntherefore all FDs conte-tin a key in the left side. Thus, each of the schernas ST)P,\n.IS, and C:.I1J(J V is in BCNF~ and this collection of schcrnas also represents a\nlossless-join decornposition of ()SJD(J V.\nThe\nst(~PS in this deC(nllposition process can be visualized as a tree, as sho\\vn\nin Figure 19.10. rrhe root is the original relation CSJIJPQ \\/, and the leaves are\nthe BCNl~~ relations that result frorn the deccHnposition aJgorithrn: 3D?, .IS,\nand CSDC2 V. Intuitively, ea.ch internal node is replaced by its children through\nEt single decOruI}osition step guided by the FD shown just belo\\v the node.\nRedundancy in BCNF Revisited\nT'he decolnposition of (}SJDQ V iuto ,S])}), J5'1, and C'JI)(J \\l is not dependency-\npreserving. Intuitively, dependency Jp ..._...~ (} carlllot be enforced without a, join.\n()ne \\vay to deal \\vith this situation is to add a relation \\vith attributes G\n1J}). In\n\n624\nC~HAPTER 19\nFigure 19.10\nDecomposition of CSJDQV into SDP, JS, and CJDQ V\neffect, this solution arnounts to storing SOITle information redundantly to rnake\nthe dependency enforcement cheaper.\nThis is a subtle point: Each of the schemas CJP, SDP, JS, and CJDQV is in\nBCNF, yet some redundancy can be predicted by FD infonnation. In particu-\nlar, if we join the relation instances for SDP and CJDQVand project the result\nonto the attributes CJP, we rnust get exactly the instance stored in the relation\nwith scherna CJP. We saw in Section 19.4.1 that there is no such redundancy\nwithin a single BCNF relation. This exarnple shows that redundancy can still\noccur across relations, even though there is no redundancy within a relation.\nAlternatives in Decomposing to BCN~-'\nSuppose several dependencies violate BCNF. Depending on ·which of these de-\npendencies we choose to guide the next decornposition step, we rnay arrive at\nquite different collections of BeNF relations.\nConsider Contracts.\n\\Ve just\ndecornposed it into SDP, is, and CJDCj V. Suppose we choose to decornpose\nthe original relation (}SJDPC2 V into JS and CJIJPCj V, based on the FD .I -+\nS. The only dependencies that hold over (}JIJPQ V Etre .IP\n----7 C and the key\ndependency C ~ C.IDPQV. Since iP is a key, C:J.DPC2Vis in BeNF. Thus, the\nschernas JS and CJDPQ V represent a lossless-join decornposition of Contracts\ninto BCNF relations.\nThe lesson to be learned here is that the theor,Y of dependencies can tell us ·when\nthere is redundancy and give us clues about possible clecornpositions to address\nthe problern, but it cannot discrirninate arnong decornposition alternatives. A\ndesigner has to consider the alternatives and choose one based on the scrnantics\nof the application.\n\n*625\nBCNF and Dependency-Preservation\nSoruethnes, there siIuply is no decomposition into BCNF that is dependency-\npreserving. .i\\s an exaruple, consider the relation schelna SBD, in which a tuple\ndenotes that sailor S ha.s reserved boat ,8 OIl date [J. If we have the 11\"'Ds 8B\n~ D (a sailor can reserve a given boat for at nlost one day) and D -+ B (on\nany given day at rllost one boat can be reserved), SBn is not in BCNF because\nD is not a key.\nIf we try to dec(nnpose it, however, we cannot preserve the\ndependency BB \"-7 D.\n19.6.2\nDecomposition into 3NF\nClearly, the approach \\ve outlined for 10ssless-joiIl decornpositioIl into BCNF\nalso gives us a lossless-join decomposition into 3NF. (Typically, we can stop\na little earlier if we are satisfied with a collection of 3NF relations.) But this\napproach does not ensure dependency-preservation.\nA siInple rllodification, however, yields a deco111position into 3NF relations that\nis lossless-join and dependency-preserving.\nBefore we describe this modifica-\ntion, we need to introduce the concept of a lninirnal cover for a set of FDs.\nMinimal Cover for a Set of FDs\nA minimal cover for a set F of FDs is a set G of FDs such that:\n1. Every dependency in G is of the forIn ..¥ ---+ A, where A is a single attribute.\n2. The closure F+ is equal to the closure (;+.\n:3. If we obtain a set II of dependencies frorn G by deleting one or 1110re depen-\ndencies or by deleting attributes frorn a dependency in G, then p+' i= II+.\nIntuitively, a rninirnal cover for a set }-' of FDs is an equivalent set of depen-\ndencies that is 'minirnal in two respects: (1) Every dependency is as slIlall as\npossible; tha,t; iS 1 each attribute on the left side is necessary and the right side\nis a single attribute. (2) Every dependency in it is required for the closure to\nbe equal to j?-+',.\nAs an exarnplc, let J? be the set of dependencies:\n1\nB\n1BCID\nE' E'j-'\n(\"'I\n1'\" T;'\nlIlA (-'11)}'--'\nE-'G'1\nit\n..-+ ',j\n--\n----+\n.../,\n.f\n...._--t\n>T,\n-~r --+\n,. 1\n~Ul(\n./.\nf\n----+\n.../.J.\nFirst 1 let us rewrite it()DF -_...+ BG so that every right side is a single attribute:\n\n626\nACDF-tEand ACDF-t G,\nCHAPTERt9\nNext consider ACDF -+ G, This dependency is irnplied by the following FDs:\nA -7 B, ABC'D -7 E, and EF -7 G,\nTherefore, \\ve can delete it, Sirnilarly, we can delete A CDF -7 1:7, Next con-\nsider ABCD -7 E, Since A -7 B holds, we can replace it with ACD _..._~ E, (At\nthis point, the reader should verify that each rernaining FD is rninilnal and\nrequired,) Thus, a rninilnal cover for F is the set:\nA -7 B, ACD -7 E, EF ---7 Ci, and EF --+ H,\nThe preceding exarnple illustrates a general algorithrn for obtaining a rninimal\ncover of a set }i' of FDs:\n1. Put the FDs in a Standard Form: Obtain a collection G of equivalent\nFDs with a single attribute on the right side (using the decornposition\naxiolIl),\n2. Minimize the Left Side of Each FD: For each FD in G, check each\nattribute in the left side to see if it can be deleted while preserving equiv-\nalence to F+,\n3. Delete Redundant FDs: Check each reluaining FD in G to see if it can\nbe deleted while preserving equivalence to .F+,\nNote that the order in which we consider FDs while applying these steps could\nproduce different rninilnal covers; there could be severa'! rninirnal covers for a\ngiven set of FDs,\nlV101'8 irnportant, it is necessary to rniniInize the left sides of F'Ds befoTc checking\nfor redundant FI)s, If these two steps are reversed, the final set of FI)s could\nstill contain senne redundant FDs (i,e., not be a rninirnal cover), as the following\nexarnple illustrates, LetF be the set of dependencies, each of \"\"vhich is already\nin the standard fornl:\n.A13CTJ -t E',E \" ~~ D, A ----;. 13, and A C ----;. I),\nObserve that none of these FDs is redundant; if \\ve checked for redundantFDs\nfirst, \\ve \"\"vould get the saIne set of FI)s I?, The left side of il13CIJ\n}; can be\nn~l)hiced by ACt\\vhile preserving equivalence to\n1~\"1+, and ,ve \\vould stop here if\n\\ve checked for reclunda.ntF'Ds in I? before rnillilnizing the left sides.\nHO\\V8Ver,\nthe set of FDs \"\"ve 11(lVe is not a Inininlal cover:\n\nSche17~a Ilefinerncnt and NOTrnal j?OT'lnS\nACt .......:;. E,E _..-+ D,A --t B, and AG1 -+ D.\n~127\nFrOlIl transitivity, the first two FDs irnply the la.\",'3t FD, ,v-hich can therefore be\ndeleted while preserving equivalence to\n1~1+. The irnportant point to note is\nthat A C---+ D becc)lnes redundant only after we replace ABeD -)- E with AC\n-)- E. If \"ve Ininirnize left sides of FDs first and then cheek for redundantFDs,\n,ve are left \"vith the first three FDs in the preeeding list,whieh is indeed a\nIninirna1 cover for F.\nDependency-Preserving Decomposition into 3NJ.1-'\nReturning to the problenl of obtaining a lossless-join, dependency-preserving\ndecornposition into 3NF relations, let R be a relation with a set [/' of FDs that\nis a minirnal cover, and let R 1 , R2 ,\n...\n, Rn be a lossless-join decolnposition\nof R. For 1 < i < n, suppose that each Ri is in 3NF and let Fi denote the\nprojection of F onto the attributes of Ri . Do the following:\n•\nIdentify the set N of dependencies in F that is not preserved, that is, not\nincluded in the closure of the union of Fis.\n•\n:F'or each FD X\n---t A in N, create a relation schelna XA and add it to the\ndecomposition of R.\nObviously, every dependency in F is preserved if we replace R by the R'iS plus\nthe schernas of the forn1 XA added in this step. The Ris are given to be in\n3NF. We can show that each of the schelnas XA is in 3NF as follows: Since X\n----7' A is in the lninirnal cover F, Y ---+ A does not hold for any Y that is a strict\nsubset of X. Therefore, X is\n(1, key for XA. :F\\llrther, if any other dependencies\nhold over XA, the right side can involve only attributes in X' because A is a\nsingle attribute (because X\n-~ A is an FD in a rninhnal cover). Since X is a\nkey for ..:YA, none of these additional dependencies causes a violation of ~3NF\n(although they rnight cause a violation of BCNF).\nAs an optilYlization, if the set N contains several FI)swith the saIne left\nside, say, X\n-~~.-t\n..41, X\n-t A 2 ,\n,\n..X- ..+ /In , we can replace thern \\vith\n(.I., single equivalent FD X\n-t AI\n..I4·n .\nTherefore, \\ve produce one relation\nscherna ..X' ..14 1 ... /In , instead of several schernas XA 1 , .... ~X'\"An, \\vhich is gener-\nally preferable.\n(~onsider the Contracts relation vvith attrilnltes C:SJDPQV etnel FI)s JP\n-j> C:,\n81)--4 P. and J\n....-» S. If \\ve decolnpose (}SJIJPe) V into SDIJ and C}SJl)(JV,\nthen 8DP is in BCNF, but C\\'1J1)(2 V is not\nev(~n in :3NI;'. So \\ve dec.olupose it\nfurther into JS and C\n1JDe2 V. rrhe relation schcrnas 19IJ.P, .I8, Etnel C7JDQVare\nin\n~3NF (in fact, in BCNF) 1 and the decoInposition is lossless-join.\nlIowever,\n\n628\n(;HAPTElt 19\nthe dependency JP ---:,. C' is not preserved. This problerIl can be addressed by\nadding a relation schenut e'Jp to the decornposition.\n3NF Synthesis\nvVe assurned that the design process starts with an Ell diagraII1, and that our\nuse of FDs is prilnarily to guide decisions about decolnposition.\nThe algo-\nrithill for obtaining a lossless-join, dependency-preserving decornpositiol1 was\npresented in the previous section fro111 this perspective--------a lossless-join decoru-\nposition into 3NF is straightforward, and the algorithrn addresses dependency-\npreservation by adding extra relation schcrnas.\nAn alternative approach, called synthesis, is to take all the attributes over the\noriginal relation R and a rnininlal cover F for the FDs that hold over it and\nadd a relation scherna XA to the decomposition of R for each FD X\n-----+ A in F.\nThe resulting collection of relation schernas is in 3NF and preserves all FDs.\nIf it is not a lossless-join decomposition of R, we can Dlake it so by adding a\nrelation schenla that contains just those attributes that appear in sorne key.\nThis algorithrn gives us a lossless-join, dependency-preserving decornposition\ninto 3NF and has polynornial corIlplexity-----polynornial algorithms are available\nfor coruputing rninirnal covers, and a key can be found in polync)Inial tirHe\n(even though finding all keys is known to be NP-cornplete).\nThe existence\nof a polynornial algorithnl for obtaining a lossless-join, dependency-preserving\ndecornposition into 3NF is surprising when we consider that testing whether a\ngiven schenu:l, is in ~~NF is NP-cornplete.\nAs a.n exarnple, consider a relation ABG1 with FI)s F:::::::: {A\n-----+ B, C -----+ B}.\n~rhe\nfirst step yields the relation scheluas AB and BG. T'his is not a lossless-join\ndeC0l11position of AilC; AB nBC is B, and neither B -----+ A nor IJ -, C: is in /?+.\nIf we add a, scherna A C:, \\ve have the lossless-join property\n<:I\",\" well. Although\nthe collectic)ll of relations AB,BC, and A C is a depenclency-preserving, lossless-\njoin decornposition of ABC, we obtained it through a process of synthesis,\nrather tllan through a process of repeated decornposition.\n\\\\Te note that the\ndecoIIIposition produced by the synthesis approa,ch heavily dependends on the\nrninirnal cover used.\nAs another exarnple of the synthesis approach, consider the Contracts relation\nwith attributes (}SJDP(JVand the follovving FI)s:\nC\nC\\').IIJ.P(J V, .IP -----+ C, 8D --+P, aDd J -7 s.\nThis set of FI)s is not a rninirnal cover, and so \\ve IllUSt find OI18.\n\\Ve first\nreplace G -.-7 (}S'IJ[J.P(J V v'lith tllcF'I)s:\n\n5~che'Tna Refincrnent and j\\forlnal FC)'t'rns\nC' -+ 5, Cf -\" J, (: _.+ IJ, C' _...+ .P, C --+\nQ~ and () -+ \\1.\n1\n1he FD C --+ P is irnplied by C\n1 -to S, C _.......;. D, and SD\n-_··+P~ so we can delete\nit.\n~rhe FD C-r ~ S is irnplied byC -;. J and J _..u} S; so \\ve ean delete it. This\nleaves us with a rninirnal cover:\nC _.-+ ,I, C _.-7 1), C·--:,. Q, C -;. V, JP --...;. C, 3D ---;. P, and J ~.._+ S.\nlJsing the algorithrll for ensuring dependency-preservation, we obtain the re-\nlational scherna CJ, CD, CQ. CV, GlJP, SDP, and JB. We can irnprove this\nschenla by cornbining relations for which C is the key into CDJP(J V. In addi-\ntion, we have SDP and ,IS in our decorllposition. Since one of these relations\n(CDJPQ V) is a superkey, \\ve are done.\nConlparing this decomposition with that obtained earlier in this section, we\nfind they are quite close, with the only difference being that one of them has\nCDJPQV instead of CJP and CJDQV. In general, however, there could be\nsignificant differences.\n19.7\nSCHEMA REFINEMENT IN DATABASE DESIGN\nWe have seen how normalization can elilninate redundancy and discussed sev-\neral approaches to nonnalizing a relation.\nWe now consider how these ideas\nare applied in practice.\nDatabase designers typically use a conceptual design rnethodology, such as ER\ndesign, to arrive at an initial databa,,\"3e design.\nGiven this, the approach of\nrepeated decorllpositions to rectify instances of redundancy is likely to be the\nrnost natural use of PI)s and nonnalization techniques.\nIn this section, \\ve Inotivate the need for a schcrna refinernent step follovving\nEll design. It is natural to aBk whether \\ve even need to decornpose relations\nproduced by translating an Eli diagranl. Should a good ER design not lead to a\ncollection of relations free of redundancy prob.lerns? Unfortunately, Fjll design\nis a c()!nplex, subjective process, and certain constraints are not expressible\nin tenns of Ell diagraJns. 1'he exaruples in this section are intendecl to illus-\ntrate \\vhy decornposition of relations produced through Ell design rnight be\nnecessary.\n\n6:30\nC~lIAPTgR 19\n19.7.1\nConstraints on an Entity Set\nConsider the Hourly_ElllPS relation again. rrhe constraint that attribute 8/)'n is\na key can be expressed a..s an FI):\n{ssn} ---} {SS11, naTne, lot, l>ai'ing, ho'U,'rIY_J.oages, hOUTS_'U)orke.d}\n:For brevity, we \\vrite this FD &'3 S ---} 8NLR~VII, using a single letter to denote\neach attribute and ornitting the set braces, but the reader should rernernber\nthat both sides of an FD contain sets of attributes. In addition, the constraint\nthat the hourl:y~wages attribute is deterruined by the rating attribute is an FD:\nR\n.,. W.\nAs \\ve saw in Section 19.1.1, this FI) led to redundant storage of rating wage\nassociations.\nIt cannot be expressed in, teTrns of the ER\nrr~odel.\nOnly FDs\nthat\ndeteTrr~ine all attTilndes of a relation (i. e., key constraints) can be ex-\npTessed in the ER rnodel. rrherefore, we could not detect it when we considered\nHourly_EIIlPS as an entity set during ER IIlodeling.\nWe could argue that the problenl with the original design was an artifact of a\npoor ER design, which could have been avoided by introducing an entity set\ncalled Wag(~_Table (with attributes rating and houTly_wages) and a relationship\nset lIas_Wages associating IIourly_.ErIlps and vVagc_Table. The point, however,\nis that we could easily arrive at the original design given the subjective nature of\nEll rnodeling. Having forInal techniques to identify the problenl with this design\nand guide us to a, better design is very useful. 1'he value of such techniques\ncannot be underestirnated when designing large schernas····-schcrnas with rnore\nthan a hundred tables are not unCOIllIHon.\n19.7..2\nConstraints on a Relationship Set\n1\"'he previous exarnple illustrated how FDs can help to refine the subjective\ndecisions Blade during ER. design, but one could argue that the best possible\nER, eliagrarn \\vould have led to the sc:Hne final set of relations. ()ur next exarnple\nshovvs ho\\v Ff) inforrnation ca.ll lead to a. set of relations unlikely to be arrived\nat\nsol(~ly through ER. design.\n\\Ve revisit an exanlple frOTH Chapt<~r 2. Suppose that \\ve have entity sets Parts,\nSuppli(~rs, and I)epartrnents, as \\vell as a relationship set Contracts that involves\nall of theIn. \\lVe n~fer to\nth(~ scherna for Contra(:ts as C7C2}>S'1IJ. A contra,ct \\vith\ncontract id () specifies that\n<1, supplier 8 \\vill supply sorne ql.Hlntity (2 of a part\nP to a departrnent J). (\\,Ve have adderl the contract ieI field C' tC) tIle versiorl of\nthe\nC~ontracts relation discussed in Chapter 2.)\n\nSchCTna Refinernc'n.t anr11VoT7nalFoTrn8\n6431\nvVe Blight have a policy that a departrnent purchases at Inost one paJt fror11\nany given supplier. 'Therefore, if there are several contracts between the saIne\nsupplier and departrnent, \\ve know that the saIne part Inus!; be involved in all\nof thcrn. This constraint is cUI FD, DS·--.+ P.\nAgain \"ve have redundancy and its a\",c;sociated problclns. \\Ve can address this\nsituation by decornposing Contracts into two relations with attributes CQSD\nand 3DP. Intuitively, the relation 3DP records the part supplied to a depart-\nrllent by a supplier, and the relation C:QSD records additional infornlation\nabout a contract. It is unlikely that we would arrive at such a design solely\nthrough ER rIlodeling, sinee it is hard to fOfrnulate an entity or relationship\nthat corresponds naturally to CQSD.\n19.7.3\nIdentifying Attributes of Entities\nThis exarIlple illustrates how a careful examination of FDs can lead to a better\nunderstanding of the entities and relationships underlying the relational tables;\nin particular, it shows that attributes can easily be associated with the 'wrong'\nentity set during ER design.\n'I'he ER diagrarn in Figure 19.11 shows a rela-\ntionship set called Works_In that is silnilar to the Works.ln relationship set of\nChapter 2 but with an additional key constraint indicating that an employee\ncan work in at rnost one departrIlent. (Observe the arrow connecting Employees\nto Works_In.)\nFigure 19.11.\nThe Works._In Relationship Set\nUsing the key constraint, \\ve can translate this Ell diagrarn into two relations:\n\\VorkersC58n, narne, lot, d'id, since)\nDepartrnents( did, dna1nc, budget)\nThe entity set Ernployees and the relationship\nset\\iVorks~n are rnapped to\na sil.lgle relation, vVorkers. This translation is ba\".'Scd on the second approach\ndiscussed in Section 2.4.1.\n\n632\nCHAPTER »9\nNo\\v suppose elIlployees are a,,'3signed parking lots ba.rged on their departrnent,\nand that all enlployees in a given departrnent are ~'h'3signed to the saIne lot. This\nconstraint is not expressible vvith respect to the ER, diagrarIl of Figure 19.1l.\nIt is another exarnple of an FD: did --: lot. The redundancy in this design can\nbe elirninated by decornposing the Vlorkers rela.tion into two relations:\nvVorkers2( 8sn, narne, did, since)\nDept_Lots(did, lot)\n'rhe new design has lnuch to reconunend it. VVe can change the lots associated\nwith a departlnent by updating a single tuple in the second relation (i.e., no\nupdate anornalies). \\Ve can associate a lot with a department even if it cur-\nrently has no crnployees, without using null values (i.e., no deletion anornalies).\n\\Ve can add an eruployee to a department by inserting a tuple to the first rela-\ntion even if there is no lot associated with the enlployee's departrnent (i.e., no\ninsertion anornalies).\nExalnining the two relations Departrnents and Dept_Lots, which have the saIne\nkey, we realize that a Departrnents tuple and a Dept_Lots tuple with the sarne\nkey value describe the sarne entity.\nThis observation is reflected in the ER,\ncliagrarn shown in Figure 19.12.\nFigure 19.12\nRefined\\Norks_In Relationship Set\nTranslating this diagrarn into the relational rnodel would yield:\n\\iVorkers2 (8871\" narne, did, since)\nI)epartrnentsCdid, dnarne, budget, lot)\nIt SeClllS intuitive to associate lots ,vith crnployees; on the other hand, the les\nreveal tllat ,in this exarnple lots are rea.1ly a\"ssociated ,\\lith departrnents. The\nsubjective pr()c(~ss of EIlrnodeling could Iniss this point. T'he rigorous process\nof norrnaliza,tion would not.\n\nSchC'lna Ilejinernc'rd (uul IVOTTTUll Forrns\n19.7.4\nIdentifying Entity Sets\nConsider a variant of the lleserves scherna used in earlier chapters.\nLet Re-\nserves contain attributes S, B, and D\n:::1.'3 before, indicating that sailor S has\na reservation for boat B on day D. In addition, let there be an attribute\nG\"1\ndenoting the credit card to which the reservation is charged. vVe use this ex-\narnple to illustrate how FD illfonnation can be used to refine an ER design. In\nparticular, \\ve discuss how FD inforluation can help decide whether a concept\nshould be rnodeled as an entity or as an attribute.\nSuppose every sailor uses a unique credit card for reservations. This constra,int\nis expressed by the FD S -7- C. This constraint indicates that, in relation Re-\nserves, we store the credit card rnllnber for a sailor as often as we have reserva-\ntions for that sailor, and we have redundancy and potential update anolnalies.\nA solution is to deconlpose Reserves into two relations with attributes SBD\nand SC. Intuitively, one holds inforrnation about reservations, and the other\nholds infonnation about credit cards.\nIt is instructive to think about an ER design that would lead to these rela-\ntions. One approach is to introduce an entity set called Credit_Cards, with the\nsale attribute cardno, and a relationship set Has_Card associating Sailors and\nCredit_Cards. By noting that each credit card belongs to a single sailor, we can\nInap Has_Card and Credit_Cards to a single relation with attributes SC. We\nwould probably not rnodel credit card nUlnbers as entities if our Inain interest\nin card nurnbers is to indicate how a reservation is to be paid for; it suffices to\nuse an attribute to rnodel card nUlnbers in this situation.\nA second approach is to rnake cardno an attribute of Sailors. But this approach\nis not very natural-\" ..··(1 sailor lllay have several cards, and we are not interested\nin all of theln. Our interest is in the one card that is used to pay for reservations,\nwhich is best lnodeled as an attribute of the relationship Iteserves.\nA helpful way to think about the design problern in this exarnple is tlHlt we\nfirst lnake carrino an attribute of H,eserves and then refine the resulting tables\nby taking into account the 1\"1) inforrnation. C\\Vhether \\VC refine the design by\nadding cardno to the table obtained froTIl S<dlors or by creating a new table\n\\vith attributes S'f(} is\n(1, separate issue.)\n19.8\nOTHER KINDS O~., D~:PENDENCIES\nFI)s are probal)l.y the rn08t conunon and irnportant kind of\n(~onstraiIlt fr0 III\nthe point of vie\"\\! of database design.\nTIowever, there axe several other kinds\nof dependencies. In particular, there is a 'Vvell-developccl theory for database\n\n634\n(;HAPTER 19\ndesign llsing rrLultival'ued dependenc'ics and join dependencies. By taking sueh\ndependencies into account, we C,Ul identify potential redundancy problenls that\ncannot be detected using FDs alone.\n'rhis section illustrates the kinds of redundancy that can be detected using II1Ul-\ntivalued dependencies. Our Inain observation, however, is that sirnple guidelines\n(which can be checked using only FD reasoning) can tell us whether we even\nneed to worry about complex constraints such as 111ultivalued and join depen-\ndencies. We also conunent on the role of inclusion dependencies in database\ndesign.\n19.8.1\nMultivalued Dependencies\nSuppose that we have a relation with attributes course, teacher, and book, which\nwe denote as CTB. The Ineaning of a tuple is that teacher T can teach course\nC, and book B is a reccnnmended text for the course. There are no FDs; the\nkey is CTB. However, the recolnlnended texts for a course are independent of\nthe instructor. The instance shown in Figure 19.13 illustrates this situation.\n~ofurse\n.~ teache2J book\nGreen\n..-\nPhysics101\nMechanics\nPhysicslOl\nGreen\nOptics\nPhysicslOl\nBrown\nMechanics\nPhysics101\nBrown\nOptics\nGreen\nMechanics\n._..\nMath301\n..\nMath301\nGreen\nVectors --\nMath301\nGreen\nGeometry\nL..--._..._\n........\nFigure 19.13\nBCNF R.elation with Redundancy That Is Revealed by MVDs\nNote three points here:\nII\n1'he relation sehcrna CTB is in BCNF; therefore we would not consider\ndecolnposing it further if we looked only at the FDs that hold over (JTB.\nII\nThere is redundancy. rrhe fact that G·reen can teach Physics101 is recorded\nonce per reeonunendecl text for the course. Sirnila.rly, the fact that Optics\nis a text for Physics101 is recorded once per potential teacher.\n..\nT'he redundancy can be elirninated by decornposing C\n1fTB into CT and CE.\nThe redundaJ1cy in this exarnple is due to the constraint that the texts for a\ncourse are independent of tIle instructors, which cannot be expressed in tenns\n\nScherna liefine'lnent an.d iVoT1nal FO'T\"rns\nG35\nof FDs. rrhis constraint is an example of a rn'll,ltival'ued dependency,\nor~1VD.\nIdeally, we should rnodel this situation using two binary relationship sets, In-\nstructors \\vith attributes G1T and l'ext with attributes CB. Because these are\ntwo essentially independent relationships, rnodeling them with a single ternary\nrelationship set with attributes CTE is inappropriate. (See Section 2.5.3 for a\nfurther discussion of ternary versus binary relationships.) Given the subjectiv-\nity of ER design, ho\\vever, we rnight create a ternary relationship. A careful\nanalysis of the fvIVD infonnation would then reveal the problern.\nLet R be a relation scheIna and let X and Y be subsets of the attributes of R.\nIntuitively, the multivalued dependency X --+--+ Y'is said to hold over R if,\nin every legal instance r of R, each X value is associated with a set of Yvalues\nand this set is independent of the values in the other attributes.\nForInally, if the MVD X -+-+ Y holds over\nl~ and Z = R - XY, the following\nlllUSt be true for every legal instance r of Fl:\nIf tl E r, t2 E rand tl.X == t2.X, then there must be sorne t3 E r such\nthat tl'XY = t3.XY and t2· Z = t3'Z,\nFigure 19.14 illustrates this definition. If we are given the first two tuples and\ntold that the MVD X ,-+---+ Y holds over this relation, we can infer that the\nrelation instance must also contain the third tuple. Indeed, by interchanging the\nroles of the first two tuples~treating the first tuple &'3 t2 and the second tuple\nas tl----we can deduce that the tuple t4 must also be in the relation instance.\nLx I Y I Z ]\nt: B~\nCI\n___'n tuple t1\nC2\n____.n_ tuple t2\n--'-\"-\na\nbl\nC2\n.--- tuple\nt:~\na\nb2\nCI\n-........... tuple t4\nFigure 19.14\nIllustration of MVD Definition\nThis table suggests another \\vay to think about lVIVDs:\nIf X\n----7-+ Y holds\nover Ii, then 1fyz(ax=:r(R)) = 1fy(ax=:r(R)) x 1fz(ax:::::.:x(Ii)) in every legal\ninstance of R,' for any value x that appears in the X colurnn of R. In other\nwords, consider groups of tuples inR with the sarne X-value.\nIn each such\ngroup consider the projection onto the attributes lTZ. This projection HUlst be\nequal to the cross-product of the projectiolls onto Yand Z. That is, for a given\nX-value, the Y-values and Z-values are independent. (Froln this definition it is\neaAsy to sec that\n~Y ----7-----;0 y'l11Ust hold wherlever ..!Y --..~\n}T holds. If the FI) X _...;.\n\n636\nY holds, there is exactly one Y:'value for a given X-value, and the conditions in\nthe ~!IVD definition hold trivially.\n~rhe converse does not hold, a.s Figure 19.14\nillustrates.)\nReturning to our G't1\"'B exaIllple, the constraint that course texts are indepen-\ndent of instructors can be expressed as G1 ---+---+ T. In terlllS of the definition of\nlVIVDs, this constraint can be read as follo\\vs:\nIf (there is a tuple showing that) () is taught by teacher T,\nand (there is a tuple showing that) G has book B as text,\nthen (there is a tuple showing that) G is taught by T and has text B.\nGiven a set of FDs and :NIVDs, in general, we can infer that several additional\nFDs and :NIVDs hold. A sound and complete set of inference rules consists of\nthe three ArIllstrong AxioIllS plus five additional rules. Three of the additional\nrules involve only :NIVDs:\n•\nMVD Complementation: If X\n---+~ Y, then X\n~---+ II ~ XY.\n•\nMVD .Augmentation: If X\n---+~ Yand W:2 Z, then WX --+--t YZ.\n•\nMVD Transitivity: If X --7-> Yand Y -+--+ Z, then X --+--+ (Z - Y).\nAs an exanlple of the use of these rules, since we have () --+---+ T over GTB,\nMVD complelnentation allows us to infer that C -7-+ OTB ~ CT as well, that\nIS, C ---+---+ B. The remaining two rules relate FDs and MVDs:\n•\nReplication: If X --+ Y, then X --+--+ Y.\n•\nCoalescence: If ~y --+---+ Yand there is a W such that W n Y- is elnpty,\nW ---+ Z, and Y:2 Z, then X -7 z.\n()bserve that replication states that every FD is also an l\\1VT).\n19.8.2\nFourth Normal Form\nFourth Horrnal fonn is a direct generalization of BeNF. Let R be a relation\nscherna, X and Y be nonernpty subsets of the attributes of R, and F' be a set\nof dependencies that includes both FDs and lVIVDs. R is said to be in fourth\nnormal form (4NF), if, for every l\\1.VI) -'X',.>----'t }i that holds over R, one of\nthe following staternents is true:\n•\ny\"\n~\n~y or XI\nT\n:::::::: .R, or\n•\nX is a superkey.\n\nScherna Ilefinernent and iVorrnal Fen'IlLs\n637\nIn reading this definition, it is irnportant to understand that the deflnition of a\nkey has not changed··········-the key rnust uniquely deterrnine all attributes through\nFDs alone. X ---+-; Y'is a trivial MVD if Y\nC\n.LX\"\n~ R or .LX\"Y·\n:::: R; such\nwIVDs always hold.\nThe relation CTB is not in 4NF because C ~-> T is a nontrivial MVD and C\nis not a key. vVe can elirninate the resulting redundancy by deconlposing CTB\ninto cr and CB; each of these relations is then in 4NF.\nrfo use 1t1VD inforrnation fully, we nUlst understand the theory of :NIVDs. IIow-\never, the following result due to Date and Fagin identifies conditions-detected\nusing only FD infornlation!~-underwhich we can safely ignore MVD inforrna-\ntion. That is, using MVD information in addition to the FD infornlation will\nnot reveal any redundancy. Therefore, if these conditions hold, we do not even\nneed to identify all MVDs.\nIf a relation schema is in BCNF, and at least one of its keys consists\nof a single attribute, it is also in 4NF.\nAn in1.portant assl.unption is inlplicit in any application of the preceding result:\nThe set of FDs identified thus far is 'indeed the set of all FDs that hold over the\n'('elation. This assulllption is important because the result relies on the relation\nbeing in BCNF, which in turn depends on the set of FDs that hold over the\nrelation.\nWe illustrate this point using an exalnple. Consider a relation scherna ABCD\nand suppose that the FD A -+ BCD and the MVD B -+-+ C are given. Consid-\nering only these dependencies, this relation schema appears to be a counterex-\nalnple to the result. The relation has a sirnple key, appears to be in BCNF, and\nyet is not in 4NF because B\n..~-+ C: causes a violation of the 4NF conditions.\nLet us take a closer look.\nb\nCl\n0:1\nell --- tUP!ti§\n-'\n-~ tUI?-ie t2 -\nb\nC2\n([,2\nd2\nb\nCl\n([,2\nd2\n--_... tuple t:3\n_..__...__..-\n:Figure 19.15\nThree Tuples [rorn a Legal Instance of ABCD\nFigure 19.15 8ho\\v8 three tuples f1'orn an instance of ABCD that satisfies the\ngiven lVIVI) B --+-+ C\nr\n, Frolu the definition of an lVIVD, given tuples tl and \"t2: it\nfollows that tuple t:3 Inust also be included in the instaJ1ce. Consider tuples \"t2\nand 1:3. FrOlJl the givenFD A -,B(/1) and the fact that these tuples have the\n\n638\n(;HAPTER L9\nsame A-value~ we GaIl deduce that Cl =\nC2. Therefore, ,ve see that the FD B ---+\nC rnust hold overilBCD \\V\"henever the FI) A ~ BCD and theNIVI)\nB··_·-7~ (:\nhold. If B\n-·-4\n() holds~ the relation ABeD is not in BeNF (unless additional\nFDs lllake B a key)!\nThus, the apparent counterexalnple is really not a counterexalllple···········-rather,\nit illustrates the iInportance of correctly identifying all FDs that hold over a\nrelation.\nIn this exarnple, A -» BCI) is not the only FD; the FD B -+ C\nalso holds but ·wa..s not identified initially. Given a set of FDs and IvIVI)s, the\ninference rules can be used to infer additional FDs (and l\\1VDs); to apply the\nDate-Fagin result without first using the l\\1VD inference rules, we IUUSt be\ncertain that we have identified all the FDs.\nIn summary, the Date-Fagin result offers a convenient way to check that a\nrelation is in 4NF (without reasoning about l\\1VDs) if we are confident that\nwe have identified all FDs. At this point, the reader is invited to go over the\nexamples we have discussed in this chapter and see if there is a relation that is\nnot in 4NF.\n19.8.3\nJoin Dependencies\nA join dependency is a further generalization of MVDs. A join dependency\n(JD)\n[><J {R1 ,\n...\n, R'71,} is said to hold over a relation R if R1,\n...\n, Rn is a\nlossless-join decolnposition of R.\nAn MVD X ~~ Yover a relation R can be expressed as the join dependency\n[)<J {XV, X(R,--Y)}..As an excunple, in the GTB relation, the MVD C ~-+ T\ncan be expressed as the join dependency [)<J {Crr, CB}.\nU·nlike FDs and l'v1VDs, there is no set of sound and cornplete inference rules\nfor .IDs.\n19.8.4\nFifth Normal Form\nA relation schcrna R is said to be in fifth normal form (5NF) if, for every\n.II)\n[XJ\n{.R 1. ,\n•••\n, Tin} that holds over Il, one of the follo\"ving statcrnents is\ntrue:\n•\nIli\n==\nR, for scnne i, or\n•\nThe .lD is irnplied by the set of those FDs over Il in ·which the left side is\na key for R.\n\n&39\n'The second condition deserves S(Hne explanation, since \\ve have not presented\ninference rules for FDs and .00Ds taken together. Intuitively, \\ve rnust be able to\nsho\\v that the decolnposition of R into {Rl , ... ) R'TI} is lossless-join whenever\nthe key dependencies (FDs in which the left side is a key for R) hold. JI)\nt><J\n{.R I ,\n.. ,\n, Rn } is a trivial JD if R j\n=\nR for SaIne i; such a .JD always\nholds.\n~rhe following result, also due to Date and Fagin, identifies conditions-H' ·again,\ndetected llsing only FD inforlnation---under -which we can safely ignore JD\ninforlnation:\nIf a relation schenla is in 3NF and each of its keys consists of a single\nattribute, it is also in 5NF.\nThe conditions identified in this result are sufficient for a relation to be in 5NF\nbut not necessary. rrhe result can be very useful in practice because it allcnvs\nus to conclude that a relation is in 5NF 'Without ever 'identifying the!'v1VDs and\nJDs that 'may hold oveT the relation.\n19.8.5\nInclusion Dependencies\nlVIVDs and JDs can be used to guide database design, as we have seen, although\nthey are less COllUllon than FDs and harder to recognize and rea..')on about. In\ncontrast, inclusion dependencies are very intuitive and quite cornrnon. IIowever,\nthE~y typically have little influence on database design (beyond the ER design\nstage).\nInfonnally, an inclusion dependency is a statcrnent of the fOlTH that\nS0111e\ncohunns of (1, relation are contained in other cohunns (usually of a second re-\nlation).\nA foreign key constraint is an exarnple of an inclusion dependency;\nthe referring colurnn(s) in one relation rnust be contain.ecl in the prirnary key\ncohnnn(s) of the referenced relation. As another exarnple, if!? and S are tv\\to\nrelations obtained 1)y translating t\\VO entity sets that every R entity is also\nan 8 erltity, vve \\vollid have an inclusion dependency; projecting If on its key\nattributes yields a relation conta,ined in the relation obtained by projecting 8\non its key attributes.\nThe rnain point to bear in rnind is that \\ve should not split groups of attributes\nthat participate in aJl inclusion dependency. For exarnple, if ,ve have an inclu-\nsion dep(~ndenc'y Al~ ~ Of), \\vhile decornposing the relation scherna containing\nA 13, \\\\le should ensure that at lea\"st one of the scherna.g obtained in the de-\nccnnposition contains botJ1 A and f3. ()ther\\vise,v.re cannot check the inclusion\nclependency .1113 ~ C:/) ·without reconstructing the relation containing A 13.\n\n640\n(;If.APTER W\nIvlost inelu.sioIl dependencies in practice are kelJ-based~ that is, involve only keys.\nForeign key constraints are a good exalnple of key-ba.'3ed inclusion dependencies.\n~A.n E~I{ diagralIl that involves ISA hierarchies (see Section 2.4.4) also leads to\nkey-based inclusion dependencies. If all inclusion dependencies are key-ba,sed,\n\\ve rarely have to ''lorry about splitting attribute gTOUps that participate in\ninclusion dependencies, since decornpositions usually do not split the priInary\nkey.\nN'ote, ho\\vever, that going fn:)111 3NF to BCNF ahvays involves splitting\nSOlne key (ideally not the prirnary key!), since the dependency guiding the split\nis of the fornl ..x -7\n.4 where A is part of a key.\n19.9\nCASE STUDY: THE INTERN'ET SHOP\nR,ecall froIn Section 3.8 that DBDudes settled on the following scherna:\nBooks(i~~~n: CHAR(10), title: CHAR(8) , author: CHAR(80) ,\nqty_iTL.stock: INTEGER, price: REAL, year_published: INTEGER)\nCustolllers( cid: INTEGER, cnaTne: CHAR(80) , address: CHAR(200))\nOrders(orde.rnum,: INTEGER, .....~sbn: CHAR(.10), cid: INTEGER,\ncardnu'm: CHAR(16), qty: INTEGER, ordeT_date: DATE, ship....date: DATE)\nDBDudes analyzes the set of relations for possible redundancy.\nThe Books\nrelation has only one key, (isbn), and no other functional dependencies hold\nover the table. Thus, Books is in BCNF. The Custorners relation also has only\none key, (cid), and no other functional depedencies hold over the table. T'hus,\nCustorners is also in BCNF.\nDBI)udes has already identified the pair\n(o7'(lerTl,'urr~7 isbn) as the key for the\nOrders table. In addition, since each order is placed by one custorner on one\nspecific date with one specific credit card nurnber, the following three functional\ndependencies hold:\nordcrnuTn -_......j. cid, ordernuTrl,-+ order_date, and oTderTru,Tn -7 co:rdrHlTn\nThe experts at DBDudes conclude that Orders is not even in 3NF. (Can you\nsee \\vh.y?) They decide to clecornpose ()rders into the follc)\\ving t\\VO relations:\n()rders(Q!..:clcT71'u:rn, c1.:d, order_date, caTdnurtl, a.nd\n()rderlists( ordernurn,i8b..!.~, qty,\n8hip~date)\nThe resulting t\\VO relation,s~ ()rders and ()rderlists, are both in BCNF', and the\ndecornposition is lossless-join since oTcle'rn,'ll'rn is a key for (the new) ()rders. The\nr(-~ader is invited to check that this decolnposition is also dependency-preserving.\nl?or cornpleteness, ,ve give th,e S(~L DIJL for the ()rders and Orderlists relations\nbelow:\n\n641\nFigure 19.16\nER Diagram Reflecting the Final Design\nCREATE TABLE Orders ( ordernurn\nINTEGER,\ncid\nINTEGER,\norder_date\nDATE,\ncardnum\nCHAR(16),\nPRIMARY KEY (ordernlllll),\nFOREIGN KEY (cid) REFERENCES Custolllers )\nCREATE TABLE Orderlists (ordernurll\nINTEGER,\nisbn\nCHAR (10),\nqty\nINTEGER,\nship~date\nDATE,\nPRIMARY KEY (ordernurn, isbn),\nFOREIGN KEY (isbn) REFERENCES Books)\nF'igure 19.16 shc)\\\\TS an updated ER diagrarn that reflects the new design. Note\nthat DBDudes could have arrived inunedia,tely at this diagrarn if they ha\"d llla.de\n()rders an entity set instead of a relationship set right at the beginning. But at\nthat tilne they did not understand the requirernents cornpletely, and it seeTHed\nnatural to rnodel Orders a.I) a relationship set. This iterative refinernent process\nis typical of real-life da,tabase design processes. As DBI)udes has learned over\ntirne, it is rare to achieve an initial design that is not changed as a project\nprogresses.\nT'he DBI)udes tea,lll celebrates the successful cornpletion of logical dataJn1,.'3c\ndesign and scherna refinelnent by opening a bottle of charnpagne and charging\nit to B&:N. After recovering frorn the celebration~ they lIlove on to the physical\ndesign phase.\n\n642\nCHAPTER 19\n19.10\nREVIEW QUESTIONS\nAnswers to the review' questions can be found in the listed sections.\n•\nIllustrate redundancy and the problerns that it ca.n cause. Give excunples\nof insert, delete, and npdate anoInalies. Can 'fudl values help address these\nproblerlls? Are they a cOlnplete solution? (Section 19.1.1)\n•\n\"VVhat is a decoTnpositio'n and how does it address redundancy?\nWhat\nproblerlls Inay be caused by the use of decolupositions? (Sections 19.1.2\nand 19.1.3)\n•\nDefine junctional dependencies.\nHow are pr'lnLary keys related to FDs?\n(Section 19.2)\n•\nV\\Then is an PD j implied by a set F of FDs? Define Armstrong's Axioms,\nand explain the statement that \"they are a sound and cornplete set of rules\nfor FD inference.\" (Section 19.3)\n•\nWhat is the dependency closure F+ of a set F of FDs? What is the at-\ntribute closure X+ of a set of attributes X with respect to a set of FDs F?\n(Section 19.3)\n•\nDefine INF, 2NF, 3NF, and BCNF. What is the nlotivation for putting a\nrelation in BCNF? What is the motivation for 3NF? (Section 19.4)\n•\nWhen is the decomposition of a relation schenla R into two relation schemas\nX and Y said to be a lossless-join decomposition? Why is this property\nso irnportant? Give a necessary and sufficient condition to test whether a\ndecc)1nposition is lossless-join. (Section 19.5.1)\n•\nWhen is a decornposition said to be depc'ndency-preserving? \\Vhy is this\nproperty useful? (Section 19.5.2)\n•\nDescribe how we can obtain a. lossless-join decornposition of a relation into\nBCNI\"\"\"'. Give an exanlple to show that there rnay not be a dependency-\nprest~rvingdecornposition into BCNF. Illustrate how a given relation could\nbe decornposed in different ways to arrive at several alternative decornposi-\ntions, and discuss the irnplications for clatabc~\"e design. (Section 19.6.1)\nIlIll\n(jive an cx.:arnple that illustrates how a collection of relations ill BCNF\ncould have redundancy even though each relation, by itself, is free fronl\nredundancy. (Section 19.6.1)\n•\nWhat is a. Tninirnal cover for a set of I:(I)s?\nDescribe an algorithrn for\ncornputing the rninirnaJ cover of B.\nS(~t of FI)s, and illustrate it with an\nexarnple. (Section 19.6.2)\n\nSche77~a Refincrnent and N oT'Tnal FOTin8\n64:3\n•\nDescribe hovv the algorithl11 for lossless-join decolnposition into BCNF can\nbe adapted to obtain a lossless-join, dependency-preserving decornposition\ninto 3NF. Describe the alternative synthesis approach to obtaining such\na decorllposition into 3NF. Illustrate both approaches using an exarnple.\n(Section 19.6.2)\n•\nDiscuss how scherna refinernent through dependency analysis and norrnal-\nization can iInprove scheIna.-') obtained through ER, design. (Section 19.7)\n•\nDefine 'Tn'l1,ltival'Ued dependencies, .Join dependencies, and inclusion depen-\ndencies. Discuss the use of such dependencies for database design. Define\n4NF and 5NF, and explain how they prevent certain kinds of redundancy\nthat BCNF does not eliminate. Describe tests for 4NF and 5NF that use\nonly FDs. What key assumption is involved in these tests? (Section 19.8)\nEXERCISES\nExercise 19.1 Briefly answer the following questions:\n1. Define the term functional dependency.\n2. Why are some functional dependencies called trivial?\n3. Give a set. of FDs for the relation schema R(A,B, C,Dj with prilnary key AB under which\nR is in 1NF but not in 2NF.\n4. Give a set of FDs for the relation schelna R(A,B, C,Dj with prilnary key AB under which\nR is in 2NF but not in 3NF.\n5. Consider the relation schelna R(A,B, OJ, which has the FD B ~ C. If A is a candidate\nkey for R, is it possible for R to be in BCNF? If so, under what conditions? If not,\nexplain why not.\n6. Suppose we have a relation schema R (A, B, OJ representing a relationship between two\nentity sets with keys A and 13, respectively, and suppose that R has (aIIlong others) the\nFDs A ..._+ Band 13 -ot A. Explain what such a pair of dependencies means (i.e., what\nthey irnply about the relationship that the relation nlOdels).\nExercise 19.2 Consider a relation R with five attributes ABCDE. You are given the follc)\\ving\ndependencies: A --t B, Be ~ B, and BD ~ A.\n1. List all keys for R.\n2. Is R in :3NF?\n3. Is R in BCNF?\nExercise 19.3 Consider the relation shown in Figure 19.17.\n1. l...ist all the functional dependencies that this relation instance satisfies.\n2. ASSUIlIe that the value of attribute Z of the la..<:;t record in the relation is cluLuged frorH\n23 to Z2. Now list all the functional dependencies that this relation instance satisfies.\nExercise 19.4 Assurne that you are given a. relation with attributes A BCD.\n\n644\nXl\nYl\nZl\nXl\nYl\nZ2\nX2\nYl\nZl\n~r2\nYl\nZ~~\n'--.\nFigure 19.17\nRelation for Exercise 19.:3.\nCJHAPTER }g\n1. Asslune that no record has NULL values. \\Nrite an SQL query that checks whether the\nfunctional dependency A\n---,)0 B holds.\n2. Assulne again that no record has NULL values. \\tVrite an SQL assertion that enforces\nthe functional dependency A -+ B.\n3. Let us now aSSUlne that records could have NULL values.\nRepeat the previous two\nquestions under this assurnption.\nExercise 19.5 Consider the following collection of relations and dependencies. Assume that\neach relation is obtained through decomposition from a relation with attributes ABCDEFGHI\nand that all the known dependencies over relation ABCDEFGHI are listed for each question.\n(The questions are independent of each other, obviously, since the given dependencies over\nA BCDEFGHI are different.) For each (sub)relation: (a) State the strongest nonnal fonn that\nthe relation is in. (b) If it is not in BCNF, decompose it into a collection of BCNF relations.\n1. Rl (A. C,B.D,E), A -+ 13, C -+ D\n2. R2(A,B,F), AC -+ B, B -+ F\n3. R3(A~DJ.. D\n\"\".of G, G -+ H\n4. R4(D, C,H, G), A\n-~·t I, I -+ A\n5. R5(A.I,C.B)\nExercise 19.6 Suppose that we have the following three tuples in a legal instance of a relation\nschema S with three attributes ABC (listed in order): (1,2,:3), (4,2,3), and (5,3,3).\n1. \\Vhich of the follc)\\\\ring dependencies can you infer does 'lUJl hold over scherna S?\n(a) A -+ 13, (b) Be ---'1 A, (c) 13 ..._-, C\n2. Can you identify allY dependencies that hold over 5''?\nExercise 19.7 Suppose you are given a rebltion R \\vith four attributes AB()D.F'or each of\nthe following sets of FDs, assurning those are the only dependencies that hold for R, do the\nfollowing:\n(a) Identify the candidate key(s) for R. (b) Identify the best Honnal forBl thatR\nsatisfies (lNF, 2NF, :JNF, or BeNF). (c) If Ii is not in\nBCNl~\\ decOlnpose it into a set of\nBCNF relations that preserve the dependencies.\n1.\nC:-······ D, C----+ A. 13\nC\n2.\nFJ\n'-~-f C'. D --> A\n:3. ABC\n-~f D, D --> A\n4.\nA -+ B. B(}\n,. D. A\n~. C\n5.\nA13 _....... C, AB···_··.• D. C -+ A, D·_·-+ 13\n\nScherna llefincl1H;nt (nul IVorrnal\n1:\"~oTn18\n645\nExercise 19.8 Consider the attribute set Ii = ABCDEGH (Lud theFD set F= {A.B·--+ C:,\nAC --+ B: AD ---4 E, B\n-----+ D, Be --+ 11, B -!- G}.\n1. For each of the following attribute sets, do the following:\nCornpute the set of depen-\ndencies that hold over the set and write down a rninirnal cover. (ii) Narne the strongest\nnonnal [onn that is not violated by the relation containing these attributes.\n(iii) De-\nCOlnpose it into a. collection of BCNF relations if it is IH)t in BeNF'.\n(a) ABC, (b) ABCD, (c) ABCEG, (d) DC:BGII, (e) ACEH\n2. vVhich of the following decOIllpositions of R = ABCDEG, with the saIne set of depen-\ndencies F', is (a) dependency-preserving? (b) lossless-join?\n(a) {AB, BC, ABDE. EG }\n(b) {ABC, ACDE, ADG }\nExercise 19.9 Let R be decOIllposed into R 1 , R2 , ... , Rn . Let F be a set of FDs on R.\n1. Define what it rlleans for F to be pre8erved in the set of decOlllposed relations.\n2. Describe a polynomial-tirne algorithm to test dependency-preservation.\n3. Projecting the FDs stated over a set of attributes X onto a subset of attributes Y requires\nthat we consider the closure of the FDs. Give an exarnple where considering the closure\nis irnportant in testing dependency-preservation, that is, considering just the given FDs\ngives incorrect results.\nExercise 19.10 Suppose you are given a relation R(A,B, C,D). For each of the following\nsets of FDs, assuming they are the only dependencies that hold for R, do the following: (a)\nIdentify the candidate key(s) for R. (b) State whether or not the proposed decOlnposition of\nR into smaller relations is a good decolllposition and briefly explain why or why not.\n1. B --+ C, D --+ A; decornpose into BC and AD.\n2. AB -+ C, C -~~ A, C ,--)- D; decompose into A CD and Be.\n~~. A -!- BC, C -+ AD; decornpose into ABC and AD.\n4. A -!- B, B\nC, C '-1 D; decornpose into AB and A CD.\n5. A --+ B, B -+ C, C -!- D; decOInpose into AB, AD and CD.\nExercise 19.11 Consider a relation R that has three a\"ttributes ABC. It is decornposed into\nrelations R 1 with attributes AB and R 2 with attributes Be.\n1. St<lte the definition of a lossless-join decOlnposition with respect to this exarnple. Answer\nthis question concisely by \\-vriting a relational algebra equation involving R, R 1 , and H2.\n2. Suppose that B --+--+ C. Is the decorHposition of R into .R! and R2 lossless-join? Reconcile\nyour (l.,nswer with the observation that neither of the FDs HI nR2\n----'> R I nor R I n R2 --+ .H..2\nhold, in light <;)f the siInple test offering a necessary and sufficient condition for lossless-\njoin decmnposition into two relations in Section 15.6.1.\n:3. If you are given the follc}\\\\ring justa.nees of R 1 a,nd 112, what can you say about the\ninstance of R from which these were obtained? Answer this question by listing tuples\nthat are definitely ill R and tuples that a.re possibly· in R.\nInstance of RJ. = {(5,l), (6,l)}\nInstance of R 2\n:::::: {(l,8), (1,9)}\nCan you say that attribute B definitely is or 'is not. a key for R?\n\n646\nCHAPTER 19\nExercise 19.12 Suppose that we have the following four tuples in a relation 5' with three\nattributes ABC: (1,2,:3), (4,2,:3), (5,3,:3),\n(5,::~A). \\Vhich of the following functional (-+) and\nrIlultivalued (-+--»\ndependencies can you infer does not hold over relation S?\n1. A -+ 13\n2. A _....-} ..._,,-} B\n3. Be --,} A\n4. BG -+_..~> A\n5. 8 -+ C\n6. B\n~'---'o C\nExercise 19.13 Consider a relation R with five attributes A BCDE'.\n1. For each of the following instances of R, state whether it violates (a) theFD Be \",,\"-, D\nand (b) the J\\fVD Be _..-t--+ D:\n(a) { } (i.e., mnpty relation)\n(b) {(0,,2,3,4,5), (2,a,3,5,5)}\n(c) {(o,,2,3,4,5), (2,0,,3,5,5), (o,,2,3,4,6)}\n(d) {( a,2)~,4,5), (2,0,,3,4,5), (o,,2,3,6,5)}\n(e) {(0,,2,3,4,5), (2,0,,3,7,5), (a,2,3,4,6)}\n(f) {(o,,2,3,4,5), (2,0,,3,4,5), (0,,2,3,6,5), (o,,2,3,6,6)}\n(g) {(a,2,~3,4,5), (0,,2,:3,6,5), (0,,2,3,6,6), (o,,2,3,4,6)}\n2. If each instance for R listed above is legal, what can you say about the FD A -+ B?\nExercise 19.14 JDs are lllotivated by the fact that sornetilnes a relation that cannot be\ndecoruposed into two sinaller relations in a lossless-join rnanner can be so deC0111pOsed into\nthree or rnore relations. An exa,rnple is a relation with attributes 8upplier, part, clnd J}Toject,\ndenoted SPJ, with no FDs or l'vlVDs. The JD [Xl {SP, P J, J S} holds.\nFrorn the JD, the set of relation scheines SP, PJ, and JS is a IORsless-join decornposition of\nSPJ. Construct an instance of HPJ to illustrate that no two of these schernes suffice.\nExercise 19.15 Answer the following questions\n1. Prove that the algorithrn shown in Figure 19.4 correctly cornputes the ::l.ttribute closure\nof the input attribute set X.\n2. Describe a linear-tirne (in the size of the set of FI)s, where the size of each FD is the\nnurnber of attributes involved) algoritlun for finding the attribute closure of a set of\nattributes with respect to a set of FDs. Prove that your algoritlun correctly COInputes\nthe attribute closure of the input attrilnlte set.\nExercise 19.16 Let us say that an 'Fl) )( -_.,} Y is si'mple if Y is a single attribute.\n1. Replace the FD AB........, CD l.Jy the srnallest equivalent collection of sirnple FDs.\n2. Prove that everyFD X _..crY\" in\n(.'L set of FDs F Cetn be replaced by a set of sirnple F'Ds\nsuch that p+ is equal to the closure of the new set of FDs.\n\nScherria RejineTnent and NOT'nial 1?oT7ns\n641\nExercise 19.17 Prove that Arrnstrong's i-\\xiorns are sound and cOluplete for FD inference.\nrT'hat is, sh()\\v that repeated application of thesf; ttxiOlllS on a set F ofF'Ds produces eX(lctly\nthe dependencies in P+.\nExercise 19.18 Consider a relation R with attributes A BCDE. aLet the following FDs be\ngiven: A ~ BC, Be ----'!\" E, and E ~ DA. Siluilarly, let S be a relation with attributes ABC\n1DE\nand let the follo\\ving FDs be given:\nA-+ BC, B -+ IE, and E\n----'!\" DA. (Only the second\ndependency differs frolll those that hold over R.) You do not know whether or which other\n(join) dependencies hold.\n1. Is R in BCNF?\n2. Is R in ,1NF?\n3. Is R in 5NF?\n4. Is Sin BeNF?\n5. Is Sin 4NF?\n6. Is Sin 5NF'?\nExercise 19.19 Let R be a relation schelna with a set F of FDs.\nProve that the decOIll-\nposition of R into HI and R2 is lossless-join if and only if p+ contains HI n R 2\n----'!\"\nR 1 or\nR 1 n R2\n----'!\" R 2 .\nExercise 19.20 Consider a scheme R with FDs F that is decOlnposed into schelnes with\na..ttributes X and Y. Show that this is dependency-preserving if F'\n~\n(}~x U py)+.\nExercise 19.21 Prove that the optiInizatioll of the algorithrn for lossless··-join, dependency-\npreserving decornposition into ~)NF relations (Section 19.6.2) is correct.\nExercise 19.22 Prove that the 3NF synthesis algoritlull produces a lossless-join decOlnposi-\ntion of the relation containing all the original attributes.\nExercise 19.23 Prove that an ]\\IlVn .X -+-+ Y over a rehltion R can be expressed a,,') the\njoin dependency\n[Xl {-,YY, X(R - Y)}.\nExercise 19.24 Prove that, if R has only one key, it is in BCNF if and only if it is in ~3NF.\nExercise 19.25 Prove that, if R is in 3NF and every key is shnple, then R is in HeNF.\nExercise 19.26 Prove these staternents:\n1. If a relation scherne is in BCNF and at lea.st one of its l\\.eys consists of }1 single attrilmte,\nit is also in 4NF.\n2. If a relation scherne is in :3NF and each key has a single attribute, it is also in 5NF'.\nExercise 19.27 Give <ill algorithrn for testing whether a relation scheme is in BCNF. ]'he\n:.llgorithrn should l)e polynorniaJ in the size of the set of given FDs. ('rhe size is the surn over\nall FI)s of the nurnber of attributes that a,ppear in theFJ).) Is there ::t polyuOlnial algorithrn\nfor testing whether a relation scheme is in :~NF?\nExercise 19.28 Give ('UI algorithm for testing \\vhether a relation scherne is in BCjNF'.\n'I'hf~\naJgorithrn should be polynomial in the size of the set of given FI)s. CI'he 'size' is the SUln over\nall FI)s of the nurnber of attributes that appear in theFD.) Is there a polynomial algorithrn\nfor testing whether a relation scherne is in :~NF?\nExercise 19.29 1)rove that the algorithm for decomposing a re1cltion scherna with a set of\nFI)s into a collection of BC~NS relation schenlas as describerl in Section 19.G.l is correct (i.e.,\nit produces a collection of BCNF relations, and is lossless-join) and terrninates.\n\n648\nBIBLIOGRAPHIC NOTES\n(;I'IAPTER 19\nTextbook presentations of dependency theory and its use in database design include [3, 45,\n501) 509, '747J. Good survey articles on the topic include [755,415].\nFDs \\vere introduced in [187], along with the concept of ::3NF, and aximlls for inferring FDs\nwere presented in P38]. BCNF was introduced in [188]. The concept of a legal relation instance\nand dependency satisfaction are studied fonnaUy in [828] .FDs were generalized to scrnantic\ndata Illodels in [768].\nFinding a key is shown to be NP-COlnplete in [497]. Lossless-join decOlnpositions were studied\nin [28, 502, 627]. Dependency-preserving decorIlpositions were studied in [74]. [81] introduced\nrninirnal covers. DecOlnposition into :3NF is studied by 1.81, 98] and decOlnposition into BCNF\nis addressed in [742]. [412] shows tha,t testing whether a relation is in ~1NF is NP-cOlnplete.\n[253] introduced 4NF and discussed decoillposition into 4NF. Fagin introduced other nonnal\nforIlls in [254J (project-join nonnal fonn) and [255] (doHluin-key nonnal forIll). In contrast to\nthe extensive study of vertical decOlnpositions, there has been relatively little formal investi-\ngation of horizontal decompositions. [209] investigates horizontal decoillpositiolls.\nIvIVDs were discovered independently by Delobel [211] ,Fagin [253], and Zaniolo [789]. AxiOlllS\nfor FDs and MVDs were presented in [73}.\n[593] shows that there is no axiornatization for\nJDs, although [662] provides an axioHlatization for a more general class of dependencies. The\nsufficient conditions for 4NF and 5NF in tenns of FDs that were discussed in Section 19.8 are\nfrom [205]. An approach to database design that uses dependency inforrnation to construct\nsarnple relation instances is described in [508, 509].\n\n20\nPHYSICAL DATABASE\nDESIGN AND TUNING\n--What is physical database design?\n..\nWhat is a query workload?\n...\nHow do we choose indexes? What tools are available?\n..\nWhat is co-clustering and how is it used?\n..\nWhat are the choices in tuning a database?\n..\nHow do we tune queries and view?\n..\nWhat is the impact of concurrency on perforrnance?\n..\nHow can we reduce lock contention and hotspots?\n..\n\"'That are popular database benchnlarks and how are they used?\n...\nKey concepts: Physical database design, database tuning, workload,\nco-clustering, index tuning, tuning wizard, index configuration, hot\nspot, lock contention, database benchmark, transactions per second\nAdvice to a client who cornplained al)out rain leaking through the roof onto the\ndining Utble: \"J\\!!ove the table.\"\n............Architect Frank IJoyd \\\"lright\nThe perfonnance of a 1)131:18 on cornrnonly &':lked queries and typical update\noperations is the ultirnate Ineasure of a database desigIl. A I}BA can irnprove\nperforrnance by identifying perforrnance bottlenecks and adjusting sorne DBIVlS\npararneters (e.g., the size of the buffer pool or the frequency of checkpointing)\nor adding hanhvare to elirninate such bottlenecks. rIhe first step in achieving\nGi19\n\n650\nCHAPTER 20\ngood perforlnancc, however, is to Inake good databa.se design choices, which is\nthe focus of this chapter.\nAfter we design the conceptual and exte'rnal scherna..'3, that is, create a collection\nof relations and views along 'with a set of integrity constraints, we .Il1ust address\npe1'forlna11c8 goals through physical database design, in which we design the\nphysical sche111a. As user requirernents evolve, it is usually necessary to tune,\nor adjust, all &\"3pects of a database design for good perforrnance.\nThis chapter is organized as follows. We give an overview of physical database\ndesign and tuning in Section 20.1. The 1nost irnportant physical design deci-\nsions concern the choice of indexes. We present guidelines for deciding which\nindexes to create in Section 20.2. These guidelines are illustrated through sev-\neral exalnples and developed further in Sections 20.3. In Section 20.4, vve look\nclosely at the irnportant issue of clustering; we discuss how to choose clustered\nindexes and whether to store tuples fron1 different relations near each other (an\noption supported by sorne DBMSs). In Section 20.5, we empha..\"3ize how well-\nchosen indexes can enable some queries to be answered without ever looking at\nthe actual data records. Section 20.6 discusses tools that can help the DBA to\nautornatically select indexes.\nIn Section 20.7, we survey the lnain issues of database tuning.\nIn addition\nto tuning indexes, we 111ay have to tune the conceptual schema as well as\nfrequently used query and view definitions. We discuss how to refine the con-\nceptual schelna in Section 20.8 and how to refine queries and view definitions\nin Section 20.9. We briefly discuss the perforrnance irnpact of concurrent access\nin Section 20.10. We illustrate tuning on our Internet shop exarnple in Section\n20.11. \\Ye conclude the chapter with a short discussion of DBMS benchrnarks in\nSection 20.12; benchrnarks help evaluate the perfOI'lnanCe of alternative DBl\\1S\nproducts.\n20.1\nINTRODUCTION TO PHYSICAI.J DATABASE\nDESIGN\nLike all other a.spects of elataba.se design, physical design rnust\nb(~ guided by\nthe nature of the data, a,nd its intencled use. In particular, it is irnportant tonn-\nderstand the typical workload th<:tt the database lI1Ust support;\ntlH~ vvorklc)ad\nconsists of a rnix of queries a.nel updates. 1Jsers (:tlso have certain requirenl(~nts\nabout ho\\v fast cert;:lin queries or llpdat(~s 11111st run or ho\\v rnan.y tran.sactions\nrnust be processed per second.\nrrhe \\vorkload\n(l<~scription and users' perfor-\nIn(U1C(~ reqllirernents are the ba\"sis on \\vhich a nurnber of decisions have to be\nrnade during pl1ysical datab':lse design.\n\nPhysical Dat;abase Design, an,cl l1zJ,ni'ltg\n651\nr-..····_·_,,·,_·.._~.._···_··_-.\"_.~ _-_.-.-.._-_._\n_-~\"\" __\n_~ _\n._\n_\n..~ --·_ l\nI\nIdentifying Perfornlance Bottlenecks: All cornrrlercial systeuls pro-\nI\nvide a suite of tools for rnonitoring a wide range of systeIll paralneters.\n'I'\nThese tOOlS. ' used properlY\" C,Ul help identify perfOfIna.nc.e bO.ttlenecks and\nsuggest aspects of the databc1..'Se design and application code that need to\nI\nbe tuned for perforlnance. For exarnple 1 •\"ve can ask the DBMS to rnonitor\nI\nthe execution of the database for a certain period of tinle and report on\ni\nthe nurnber of clustered scans, open cursors, lock requests, checkpoints,\nbuffer scans, average wait titne for locks, and many such statistics that\ngive detailed insight into a snapshot of the live system. In Oracle, a report\ncontaining this inforlnation can be generated by running a script called\nUTLBSTAT. SQL to initiate monitoring and a script UTLBSTAT. SQL to termi-\nnate rnonitoring.\nThe system catalog contains details about the sizes of\ntables, the distribution of values in index keys, and the like. The plan gen-\nerated by the DBMS for a given query can be viewed in a graphical display\nthat shows the estimated cost for each plan operator.\nWhile the details\nare specific to each vendor, all InajaI' DBlIIS products on the market today\n~rovide a sUi~e of such tools.\n__~..\nTo create a, good physical database design and tune the systenl for perfor-\nmance in response to evolving user requirelnents, a designer HUlst understand\nthe workings of a DBMS, especially the indexing and query processing tech-\nniques supported by the DBMS. If the database is expected to be accessed\nconcurrently by rnany users, or is a d'istr'ibuted database, the task beeornes\nlnore cornplicated and other features of a, DBl\\1S CaIne into play. 'iVe discuss\nthe ilnpact of concurrency on database design in Section 20.10 and distributed\ndatabases in Chapter 22.\n20..1..1\nDatabase Workloads\nThe key to good physical design is DTriving at an accurate description of the\nexpectedworkloa.d. A workload description includes the follCJ\\ving:\n1. A list of queries (with their frequency, as\n<'1 rc:ttio of all queries / npdcltes).\n2, A list of updates and their frequencies.\n:3.Perfonnanc(~ goals for each type of query and update.\nFor each quer.y in the \\vorklo;:vL vve HUlst identify\n1\\1II\n\\Vhich relations (11'e accessecl.\n1\\1II\n\\Vhich attributes are n:~tained (in the SELECT clru.lse).\n\n652\nCHAPTER 20\n•\n\\Vhich attributes have selection Of join conditions expressed on thern (in\nthe WHERE clause) (UH:! hovv selective these conditions are likely to be.\nSiInilarly, for each update in the \\vorkloacl, \\ve Blust identify\n•\nvVhich attributes have selection or join conditions expressed on therll (in\nthe WHERE clause) and ho\\v selective these conditions are likely to be.\nB\nThe type of update (INSERT, DELETE, or UPDATE) and the updated relation.\n•\nFor UPDATE cOHnuands, the fields that are rnodified by the update.\nR.ellleluber that queries and updates typically have parameters, for exarnple, a\ndebit or credit operation involves a particular account nUlnber. rrhe values of\nthese paralneters deterlnine selectivity of selection and join conditions.\nUpdates have a query cornponent that is used to find the target tuples. This\ncOlllponent can benefit froIn a good physical design and the presence of indexes.\nOn the other hand, updates typically require additional work to ITlaintain in-\ndexes on the attributes that they 111odify. Thus, while queries can only benefit\nfroill the presence of an index, an index rnay either speed up or slow down\na given update.\nDesigners should keep this trade-off in rnind when creating\nindexes.\n20.1.2\nPhysical Design and Tuning Decisions\nIrnportant decisions rnade during physical datab&'3e design and database tuning\ninclude the follovving:\n1. Choice of indexes to create:\nII\n\\Vhich relations to index and which field or cornbination of fields to\nchoose as index search keys.\nII\nFor each index, should it be clustered or ul1clustered?\n2. Tuning the conceptual 8chenLa:\nl1li\nAlternative 'fuJTYnalized 8cherna/): \\Ve usually have rnore than one way\nto dec()1npose a schclua into a desired IlOl'Inal fOITn (BCNF or 3NF).\nA choice can be rnade on the basis of perforrnance criteria,.\nII\nDen,OT7Tl.alizat'io'n: \\Ve Inight\\vant to reconsider scherna decolnposi-\nbons ca.rried Ollt for norrnalization. during the conceptual schern.a de-\nsign process to irnprove the perforrnance of queries that involve at-\ntributes fr0111 several pn:~viously decornposed relations.\n\nII\nl/cr\nvtical part'itioning: LJnder certain circurnstances we rnight 'want to\nfurther decornpose relations to ilnprove the perfornlance of queries\nthat involve only a fevv attributes.\nII\nViC'U1S: \\Ve luight 'want to add sorne vie\\vs to nlask the changes in the\nconceptual scherna fr0111 users.\n3. Query and tTansact'ioTl, t'UJLing: Frequently executed queries and transac-\ntions ulight be re\\\\rritten to run fc1..ster.\nIn paTallel or distributed databases, \\vhichwe discuss in Chapter 22, there are\nadditional choices to consider, such (laS whether to partition a relation across\ndifferent sites or whether to store copies of a relation a,t rnultiple sites.\n20.1.3\nNeed for Database Thning\nAccurate, detailed workload infonnation 111Cly be hard to corne by while doing\nthe initial design of the systen1. Consequently, tuning a database after it has\nbeen designed and deployed is ilnportant---we HlllSt refine the initial design in\nthe light of actual usage patterns to obtain the best possible perfonnance.\nThe distinction bet\\veen database design and database tuning is soruewhat\narbitrary.\nWe could consider the design process to be over once an initial\nconceptual sche1na is designed and a set of indexing and clustering decisions\nis nlade.\nAny subsequent changes to the conceptual scherna or the indexes,\nsay, would then be regarded as tuning. Alternatively, we could consider sorne\nrefinernent of the conceptual scheula (and physical design decisions afl'ected by\nthis refinernent) to be part of the physical design process.\nvVh,ere we draw the line between design and tuning is not very irnpoItant, and\nwe sirnply discuss the issues of\nin(lE:~x selection and databa..'3c tuning without\nregard to when the tuning is carrier} out.\n20.2\n(;UIDEI.JINES FOR INDEX SELECTION\nIn considering \\vhich indexes to create; we begin \\\\rith the list of queries (includ-\ning queries tha,t a.ppear as paTt of update operations). ()bviously, only relations\naccessed by sorTle qu(~ry need to be considered as candidates for indexing, and\nthe choice of attributes to index is guided by the conditions that appear in the\nWHERE clauses of the queries in the \\vorkload. 1'he presence of suitable indexes\ncan significa.,ntly irnprove the evaluation plan for\n(1, query, EtS\\Ve saw in Chapters\n8 and 12.\n\n654\n(;HAPTER20\n()ne approach to index selection is to consider the Ulost irnportant queries in\nturn, and, for each, deterrnine \\vhich plan the optiInizer \\vould choose given\nthe indexes c:urrently on our list of (to be crc(;tted) indexes. Then\\ve consider\n\\vhetherwe can aTrive at a substantially better plan by a,clding 1110re indexes; if\nso. these additional indexes are candidates for inclusion in our list of indexes.\nIn general, range retrievals benefit froIn a 13'-f- tree index, and exact-IHatch\n..\n,-\nretrievals benefit frorn a hash index. Clustering benefits range queries, and it\nbenefits exact-rnatch queries if several data entries contain the saIne key value.\nBefore adding an index to the list, hovvever, \\ve lnust consider the irnpact of\nhaving this index on the upda,tes in our workload. As \\ve noted earlier, although\nan index can speed up the query cornponent of an update, all indexes on an\nupdated attribute---{)n any attribute, in the case of inserts Cl.,nd deleteslnust\nbe updated whenever the value of the attribute is changed.\nTherefore, we\nJ.nust sOlnetirnes consider the trade-off of slo\\ving sorne update operations in\nthe workload in order to speed up sorne queries.\nClearly, choosing a good set of indexes for a given workload requires an un-\nderstanding of the available indexing techniques, and of the workings of the\nquery optiruizer.\nThe following guidelines for index selection sunnnarize our\ndiscussion:\nWhether to Index (Guideline 1): The obviollS points are often the lnost\nirnportant.\nDo not build an index unless sorne query including the query\ncOlnponents of updates\nbenefits frolu it.\nWhenever possible, choose indexes\nthat speed up rIlore than one query.\nChoice of Search Key (Guideline 2): Attributes rnentioned HI a, WHERE\nclause <),re ca,ndidates for indexing.\n11I'I\nAn exact-rnatch selection condition suggests that \\ve consider an irldex on\nt}le selected attributes, ideally,\n<:-}, hash index.\nII\nj\\ range selection condition suggests that we consider a 13+- tree (Of ISA1/1)\nindex on the selected attrilnltes. j\\ B+ tree index is 11S1U111y preferaJ)le to\nan ISA1/1 index. A.n ISA:NI irlclex rnay l)e vvorth considering if the relation is\ninfrequently updated, but we assurne that a B-t--- tree index is (lhvays chosen\nover an lSft.\\i\\,1 index, for sirnplicity.\nMulti-Attribute Search :Keys (Guideline 3): Indexes\\vith Inllitipl(~-attributc\nsea,rch keys slH)uld l)e considered in the follc)\\ving two situ<ltion.s:\nill\nj\\ WHERE clause includes conditinns 011 Inore t.han\non(~ attribute of a rela-\ntion.\n\n&--\n.\")'\"')\nI\n,_\nIII\nThey ellilble index-only evaluation strategies (i.e., accessing the relation can\nbe avoided) for irnportCtnt queries. (This situation Gould lead to attributes\nbeing in the sCc:lrch key even if they do not appear in WHERE clauses.)\nvVhen creating indexes on search keys wit.h rnultiple attributes, if range queries\naxe expected, be careful to order the attributes in the search key to Ina.tch the\nquenes.\nWhether to Cluster (Guideline 4): At lllost one index on a given relation\ncan be clustered, and clustering affects perfonnance greatly; so the choice of\nclustered index is iInportant.\nII\nAs a rule of tlnunb, range queries are likely to benefit the 1110St froIll clus-\ntering. If several ra,nge queries are posed on a relation, involving different\nsets of attributes, consider the selectivity of the queries and their relative\nfrequency in the workload when deciding which index should be clustered.\nII\nIf an index enables an index-only evaluation strategy for the query it is\nintended to speed up, the index need not be clustered. (Clustering Inatters\nonly when the index is used to retrieve tuples fr(nll the underlying relation.)\nIfash versus Tree Index (Guideline 5): A B-·t- tree index is usually prefer-\nable because it supports range queries as well as equality queries. A hash index\nis better in the following situations:\nIIi1\nThe index is intended to support index nested loops join; the indexed\nrelation is the inner relation, and the search key includes the join colurllns.\nIn this case, the slight ilnproveIllent of a hash index over a B·+ tree for\nequality self~ctions is rnagnified, because an equality selection is generated\nfor each tuple in the outer relation..\nII\nrrllcre is a very iInportant equality query, and no range queries, involving\nthe sea.rch key attributes.\nBalancing the C~ost of Index Maintenance (Guideline 6): After drawing\nup a\n~\\vishlist' of indexes to create, consider the irnpact of each index on the\nupdates in tl1cvvorkload.\n•\nIf Inaintainlng an index sl()\\vs do\\vn\nfrc~quent update operations, consider\ndropping the index.\nII\nI{eep ill rnind,\nhOvv(~ver, thaJ adding an index Illay 'well speed up a given\nupdate operation. For exanlplc, <111 index on enlployec~ IDs could speed up\nth(~ operaJion of increa,'3ing the salary of a given ernployee (specified b:y ID).\n\n20.3\nCHAPTER.20\nBASIC EXAMPLES OF INDEX SELECTION\nThe follawing examples illustrate hen¥ to choose indexes during databa..sc design,\ncontinuing the discussion froln Chctpter 8, \\vherewe focused on index selection\nfor single-table queries. The schernas used in the exarnples are not described in\ndetail; in general, they contain the attributes nalned in the queries. Additional\ninforlnation is presented \"when necessary.\nLet us begin with a silnple query:\nSELECT E.enaIne 1 D.rugI'\nFROM\nEnlployees E, DepartInents D\nWHERE\nD.dncune='Toy' AND E.dno=D.dno\nThe relations rnentioned in the, query are Enlployees and Departnlents, and\nboth conditions in the WHERE clause involve equalities. Our guidelines suggest\nthat we should build hash indexes on the attributes involved. It seeIns clear\nthat we should build a hash index on the dnaTne attribute of DepartInents. But\nconsider the equality E. dno=D. dno. Should we build an index (hash, of coursf~)\non the dno attribute of Departrnents or Ernployees (or both)? Intuitively, we\nwant to retrieve Departments tuples using the index on dnarne because few\ntuples are likely to satisfy the equality selection .D. dnaTne= 'Toy '.1\nFor each\nqualifying Departrnents tuple, we then find lnatching EInployees tuples by using\nan index on the dno attribute of Ernployees. So, we should build an index on the\ndno field of En1ployees. (Note that nothing is gained by building an additional\nindex on the dno field of Departrnents because Departnlents tuples are retrieved\nusing the dna:rne index.)\nOur choice of indexes was guided by the query evaluation plan we \\vanted\nto utilize. This consideration of a, potential evaluation pla.n is connnon while\nrnaking physical design decisions.\nU·nderstanding query optirnization is very\nuseful for physical design.\n\"VVe show the desired plal1 for this query in Figure\n20.1.\nAs a variant of this query, suppose that the WHERE clause is rnodified to be\nWHERE J). dnarne= 'Toy\n~ AND E'.dno=D. dno AND\nE'1. 0,oge=25. Let us consider al-\nternative evaluation plans.\n()ne good plan is to retrieve Departrnents tuples\nthat satisfy the selection on dnarne and retrieve rnatching Ernployees tuples by\nusing an index on the dno field; the selection on age is then applied on-the-fly.\nTIc)\\vever, unlike the previous variant of this Cjllcry, vve do not really need to\nhave an index on the dna field of Ernployees if \\ve have an index. on age. In this\n------_......\n_._--\nlThis is only a heuristic. If dnarne is not the key, and we have no statistics to verify this cla.inl. it\nis possible that several tuples satisfy this condition.\n\nPhysical Databa8c De,sign and Tuning\nename\nIndex Nested Loops\ndno:::dno\n657\nIt\nOdname::::'Toy'\nDepartment\nEmployee\nFigure 20.1\nA Desirable Query Evaluation Plan\ncase we can retrieve Departrnents tuples that satisfy the selection on dnarrte (by\nusing the index on dname, as before), retrieve Ernployees tuples that satisfy the\nselection on age by using the index on age, and join these sets of tuples. Since\nthe sets of tuples we join are srnall, they fit in 111ernory and the join Inethod is\nunirnportant. This plan is likely to be sornewhat poorer than using an index on\ndno, but it is a reasonable alternative. rrherefore, if we have an index on age\nalready (prolnpted by sorne other query in the workload), this variant of the\nsarnple query does not justify creating an index on the dno field of Ernployees.\nOur next query involves a range selection:\nSELECT\nFROM\nWHERE\nE.enan1e, I),dnarne\nElnployees E, Departrnents D\nB.sal BETWEEN 10000 AND 20000\nAND E.hobby='Starups' AND E.dno=D.dno\nT'his query illustrates the use of the BETWEEN operator for expreSSIng range\nselections. It is equivalent to the condition:\n10000 ::; E.sal AND E.sal :S 20000\nl~he use of BETWEEN to express rarlge conditions is reconunended; it lnakes it\ncc1.5ier for both the user and the optilnizer to recognize both parts of the range\nselection.\nlleturning to the exarnple query, both (nonjoin) selections are on the Ernployees\nrelation.\nTherefore, it is clear that a plan in which Eluployees is the outer\nrelation and I)epartrnents is the inner relation is the best, as in the previous\nquery, and\\ve should build a hash index on the dno attribute of Departlnents.\nBut vvhich index should vve build on Ernployees?\n1\\. 13+ tree index on. the sal\nattribute would help with the range selection, especially if it is clustered.\nA\n\n658\nCHAPTER 20\nhash index on the hobby attribute \\\\Tould help ·with the equality selection.\nIf\none of these indexes is available, we could retrieve Ernployees tuples using this\nindex, retrieve rnatching Departruents tuples using the index on dno~ aJld apply\nall rernaining selections and projections on-the-fly. If both indexes are available,\nthe optirnizer would choose the rno1'e selective index for the given query; that is,\nit \\vollld consider \\vhich selection (the range condition on salary or the equality\non hobby) ha\",,; fe\\ver qualifying tuples. In general, which index is rnore selective\ndepends on the data. If there are very few people with salaries in the given\nrange and rnany people collect starnps, the B-t- tree index is best. Otherwise,\nthe hash index on hobby is best.\nIf the query constants are known (H,,\" in our exarnple) ~ the selectivities can be\nestiInated if statistics on the data are available. Otherwise, as a rule of thurnb,\nan equality selection is likely to be rnore selective, and a rea.sonable decision\nwould be to create a hash index on hobby.\nSornethnes, the query constants\nare not knowIl\",,··----we rnight obtain a query by expanding a query on a view at\nrUIl-tirrle, or we rnight have a query in Dynalnic SQL, which allows constants\nto be specified as wild-card variables (e.g., %X) and instantiated at run-tinle\n(see Sections 6.1.3 and 6.2). In this case, if the query is very important, we\nlllight choose to create a B+ tree index on sal and a hash index on hobby and\nleave the choice to be rnade by the optirnizer at run-tirrle.\n20.4\nCLUSrrERING AND INDEXING\nClustered indexes can be especially iInportant while accessing the inner relation\nin an index nested loops join. To understand the relationship between clustered\nhldexes and joins, let us revisit our first exarnple:\nSELECT E.enanle, D.rngr\nFROM\nI;~Inployees E, I)epartrnentsD\nWHERE\nI).dnalne=uroy~\nAND\nIi~.clno:=I).dno\n\\Ve concluded that a good evaluation plan is to use an index on dna:rne to re-\ntrieve DepartlnE:~ntstuples satisfying the condition on dnarne and to find. rnatch-\ning Ernployees tuples using an index on dna. Should these indexes be clustenxl?\nG·iven our asslunption that the\nnUlnl.H~r of tuples satisfying 1). dnarne:= 'Toy' is\nlikely to be srnall, \\ve should build an unclustered index on\ndnanu~. ()n the\nother IH:ind, Ernployees is the inner relation in an index nested loops join and\ndna is not El candidate key. 'fhis situation is a strong Etrgulnent that the index\non the dno field of Ernployees 8ho111(1 be clustered.\nIn\nfact~ bec;:Luse the join\nconsists of repeatedly posing equ.ality selections\nCH1 the dnofield of the inner\nrelation, this type of quer,Y is a stronger justification for rnaking tIle index on\ndno clustered than\n(1, sirnple\nsc~lecti(}n query such as the previous selection on\n\nPhysical Database Design and 7;l.lning\n659\nhobby. (Of courso, factors such as selectivities and frequency of queries have to\nbe taken into account a\",swell.)\n'rhe follc)\\ving oxaluplc, very sirnilar to the previous one, illustrates ho\\v clus-\ntered indexes can be used for sort-rnerge joins:\nSELECT E.enarne,D.rngr\nFROM\nErnployees E, DepartlTlents D\nWHERE\nE.hobby='Starnps' AND E.dno=D.dno\nThis query differs froIll the previous query in that the condition E. hobby-='Starnps i\nreplaces D. dnarne== 'Toy'. Based on the t1.'3sl.unption that there are few ernploy-\nees in the Toy departrnent, we chose indexes that would facilitate an indexed\nnested loops join with DepartlTlents as the outer relation. Now, let us suppose\nthat rllany ernployees collect stamps. In this case, a block nested loops or sort-\nrnerge join Blight be rnore efficient. A sort-rnerge join can take advantage of a\nclustered B+ tree index on the dno attribute in Departrnents to retrieve tuples\nand thereby avoid sorting Departrnents. Note that an unelustered index is not\nuseful----since all tuples are retrieved, performing one I/O per tuple is likely to\nbe prohibitively expensive. If there is no index on the dno field of Ernployees,\nwe could retrieve Ernployees tuples (possibly using an index on hobby, especially\nif the index is clustered), apply the selection E. hobby= 'Starnps ' Oll-the-fly, and\nsort the qualifying tuples on dno.\nAs our discllssion has indicated, when we retrieve tuples using an index, the\nirnpact of clustering depends on the rnunber of retrieved tuples, that is, the\nnuruber of tuples that satisf\\r the selection conditions that rnatch the index.\nAn unclustered index is just a.s good as\n(1, clustered index for a selection that\nretrieves a single tuple (e.g., an equality selection on a candidate key). As the\nllurnber of retrieved tuples increases, the unclustered index quickly becoHlcs\nrnore expensive than e'ven a sequential scan of the entire relation.\nAlthough\nthe sequential scan retrieves all tuples, each page is retrieved exactly\nonc(·~,\nwherea,s a page rIlay be retrieved as often as the rnunber of tuples it contains\nif an unclustered index is usee!' If blocked l/C) is perforrned (as is COl1nnon),\nthe relative advantage of sequential scan versus an llnclustered index increases\nfurther. (Blocked T/C) also speeds up access using a clustered index, of c(nus(~.)\n\\Ve illustrate the relationship bet\\veen the\nnUlnb(~r of r(~trieved tuples, vicv.red\nas a percentage of the total nurnber of tuples in the rehttioIl, a,nd the cost of\nvarious access rnethods in .Figure 20.2. vVe assurne that the query is a selection\non a single relation, for sirnplicity.\n(Note that this figure reflects the C()st of\nwriting out the result:\nother\\vise~ the line for seqnential scan weHlld be flat.)\n\n660\nCHAPTER 20\nCost\nUnclustered index\nRange in which\nuncl'Ustered index is\nbetter than sequential\nscan of entire relation\no\nPercentage of tuples retrieved\nFigure 20.2\nThe Impa.ct of Clustering\n20.4.1\nCo-clustering Two Relations\n100\nIn our description of a typical database systern architecture in Chapter g, we\nexplained how a relation is stored as a file of records. Although a file usually\ncontains only the records of SOIn8 one relation, SCHue systeIlls allow records\nfrorn Inore than one relation to be stored in a single file.\nrrhe database user\ncan request that the records froIll two relations be interleaved physically in this\n111anne1'. This data layout is sornetiInes referred to as co-clustering the two\nrelations. We now discuss when co-clustering can be beneficial.\nAs an exarnple, consider two relations with the following schernas:\nParts(pid: integer, pnarne: string, cost: integer, 8upplierid: integer)\nAsserllbly\"(part'id: integer, cOT!.!,ponent~d: integer, quantity: integer)\nIn this scherna the cO'nl,ponentid field of Assernbly is intended to be the pid\nof sorne part that is used as a cornponent in assernbling the part with pid\nequal to partido\nTherefore, the Assernbly table represents a l:N relationship\nbetween parts and their subparts; a part can have rnany\nS11bparts, but each\npart is the subpart of at rnost one part. In, the Parts table, pid is the key. For\ncOlnposite parts (those assernbled frorIl other parts, a.s indicated by the contents\nof Assclnb1y), the cost field is taken to be the cost of a.sselubling the part frorn\nits subparts.\nSuppose tha.t a frequent query is to find the (inllnediate) subparts of all parts\nsupplied by a given supplier:\nSELECT P.piel, .A.componentid\nFROM\nPql't'c' 'J\") AS''''(.'·\\I'Xll'')I't\" A\n.\n(L\"\n11:::1\n..\n,\n\\,S ..'\n\",r\n\"\n\nPhysical Database Design atut Tuning\nWHERE\nP.piel = A.partid AND P.supplierid = 'Acrne'\n6ljl\nA good evaluation plan is to apply the selection condition on Parts a,nel then\nretrieve rnatching Asselnbly tuples throngh an index on the partid field. Ideally,\nthe index on part'id should be clustered. rrhis plan is rea,,'30rHlbly good. :However,\nif such selections are COHU110n and we \\.vant to optirnize thorn further, ,ve can\nco-cZ'usteT' the tvvo tables. In this approach, we store records of the two tables\ntogether, \\vith each Parts record P follc)\\ved by all the Assernbly records ./1 such\nthat P.pid = A.partid. This approach improves on storing the two relations\nseparately and having a, clustered index on paTtid because it does not need an\nindex lookup to find the A.ssernbly records that rnatch a given Parts record.\nThus, for each selection query, we save a few (typically two or three) index\npage l/Os.\nIf we are interested in finding the imrnediate subparts of all parts (i.e., the\npreceding query with no selection on supplierid) , creating a clustered index on\npaTtid and doing an index nested loops join with Assembly as the inner relation\noffers good perfonnance. An even better strategy is to create a clustered index\non the paTtid field of Assernbly and the pid field of Parts, then do a sort-rnerge\njoin, using the indexes to retrieve tuples in sorted order.\nThis strategy is\ncomparable to doing the join using a co-clustered organization, which involves\njust one scan of the set of tuples (of Parts and Asselnbly, which are stored\ntogether in interleaved fashion).\nThe real benefit of co-clustering is illustrated by the following query:\nSELECT P.pid,A.componentid\nFROM\nParts P, Assernbly A\nWHERE\nP.pid = A.partid AND P.cost=10\nSuppose that rnany parts have cost = 10. This query essentially a,rnonnts to\na collection of queries in which we are given a Parts record and want to find\nrnatching Assernbly records. If we have an index on the cost field of Parts, we\ncan retrieve qualifying Parts tuples.\nI~'or each such tuple, we haNe to use the\nindex on Assernbly to locate records with the given pid. rrhe index access for\nA.ssernbly is avoided if we have a co-clustered organization. (()f courS8, vve still\nrequire all index on the cost attribute of Parts tuples.)\nSuch an optirnization is especiftlly irnportant if we ,vant to traverse several\nlevels of the part-subpart hierarchy..For excunplc, a COnll110Il query is to find\nthe totaJ cost of a part, vvhich requires us to\nrep(~atedly carry out joins of\nPa,rts (lIlel Asscrnbly.\nIncidentally~ if\n'\\:V(~ do not know the nurnber of levels in\nthe hierarchy ill adVallCf\\ the nUlnber of joins varies and the query cannot be\n(~xpressed in S(~L. The query can be ansvvered by ernbedcling an S(~L staterneIlt\n\n662\n(;IIAPTER 20\nfor the join insicie an iterative host language prograrll. fluw to express the query\nis orthogonal to our lnain point here, which is that co-clustering is especially\nbeneficial \\vhen the join in question is carried out very frequently (either because\nit arises repeatedly in an ilnportant query such as finding total cost, or because\nthe join query itself is asked frequently).\nTo sUIlunarize co-clustering:\nIII\nIt can speed up joins, in pa,rticular key· foreign key joins corresponding to\nl:N relationships.\nIII\nA sequential scan of either relation becornes slower. (In our exalnple, since\nseveral Assenlbly tuples are stored in bet\\veen consecutive Parts tuples, a\nscan of alll:>arts tuples becornes slower than if Parts tuples \\vere stored sep-\narately. SiInilarly, a sequential scan of all Assernbly tuples is also slower.)\nIi\nAll inserts, deletes, and updates that alter record lengths becorne slower,\nthanks to the overheads involved in ruaintaining the clustering.\n(We do\nnot discuss the irnplernentation issues involved in co-clustering.)\n20.5\nINDEXES THAT ENABLE INDEX--ONLY PLANS\nThis section considers a nU111ber of queries for which we can find efficient plans\nthat avoid retrieving tuples froln one of the referenced relations; instead, these\nplans scan an associated index (which is likely to be lnuch srnaller). An index\nthat is used (only) for index-only scans does not have to be clustered because\ntuples fronl the indexed relation are not retrieved.\nThis query retrieves the lnanagers of depal'truents with at least one ernployee:\nSELECTD.rugr\nFROM\nDepartrnents I) ~\nF~rnployees E\nWHERE\nI).dno=E.dno\n()bserve that no attributes of Ernployees are retahlcd. If ¥lC have an index on\nthe dno field of Ernployees: tl1e optirnization of doirlg an index nested loops join\nusing an index-onl:y searl for the inner relation is applicable. C;iven this variant\nof the quer:y, the correct\nd(-~cision is to build an uncIustered index on\ntll(~ dna\nfield of Elfll>loyees, rather thaIl a clustered index.\nrrhe next query takes this idea a step further:\nSELECT ]).rngr,E.eid\nFROM\n])epartrnents I), Ernployees E\nWHERE\nD.dno=E.dno\n\n&63\nIf \\ve have an index on the lino field of :Elnployees~we Gan use it to retrieve\nEU.lployees tuples during the join (\\vith Departrnents\na\",~ the alIter relation),\nbut unless the index is clustered, this approach is not be efficient.\n()n the\nother hand, suppose that vve have a B+- tree index on (dna, e'id).Now all the\ninforrnation \\ve need about an Ernployee,s tuple is contained in the data entry\nfor this tuple in the index. We can use the index to find the first data entry\n\\vith a given elno; all data entries 'with the SeHne dno are stored together in the\nindex. (Note that a ha'3h index on the cOlnposite key (dna, eid) cannot be used\nto locate an entry with just a given dno!) \\\\Te can therefore evaluate this query\nusing an index nested loops join with Departlnents as the outer relation and\nan index-only scan of the inner relation.\n20.6\nTOOLS TO ASSIST IN INDEX SELEC\"fION\nThe rUllnber of possible indexes to consider building is potentially very large:\nFor each relation, we can potentially consider all possible subsets of attributes\nas an index key; we have to decide on the ordering of the attributes in the index;\nand we also have to decide which indexes should be clustered and which un-\nclustered. Many large applications---for exalnple enterprise resource planning\nsysterns~--··createtens of thousands of different relations, and rnanual tuning of\nsuch a large schelna is a daunting endeavor.\nThe difficulty and irnportance of the index selection tc'ksk rnotivated the devel-\nopment of tools that help database adrninistrators select appropriate indexes\nfor a given workload. The first generation of such index tuning wizards, or\nindex advisors, were separate tools outside the database engine; they sug-\ngested indexes to build, given a workload of SQL queries. rfhe rnain drawback\nof these systerns was that they had to replicate the database query optirnizer's\ncost rnodel in the tuning tool to rnake sure that the optilnizer would choose the\nsanlC query evaluation plans as the design tool. Since query optirnizers cha,nge\nfroIn release to release of a conunercial databa.se systern, considerable effort was\nneeded to keep the tuning tool and the database optirnizer synchronized. The\nrnost recent g(~neration of tuning tools are integrated \\vith the database engine\nand use the database query optiluizer to estirnate\nth(~ cost of a workload given\na set of indexes, cl,voiding duplication of the query optirnizer's cost rnodel into\nan external tool.\n20.6.1\nAutomatic Index Selection\n\\{Ve call a set of indexes for a given database scherna. an index configuration.\n\\Ve aSSlune that a, query workload is a set of queries over a databc'kse scherna\n'INhere each query has a frequency of occurrence assigned t.o it. (jiven a database\nschelna and a, workload, the cost of an index configuration is the expected\n\n664\nCHAPTEI{ 2JJ\ncost of running the queries in the 'workload given the index configuration\ntaking the different frequencies of queries in the workloa-d into account. (jiven\na database schclna and a query workload, we can no\\v define the problel11 of\nautomatic index selection as finding an index configuration \\vith nlinirnal\ncost.\nA.s in query optinlization, in practice our goaJ is to find a good index\nconfiguration rather than the true optirnal configuration.\n\\Vhy is autollultic index selection a hard problern? Let us calculate the nUlnber\nof different indexes \\vith c attributes, assurning that the table hc),sn attributes.\nFor the first attribute in the index, there are n choices, for the second attribute\nn ~ 1, and thus for a, c attribute index, there are overall n· (n -1) ... (n -'- c+ 1) =\n(, ~! )' different indexes possible. The total nurnber of different indexes with up\nn\nc.\nto c attributes is\nc\n,\n2: _!!-_\n.. \"\n'i=l (n - 1,).\nFor a table with 10 attributes, there are 10 different one-attribute indexes, 90\ndifferent two-attribute indexes, and 30240 different five-attribute indexes. For\na cornplex workload involving hundreds of tables, the nurnber of possible index\nconfigurations is clearly very large.\nThe efficiency of autornatic index selection tools can be separated into two\ncomponents: (1) the nurnber of candidate index configurations considered, and\n(2) the nurnber of optimizer calls necessary to evaluate the cost for a configura-\ntion. Note that reducing the search space of candidate indexes is analogous to\nrestricting the search space of the query optiInizer to left-deep plans. In lnany\ncases, the optirnal plan is not left-deep, but alllong all left-deep plans there is\nusually a plan whose cost is close to the optirnal plan.\nWe can easily reduce the tiIne taken for autornatic index selection by reducing\nthe nUlnber of candidate index configurations, but the srnaller the space of\nindex. configurations considered, the farther away the final index configuration is\n[1'0111 the optirnal index configllration. rrherefore, different index tuning \\vizards\nprune the search space differently, for exarnple, by considering onl:y one- or two-\nattribute indexes.\n20.6.2\nHow Do Index Thning Wizards Work?\nAll index tuning \\vizards\ns(~arch a, set of candidate indexes for an index con-\nfiguration '''lith lowest cost. 1hols differ in the spa.ce of candidate index con-\nfigurations they consider aJld how they search this space.\n\\Ve describe one\nrepresentative algoritlun; existing tools iInplernent 'variants of this algorithrn,\nhut their irnplernentations have the sanIe basic structun~.\n\nPhysical Databa,se Design and 11_ln'ing\nG65\nr-·······..-··---...----·····..·.··....··.-~-·· ..\"..--·-_............\n..\n...\n.....~.--------l\nI\nThe DB2 Index Advisor. The DB2 Index Advisor is a tool for auto-\nI\nI\nmatic index recon1nl~ndationgiven a workload.. The workl?adis stored i~\nI\nthe databa~~e systell1 In a table called ADVISE_WORKLOAD. It IS populated e1-\nI\nther (1) by SQL statcrnents 1'1'0111 the DB2 dynanlic SQL statelnent cache,\n.\na cache for recently executed SQL statenlents, (2) with SQL staternents\nfrol11 packages·....--groups of statically cornpiled SQL statenlents, or (3) with\nSQL statelnents frolIl an online monitor called the Query Patroller. The\nDB2 Advisor allows the user to specify the lnaximuill arnount of disk space\nfor ne\\v indexes and a rnaxirrnul1 tiTne for the cornputation of the recom-\nrnended index configuration.\nThe DB2 Index Advisor consists of a prograrrl that intelligently searches\na subset of index configurations.\nGiven a candidate configuration, it\ncalles the query optirnizer for each query in the ADVISE_WORKLOAD table\nfirst in the RECOMMEND_INDEXES rnode, where the opthnizer recommends\na set of indexes and stores thern in the ADVISE_INDEXES table.\nIn the\nEVALUATE_INDEXES mode, the optimizer evaluates the benefit of the index\nconfiguration for each query in the ADVISE-WORKLOAD table. The output of\nthe index tuning step is are SQL DDL statenlents whose execution creates\nthe recomrnended indexes.\nGhe M~crosoft-~Q~~~~er-;~OO ~~x ;:nin;-Wiz~d. Microsoft l\n,\npioneered the irnplell1entation of a tuning wizard integrated with the\nI\ndatabase query optiInizer. The l\\1icrosoft Tuning vVizard has three tuning\nI\nrnodes tha.t perrnit the user to trade off running tiIne of the analysis and\n!\nnurnber of candidate index configurations exarnined: fast, rned-itlm, and\nI\nthOTOUgh, with fast having the lo\\vest running tirne aJld thoTo'ugh exalnin-\ning the h1rgest nUlnber of configurations.\nrro further reduce the running\nI\ntiIne, the tool has a salnpling Inode in which the tuning wizard randoruly\n:1'\nsalllpics queries fronl the input workload to speed up analysis. Other pa-\nnuneters include the lnaxirnurn space allowed for the reeornmended indexes,\n!\nthe. rn~x.iInu~n nurnber of attributes per i~ldex considered, a~d th:.tables on\nIi\n\\Vl11C}~ Indexes can. be generated. The ~llcroso~'~' Index ~u.lung \\\\1 Izard also,\nI\nperunts table scall,ng, \\vhere the user can specIfy an antIcIpated nurnber of\n;\ni\nrecords for the tables involved in the workload. This allows users to plan\nI\nI.._f~:t~~_~~~th_of::~_~~abl~~:__.____. ~\" \"\" _\n__\".._\"\n_.__~,._ __\"..__.._\n__.J\n\n666\nCHAPTER 20\nBefore we describe the index tuning algoriUlIn, let us consider the problell1 of\nestiInating the cost of a configuration.\nNote that it is not fea...sible to actu-\nally create the set of indexes in a candidate configuration and then optirnize\nthe query workload given the physical index configuration. Creation of even a\nsingle candidate configuration with several indexes lIlight take hours for large\ndatabases and put considerable load on the database systerIl itself. Since we\nvvant to exauline a large nUlnber of possible candidate configurations, this ap-\nproach is not feasible.\nTherefore index tuning algorithrIls usually .sim,ulate the effect of indexes in\na candidate configuration (unless such indexes already exist).\nSuch what-if\nindexes look to the query optilIlizer like any other index and are taken into\naccount when calculating the cost of the workload for a given configuration,\nbut the creation of what-if indexes does not incur the overhead of actual index\ncreation.\nCommercial databa..'3e systelIls that support index tuning wizards\nusing the database query optirnizer have been extended with a module that\npermits the creation and deletion of what-if indexes with the necessary statistics\nabout the indexes (that are used when estirnating the cost of a query plan).\nWe now describe a representative index tuning algorithm. The algorithm pro-\nceeds in two steps, candidate index selection and cor~figurationenumeration. In\nthe first step, we select a set of candidate indexes to consider during the second\nstep as building blocks for index configurations. Let us discuss these two steps\nin Inore detail.\nCandidate Index Selection\nWe saw in the previous section that it is iInpossible to consider every possible\nindex, due to the huge nUluber of candidate indexes available for larger databa.'3c\nschernas. ()ne heuristic to prune the large space of possible indexes is to first\ntune each query in the workload independently and then select the union of\nthe indexes selected in this first step as input to the second step.\n:F'or a query, let us introduce the notion of an indexable attribute, which is an\nattribute whose appearance in an index could change the cost of the query. An\nindexable attribute is an attribute on \\vhich the WHERE-part of the query h(4'3\na condition (e.g., an equality predicate) or the attribute appears in a GROUP BY\nor ORDER BY clause of the SC~L query. An admissible index for a query is an\nindex that contains only indexable attributes in the query.\nJIo\\v do we select candidate indexes for an individual query? ()ne approach is\na ba.sic ellurnenttion of all indexes with up to k attributes. \\Ve start \\ivith aU\nindexable attributes as single attribute candidate\nind(~xes, then add all corn-\n\n[>hysical Database ]Jc8ign and T11ning\nbinations of two indexable attributes\n&'3 candidate indexes, and repeat this\nprocedure until a user-defined size threshold k.\n'This procedure is obviously\nvery expensive a., we add overall n + n· (n - 1) +... + n· (n - 1) ... (n·- k + 1)\ncandidate indexes, but it guarantees that the best index with up to k attributes\nis aIl10ng the candidate indexes. The references at the end of this chapter COIl-\ntain pointers to faster (but less exhaustive) heuristieal search algorithrns.\nEnumerating Index Configurations\nIn the second phase, we use the candidate indexes to enUInerate index con-\nfigurations.\nAs in the first phase, we can exhaustively enurnerate all index\nconfigurations up to size k, this time cornbining candidate indexes. As in the\nprevious phase, more sophisticated search strategies are possible that cut down\nthe number of configurations considered while still generating a final configu-\nration of high quality (i.e., low execution cost for the final workload).\n20.7\nOVERVIEW OF DATABASE TUNING\nAfter the initial phase of databa.se design, actual use of the database provides\na valuable source of detailed information that can be used to refine the initial\ndesign. Many of the original a.ssulnptions about the expected workload can be\nreplaced by observed usage patterns; in general, some of the initial workload\nspecification is validated, and some of it turns out to be wrong. Initial guesses\nabout the size of data can be replaced with actual statistics frorn the sys-\ntern catalogs (although this inforrnation keeps changing as the systern evolves).\nCarefulrnonitoring of queries can reveal unexpected problerlls; for eXaJnple, the\noptirnizer lllay not be using SOIne indexes &'3 intended to produce good plans.\nContinued database tuning is irnportant to get the best possible perforrnance.\nIn this section, we introduce three kinds of tuning: tun'ing\n'inde1;(~8J tun'ing the\nconceptual scherna, and tuning queT'ies.\nOUf discussion of index selection also\napplies to index tuning decisions.\nConceptual schcrna and query tuning are\ndiscussed further in Sections 20.8 and 20.9.\n20.7.1\nThning Indexes\nThe initial choice of indexes rnay be refined for one of several reasons.\n'fhe\nsirnplest reeL-son is that the observed \\vorkload reveals that scnne queries and\nupdates considered irnportant in the initial\\vorkload specification are not very\nfrequent. 1'he observed \\vorkload rnay a1so identi~y SCHne ne\\v queries and up-\ndates that aTe inlportant.The initial choice of indexes has to be revievved in\nlight of this new inforrnation. Scnne of the original in,dexes rnay be dropped and\n\n668\nCHAPTER 20\nnew ones added. The rea.solling involved is siInilar to that used in the initial\ndesign.\nIt Inay also be discovered that the optimizer in a given systenl is not finding\nsome of the plans that it vvc1\"s expected to. For exaluple, consider the following\nquery, which \\ve discussed earlier:\nSELECT D.Ingr\nFROM\nErnployees E, Departulents D\nWHERE\nD.dname=='Toy' AND E.dno=D.dno\nA good plan here would be to use an index on dnarne to retrieve Departnlents\ntuples with dnarne= 'Toy' and to use an index on the dno field of Employees as\nthe inner relation, using an index-only scan. Anticipating that the optirnizer\nwould find such a plan, we rnight have created an unclustered index on the dno\nfield of Ernployees.\nNow suppose queries of this fonn take an unexpectedly long time to execute. We\ncan ask to see the plan produced by the optiInizer. (Most commercial systerIls\nprovide a simple cOillrnand to do this.) If the plan indicates that an index-only\nscan is not being used, but that Employees tuples are being retrieved, we have\nto rethink our initial choice of index, given this revelation about our system's\n(unfortunate) lhnitations. An alternative to consider here would be to drop the\nunclustered index on the dno field of EUlployees and replace it with a clustered\nindex.\nSOUle other COllllnon lirnitations of optiInizers are that they do not handle\nselections involving string expressions, arithrnetic, or null values effectively.\nWe discuss these points further when we consider query tuning in Section 20.9.\nIn addition to re-exarnining our choice of indexes, it pays to periodically reor-\nganize S(Hne indexes. For example, a static index, such <he.:; an ISAl\\Il index~ Illay\nhave developed long overflow chains. Dropping the index and rebuilding it..···_·-if\nfeasible, given the interrupted access to the indexed relation----··can substantially\nirnprove access tiTHes through this index. Even for a dynarnic structure such\nas a 13+ tree, if the implernentation does not rnerge pages on deletes, space\noccupancy can decrea\"se considerably in SaIne situations. This in turn rnakes\nthe size of the index (in pages) larger than necessary, and could increase the\nheight and therefore the access tilne. Ilebuilding the index should be consid-\nered.Extensive updates to a clustered index rnight also lead to overflow pages\nbeing allocated, thereby decreasing the degree of clustering. Again, rehuilding\nthe index Inay be vvorthwhile.\n\nPhysical [Jatubasc Design (l,'ndTnnin,g\n669\n@\nl?inally, note that the query optinlizer relies on statistics rnaintained in the\nSystCIll catalogs. These statistics are updated only \"til/hen a special utility pro-\ngranl is run; be sure to run the utility frequently enough to keep the statistics\nreasonably current.\n20.7.2\nThning the Conceptual Schema\nIn the course of datab&'Se design, \\ve rnay realize that our current choice of\nrelation SChelllaS does not enable us rneet our perforrnance objectives for the\ngiven vvorkload with any (feasible) set of physical design choices.\nIf so, \\ve\n11lay have to redesign our conceptual scherna (and re-exarnine physical design\ndecisions affected by the changes we rnake).\nWe rnay realize that a redesign is necessary during the initial design process or\nlater, after the systern has been in use for a while. Once a database has been\ndesigned and populated with tuples, changing the conceptual scherna requires\na significant effort in tenrlS of rnapping the contents of the relations affected.\nNonetheless, it rnay be necessary to revise the conceptual scherna in light of\nexperience with the systern.\n(Such changes to the schema of an operational\nsysterll are sometirnes referred to as schema evolution.)\n\\lVe now consider\nthe issues involved in conceptual scherna (re)design frorn the point of vie\\v of\nperforrnance.\nrrhe rnain point to understand is that OUT choice of concepb.lal 8cherna should\nbe gv-ided by a cons'ideration of the quer\nflc8 and 'updates in our 'workload\" in\naddition to the issues of redundancy that rllotivate nonnalization (which we\ndiscussed in Chapter 19). Several options rnust be considered while tuning the\nconceptual scherna:\nIlIII\n\\Ve lIlay decide to settle for a :3NF design instead of a BCN'F design.\nIII\nIf there are two ways to decornpose a given schelna into 3NF or BCNF\\ our\nchoice should be guided by the workload.\nIII\nSornetilnes we rnight decide to further decornpose a relation that is already\n.\nBC1NF'-'\nrn\n:./1\n.\nIII\nIn other situations, we rnight denorrnalizc.\nrrhat\nis~ \\ve rnight choose to\nreplace a cOllection of relations obtained by a dec(nnposition frorn a larger\nrelation ,vith the original (larger) relation, even though it suffers frorn 80rne\nredundancy problerl1s. Alternatively, we rnight choose to H.del sorne fields\nto certain relations to speed up SCHne irnportant queries, even if this leads\nto a redundant storctge of 80rne infonnation (anei, consequently, a scherna\nthat is in neither :3NF nor BCNF).\n\n670\nC~IIAPTER 20\nII\nThis discussion of nonnalization has concentrated on the technique of de-\nCO'f17JJo8'ition, 'which arnounts to vertical partitioning of a relation. Another\ntechnique to consider is horizontal ],Jartit'ion/ing of a relation, 'which \\vould\nlead to having two relations v'lith identical schernas. Note that we are not\ntalking about physically partitioning the tuples of a single relation; rather,\n\\ve \\vant to create two distinct relations (possibly \\vith different constraints\nand indexes on each).\nIncidentally, when \\ve redesign the conceptual scherna, especially if we are tun-\ning an existing database sche1na, it is \\vorth considering whether V'le should\ncreate vic\\vs to rnask these changes fronl users for WhOlll the original schcrlla is\n]nore natural. \\\\le discuss the choices involved in tuning the conceptual scherna\nin Section 20.8.\n20.7.3\nThning Queries and Views\nIf we notice that a query is running rnuch slower than we expected~ we have to\nexarnine the query carefully to find the problern. SaIne rewriting of the query,\nperhaps in conjunction with SCHne index tuning, can often fix the problern. Sirn-\nilar tuning rnay be called for if queries on SaIne view run slower than expected.\nWe do not discuss view tuning separately; just think of queries on views as\nqueries in their own right (after all, queries on views are expanded to account\nfor the view definition before being optirnized) and consider hcnv to tune thern.\nvVhen tuning a query, the first thing to verify is that the systern uses the plan\nyou expect it to use.\nPerhaps the systelll is not finding the best plan for a\nvariety of rcclsons.\nSorne COIllrllon situations not handled efficiently by rnany\noptinlizers follow:\n..\nA selection condition involving null values.\nl1li\nSelection conditions involving aritlunetic or string expressions or concli-\ntions using the OR connective. For exarnple, if we have a conclitionE. age\n= 2*]). age in the WHERE clause, the optirnizer rnay correctly utilize an\navailable index onE. age but fail to utilize an availclble index on 1). age.\nR,eplacing the condition by 1;;. age/2 = 1). age \\vould reverse the situation.\nIIi1I\nInability to recognize a sophisticated plan such as an index-only scan for\nan aggregation query involving a GROUP BY clause. ()f course, virtually no\noptirnizer looks for plans outside the plan space described in Chapters 12\nand 15, such cL.snonleft-deep join trees. So a good\nurHl(~rstandingof ·what\nan optirnizer typically does is irnportant. In addition, the rnore a:ware you\nare of a given systeur's strengths arrd lirnitations, the better off Y01J arc.\n\nPhysical IJataba.se Dcs'ign and 'Tun'ing\n611\nIf the optirnizer is not SIl1art enough to find the best pla.n (using access Inethods\nand evaluation straJegies supported by the DB.wIS), SOHle systern.s allo\\v users\nto guide the choice of a plan by providing hints to the opthnizer; for exalnplc,\nusers rnight be able to force the use of a particular index or choose the join\norder and join rnethod. A user who wishes to guide optirnization in this Inanner\nshould have a thorough understanding of both optirnizatioll and the capabilities\nof the given DBNIS. We discuss query tuning further in Section 20.9.\n20.8\nCHOICES IN TUNING THE CONCEPTUAL\nSCHEMA\nWe novv illustrate the choices involved in tuning the conceptual scheIua through\nseveral exarnples using the following schelnas:\nContracts( cid:\ni~.~eger, s'Upplierid: integer, projectid: integer,\ndeptid: integer, partid: integer, qty: integer, value: real)\nDepartments(did: integer, budget: real, annualreport: varchar)\nParts(pid: integer, cost: integer)\nProjects(jid: integer, rngr: char(20))\n_.-\n-\nSuppliers(sid: _..~.:nteger, address: char(50))\nFor brevity, we often use the cornrnon convention of denoting attributes by\na single character and denoting relation schernas by a sequence of characters.\nConsider the scherna for the relation Contracts, whic.h we denote as CSJDPQV,\nwith each letter denoting an attribute. The Ineaning of a tuple in this relation\nis that the contract with cid C is an agreernent that supplier S (with sid equal\nto supplierid) will supply (~ iterns of part P (with pid equal to partid) to project\nJ (with j'id equal to projectid) a.'3sociated with departrnent D (with deptid equal\nto did), and that the value V of this contract is equal to value. 2\nThere are two known integrity constraints with respect to Contracts. A project\npurdHk')cs a given part using a single contract; thus, there cannnot be two\ndistinct contracts in which the saIne project buys the saIne part. This constraint\nis represented using th.e FI) .II) ----;. (.1. Also, a departrnent purchases at rnost\none part frolll any given supplier. This constraint is represented llsing the I?D\n8D ----;. I).\nIn addition, of course, the contract ID C is a key.\nl\"1he rneaning\nof the other relations should be obvious, and we do not describe thcrn further\nbecause we focus on the Contra.cts rela.tion.\n2If this schema seems cornplicated, note that real-life situations often call for considerably more\ncornplex schema.,,':>!\n\n672\nCHAPTER 4()\n20.8.1\nSettling for a Weaker Normal Form\nConsider the Contracts relation. Should we deconlpose it into sHlaller relations?\nLet us see what norrnal fann it is in. '1'he candidate keys for this relation are C\nand .1P. (C is given to be a key~ and tIP functionally deterrnines C.) The only\nnonkey dependency is 3D -+ P, and P is a pri'rne attribute because it is part\nof candidate key JP. rrhus, the relation is not in BC:NF···because there is a\nnonkey dependency-..·..·....but it is in 3NF.\nBy using the dependency 8D -tP to guide the decornposition, we get the\nt\"vo\nsclH~rnas SDP and CSJDQV. This decornposition is\nlossless~ but it is not\ndependency-preserving. lfo\\vever, by adding the relation schelne CJP, we ob-\ntain a lossiess-join, dependency-preserving decoruposition into BCNF. Using\nthe guideline that such a decorllposition into BCNF is good, we might decide\nto replace Contracts by three relations with schernas CJP, SDP, and CSJDQV.\nHowever, suppose that the following query is very frequently asked: Find the\nnurnber of copies Q of part P ordered in contract C. 'rhis query requires a join of\nthe decornposed relations CJP and CSJDQV (or SDP and CSJDQV), wherea..'3\nit can be answered directly using the relation Contracts. The added cost for\nthis query could persuade us to settle for a 3NF design and not decompose\nContracts further.\n20.8.2\nDenormalization\nThe rea.'3ons rTIotivating us to settle for t1 weaker norrnal forIn lIlcl;Y lead us to\ntake an even rnore extrerne step: deliberately introduce SOlllC redundancy. As\nan exarnple, consider the Contracts relation, 'which is in 3NF. Now, suppose\nthat a frequent query is to check that the value of a contract is less than\nthe budget of the contracting departruent. \\Ve lllight decide to add a budget\nfield B to Contracts.\nSince did is a key for Departrnents, \\ve now have the\ndependency D -+ B in Contracts, \\vhich InCt1IlS Contracts is not in 3NF any\nr11orc.\nNonethelf~ss,\nvVf~ rnight choose to stay vvith this design if the rnotivating\nquery is sufficiently irnportant. Such a decision is clearly subjective and CaInes\nat the cost of significant redundancy.\n20.8.3\nChoice of Decomposition\nConsider the Contracts relation again. Several choices are possible for dealing\nwith the redundancy in this relation:\nIIiI\n\\Ve can leave C\\)ntracts as it is ::uld accept the rcchu1dancyr associaJed \\\\\"ith\nits being in :3N:F ratller than .BCNF.\n\nPh:lp\"ical Database Desiign and\n~r1tning\n673\nif\n•\n\\Ve Inight decide that we want to avoid the anolIlalies resulting froIn this re-\ndundancy by deeornposing Contracts into BC;NF using one of the following\nruethods:\n·0__ \\Ve have a lossless-join decornposition into Partlnfovvith attributes\nSDP and Contractlnfo \\vith attributes CSJDQ\\l. As noted previously,\nthis decornposition is not dependency-preserving, and to rnake it so\n\\vould require us to add a third relation CJP: \\vhose sale purpose is to\nallow us to cheek the dependency J P -+ C.\n- \\Ve could choose to replace Contracts by just Partlnfo and Contract-\nInfo even though this decornposition is not dependency-preserving.\nR,eplacing Contracts by just Partlnfo and Contractlnfo does not prevent us\nfrorll enforcing the constraint JP -+ C; it only makes this n10re expensive. We\ncould create an assertion in SQL-92 to check this constraint:\nCREATE ASSERTION checkDep\nCHECK\n( NOT EXISTS\n(SELECT *\nFROM\nPartlnfo PI, Contractlnfo Cl\nWHERE\nPI. supplierid==CI. suppl'ierid\nAND PI. deptid==CI. deptid\nGROUP BY C1.projectid, PI.partid\nHAVING\nCOUNT (cid) > 1 ) )\nThis assertion is expensive to evaluate because it involves a join followed by a\nsort (to do the grouping). In cornparison, the systerll can check that JP is a\nprirnary key for table CJP by rnaintaining an index on J P. This difference in\nintf~grity-checking cost is the rl1otivation for dependency-preservation. On the\nother hand, if updates are infrequent, this incrcclE;ed cost IIlay be acceptable;\ntherefore, we rnight choose not to rnaintain the table C.JP (and quite likely, an\nindex all\ni~;).\nAs another exarnple illustrating decornposition choices, consider the Contracts\nrelation again, Etud suppose that we also have the integrity constraint that a\ndepartrnent uses a given supplier for at rnost one of its projects: SPCJ\n--7 V.\nProceeding (1\"S before, we have a lossless-join decornposition of Contracts into\nSDP and CSJDQV. Alternatively:\\ve could begin by using the dependency\n8PQ ----+ V to guide our decornpositioIl: and replace Contracts with SPQV and\nCS.JI)P(~. vVe can then dec(Hnpose\nCSJI)P(~, guided by 51D -+ P, to obtain\nSDP and CS.JD(~.\n\\Ve now have two alternative lossless-join decornpositions of Contracts into\nBC:N}\"\\ neither of which is dependency-preserving. 'fhe first alternative is to\n\n674\n(~HAPTER 20\n&\nreplace Contra~~ts\\viththe relations SDP and CS.JDC~V. The second alternative\nis to replace it \\vith SPQV, SDP, and (]SJD(~. The addition of CJP HU1kes the\nsecond deC0111positioll (but not the first) dependency-preserving.\nAgain, the\ncost of lnaintaining the three relations CJP,\nSP(~V, and CSJD(~ (versus just\nCSJDQV) Illay lead us to choose the first alternative. In this ca..se, enforcing\nthe given FDs becornes Inore expensive. vVe Illight consider Ilot enforcing thern,\nbut we then risk a violation of the integrity of our data.\n20.8.4\nVertical Partitioning of BCNF Relations\nSuppose that we have decided to decornpose Contracts into SDP and CSJDQV.\nThese scheruas are in BCNF, and there is no reason to decornpose thern further\nfrom a nonl1alization standpoint. However, suppose that the following queries\nare very frequent:\n•\nFind the contracts held by supplier S.\n•\nFind the contracts placed by departrnent D.\nThese queries rnight lead us to decompose CSJDQV into CS, CD, and CJQV.\nThe decornposition is lossless, of course, and the two il11portant queries can be\nanswered by exarnining 111uch slualler relations. Another reason to consider such\na dec()l11position is concurrency control hot spots. If these queries are COIllIllon,\nand the rnost COIlunon updates involve changing the quantity of products (and\nthe value) involved in contracts, the decoulposition inlproves perforrnance by\nreducing lock contention.\nExclusive locks are now set rnostly on the\nCJ(~V\ntable, and reads on CS and CD do not conflict with these locks.\nWhenever we decornpose a relation, we have to consider which queries the\ndecolnposition rnight adverse~y affect, especially if the only rnotivation for the\ndecoInposition is iUlproved perforrnance.\n\"For exaruplc, if another illlportant\nquery is to find the total value of contracts held by a supplier, it would involve\na join of the decornposed relations CS and\nC.J(~V. In this situation, we rnight\ndecide against the decolnposition.\n20.8.5\nHorizontal Decomposition\nThus far, we have essentially considered how to replace a relation v'lith a col-\nlection of vertical decorupositions. Sornetilnes, it is \\vorth considering whethf~r\nto repla.ce a relation with t\\VO relations that have the sa.Dle attributes as the\noriginal relation, ea..eh containin.g a subsf'.t of thf~ tuples in tb.e original. Intu-\nitively, this technique is useful \\vhen different subsets of tuples are queried in\nvery distinct ways.\n\n675\nl'\nFor exanlple, different rules lnay govern large contracts, \\vhich are defined\na~\"\ncontracts with values greater than 10,000. (Perhaps, such contra<cts have to be\nawarded through a bidding process.) This constraint could lead to a nUlnber\nof queries in which Contracts tuples are selected using a condition of the forIII\nvalue > 10, 000.\nOn8 way to approach this situation is to build a clustered\nB+ tree index OIl the value field of Contracts. Alternatively, we could replace\nContracts with t\\VO relations called LargeContracts and SrnallContracts, with\nthe obvious 11leaning. If this query is the only lllotivation for the index, hori-\nzontal decornposition offers all the benefits of the index without the overhead of\nindex maintenance. This alternative is especially attractive if other irnportant\nqueries on Contracts also require clustered indexes (on fields other than val'ue).\nIf we replace Contracts by two relations LargeContracts and SrnallContracts,\nwe could r1l8sk this change by defining a view called Contracts:\nCREATE VIEW Contracts(cid, supplierid, projectid, deptid, partid, qty, value)\nAS ((SELECT *\nFROM\nLargeContracts)\nUNION\n(SELECT *\nFROM\nSmallContracts))\nHowever, any query that deals solely with LargeContracts should be expressed\ndirectly on LargeContracts and not on the view. Expressing the query on the\nview Contracts with the selection condition value> 10, 000 is equivalent to\nexpressing the query on LargeContracts but less efficient. This point is quite\ngeneral: Although we can rllth\"k changes to the conceptual scherlla by adding\nview definitions, users concerned about perforrnance have to be aware of the\nchange.\nAs another exanlple, if Contracts had an additional field yeaT and queries typ-\nically dealt with the contracts in sorne one year, we rnight choose to pa,rtition\nContracts by year. ()f course, queries that involved contracts fron1 rnore than\none year rnight require us to pose queries against each of the decolllposed rela-\ntions.\n20.9\nCHOICES IN TUNING QUERIES AND VIEWS\nT'he first step in tuning a query is to understand the plan used by the I)B1-18\nto evaluate the query.\n8ysten1s usually provide sorne facility for identifying\nthe plan used to evaluate a query. ()nce\\ve understand the plan selected by\nthe systelIl, we can consider how to irnprove perfornu:Ulce. \"\\Ve can consider a\ndifferent choice of ilHlexes or perhaps co-clustering two relations for join queries,\n\n676\nC~HAPTER 20\nguided by our understanding of the old plan and a better phtn that we want\ntheDBlVIS to use. The detaHs are sinlilar to the initial design process.\nOne point'worth rnaking is that before creating ne\"v indexes we should consider\n\\vhether rewriting the query achieves acceptable results with existing indexes.\nFor example, consider the foll()\\ving query with an OR connective:\nSELECT E.dno\nFROM\nErnployees E\nWHERE\nE.hobby='Stalnps' OR E.age==10\nIf \\ve have indexes on both hobby and age, we can use these indexes to retrieve\nthe necessary tuples, but an optiInizer ruight fail to recognize this opportunity.\nThe optinlizer rnight view the conditions in the WHERE clause\n&1;) a whole\n&'3\nnot rnatching either index, do a sequential scan of Ernployees, and apply the\nselections on-the-fly. Suppose we rewrite the query ElS the union of two queries,\none with the clause WHEREE.hobby= 'Starnps\" and the other with the clause\nWHERE E.agc==10.\nNow each query is answered efficiently with the aid of the\nindexes on hobby and age.\nWe should also consider rewriting the query to avoid sorne expensive operations.\nfor exalnple, including DISTINCT in the SELECT clause leads to duplicate elirn-\nination, which can be costly. rrhus, we should ornit DISTINCT whenever pos-\nsible.\nFor exalnple, for a query on a single relation, we can ornit DISTINCT\nwhenever either of the following conditions holds:\nII\nWe do not care about the presence of duplicates.\nII\nrrhe attributes lllentioned in the SELECT clause include a candidate key for\nthe relation.\nSOlnetirnes a query \\vith GROUP BY and HAVING can be replaced by a query\nwithout these clauses, thereby eliminating c1 sort operation. For ext1rnplc, COIl-\nsider:\nSELECT\nFROM\nGROUP BY\nHAVING\nMIN (E.age)\nErnployees E\nE.dno\nE'\n..}. \"\n'1 ()'>\n;.( no=:::, ,\n~\nThis quer:y is equivalent to\nSELECT\nFROM\nWHERE\nMIN (E.age)\nErl1ployees E\nE.dno=102\n\nPhU,'.fical Database ]Jesign and 1'lf,n'ing\n677\nt\nCornplex queries are often \\vritten in steps, using a ternporary relation.\n\\\\le\ncan usually re\\vrite such queries without the tClnporary relation to rnake thcrn\nrun faster.\nConsider the following query for cornputi.ng the average salary of\ndepartrnents rnanaged by Robinson:\nSELECT\nINTO\nFROM\nWHERE\n*\nTernp\nErnployeesE, Depa:rtruents D\nE.dno==D.dno AND D.rngrnanle='Robinson'\nSELECT\nT.dno, AVG (T.sal)\nFROM\nT'clnp T\nGROUP BY T.dno\nThis query can be rewritten a.s\nSELECT\nFROM\nWHERE\nGROUP BY\nE.dno, AVG (E.sal)\nElnployees E, Departlnents D\nE.dno==D.dno AND D.rngrnarne=='llobinson'\nE.dno\nThe rewritten query does not 111aterialize the interrnediate relation ,Ternp and is\ntherefore likely to be faster. In fact, the optimizer may even find a very efficient\nindex-only plan that never retrieves Ernployees tuples if there is a cornposite\nB+· tree index on (d'no, sal). This exanlple illustrates a general observation: By\nTewriting queries to avoid 'Unnecessary temporaries, we not only avoid creating\nthe ternporary relations, we also open up rnore opt'im,ization possibilit'ies for the\noptim,izer to el;plore.\nIn SCHne situations, ho\\vever, if the optirnizer is unable to find a good plan for a\ncornplex query (typically a nested query with correlation), it rnay be worthwhile\nto re\\vrite the query using tenlporary relations to guide the optirnizer toward\na good plan.\nIn fact, nested queries are a conunon source of inefficiency because luany opti-\nrnizers deal poorly with theIn, as discussed in Section 15.5.v'Vllenever possible,\nit is better to l:e\\vrite a nested query \\vithout nesting and a correlated query\nwithout correlation. As already notfxl, a good reforrIlulation of the query rnay\nrequire us to introduce ne\\v, ternporary relations, and techniques to do so sys-\ntenlatically (ideally, to be done by the optirnizer) have been \\videly studied.\n()ften tllough, it is possible to re\\vrite nested queries ,vithout nesting or the use\nof ternpora,ry relations, a\"s illustrated in Section 15.5.\n\n678\nCHAPTER $20\n20.10\nIMPACT OF CONCURRENCY\nIn a system with IIlaIlY concurrent users, several additional points IllUSt be\nconsidered.\nTransactions obtain locks on the pages they a,ccess, and other\ntransactions Ina)' be blocked waiting for locks on objects they wish to access.\nvVe observed in Section 1().5 that blocking delays 111ust be IniniInized for good\nperforrnance and identified two specific ways to reduce blocking:\nII\nR,educing the tilne that transactions hold locks.\nII\nR,edllcing hot spots.\nWe now discuss techniques for achieving these goals.\n20.10.1\nReducing Lock Durations\nDelay Lock Requests: Tune transactions by writing to local prograrn vari-\nables and deferring changes to the database until the end of the transaction.\nThis delays the acquisition of the corresponding locks and reduces the time the\nlocks are held.\nMake Transactions Faster: The sooner a transaction c01npletes, the sooner\nits locks are released.\nWe have already discussed several ways to speed up\nqueries and updates (e.g., tUllillg indexes, rewriting queries).\nIn addition, a\ncareful partitioning of the tuples in a relation and its &'3sociated indexes across\na collection of disks can significantly irnprove concurrent access. :B-'or exarnple,\nif we have the relation on one disk and an index on another, accesses to the\nindex can proceed without interfering with accesses to the relation, at lea\"\"t at\nthe level of disk reads.\nReplace Long Transactions by Short Ones: SometiInes, just too ruuch\nwork is done within a transaction, and it takes a long tirne and holds locks a\nlong tirne.\nConsider rewriting the transaction as two or Inore\nsInall(-~r trans-\nactions; holdable cursors (see Section 6.1.2) can be helpful in doing this. The\nadvantage is that each new transaction cornpletes quicker and releases locks\nsooner.\n~rhe disadvantage is that the original list of operations is no longer ex-\necuted atolni(~ally, and the application code Illust deal with situations in which\none or rnore of the new transactions fail.\nBuild a Warehouse: CC)lnplex queries can hold shared locks for a long tirne.\n()ften,\nhowev('~r, these queries involve statistical analysis of business trends and\nit is a,cceptable to run theln on a copy of the da.ta that is a little out of date. rrhis\nled to the popularity of data ?LJaTeho1LBCS, which are databa.'3cs that cornplcnlcnt\n\n}Jhys'ical Database Design and Tnn'ing\n61~\nthe operational datab&\"c by rnaintaining a copy of data USE:xl in cornplex queries\n(Chapter 25).\nH~unning these queries against the \\varehouse relieves the burden\nof long-running queries froln the operational datal:H1.'3c.\nConsider a Lower Isolation Level: In rnany situations, such as queries gen-\nerating aggregate infonnation or statistical sununaries, we can use a lo\\ver SQL\nisolation level such as REPEATABLE READ or READ COMMITTED (Section 16.6).\nLo\\ver isolation levels incur lower locking overheads, a,nel the application pro-\ngrannner rllust nUlke good design trade-offs.\n20.10.2\nReducing Hot Spots\nDelay Operations on Hot Spots: We already discussed the value of delaying\nlock requests.\nObviously, this is especially irnportant for requests involving\nfrequently used objects.\nOptimize Access Patterns: The patteTn of updates to a relation can also be\nsignificant. For exanlple, if tuples are inserted into the Ernployees relation in\neid order and we have a B+ tree index on eid, each insert goes to the last leaf\npage of the B+ tree. This leads to hot spots along the path froIn the root to the\nrightrnost leaf page. Such considerations nlay lead us to choose a hash index\nover a B+- tree index or to index on a different field. Note that this pattern of\naccess leads to poor perforrnance for ISAM indexes as well, since the last leaf\npage beCOlIles a hot spot. rrhis is not a problcln for hash indexes because the\nhashing process randornizes the bucket into which a record is inserted.\nPartition Operations on Hot Spots:\nConsider a data entry transaction\nthat appends new records to a file (e.g., inserts into a table stored as a heap\nfile).\nInstead of appending records one-per-transaction and obtaining a lock\non the hhst page for each record, we can replace the transaction by several\nother transactions, each of which writes records to a local file and periodically\nappends a batch of records to the rnain file. While we do rnore work overall,\nthis reduces the lock contention on the last page of the original file.\n_As a further illustration of partitioning, suppose \"\\ve track the nU111ber of re(~ords\ninserted in a counter. Instead of updating this counter once per record, the pre-\nceding approad,l results in updating several counters and periodically updating\nthe HULin counter.\nrrhis idea can IJe\naclapt(~d to rnany uses of counters, \\vith\nvarying degrees of effort. For exaInple, consider a counter that tracks the Ilurn-\nber of reservations, with the rule that a nc\\v reservation is allowed\nonl~y if the\ncounter is belo\"v a, rnaxiullun value. vVe can replace this by three counters, each\n\\vith one-third the origina11naxirIlurn threshold, and three transactions that use\nthese counters rather than the original.\n\\\\le obtain greater concurrency, but\n\n680\n(;HAPTERQO\nhave to deal with the cc1..58 where one of the counters is at the 111axirnum value\nbut SOHle other counter can still be incrcrnented.\n1~hus, the price of greater\nconcurrency is increased cornplexity in the logic of the application code.\nChoice of Index: If a relation is updated frequently, B+ tree indexes can\nbecolne a concurrency control bottleneck, because all accesses through the index\nHUlst go through the root. Thus, the root and index pages just below it can\nbec()lne hot spots.\nIf the DBMS uses specialized locking protocols for tree\nindexes, and in particular, sets finc-granularity locks, this problenl is greatly\nalleviated. l\\Ilany current systeuls use such techniques.\nNonetheless, this consideration lllay lead us to choose an ISA~1 index in SOllIe\nsituations. Because the index levels of an ISAM index are static, \\ve need not\nobtain locks on these pages; only the leaf pages need to be locked. An ISAl\\!l\nindex rnay be preferable to a B·+ tree index, for exalllple, if frequent updates\noccur but we expect the relative distribution of records and the nUlnber (and\nsize) of records with a given range of search key values to stay approxirnately\nthe saIne.\nIn this case the ISAM index offers a lower locking overhead (and\nreduced contention for locks), and the distribution of records is such that few\noverflow pages are created.\nI-Iashed indexes do not create such a concurrency bottleneck, unless the data\ndistribution is very skewed and lnany data itenlS are concentrated in a few\nbuckets. In this ca..'SC, the directory entries for these buckets can beccnne a hot\nspot.\n20.11\nCASE STUDY: THE INTERNET SHOP\nRevisiting our running case study, I)BDudes considers the expected workload\nfor the B(~N 1)00kstore. rrhe owner of the bookstore expects rnost of his CllS-\ntorners to search for books by ISBN nUluber before placing an order. Placing\nan order involves inserting one record into the ()rders table and inserting one\nor lllore records into the Orderlists relation. If a sufficient nurnber of books is\navaihtble, a, shiprnent is prepared and a value for the ship.Jlale in the Orderlists\nrelation is set. In addition, the available quantities of books in stock changes\nall the tirne, since orders are placed that; decrease the quantity available and\nnew books arrive frorn suppliers and increase the quantity available.\nThe DBDudes tearn begins by considering searches for books by ISBN'. Since\nisbn, is a\nkey~ (l,n equality query on isbn returns at rnost one record. rrhereforc,\nto speed up queries frolll Cllstolllers who look for books \"with a given ISBN,\nI)BIJudes decides to build an unclustered hash index on L\"bn.\n\n]:>hysical jJatabase IJesign, and Tuning\n681\nNext, it considers updates to book quantities. Tb update the qtll_iTLstock value\nfor a book, we IllUSt first search for the book by ISBN; the index on 'isbn speeds\nthis up.\nSince the qty_irLstock value for a book is updated quite frequently,\nDBDudes also considers partitioning the Books relation vertically into the fo1-\n1c)\\ving two relations:\nBooks(~ty(isbn, qty)\nBookH,(~st( 'isbn, title, author\" price, yeQ.T_IHlblished)\nUnfortunately, this vertical partitioning slows do\\vn another very popular query:\nEquality search on ISBN to retrieve all infonnation about a book no\\v requires\na join between BooksQty and BooksH,est. So DBDudes decides not to vertically\npartition Books.\nDBDudcs thinks it is likely that custonlers \"vill also want to search for books by\ntitle and by author, and decides to add unclustered hash indexes on title and\nauthor-these indexes are inexpensive to rnaintain because the set of books is\nrarely changed even though the quantity in stock for a book changes often.\nNext, DBDudes considers the Custorners relation. A custorner is first identi-\nfied by the unique custorner identifaction nurnber. So the rnost COlnrnon queries\non Custorners are equality queries involving the custolner identification nurn-\nber, and DBDudes\ndecid(~s to build a clustered ha..'3h index on cid to achieve\nmaxirnum speed for this query.\nl\\!Ioving on to the Orders relation, DBDudes sees that it is involved in two\nqueries: insertion of new orders and retrieval of existing orders. Both queries\ninvolve the ordcrrl/urn attribute as search key and so DBDudes decides to huild\nan index on it. What type of index should this be~\"\"·\"a 13+ tree or a hash index?\nSince order nurnbers are assigned sequentially and correspond to the order date,\nsorting by onleT\"n'lun effectively sorts by order date as well. So DBDudes decides\nto build a clustered B-t- tree index OIl onlernurn.\nA.lthough the operational\nrequirernents rncntioned until no\\v favor neither a 13+ tree nor a hash index,\nB&N\\vill probably want to rnonitor daily a,ctivities and the clustered 13+ tree\nis a better choice for such range queries. ()f course, this 1118ans that retrieving\nall orders for a given custorner could be expensive for custolllers with InallY\norders, since clustering by o'(ylerntlTn precludes clustering by other attributes,\nSllCh as cicio\nl'he ()rderlists rela,tion involves lnostly insertions, vvith an occfLsionaJ update of\na shiprnent date or a query to list all cOlnponents of a given order. If Orderlists\nis kept sorted on oT'dcrnvxn, all insertions are appends at the end of the relation\nand thus verJr efficient. A clustered 13+ tree index on oT'dernuTn rnaintains this\nsort order and also speeds up retrieval of aU iterns for a given order. lc) update\n\n682\nCHAPTER~O\na shiprnent date, we need to search for a tuple by\noT(le1~rrnmj and isbn.\nThe\nindex on ordeT'n'Urn helps here as well. Although an index on (ardern:u,rn, 'isbn)\nwould be better for this purpose, insertions would not be as efficient a..,,\\vith\nan index on just oTdeT7rurn; DBDudes therefore decides to index ()rderlists on\njust oTCiern7lrn.\n20.11.1\nTuning the Database\nSeveral rnonths after the launch of the B&N site, DBDudes is called in and told\nthat custorner enquiries about pending orders are being processed very slowly.\nB&N has becorne very successful, and the Orders and Orderlists tables have\ngrown huge.\nl'hinking further about the design, DBDudes realizes that there are two types of\norders: completed orders, for which all books have already shipped, and partially\nco'mpleted order'S, for which sorne books are yet to be shipped. l\\Ilost custorIler\nrequests to look up an order involve partially corIlpleted orders, which are a\nsInall fraction of all orders. DBDudes therefore decides to horizontally partition\nboth the Orders table and the Orderlists table by ordernu'Tn. This results in\nfour new relations: NewOrders, OldOrders, NewOrderlists, and OldOrderlists.\nAn order and its cornponents are always in exactly one pair of relations····· ..·--and\nwe can deterrIline which pair, old or new, by a sinlple check on ordernurn-----\"and\nqueries involving that order can always be evaluated using only the relevant\nrelations.\nSCHIle queries are now slower, such as those asking for all of a cus-\ntoruer's orders, since they require us to search two sets of relations. lIowever,\nthese queries are infrequent and their perforrnance is acceptable.\n20.12\nDBMS BENCHMARKING\n~rhus far, we considered ho\\v to irnprove the design of a database to obtain bet-\nter perforrnance. 1\\S the database grows, however; the underlying IJB1tlS rnay\nno longer be able to provide adequate perforrnance, even with the best possi-\nble design, and \\ve have to consider upgrading our systcrn, typically by buying\nfaster harchva,re and additional rnernory. We IIlay also consider rnigrating our\ndatabase to\n(1, new DBIVIS.\n\\\\Then evaluating IJBl'vlS products, perforrnal1ce is an iUlportant consideration.\nADBIVIS is a cornplex piece of sofb,va,rc, and different vendors rnay target\ntheir systerns to\\vard cliff'erent 1I1Etrket segrnents by putting rnore effort into\noptirnizirlg certa,in parts of the systern or choosing different systern designs.\nFor exc:unple, sorne systcrIls are designed to run cornplex queries efficiently,\nwhile others are designed to run Inany sirnple transactions per second. \\iVithin\n\nPhysical Database Des'ign and Tlln'ing\n683\neach category of systcrIls, there are lnany cornpeting products. To assist users\nin choosing a DBi'vIS that is 'well suited to their needs, several performance\nbenchmarks have been developed. These include benchrnarks for Inea.'Hlring\nthe perforlnance of a certain class of applications (e.g., the TPC benclnnarks)\nand benchrnarks for rnecl,.c:;uring how well a DBIVlS perfOrII1S various operations\n(e.g., the \\Visconsin benchrnark).\nBenchnuuks should be portable, easy to understand, and scale naturally to\nlarger problenl instances. 'rhey should II1eaSUre peak performance (e.g., trans-\nactions per second, or ips) &s well as pTice/perforrnance ratios (e.g., $/tps) for\ntypical workloads in a given application donlain. The Transaction Processing\nCouncil (TPC) was created to define benchlnarks for transaction processing\nand database systerns. Other well-known benchlnarks have been proposed by\nacadelnic researchers and industry organizations.\nBenchrnarks that are pro-\nprietary to a given vendor are not very useful for cornparing different systerns\n(although they rnay be useful in deterrnining how well a given systern would\nhandle a particular workload).\n20.12.1\nWell-Known DBMS Benchmarks\nOnline 'Transaction Processing Benchmarks: The TPC-A and TPC-B\nbenchrnarks constitute the standard definitions of the ips and $/tps measures.\nTPC-A rneasures the perfonnance and price of a computer network in addition\nto the DBMS, whereas\nthE~ TPC-B benclnnark considers the DBMS by itself.\nThese bencln11arks involve a sirnple transaction that updates three data records,\nfrolIl three different tables, and appends a record to a fourth table. A 11urnber\nof details (e.g., transaction arrival distribution, interconnect rnethod, systern\nproperties) are rigorously specified, ensuring that results for different systenls\ncan be rneaningfully cOI11pared.\nThe T'PC-C benchrna,rk is a l110re cornplex\nsuite of transactional ta.,,'3ks than TPC-A and TPC-B. It rnodels a waxehouse\nthat tracks iterns supplied to custorners and involves five types of transrtctions.\nEach TPC-C transaction is rnuch rIlore expensive than a 1'PC-A or TPC-B\ntransaction, a,nel TPC-C exercises a rnuch ,videI' range of systern capabilities,\nsuch as use of secondary indexes and transaction aborts. It ha,,'3 Inore or less\ncOlnpletely replaced 'I'PC-A and rrpC-B as the standard transaction processing\nbencillnark.\nQuery Benchmarks: '1'he \\Visconsin l)cnchrnark is \\videly used for 1neasnr-\ning the perforrnance of sirnple relational queries. ]'he Set\n(~ueI'Y benclunark\nHleasures the perforrnance of Et suite of rJlore cornplex queries, and the .AS:{A.P\nl)enchrnark rneaBures the perfonnance of (1, Inixed ~Torkloa,d of transactions 1 re-\nlatiol1(l] queries, (lnd utility fUllctions.\n'The rrpC-I) benchn.lark is a suite of\ncornplex\nS(~I.J queries intended to be representative of the (Incision-support ap-\n\n684\nCHAPTER 20\nplication dCHuain. 'fhe ()LAP (]ouneil also developed a benehlnark for cornplex\ndecision-support queries, including sor11e queries that cannot be expressed eas-\nily in SQL; this is intended to rnea..'3ure systerIls for online a'nalyt'ic ]JTocessing\n(OLAP),\\vhieh we discuss in\n(~hapter 25, rather than traditional\nS(~L sys-\nterns. The Sequoia 2000 benchrnark is designed to cornpare DBNIS support for\ngeographic inforrnation systerns.\nObject-Database Benchmarks:\n'The 001 and 007 benclunarks rneasure\nthe perforrnance of object-oriented database systelns. 'rhe Bucky benclunark\nrneasures the perforrnance of object-relational database systcrns. (We discuss\nobject-database systelns in Chapter 23.)\n20.12.2\nUsing a Benchmark\nBenchrnarks should be used with a good understanding of what they are de-\nsigned to rnea8ure and the application environrnent in \\vhich a DBMS is to be\nused.\n\\Vhen you use benchrnarks to guide your choice of a DBMS, keep the\nfollowing guidelines in rnind:\nII\nHow Meaningful is a Given Benchmark?\nBenchrnarks that try to\ndistill perforrnance into a single nunlber can be overly sirnplistic. A DBMS\nis a cOlnplex piece of software used in a variety of applications.\nA good\nbenchlnark should have a suite of tasks that are carefully chosen to cover a\nparticular application dornain and test DBJ\\lIS features irnportant for that\nd01nain.\nII\nHow Well Does a Benchrnark Reflect Your Workload? Consider\nyour expected workload and corupare it with the benchrnark. C;ive 11101'8\n\\veight to the perfonnance of those l)enchrnark tasks (i.e., queries and up-\ndates) that are siInilar to irnportant tasks in your workload. Also consider\nhow benclunark nurnbers are rnect..sured. For exarnple, elapsed tirne for in-\ndividual queries rnight be rnisleading if considered in a rnultiuser setting:\nA systern rnay have higher elapsed tirr18s because of slo\\ver l/C). On a 1nul-\ntiuser workloa,d, given sufficient disks for parallel l/C), such a systern lnight\nolltperfofrn <1 SY8t8111 'with a leJ\\ver elapsed tirne.\nII\nCreate Your Own Benchmark:\nVendors often tweak their systerns\nin ad hoc ways to obtctin good nurnbers on irnportant benchrnctrks. fro\ncounter this, create your own benc1unark by rnodi(ying standard bench-\nrnarks slightly or by replacing the ta,'3k8 in\nf), standard benchrnark \\vith\nsiInilar tct..sl<:s frarn your workload.\n\nPhys'ical Database Design and T1uni'ng\n20.13\nREVIEW QUESTIONS\nAllS\\VerS to the revie\\v questions can be found in the listed sections.\n685\nII\nvVhat aTe the cornponents of a \\vorkload description? (Section 20.1.1)\nII\n\\Vhat decisions need to be rnade during physical design? (Section 20.1.2)\nII\nDescribe six high-level guidelines for index selection. (Section 20.2)\nII\n\\\\Then should \\ve create clustered indexes? (Section 20.4)\n..\nWhat is co-clustering, and when should vve use it? (Section 20.4.1)\nII\nvVhat is an index-only plan, and how do we create indexes for index-only\nplans? (Section 20.5)\nII\n\\rVhy is automatic index tuning a hard problern? Give an exarnple. (Sec-\ntion 20.6.1)\nII\nGive an exarnple of one algorithrn for autonlatic index tuning. (Section\n20.6.2)\nII\nWhy is database tuning irnportant? (Section 20.7)\nII\nHow do we tune indexes, the conceptual scheula, and queries and views?\n(Sections 20.7.1 to 20.7.3)\nII\nWhat are our choices in tuning the conceptual scherna? What are the fol-\nlowing techniques and when should \\ve apply thern: settling for a weaker\nnorrnal forrn, denorrnalization, and horizontal and vertiacal decornposi-\ntions. (Section 20.8)\n11\nvVhat choices do \\ve have in tuning queries and vie\\vs? (Section 20.9)\nII\n\\Vhat is the irnpact of locking\n011 databa..se perforluance?\nI-Iow can we\nreduce lock contention and hot spots? (Section 20.10)\nII1II\n\\Vhy\ndO\\~le have standaTdized database benclllnarks, and \\vhat conunon\nInetrics are used to evaluate datalH:t..'Se systelns?\nCan ;you describe a few\npopular database benchrnarks? (Section 20.12)\nEXERCISES\nExercise 20.1 Consider the following BCNF schcrna for a portion of a sirnple corporate\ndatabase (type infonnation is not relevant to this question and is ornitted):\nErnp (e'iq, enarne, addl', sal, age, yT8, deptid)\nDept (did,\ndnarru~; flooT, [nalget)\n\n686\nSuppose you know that the following queries are the six rIlC>\"'it COUUIlcm queries in the \\vorkload\nfor this corporation and that all six are roughly equivalent in frequency and inlportance:\nII\nList the irC\nIHUn(~, and address of eUlployees in a user-specified age range.\nII\nList the id, naUIe, and address of crnployees vv'ho work in the departHwnt \\vith a llser-\nspecified departInent narne.\nII\nList the id and address of elnployees with a user-specified eluployeenanle.\nII\nList the overall average salary for ernployees.\nII\nList the average salary for eInployees of each age; that is, for each age in the datal)(1se,\nlist the age and the corresponding average salary.\nII\nList all the departrnent infonnation, ordered by departrnent floor nurnbers.\n1. Given this infonnation, and assuIning that these queries are lnore iluportant than any\nupdates, design a physical scherna for the corporate database that will give good perfor-\nrnance for the expected workload. In particular, decide which attributes will be indexed\nand whether each index will be a clustered index or an unclustered index. Assuuw that\n13+ tree indexes are the only index type supported by the DBMS and that both single-\nand nnrltiple-attribute keys are pernlitted. Specify yOllr physical design by identifying\nthe attributes you recornnlCnd indexing on via clustered or unclustered 13+ trees.\n2. Redesign the physical schelna assuIning that the set of iInportant queries is changed to\nbe the following:\nIII\nList the id and address of enlployees with a user-specified ernployee narne.\nII\nList the overall rnaxinHun salary for eruployees.\nIII\nList the average salary for ernployees by departnlent; that is, for each deptid value,\nlist the rlcpt'id value and the average salary of ernployees in that departrnent.\n..\nList the Slun of the budgets of all departrnents by floor; that is, for each floor, list\nthe floor and the sum.\nII\nAssuIne that this workload is to be tuned with an autornatic index tuning wizard.\nOutline the rnain steps in the execution of the index tuning algorithrn and the set\nof candidate configurations that would be considered.\nExercise 20.2 Consider the follo\\ving BCNF' relational scherna for a portion of a universit.y\ndatabase (type infonnation is not relevant to this question and is ornitted):\nProf(ssno, pnamc, office, age, 8ex:, specialt:y. dept-did)\nDept(did, drw:rnc, budget, T//wlLTTI,ajoT8, cha'lT...ssno)\nSuppose you kno\\v that the folknving queries (I,rc the five rnost connnon queries in the workloa,d\nfor this university and that all five an~ roughly equivalent in frequency ::lud iInportance:\n-List the\nn<un~~s, <lges, and offices of pn)fessors of\n(1\nusc~r-specified sex (rnale or fernale)\nwho have a llser\"specified resean:h specialty (e.g., TeC'lLT8i'ue qtLer7J fJTOco3.'ring).\nAssurne\nthat the university hEtS a diverse set of faculty rnernbers, rnaking it very unCOlnmon for\nInore than a fe\\\\! professors to have the sarne r(~search specialty.\nIIIi\nList all tite departrnent information for departrnents ''lith professors in a. user-specified\n<lge range.\nIl\nList the <lepartlnent i<l, dep<lrtrnent BaIne, and chaiq)erscmnarne for depcutrnents ,viti!\na user-specified nurnber of majors.\n\nPhysical Databa,se De.,'rign and T!zlTLing\n..\nIJist the lowest budget for a departInent in the university.\n..\nList all the infornultion about professors \\vho are departlnent chairpersons.\nf>&7\n'These queries occur runch 1nore frequently than updates, so you should build whatever in-\ndexes you need to speed up these queries.\nHo\\\\'ever, you should not build any unnecessary\nindexes, as updates will occur (and would be slowed down by unnecessary indexes). (jiven\nthis information, design a physical schc1na for the university database that will give good per-\nfonnance for the expected workload. In particular, decide which attributes should be indexed\nand 'whether each index should be a clustered index or an unclustered index.\nAssulne that\nboth B·+ trees and hashed indexes are supported by the DBlVIS and that both single- and\nlImltiple-attribute index search keys are perrnitted.\n1. Specify your physical design by identifying the attributes you recomlnend indexing on,\nindicating whether each index should be clustered or unclustered and whether it should\nbe a B+ tree or a hashed index.\n2. Assurne that this workload is to be tuned with an autornatic index tuning wizard. Outline\nthe rnain steps in the algorithrn and the set of candidate configurations considered.\n3. Redesign the physical schema, assurning that the set of irnportant queries is changed to\nbe the following:\nIII\nList the nUlnber of different specialties covered by professors in each department,\nby department.\nIII\n,Find the departrnent with the fewest rnajors.\nII\nFind the youngest professor who is a department chairperson.\nExercise 20.3 Consider the following BCNF relational schmna for a portion of a cornpany\ndatabase (type inforrnation is not relevant to this question and is OInitted):\nProject(pno, p'l'o}_narne. pro}_bascdept, ]YT'o}_'mgT, topic, budget)\n:rvianagerC!.Itl..d, rngT.\"narne, rngr...dept, salary, age, sex:)\nNote that each project is based in sorne cleprtrtrnent, each manager is e1Ylployed in some\ndepartIllEmt, and the lTu.tnager of a project need not be e1nployed in the sarne departrnent\n(in which the project is ba'Sed).\nSuppose you know that the following queries are the five\nmost COHUllon queries in the workload for this university and aJl five are roughly equivalent\nin frequency and i1\"nportance:\nII1II\nList the IH1IlIeS, ages, and salaries of lnanagers of a user-specified sex (rnale or feluale)\nworking in a given department. You can assurne that, while there are rnany deparhnents,\neach departInent contains very fe\\v project IlHlnagers.\niIIIIl\nljst the narnes of (ill projects with lnanagers whose ages are m a user-specified range\n(e.g., younger than :30).\nIii!l\nList the na\"rnes of all departrnents such that a rnanager III this deparhnent manages a\nproject based in this department.\nm\nI..list the nan1C of the project \\'vith the l()\\vest budget.\nI11III\nList the narrIeS (Jf all managers in the SaITIC department as a given project.\nrI'hese queries occur nIllch Inore frequently than updates, so you should build \\vhatever in-\ndexes you need to speed up these queries.\n110weve1', you should not build any unnecessaT.Y\ninclexes,\n<.lS updates \\'lill occur (a\"nd \\vould be slmved down by urmccessary indexes).\nGiven\n\n688\nthis infonnatioll, design a physicaJ schelna for the conlpany database that win give good per-\nformance for the expected \\vorkload. In particular l clc'Ci<le which attributes should be indexed\nand whether each index should be a clustered index or an unclustered index. Assuille that\nboth B+ trees and hashed indexes are supported by the DBrvlS, and that both single- and\nIlluitiple-attribute index keys are perruitted.\n1. Specify your physical design by identifying the attributes you recOlIlrIlend indexing on,\nindicating whether each index should be clustered or unclustered and \\vhether it should\nbe a B+ tree or a hashed index.\n2. Assunle that this workload is to be tuned with an autornatic index tuning wizard. Outline\nthe lnain steps in the algorithrn and the set of candidate configurations considered.\n3. Redesign the physical schenu't assulning the set of ilnportant queries is changed to be the\nfollowing:\n•\nFind the total of the budgets for projects luanaged by each rnanager; that is, list\np'roj_rngr and the total of the budgets of projects luanaged by that manager, for\nall values of proj _mgT.\n•\nFind the total of the budgets for projects managed by each rnanager but only for\nmanagers who are in a user-specified age range.\n•\nFind the number of male rnanagers.\n•\nFind the average age of rnanagers.\nExercise 20.4 The Globetrotters Club is organized into chapters. The president of a chapter\ncan never serve as the president of any other chapter, and each chapter gives its president\nsonle salary.\nChapters keep moving to new locations, and a new president is elected when\n(and only when) a chapter rnoves. This data is stored in a relation G(C,S,L,P), where the\nattributes are chapters (C), salaries (S), locations (L), and presidents (P).\nQueries of the\nfollowing fornl are frequently asked, and you mU8t be able to answer thern without cOluputing\na join: \"Who was the president of chapter X when it was in location Y?\"\n1. List the FDs that are given to hold over G.\n2. What are the candidate keys for relation G?\na. What Honnal fornl is the scherna Gin?\n4. Design a good database scherna for the club. (Rernernber that your design 'mnst satisfy\nthe stated query requirenlent!)\n5. \\\\7hat nonnal fonn is your good scherna in? Give an exarnple of a query that is likely to\nrun slc)\\ver on this schema than on the relation G.\n6. Is there a lossless-join, dependency-preserving deCOlTlposition of G into BeNF?\n7. Is there ever\n<:::t good reason to accept sornething less than :3NF' \\vhen designing a schema\nfor ct relaJional da..tabase? Use this\nex~unple, if necessary adding further constraints, to\nillustrate your answer.\nExercise 20.5 Consider the following BCNF relation, which lists the ids, types (e.g., nuts\nor bolts), and costs of various parts, along with the mllnber available or in stock:\nParts (pid, pname, cost, n'll.1YLC/.'l)(L'il)\nYou are told that the following t\\\\'o queries are extrelnely irnportant:\n\nIII\nFind the total nunlber available by part type, for all types.\nCI'hat is, the surn of the\nnunL(l,'vail value of all nuts, the sum of the nunLuvau value of all bolts, and so forth)\nIII\nList the ]yids of parts with the highest cost.\n1. Describe the physical design that you would choose for this relation. That is, what kind\nof a file structure would you choose for the set of Parts records, and what indexes would\nyou create?\n2. Suppose your custorners subsequently cmnplain that performance is still not satisfactory\n(given the indexes and file organization you chose for the Parts relation in response to the\nprevious question). Since you cannot afford to buy new hardware or software, you have\nto consider a schenla redesign. Explain how you would try to obtain better perfonnance\nby describing the scherna for the relation(s) that you would use and your choice of file\norganizations and indexes on these relations.\n3. How would your answers to the two questions change, if at all, if your systeIl1 did not\nsupport indexes with multiple-attribute search keys?\nExercise 20.6 Consider the following BCNF relations, which describe ernployees and the\ndepartments they work in:\nErnp (eid, sal, did)\nDept. (d'id, location, budget)\nYou are told that the following queries are extrernely important:\nII\nFind the location where a user-specified enlployee works.\nII\nCheck whether the budget of a department is greater than the salary of each ernployee\nin that departrnent.\n1. Describe the physical design you would choose for this relation. That is, what kind of a\nfile structure would you choose for these relations, and what indexes would you create?\n2. Suppose that your custollwrs subsequently cOIuplain that perforrnance is still not sat-\nisfactory (given the indexes and file organization that you chose for the relations in\nresponse to the previous question).\nSince you cannot afford to buy ne\\\\' hardware or\nsoftware, you have to consider a schelna redesign. Explain how you would try to obtain\nbetter perfonnance by describing the scherna for the relation(s) that you would use and\nyour choice of file organizations and indexes on these rehltions.\n~3. Suppose that your databa.:;e systern IU1S very ineff1cient irnplenlentations of index struc-\ntures. \\Vhat kind of a design would you try in this case?\nExercise 20.7 Consider the following BCNF relations, which describe departrnents in\nH,\ncompany and ernployees:\nDept(did, dn.arne, location\" managerid)\nEnlp( cid_, sal)\n'You arE' told that the follovving queries are extrernely iruportant:\nIlIII\nList the names and ids of rnanagel's for each department in a user-specified location., in\nalphabetical order by departuwl1t narne.\nIII\nFind the aventge salary of ernployees who rnanage departments in a user-specified loca-\ntion.You can ,kssurne that no one rnanages nlOre than one depa,rtrnent.\n\n690\nC:;HAPTER ~O\n1. Describe the file structures and indexes that you would choose.\n2. You subsequently realize that updates to these relations are frequent. Because indexes\nincur a high overhead, can you think of a way to irnprove perforrnance on these queries\nwithout using indexes?\nExercise 20.8 For each of the following queries, identify one possible reason why an opti-\nInizer Illight not find a good plan.\nRevvTite the query so that a good plan is likely to be\nfound. Any available indexes or known constraints are listed before each query; assurne that\nthe relation schelnas are consistent with the attributes referred to in the query.\n1. An index is available on the age attribute:\nSELECT E.dno\nFROM\nElnployee E\nWHERE\nE.age=20 OR E.age=10\n2. A B+ tree index is available on the age attribute:\nSELECT E.dno\nFROM\nEmployee E\nWHERE\nE.age<20 AND E.age>10\n3. An index is available on the age attribute:\nSELECT E.eIno\nFROM\nEnlployee E\nWHERE\n2*E.age<20\n4. No index is available:\nSELECT DISTINCT *\nFROM\nEnlployee E\n5. No index is available:\nSELECT\nFROM\nGROUP BY\nHAVING\nAVG (B.sal)\nElnployee E\nE.dno\nE.dno=22\n6. The sid in Reserves is a foreign key that refers to Sailors:\nSELECT\nFROM\nWHERE\nS.sid\nSailors S, Reserves H\nS.sid=R.sid\nExercise 20.9 Consider two 'ways to COlupute the HaIneS of elnployees who earn rnore than\n$100,000 and whose age is equal to their rnan~tger)s age. First, a nested query:\nSELECT\nFROM\nWHERE\nF~l.en(Hne\nEnlpEI\nEl.sal > 100 AND El.age =\n( SELECT\nFROM\nWHERE\nE2.(lge\nErnp E2, Dept D2\nE1.dname = D2.dnaJ.ne\nAND D2.mgr = E2.enarne )\nSecond, a query that uses a view definition:\n\n})hysical Dai:abaseDesign and Tun;ing\nt~91\nSELECT\nFROM\nWHERE\nCREATE\n£1.enarne\nErnp El, ~igrAge A\nEl.dnarue = A.dnarne AND E1.sal > 100 AND E1.age = A.age\nVIEW :WIgrAge (dnan1e, age)\nAS SELECT D.dnanw, E.a,ge\nFROM\nErrlp E, Dept D\nWHERE\n~D.nlgr = E.erli:une\n1. Describe a situation in which the first query is likely to outperforrn the second query.\n2. Describe a situation in which the second query is likely to outperfonn the first query.\n3. Can you construct an equivalent query that is likely to beat both these queries when\nevery ernployee who earns rnore than $100,000 is either\n~35 or 40 years old?\nExplain\nbriefly.\nBIBLIOGRAPHIC NOTES\n[658] is an early discussion of physical database design.\n[659] discusses the performance\nimplications of normalization and observes that denormalization may improve perforrnance\nfor certain queries. The ideas underlying a physical design tool frorn IBl'vf are described in\n[272]. The Nlicrosoft AutoAdrnin tool that perfonns automatic index selection according to\na query workload is described in several papers [163, 164].\nThe DB2 Advisor is described\nin [750].\nOther approaches to physical database design are described in [146, 639].\n[679]\nconsiders transaction tuning, which we discussed only briefly. The issue is how an application\nshould be structured into a collection of transactions to rnaxirnize perfonnance.\nThe following books on database design cover physical design issues in detail; they are reCOIll-\nrnended for further reading. [274] is largely independent of specific products, although rnaBy\nexcunples are based on DB2 and Teradata systerl1S. [779] deals prirnarily with DB2. Shasha\nand Bonnet give an in-depth, readable introduction to database tuning [104].\n[;334] contains several papers on benchrnarking database systerns and has accompanying soft-·\nware. It includes articles on the AS:3AP, Set Query, 'I'PC-A, 'rpC-B, Wisconsin, and 001\nbendunarks written by the original developers. The Bucky benchrnark is described in [132],\nthe 007 benchrnark is described in [l:H] , and the T'pe-D benchrnark is described in [7:39].\nThe Sequoia 2000 bendunark is described in [720].\n\n21\nSECURITY AND\nAUTHORIZATION\n..\nWhat are the rnain security considerations in designing a database\napplication?\n..\nWhat IIlechanisms does a DBNIS provideto control a user's access to\ndata?\n..\nWhat is discretionary access control and how is it supported in SQL?\n..\nWhat are the weaknesses of discretionary access control?\nHow are\nthese addressed in lnandatory access control?\n..\nWhat are covert channels and how do they cornpromise lnandatory\naccess control?\n..\nWhat lTIUst the DBA do to ensure security?\n..\nWhat is the added security threat when a database is accessed re-\nrnotely?\n..\nWhat is the role of encryption in ensuring secure access? How is it\nused for certifying servers and creating digital sig11atures?\n..\nKey concepts: security, integrity, availability; discretionary access\ncontrol, privileges, GRANT,\nREVOKE; rna.ndatory access control, objects,\nsubjects, security classes, rnultilevel tables, polyinstantiation; covert\nchannels, DoD security levels; statistical databases, inferring secure\ninformation; authentication for reIllote access, securing servers, digital\nsignatures; encyption, public-key encryption.\n-\nI know that's a secret, for it's whispered everywhere.\n......··..·vVilliam Congreve\n692\n\nSeC1LTity and .A~uthoT'iz'ation\nThe data stored in a DBNIS is often vital to the business interests of the or-\nganization and is regarded &1.) a corporate a,,'Sset. In addition to protecting the\nintrinsic value of the data, corporations rnust consider O\\vays to ensure privacy\nand control access to data that must not be revealed to certain groups of users\nfor various re&'3ons.\nIn this chapter, \\ve discuss the concepts underlying access control and secu-\nrity in a DB:N.IS. After introducing database security issues in Section 21.1,we\nconsider two distinct approaches, called\nd'iscTetionar~lj and rnandatory, to spec-\nifying and lTlanaging access controls. An access control Inechanism is a way\nto control the data accessible by a given user. After introducing access controls\nin Section 21.2, we cover discretionary access control,which is supported in\nS(~L, in Section 21.3.vVe briefly cover n1andatory access control, which is not\nsupported in SQL, in Section 21.4.\nIn Section 21.6, we discuss SOIne additional aspects of database security, such\nas security in a statistical database and the role of the database adrninistrator.\nWe then consider SOlne of the unique challenges in supporting secure access to\na DBMS over the Internet, which is a central problern in e-COlllInerce and other\nInternet database applications, in Section 21.5. We conclude this chapter with\na discussion of security aspects of the Barns and Nobble case study in Section\n21.7.\n21.1\nINTRODUCTION TO DATABASE SECURITY\nThere are three rnain objectives \\vhen designing a secure database application:\n1. Secrecy: InfoI'rnation should not be disclosed to unauthorized users. EoI'\nexarnple, a student should not be allowed to exarnine other students' grades.\n2. Integrity: ()nly authorized users should be allowed to Hlodify data. For\neXHxnplc, students 1Ilay be allowed to see their grades, yet not allowed\n(obviously) to rnodify thern.\n:3. Availability: Authorized users should not be denied access. For excunplc,\nan instructor who wishes to change a grade should be allowed to do so.\nT'() achieve these objectives, a clear and consistent security policy should be\ndeveloped to describe \\vhat security Ine::1SU1'eS rnust be enforced. In particular,\nwe rnu8t detennine what part of the data is to be protected and which users\nget access to \\vhich portions of the data. Next, the security mechanisrns of\nthe underlying I)B:JVIS and operating systenl, as \\veU as externaJ rnechanisHls,\n\n694\nCHAPTER 21\nsuch as securing access to buildings, Illust be utilized to enforce the policy. \\Ve\ncrIlphasize that security rneasures IIlust l)e taken at several levels.\nSecurity leaks in the OS or network connections can cirCUlnvent databa.se secu-\nrity rnechanisrns. For exarnple, such leaks could allow an intruder to log on as\nthe database acbninistrator, 'with all the attendant I)BlVIS access rights. Hurnan\nfactors are another source of security leaks. :For exarnple, a user IHay choose a\npa.ss\\v()l·d that is easy to guess, or a user who is authorized to see sensitive data\nrnay luisuse it. Such errors account for a large percentage of security breaches.\n\\Ve do not discuss these aspects of security despite their irllportance because\nthey are not specific to data,base rnanagerllent systelIls; our IIlain focus is on\ndataba..se access control rllechanisrns to support a security policy.\nWe observe that vie\\vs are a valuable tool in enforcing security policies. The\nview rnechanisrll can be used to create a 'window' 011 a collection of data that is\nappropriate for SOllIe group of users. 'Views allow us to liUlit access to sensitive\ndata by providing access to a restricted version (defined through a view) of that\ndata, rather than to the data itself.\nWe use the following\nSCll(~InHS in our exaurples:\nSailors( s'id: integer, snarne: string, rating: integer, age: real)\nBoats( bid:\ninteg~r, bnarne: string, color: string)\nRl~serv~;s(sid: ,,~nteger, bid: _...integer,\nd~y): dates)\nIncreasingly, as database systcrlls becorne the backbone of e-COlluncrce appli-\ncations requests originate over the Internet.\nThis rnakes it irnportant to be\nable to authenticate a user to the databa..se systern.\nA.fter all, enforcing a\nsecurity policy that allows user Sarn to read a table and Ehner to write the\ntable is not of l11uch use if S~un can rnasquerade a\"s Ebner. COllversely, we Inus!;\nbe able to assure users that they a,re COIluIlunicating \\vith a legitilnate systern\n(e.g., the real Arnazoll.col11 server, and not a spurious application intended to\nsteal sensitive inforrnation such as\nc), credit card nurl11>cr).\nvVhile the details\nof authentication are outside the scope of our coverage, we discuss the role\nof authentication (uId the l)Hsic ide;:ls involved in Section 21.5, after covering\ndatabase access control rnechanisrIls.\n21.2\nACCESS CONTROL\ni\\ database for an enterprise contains a great deal of inforrnation and usually\nhas sever(.tl groups of users. 1\\IJost users need to access onl,y a sruall pa;rt of the\ndatabase to carry out their ta\",:,ks. J\\l1owing users unrestricted access to all the\n\nSecuT'it:lJ ctnd, ,4 ldlun~ization\n6'() I'\"\n.. Q\nt\ndata can be undesirable, and a !)Bl\\IlS should provide rnechanisHls to control\naccess to data.\nA DBMS offers two rnain approaches to access control. Discretionary access\ncontrol is ba,,\"ed on the concept of access rights, or privileges, and rnecha-\nnisrllS for giving users such privileges. A privilege allows a user to access Borne\ndata object in a certain IIlHnIler (e.g., to read or 11lOdify). A user ,vho creates\na databa,se object such as a table or a vie\\v autornatically gets all applicable\nprivileges on that object. The D.BMS subsequently keeps track of how these\nprivileges are granted to other users, and possibly revoked, and ensures that at\nall tirnes only users with the necessary privileges can access all object.\nS(~L sup-\nports discretionary access control through the GRANT and REVOKE conunands.\nThe GRANT cOllnnand gives privileges to users, and the REVOKE cornrnand takes\naway privileges. We discuss discretionary access control in Section 21.3.\nDiscretionary access control rnechanisrns, while generally effective, have certain\nweaknesses. In particular, a devious unauthorized user can trick an authorized\nuser into disclosing sensitive data. Mandatory access control is based on\nsystemwide policies that cannot be changed by individual users.\nIn this ap-\nproach each databal.'3e object is assigned a security class, each user is assigned\nclearance for a security cla..ss, and rules are irnposed on reading and writing of\ndatabase objects by users. The DBMS deterrnines whether a given user can\nread or write a given object based on certain rules that involve the security\nlevel of the object and the clearance of the user.\nThese rules seek to ensure\nthat sensitive data can never be 'passed on' to a user without the necessary\nclearance.\n'rhe SQL standard does not include any support for rnandatory\naccess control. 'We discuss rnandatory access control in Section 21.4.\n21.3\nDISCRETIONARY ACCESS CONTROL\nSQL supports discretionary access control through the GRANT and REVOKE corn-\nrnands. The GRANT cornrnand gives users privileges to base tables and views.\n'rhe syntax of this corllrllctnd is H.'I.'3 follows:\nGRANT privileges ON object TO users [WITH GRANT OPTION]\nFor our purpo~esobject is either a base table or a vie\\-v. SClL recognizes certain\nother kinds of objects, but we do not discuss thcrn. Several privileges can be\nspecified, including these:\nIII\nSELECT: The right to access (read) all colurnns of the table specified as the\nobject, including colurnns added later through ALTER TABLE cornrnands.\n\n696\nCHAPTER 2l\n•\nINSERT( colurnn-na'Tne):\nThe right to insert rowsvvith (non-nuU or non-\ndefault) values in the naTned cohnnn of the table rHuncd as object.\nIf\nthis right is to be gra,nted with respect to all cohunns, including coluulns\nthat rnight be added later, \\ve can sirnply usc INSERT.\n1~he privileges\nUPDATE( col't/,'rnn-narne) and UPDATE are sirnilar.\nIII\nDELETE: 1'hc right to delete rows frorn the table narned\ni:1..S object.\n•\nREFERENCES(col'Urnn-namJe): The right to define foreign keys (in other ta-\nbles) that refer to the specified cohnnn of the table object. REFERENCES\nwithout a colurnn naUIe specified denotes this right with respect to all\ncolurnns, including any that are added later.\nIf a user has a privilege with the grant option, he or she can P<:1..')S it to another\nuser (with or without the grant option) by using the GRANT conunand. A user\nwho creates a base table autolnatically has all applicable privileges on it, along\nwith the right to grant these privileges to other users. A user who creates a\nview has precisely those privileges on the view that he or she has on everyone\nof the views or base tables used to define the view. The user creating the view\nInust have the SELECT privilege on each underlying table, of course, and so is\nalways granted the SELECT privilege on the view. The creator of the view has\nthe SELECT privilege with the grant option only if\nhE~ or she has the SELECT\nprivilege with the grant option on every underlying table. In addition, if the\nview is updatable and the user holds INSERT, DELETE, or UPDATE privileges\n(with or without the grant option) on the (single) underlying table, the user\nautornatically gets the same privileges on the view.\n()nly the owner of a scherna can execute the data definition statcrnents CREATE,\nALTER, and DROP on that schcrna. The right to execute these staternents cannot\nbe granted or revoked.\nIn conjullction with the GRANT and REVOKE cOl1llnands, views are an irnportant\ncornponent of the security rnechanisrns provided by\nEt relational\nJ)B~1S. By\ndefining vie\\vs on the base tables, \\ve can present needed inforrnation to a user\n\"while hiding other inforrnation that the user should not be given access to. For\nexalnple, consider the following view definition:\nCREATE VIEW }\\ctiveSajlors (naJIle, age, day)\nAS SELECT S.snarne, S.age, R\"day\nFROM\nSailors S, H,eserves }{\nWHERE\nS.sid =: Il.sid AND S.rating > 6\nA user who can access ActiveSailors 1)11t not Sailors or R,eserves kno\\vs the\nnennes of sailors who have reservations but cannot find out the bids of boats\nreserved by a given sailor.\n\nSec1tTity an:rl Authorization\n697\nRole-Ba.'icd Authorization in SQL: Privileges are assigned to users\n(authorization 11)s, to be precise) in S(~L-92. In the real world, privileges\narE~ often LiSsociatedwith a user's job or Tole within the organizat;ion.. :Nlany\nDBMSs have long supported the concept of a role and allowed privileges\nto be assigned to roles.\nI{,oles can then he granted to users and other\nroles. (Of courses, privileges can also be granted directly to users.) l'he\nSQL:1999 standard includes support for roles.\nR.,oles eanbe created and\ndestroyed using the CREATE ROLE and DROP ROLE eornrnands.\nUsers can\nbe granted roles (optionally, \\vith the ability to P&'3S the role on to others).\nThe standard GRANT and REVOKE connnands can assign privileges to (and\nrevoke from) roles or authorization IDs.\nWhat is the benefit of including a feature that Inany systerns already sup-\nport? 'T'his ensures that, over tiIne, all vendors who comply with the stan-\ndard support this feature. 'rhus, users can use the feature without worrying\nabout portability of their application across DBMSs.\nPrivileges are assigned in SQL to authorization IDs, which can denote a sin-\ngle user or a group of users; a user lllUSt specify an authorization ID and, in\nInany systerns, a corresponding password before the DBMS accepts any C0111-\nrnancls from hirn or her. So, technically, Joe, l'vlichael, and so on are authoriza-\ntion IDs rather than user nan1es in the following exalllpies.\nSuppose that user Joe has created the tables Boats, Reserves, and Sailors.\nSenne exarnples of the GRANT cOllunand that Joe can now execute fo11o\\v:\nGRANT INSERT, DELETE ON Reserves TO Yuppy WITH GRANT OPTION\nGRANT SELECT ON Reserves TO Nlichael\nGRANT SELECT ON Sailors TO Michael WITH GRANT OPTION\nGRANT UPDATE (rating) ON Sailors TO Leah\nGRANT REFERENCES (bid) ON Boats TO Bill\nYuppy CaJl insert or delete Ileserves rO\\V8 and authorize SOlneone else to do the\nsarne. I\\1ichael can execute SELECT queries on Sailors and H,eserves, and 118 can\npass this privilege to others for Sailors but not for R,eserves. \\Vith the SELECT\nprivilege, 1-tichael can create a view that accesses the Sailors and Ileserves\ntables (for exarnple, the ActiveSailors vic\\v), but he cannot grant SELECT on\nActiveSailors to others.\n()rl the other hand, suppose that Iv1ichael cn~ates the foUo\\ving vie\\v:\nCREATE VIEWYoungSailors (sicl, age, rating)\nAS SELECT S.sicl, S.age, S.rating\n\n698\nFROM\nWHERE\nSailors S\nS.age < 18\nCHAPTER 21\nThe only underlying table is Sailors, for which Michael has SELECT with the\ngrant option. He therefore h&'3 SELECT with the grant option on YoungSailors\nand can pass on the SELECT privilege on YoungSailors to Eric and Guppy:\nGRANT SELECT ON YoungSailors TO Eric, Guppy\nEric and Guppy can now execute SELECT queries on the view YoungSailors-\nnote, however, that Eric and Guppy do not have the right to execute SELECT\nqueries directly on the underlying Sailors table.\nMichael can also define constraints based on the inforrnation in the Sailors and\nReserves tables.\nFor exarnple, Michael can define the following table, which\nhas an associated table constraint:\nCREATE TABLE Sneaky (lnaxrating\nINTEGER,\nCHECK (maxrating >=\n( SELECT MAX (S.rating)\nFROM\nSailors S )))\nBy repeatedly inserting rows with gradually increasing rnaxrating values into\nthe Sneaky table until an insertion finally succeeds, lVIichael can find out the\nhighest rating value in the Sailors table.\nThis exarnple illustrates why SQL\nrequires the creator of a table constraint that refers to Sailors to possess the\nSELECT privilege on Sailors.\nReturning to the privileges granted by Joe, Leah can update only the rating\ncolulnn of Sailors rows. She can execute the following cornmand, which sets all\nratings to 8:\nUPDATE Sailors S\nSET\nS.rating = 8\nIIuwever, she cannot execute the seune cOllunand if the SET clause is changed\nto be SET S. age = 25, because she is not allowed to update the age field. A\nrnoro subtle point is illustrated by the following cOIrllnand, which decrelnents\nthe rating of all 'sailors:\nUPDATE Sailors S\nSET\nS.ratillg = S.rating-l\nLeah cannot execute this cOlInnand because it requires the SELECT privilege 011\nthe IS. Tabng colurnn anei Leah does not have this privilege.\n\n/3ecurit:1J and .A'lLthoT\"izat'ion\n699\nBill can refer to the lxid colurnn of Boats as a foreign key in another table. For\nexalnple~ Bill can create the Reserves table through the following cOlnnland:\nCREATE TABLE R\"eserves (sid\nINTEGER,\nbid\nINTEGER,\nday\nDATE,\nPRIMARY KEY (bid, day),\nFOREIGN KEY (sid) REFERENCES Sailors ),\nFOREIGN KEY (bid) REFERENCES Boats)\nIf Bill did not have the REFERENCES privilege on the bid coh1111n of Boats, he\nwould not be able to execute this CREATE staternent because the FOREIGN KEY\nclause requires this privilege. (A sirnilar point holds with respect to the foreign\nkey reference to Sailors.)\nSpecifying just the INSERT privilege (sirnilarly, REFERENCES and other privi-\nleges) in a GRANT conlmand is not the sarne as specifying SELECT(colurnn-name)\nfor each column currently in the table. Consider the following command over\nthe Sailors table, which has cohllnns sid, snarne, rating, and age:\nGRANT INSERT ON Sailors TO J\\!Iichael\nSuppose that this conunand is executed and then a colurnn is added to the\nSailors table (by executing an ALTER TABLE cOlIllnand).\nNote that Michael\nhas the INSERT privilege with respect to the newly added colurnn. If we had\nexecuted the following GRANT cornrnand, instead of the previous one, Michael\nwould not have the INSERT privilege on the new cohllnn:\nGRANT\nINSERT ON Sailors(sid), Sailors(sna1ne) , Sailors(rating),\nSailors( age), TO J\\!Iichael\nThere is a cornplernentary corl1rnand to GRANT that allov.ls the \\vithdra:wal of\nprivileges. The syntax of the REVOKE cOllunand is as follows:\nREVOKE [GRANT OPTION FOR ] privileges\nON object FROM users {RESTRICT I CASCADE }\nThe cOIlnnand CH,n be used to revoke either a privilege or just the grant option\non a privilege (by using the optional GRANT OPTION FOR clause). One of the\ntwo a..lternatives, RESTRICT or CASCADE, HUlst be specified; we see 'what this\nchoice IneaI1S shortly.\nThe intuition behind the GRANT cOlnnlHJHl is clear: rrhe creator of a ba\",se table\nor a vh~\\v is given all the ctppropriate privileges \\vith respect to it and is alh)\\ved\n\n700\nto pass these privileges~··-·-includingthe right to pass along a privilege,,~,to other\nusers. The REVOKE comuland is, as expected, intended to achieve the reverse:\nA user who ha\",:; granted a privilege to another user rnay change his or her lnincI\nand \\vant to withdraw the gra,nted privilege. 1\n1he intuition behind exactly 'what\neffect\n<::1, REVOKE cornrnand has is conlplicated by the fact that a user Inay be\ngranted the sarne privilege rnultiple tilnes, possibly by different users.\n\\Vhen a user executes a REVOKE cornmand with the CASCADE keyword, the effect\nis to \\vithdraw the IHuned privileges or grant option froIn all users \\vho currently\nhold\ntlH~se privileges solely through a GRANT cOllunand that \"va,,') previously\nexecuted by the sallIe user who is now executing the REVOKE cOl1nnand.\nIf\nthese users received the privileges with the grant option and passed it along,\nthose recipients in turn lose their privileges as a consequence of the REVOKE\ncOIurnand, unless they received these privileges through an additional GRANT\ncomIuand.\nWe illustrate the REVOKE cOllllnand through several examples. First, consider\nwhat happens after the following sequence of eornmands, where Joe is the\ncreator of Sailors.\nGRANT SELECT ON Sailors TO Art WITH GRANT OPTION\nGRANT SELECT ON Sailors TO Bob WITH GRANT OPTION\nREVOKE SELECT ON Sailors FROM Art CASCADE\n(crecuted by Joe)\n(executed by Art)\n(executed by Joe)\nArt loses the SELECT privilege on Sailors, of course. Then Bob, who received\nthis privilege from Art, and only Art, also loses this privilege. Bob's privilege\nis said to be abandoned when the privilege froIn which it was derived (Art's\nSELECT privilege with grant option, in this exarnple) is revoked.\nvVhen the\nCASCADE keyword is specified, all abandoned privileges are also revoked (pos-\nsibly causing privileges held by other users to becOlne abandoned and thereby\nrevoked recursively). If the RESTRICT keyword is specified in the REVOKE corll-\nmctnd, the cornrnand is rejected if revoking the privileges just frorn the users\nspecified in the cOIllluand vvould result in other privileges becorning abandoned.\nConsider the following sequence, as another exarnple:\nGRANT SELECT ON Sailors TO Art WITH GRANT OPTION\nGRANT SELE\"cT ON Sailors TO Bob WITH GRANT OPTION\nGRANT SELECT ON Sailors TO Bob WITH GRANT OPTION\nREVOKE SELECT ON Sailors FROM Art CASCADE\n(e:recuted by Joe)\n(e:tecuted by Joe)\n(e.Tfc'uted by Art)\n(e~cecuted by Joe)\n.ilS before, Art loses the SELECT privilege on Sailors.\nBut vvhat about Bob?\nBob received this privilege fronl Art, but he (:llso received it: independently\n\n8ec'u'rity and ..4nthoTization\n701\n(coincidentally, directly fro111 Joe). So Bob retains this privilege. Consider a\nthird eXa,lllple:\nGRANT SELECT ON Sailors TO Art WITH GRANT OPTION\nGRANT SELECT ON Sailors TO Axt WITH GRANT OPTION\nREVOKE SELECT ON Sailors FROM Art CASCADE\n(executed by Joe)\n(executed by Joe)\n(e:l;ecuted by Joe)\nSince Joe granted the privilege to Art twice and only revoked it once, does\nArt get to keep the privilege?\nAs per the SQL standard, no.\nEven if Joe\nabsentmindedly granted the saIne privilege to Art several tirnes, he can revoke\nit with a single REVOKE cOIlunand.\nIt is possible to revoke just the grant option on a, privilege:\nGRANT SELECT ON Sailors TO Art WITH GRANT OPTION\n(executed by Joe)\nREVOKE GRANT OPTION FOR SELECT ON Sailors\nFROM Art CASCADE\n(executed by Joe)\nThis cOlnmand would leave Art with the SELECT privilege on Sailors, but Art\nno longer has the grant option on this privilege and therefore cannot pass it on\nto other users.\nThese exarnples bring out the intuition behind the REVOKE cOillllland, and\nthey highlight the cOlllplex interaction between GRANT and REVOKE cOlnnlands.\nWhen a GRANT is executed, a privilege descriptor is added to a table of such\ndescriptors Inaintained by the DElVIS. The privilege descriptor specifies the ~ol­\nlowing: the grantor of the privilege, the gTarrtee who receives the privilege, the\ngr-anted privilege (including the narne of the object involved), and whether the\ngrant option is included. When a user creates a table or view and 'autornati-\ncally' gets certain\nprivileges~ a privilege descriptor with system, a.'S the grantor\nis entered into this table.\nrrhe effect of a series of GRANT cornrnands can be described in terrns of an\nauthorization graph in which the nodes are users ......-technically~ they are au-\nthorization IDs·..·--and the arcs indicate .how privileges are passed. There is an\narc fron1 (the node for) user 1. to user 2 if user 1. executed a GRANT cOIIunand\ngiving a privilege to user 2; the arc is labeled \\vith the descriptor for the GRANT\ncOIlllnand. A GEANT cOIlnnand has no effect if the saIne privileges ha.ve already\nbeen granted to the SeHne grantee by th.e sarne grantor. The following sequence\nof COnllllcUlds illustrates the sernantics of GRANT and REVOKE connnands when\nthere is a cycle in the authorization graph:\nGRANT SELECT ON Sailors TO .Art WITH GRANT OPTION\nGRANT SELECT ON Sa.ilors TO Bob WITH GRANT OPTION\n(e:r:e.c'U,ted by Joe)\n(e;1;(~c'lded by Art)\n\n702\nGRANT SELECT ON Sailors TO Axt WITH GRANT OPTION\nGRANT SELECT ON Sailors TO Cal WITH GRANT OPTION\nGRANT SELECT ON Sailors TO Bob WITH GRANT OPTION\nREVOKE SELECT ON Sailors FROM i-\\rt CASCADE\nellAP'I'E,R $21\n(e:ceC1tted bllBob)\n(e:r:ec1lted bl/ Joe)\n(cJ;eclded by Cal)\n(executed by Joe)\nThe authorization graph for this exarnple is shown in Figure 21.1. Note that\nvve indicate hovv Joe, the creator of Sailors, acquired the SELECT privilege fror11\nthe DBlVIS by introdtIcing a 8ystern node and dravving an arc froIn this node\nto Joe's node.\n~:~\n~/\njystem, Joe, Select on Sailo\", Yes)\n(JO~rt, Select on Sailo\", Yes)\n_../\n(Art. Boh. Select on Sailors, Yes)\n(Joe, Cal. Select on Sailors. Yes)\n/\"\n(Bob. Art, Select on Sailors. Yes)\n0:/\n(Cal, Bo;;:-;elecl on Sailo\", ~)\nBob\nFigure 21.1\nExample Authorization Graph\nAs the graph dearly indicates, Bob's grant to Art and Art's grant to Bob (of the\nscl.lne privilege) creates a cycle. Bob is subsequently given the saIne privilege\nby Cal, who received it independently froIn Joe. At this point Joe decides to\nrevoke the privilege he granted Art.\nLet us trace the effect of this revocation. 1'he arc [raIn Joe to Art is\nrenlov(,~d\nbecause it corresponds to the granting action that is revoked.\nAll rernaining\nnodes have the following property: 1f node N has an otdgo'ing aTe labeled with\na pT'iv'ilege, there is a path fTorn the Systern node to 'node N in 'which each aTC\nlabel contains the sante privilege phiS the grant opt'ion. That is, any rernaining\ngranting action is justified by a privilege received (directly or indirectly) frorn\nthe Systern. The execution of Joe's REVOKE conllnand therefore stops at this\nPOiIlt,\\vith everyone continuing to hold the SELECT privilege on Sailors.\nrrhis result\nrn~.ty seenl nnintuitive because Art continues to have the privilege\nonly because he received it fr0111 Bob, and at the tiIne that Bob granted the\nprivilege to Art, he had received it only frorn Art. Although Bob acquired the\nprivilege through Cal subsequentl~y, should we not undo the effect of his grant\n\nto Art vvhen executing Joe's REVOKE cOlInlland? 'rhe effect of the grant frorn\n\"-\"\n'\"\nBob to ,Art is 'not undone in SCJL. In effecL if a user acquires a privilege rnultiple\ntilnes frolIl different grantors,\nS(~L treats each of these grants to the user <he;\nhaving occurred befoTe that user pa\"c;sed on the privilege to other users. This\niInplcrnentation of REVOKE is convenient in 111any reaJ-\\vorld situations.\n.For\nexanlple, if a lIl.anager is fired after passing on sorne privileges to subordinates\n(vvho lnay in turn have passed the privileges to others), vve can ensure that\nonly the rnanager's privileges are rernoved by first redoing all of the Illanager's\ngranting actions and then revoking his or her privileges. That is, Vle need not\nrecursively redo the subordinates' granting actions.\nTo return to the saga of Joe and his friends, let us suppose that Joe decides\nto revoke Cal's SELECT privilege a.., well.\nClearly, the arc frorn Joe to Cal\ncorresponding to the grant of this privilege is rerIloved. The arc frorH Cal to\nBob is reilloved as well, since there is no longer a path fronl SystelIl to Cal\nthat gives Cal the right to pass the SELECT privilege on Sailors to Bob. The\nauthorization graph at this interrnediate point is shown in Figure 21.2.\nE=0\n1-(SY<tem, Joe, Select on Sailoe<, Ye<)\n(o~)\n(~\n\"''--'-''~'/\n(Art, Bob, Select on Sailors, Yes)\n(Bob, Art, Select on Sailors, Yes)\nCal\nFigure 21.2\nExample Authorization Graph during Revocation\nrrhe graph 110Vv' contains t~lO nodes (Art and Bob) for vvhieh there are outgoing\narcs \\vith labels containing the SELECT privilege on Sailors; therefore, these\nusers have granted this privilege.\nlInwever, although each node contains a,n\nincorning arc carrying the saIne privilege, then:; is no .such path jnnn Systern\nto either of these nodes; so these users' right to grant the privilege lUh'S been\nabandonecL \"\\\\le therefore rernove the outgoing arcs as well. In general, these\nnodes rnight have other arcs incident on theIn, but in this exarnplc, they now\nhave no incident arcs. Joe is left as the only user\\vith the SELECT privilege on\nSailors; Art and Bob have lost their privileges.\n\n704\nCHAPTER 2J\n21.3.1\n(;rant and Revoke on Views and Integrity Constraints\n1\"'!he privileges held by the creator of a vie\\\\! (\\vitll respect to the vie\\v) change\nover tiIne cbS he or she gains or loses privileges on the underlying tables. If the\ncreator loses a privilege held 'with the grant option, users vvho \"vere given that\nprivilege on the view lose it as \\vell. 'There are SOlHO subtle &-spects to the GRANT\nand REVOKE conunands vvhen they involve vicV\\1s or integrity constraints. \\;Ye\nconsider senne exarnples that highlight the folloV\\Ting irnportant points:\n1. A view Inay be dropped because a SELECT privilege is revoked froIn the\nuser who created the\nvie\\~r.\n2. If the creator of a vie\"v gains additional privileges on the underlying tables,\nhe or she autornatically gains additional privileges on the view.\n3. The distinction between the REFERENCES and SELECT privileges is irnpor-\ntanto\nSuppose that Joe created Sailors and gave Michael the SELECT privilege on it\nwith the grant option, and J\\!Iichael then created the view YoungSailors and\ngave Eric the SELECT privilege on ·youngSailors. Eric now defines a view called\nFiIl(~YoungSailors:\nCREATE VIEW FineYoungSailors (naIne, age, rating)\nAS SELECT S.snarne, S.age, S.rating\nFROM\nYoungSailors S\nWHERE\nS.rating > 6\n\\Vhat hc1.1>pens if .Joe revokes the SELECT privilege on Sailors froln l\\1icha,el?\nlV1ichael no longer has the authority to execute the query used to define Young-\nSa,ilors because the definition refers to Sailors. rrherefore, the vieV\\! YoungSailors\nis dropped (Le., destroyed). In turn, Fine'{oungSailors is dropped as \\vell. Both\nview definitions axe rernoved fr0111 the systcln catalogs; even if (1, rerIlorseful Joe\ndecides to give ba,ckthe SELECT privilege on Sailors to l\\;1ichael, the vicV\\Ts are\ngone a11d rnust be created afresh if they are required.\nOn a Inore happy note, suppose tllat everything proceeds as just described until\nEric defines\nI~'\\ineYoungSailors; tJleu, instead of revoking the SELECT privilege\non Sailors frorll I\\:lichael, .Joe decides to also give l\\'Echctel the INSERT privilege\n011 Sailors. l\\!Iichael's privileges on th(~ vievv YoungS(tilors are upgraded to \\vhat\nhe \\vould 11<'lVe if he \\-vere to create the vie\\v no'll!.\nlI(~ therefore acquires the\nINSERT privilege on 'YourlgSailors as VilCd1. (Note that this vie\"v is updatal)le.)\n\\~nlat ab(nltEri(~? IIis privileg(~s axe\nUn(Jlfu1g(:~d.\n\\Vh(~ther\n()l' ll.ot .lVIicllael 11::18 tlle INSERT privilege 011 \\roungSailors with the\ngra11t ()ption clel)encls 011 \\vhether or not .Joe gives hirn the INSERT I)rivilege OIl\n\n70~\nSailors \\vith the grant option. 'Ib understand this situation, consider Eric again.\nIf lVIiehael has the INSERT privilege on YoungSailors with the grant option, he\ncan pass this privilege to Eric. Eric could then insert rovvs into the Sailors table\nbecause inserts on YC}llngSailors are effected by rnodifying the underlying base\ntable, Sailors. Clearly, vve do not \\vant l\\:1ichael to be able to authorize Eric to\nrnake such changes unless :I\\JIichael has the INSERT privilege on Sailors with the\ngrant option.\nrrhe REFERENCES privilege is very different froIll the SELECT privilege, kk\" the\nfollowing exarIlple illustrates. Suppose that Joe is the creator of Boats..He can\nauthorize another user, say, Freel, to create H,eserves with a foreign key that\nrefers to the bid colurnn of Boats by giving ~'red the REFERENCES privilege with\nrespect to this colulnn. ()n the other hand, if Fred has the SELECT privilege on\nthe bid colurnn of Boats but not the REFERENCES privilege, Fred cannot create\nR.eserves with a foreign key that refers to Boats. If Fred creates R,eserves with\na foreign key colunlll that refers to bid in Boats and later loses the REFERENCES\nprivilege on the bid colurnn of boats, the foreign key constraint in Reserves is\ndropped; however, the R,eserves table is not dropped.\nTo understand why the SQL standard chose to introduce the REFERENCES priv-\nilege rather than to siInply allow the SELECT privilege to be used in this sit-\nuation, consider what happens if the definition of Iteserves specified the NO\nACTION option with the foreign key------.-Joe, the owner of Boats, Inay be pre-\nvented from deleting a row fronl Boats because a row in Reserves refers to this\nBoats row. Giving Fred, the creator of Reserves, the right to constrain updates\non Boats in this rnanner goes beyond. siInply allowing hinl to read the values\nin Boats, vvhich is all that the SELECT privilege authorizes.\n21.4\nMANDATORY ACCESS CONTROL\nDiscretionary access coutrollnechanislns, while generally effective, have certain\n\\veaknesses. In particular they are susceptible to Trojan h07'se schelnes whereby\na devious unauthorized user can trick an authorized user into disclosing sensi-\ntive data. For exalnple, suppose that student rrricky Dick \\Va.nt8 to break into\nthe grade tables of instructor ]).'ustin Justin. IJick does the following:\nIIIlI\nlIe creates a nc\\v table called lVlineAlIlVIine and gives INSERT privileges\non this tahle to .Justin (who is blissfully una\\vare of aJl this attention, of\ncourse).\nlI!\nlIe rllodifies the code of SOllIe I}BlVIS application that Jllstin uses often to\ndo\n(L couple of additional things: first, read the (jrades tel-ble,\nCtlld next,\n\\vrite the result into IVIineAl1~'1ine.\n\n706\nCHAPTER ~1\nThen he sits back and waits for the grades to be copied into :NfineAllNlinc and\nlater undoes the Illodifications to the application to ensure that Justin does\nnot sOlnehow find out later that he has been cheated. Thus, despite the DB~1S\nenforcing all discretionary access controls··.,.-·--only Justin's authorized code ,vas\nallowed to access Grades....·..··....sensitive data is disclosed to an intruder. 1:'he fact\nthat Dick could surreptitiously modify Justin's code is outside the scope of the\nDB1/IS's access control rnechanisrn.\nNIandatory access control meehanisrns are airned at addressing such loopholes in\ndiscretionary access control. 1\"he popular rllodel for mandatory access control,\ncalled the Bell-LaPadula Illodel, is described in tenllS of objects (e.g., tables,\nviews, rows, columns), subjects (e.g., users, prograrlls), security classes, and\nclearances. Each databa..'3e object is a..'3signed a security class, and each subject\nis assigned clearance for a security class; we denote the class of an object or\nsubject A as class(A). The security classes in a systerll are organized according\nto a partial order, with a most secure class and a least secure class. for\nsirnplicity, we assume that there are four classes: top secTet (T8), secret (8),\nconfidential (C), and unclassified (U). In this system, T8> S > C> U, where\nA > B rneans that class A data is more sensitive than class B data.\nThe Bell-LaPadula model imposes two restrictions on all reads and writes of\ndatabase objects:\n1. Simple Security Property: Subject S is allowed to read object 0 only\nif class(8) > class(()). For exarllple, a user with TS clearance can read a\ntable with C clearance, but a user with C clearance is not allowed to read\na table with TS classification.\n2. *-Property:\nSubject S is allowed to write object 0 only if class(S) <\nclass(O). For exarllple, a user with S clearance can write only objects with\nS or TS classification.\nIf discretionary a,ccess controls are also specified, these rules represent addi-\ntionaJ restrictions. Therefore, to read or write a databa.'3e object, a user lllUst\nhave the necessary privileges (obtained via GRANT cornrnands) and the security\nclasses of the user and the object rnust satis(y the preceding restrictions. Let\nus consider how such a Inandator~y control rnech.an.isrn lui.ght h.ave foiled 1'ricky\nI)ick. rfhe Grades table could be classified aB S, .Justin could be given clearance\nfor S, and 'Il'icky Di.ck could be given a lower clearance (C\nf). Dick can create\nobjects of only C! or lc)\\ver classification; so the table l\\!IineAl1ivline can have at\nInos1, the classification (}. \\Vhen the application prograrIl running on behalf of\n..Justin (and therefore\\vith clearance S) tries to copy (irades into 1\\1ineAllIVline,\nit is not allowed to do so because clas,s(1\\llineAlllvfrin,e) < class(applicat'ion), a.nel\nthe *-Property is violated.\n\nSeC'UT'ity and Authorization\n21.4.1\nMultilevel Relations and Polyinstantiation\n7Q7\nrro apply Inandatory access control policies in a relational DBMS, a security\nclf:1SS must be ac;sig11ed to each databa...sc object.\nThe objects can be at the\ngranularity of tables, rows, or even individual colurnn values. Let us assU111e\nthat each row is a9signed a security class. This situation leads to the concept\nof a multilevel table, which is a table with the surprising property that users\nwith different security clearances see a different collection of rows when they\naccess the sarne table.\nConsider the instance of the Boats table shown in Figure 21.3. Users with S\nand TS clearance get both rows in the answer when they ask to see all rows in\nBoats. A user with C clearance gets only the second row, and a user with\n[J\nclearance gets no rows.\nI.bid ., bname J color\nI SeclJ,:citYClass.\n[~ ~~~:: J~;:wn E-··.._-.-S-=-C---\nFigure 21.3\nAn Instance B1 of Boats\nThe Boats table is defined to have bid as the prirnary key. Suppose that a user\nwith clearance C wishes to enter the row (101, Picante, Scarlet, 0). We have\na dilemrna:\n•\nIf the insertion is perlnitted, two distinct rows in the table have key 101.\n•\nIf the insertion is not pennitted because the priInary key constraint is vio-\nlated, the user trying to insert the new row, who ha...') clearance C, can infer\nthat there is a boat with {rid==101 whose security class is higher than C. This\nsituation cOlnpromises the principle that users should not be able to infer\nany infonnation about objects that have a higher security classification.\nThis dilerrlllla is resolved by effectively treating the security cla..ssification as part\nof the key. rrhus, the insertion is allo\\ved to continue, and the table instance is\nrnodified as shown in Figure 21.4.\nI bid I bna'me I color I Security-'Class I\nl(:i'T .'--'Salsa\nfled\n.--,§\n101\nPicante\nScarlet\nC\n1---------+------....-- .___._----+-------\n102\nPinto\nBrown\nC\n---_....\n.\n_---\nFigure 21.4\nInsta.nce 131 after Insertion\n\n708\nCHAPTER 21\nlJsers\\vith clearance C or [1 see just the rows for Picante and Pinto, but users\nwith clearance S or 1'8 see all three nnvs.\nThe two ro\\vs with bid=1()1 can\nbe interpreted in one of t'wo\\vays: only the rc)\\v\\vith the higher cla..~sification\n(Salsa, with classification 8) a,ctually exists, or both exist and their presence is\nrevealed to users according to their clearance level. The choice of interpretation\nis up to application developers and users.\nThe presence of data objects that appear to have different values to users\n¥lith different clearances (for exarnple, the boat with b'id 101) is called polyin-\nstantiation. If we consider security classifications associated\n\\~lith individual\ncolurnns, the intuition underlying polyinstantiation can be generalized in a\nstraightforward\nrnanner~ but SOIne additional details Inust be addressed.\n\\I\\le\nrelnark that the rnain drawback of rnandatory access control schelnes is their\nrigidity; policies are set by systeIll adrninistrators, and the classification 1necha-\nnisrns are not flexible enough. A satisfactory cornbination of discretionary and\nrnandatory access controls is yet to be achieved.\n21.4.2\nCovert Channels, DoD Security Levels\nEven if a DElVIS enforces the rnandatory access control schenle just discussed,\ninforrnation can flow frorn a higher classification level to a lower classification\nlevel through indirect rneans, called covert channels. For exanlplc, if a trans-\naction accesses data at rnore than one site in a distributed DBI\\1S, the actions\nat the tvvo sites 1nust be coordina,ted.\nThe process at one site rTlay have a\nlower cleara.nce (say, C) than the process at another site (say, S), and both\nproceSSE~S have to agree to cOllnnit before the transaction can be conunitted.\nThis requirernent can be exploited to pass i11fo1'rnatio11 with an S classification\nto the process with a () clearance: The transaction is repeatedly invoked, and\nthe process \\vith the C: clearance always agrees to cOlIllnit, whereas the prOCf~SS\nwith the 8 clearance agrees to conunit if it wants to translnit a 1 bit and does\nnot agree if it ~rants to transrnit a 0 l)it.\nIn this (adrnittedly tortuous) lIlanllcr, infonnation with an ,9 clearance can\nb(~\nsent to a process with a C: clearance as a strea111 of bits.\n1~his covert cllannel is\nan indirect violation of the int(~nt behind the *-Propert.y. Additional exarnples\nof covert channels can be found readilv in statistical dataJ)t:lses, vvhich vve cliscuss\n,j\n.\"\nS'\n.\n')!\n6'\" 2\"\nIn\nk cetlon .... ,. \". '.\nDBrv1S vendors recently started irnplcrnenting rnandatory access control rnech-\naniSlns (although they aTe not part: of the S(~L stanc!<trcl) because the lJ nit(:~(l\nStates l)epartnlent of J)efense (1)01)) requires such support for its systelns.TlH~\nDol) requirernents can be described in terrns of security levels ./1 ~ ,13, CI, E1JJd\nD. of \\vhich ./1 is the 1J10st secure and 1) is the lC<l,st secure.\n\nSeC'llTity and ..4ntluJ1\"ization\n709\nCurrent SysteIlls:ColnuilereiaIR,DBIvISs areavt\\ilable that support dis-\ncretionary controls at the C21evel andrnandat.ory controls at theBJ level.\nIf3:NI DB2, Inforruix, ivIierosoft SQL Server, Oracle 8, and Sybase ASE an\nsupport SQL's features for discretionary clccess controL In gel1eral, they\ndo not support lnandatory a.ccess control; Ora,cle offers a versio:n of their\nproduct with support for rnandatory access control.\nLevel C requires support for discretionary access control.\nIt is divided into\nsublevels Cl and C:2; C2 also requires SOll1C degree of accountability through\nprocedures such\n3,.':3 login verification and audit trails.\nLevel B requires sup-\nport for lnandatory access control.\nIt is subdivided into levels Bl, B2, and\nB3. Level 132 additionally requires the identification and clirnination of covert\nchannels.\nLevel B3 additionally requires 11laintenance of audit trails and the\ndesignation of a security administrator (usually, but not necessarily, the\nDBA). Level A, the most secure level, requires a n1athernatical proof that the\nsecurity rnechanisrn enforces the security policy!\n21.5\nSECURITY FOR INTERNET APPLICATIONS\nWhen a DBMS is accessed frorn a secure location, we can rely upon a shnple\npassword rnechanisrn for authenticating users.\nIIowever, suppose our friend\nSarn wants to place an order for a hook over the Internet. rrhis presents sorne\nunique challenges: Saln is not even a known user (unless he is a repeat cus-\ntonler). Fron1 Alnazon's point of view, we have an individual asking for a book\nand offering to pay with a credit card registered to Saln, but is this individual\nreally Sarn? f'rcnn Sarn's point of view, he sees a fornl asking for credit card\ninforrnation, but is this indeed a legitirnate part of Arnazon's site, and not a\nrogue application designed to trick hi1l1 into revealing his credit card nurnber?\nTlhis exarnple illustrates the need for a rnore sophisticated approach to authen-\ntication than a sirnple pass\"Vvord rnechanisrn.\nEncryption techniques provide\nth(-,~ foun,dation for rnodern authentica,tion.\n21.5.1\nEncryption\nThe basic ide(\\' b(~hind encryption is to apply an encryption algorithrn to the\ndata, using a user-specified or IJBA-SI)Ccified encryption key. The output of\nthe algorithrn is the en.cryptexl version of th(~ data.\nThere is aJso a, decryp-\ntion algorithrrL ·which ta,kes the encryptc\"d data and\n(:1, decryption key as\ninput and then returns the original data.\\Vithont the\ncorn:~ct decryption key~\nthe decryption aJgoritll111produces gibl)crish. rrhe\n(~llcrypti(nl and clecryption\n\n710\n()HAPTER\n2~1\nr-'-'\"\n---------·--·--·----·----·-..··-··1\nI\nDES and AES: The DES standard,. adopted in 1977, has a 56-bit en-\n!\nI\ncryption key.\nOver thne1 COII:tputers have become so ra\",t that, in 19991\n'I!!.\nI\n(1, special-purpose chip and a network ofPCs were used to cra{:kDl~S in\n!\nunder a day.\nThe systern\nW~l.'3 testing 245biHion keys per second w,hen\nthe correct key \"va,.\" fonnd! It is estirnated that a special~pu.rpose hE:trdware\ndevice can be built for under a 1l1iUioIl dollars that can crack DES in under\nfour hours. Despite growing concerns about its vulnerability, DES is still\nwidely used. In 2000, a successor to DES, called the Advanced Encryp-\ntion Standard (AES), W&'3 adopted as the new (syrrunetric) encryption\nstandard. AES has three possible key sizes: 128, 192, and 256 bits. \\\\lith\nII\na 128 bit key size, there are over 3 . 1038 possible AES keys, which is on\nthe order of 1024 Inore than the number of 56-bit DES keys. Asslllne that\nwe could build a conlputer fa.'3t enough to crack DES in 1 second. This J\nCOIllputer would. cornpnte for about 149 trillion years to crack a 128-bit\n~.=:..~~ (Experts think the universe is less than 20 billion years old.)\n_\nalgorithrns thernselves are assunled to be publicly known, but one or both keys\nare secret (depending upon the encryption scheme).\nIn symmetric encryption, the encryption key is also used as the decryption\nkey. The ANSI Data Encryption Standard (DES), which has been in use\nsince 1977, is a well-known exarnple of syllunetric encryption. It uses an en-\ncryption algorithrn that consists of character substitutions and pernlutations.\nrrhe nlain weakness of synunetric encryption is that all authorized users rnust\nbe told the key, increasing the likelihood of its becorning known to an intruder\n(e.g., by sirnple Inllnan error).\nAnother approach to encryption, called public-key encryption, ha.l;) becorne\nincrea.'3ingly popular in recent years.\nThe encryption scheniC proposed by\nHjvest, Sharnir, and Adlernan, called RSA, is a well-known exarnple of public-\nkey encryption. Each authorized user has a public encryption key, known\nto everyone, and a private decryption key, known only to hini or her. Since\nthe priv<lte decryption keys are kno\\llln only to their owners, the weakness of\n1)ES is avoided.\nA central issue for public-key encryption is ho\\v encryption and decryption\nkeys are chosen.\nTechnically, public-key encryption algorithrns rely on the\nexistence of one-way functions, whose inverses are cornplltationally very hard\nto deterrnine.\nrrhe I1SA algoritllIn, for exaJnple, is based on the observation\nthat, although checking vvhether a given nurnber is prirne is easy, deterrnining\nthe prirne factors of a nonpriIne nurnber is extrernely hard. (I)eterlnining the\n\nSec'lL'f'ity and \",4'llthoT'izat'ion\n71J\nWhy RSA Works: The essential point of the scherne is that it is easy to\ncompute d given e, 1>, and q, but ve'r.lJ hard to cornpute d given just e and\nL. In turn, this difficulty depends OIl the fact that it is hard to deterlnine\nthe priIne factors of L, \",r11ich happen to be p and q. A cavcat:Factoring\nis widely believed to be hard, but there is no proof that this is so.\nNor\nis there a proof that factoring is the only way to crack I\\SA; that is, to\nCU11111\nd frolll e and L.\nprirne factors of a nurnber with over 100 digits can take years of CPlJ tirne on\nthe fastest available COIllputers today.)\nWe now sketch the idea behind the R,SA algorithrn,\n~lssurning that the data. to\nbe encrypted is an integer I.\nTo choose an encryption key and a decryption\nkey for a given user, we first choose a very large integer L, larger than the\nlargest integer we will ever need to encode. 1 We then select a nUl1lber e as the\nencryption key and cornpute the decryption key d based on e and L; how this\nis done is central to the approach, as we see shortly. Both Land e are l1lade\npublic and used by the encryption algorithrn. However, d is kept secret and is\nnecessary for decryption.\nII\nThe encryption function is 8\nII\nThe decryption function is 1\nIe mod L.\nSd mod L.\nvVe choose L to be the product of t,vlO large (e.g., 1024-bit), distinct prirne\nnurnbers, 11 * q. 1\"he encryption key e is a randornly chosen nlunber between\n1 and L that is relatively prirne to (p\n\"~- 1) * (q - 1). The decryption key d is\ncornputed such that d*e = 1 mod ((p - 1) *(q - 1)). Ciiven these choices, results\nin nurnber theory can be used to prove that the decryption function recovers\nthe original ruessage frorll its encrypted version.\nA very irnportant property of the encryption and decryption algoritluns is that\nthe roles of the encryption and decryption keys can be reversed:\ndecrypt(el, (encrypt(c, I))) = I == decTypt(c, (cru:rypt(el, I)))\nSince In.any protocols rely on this property, \\ve henceforth sirnply refer to pub-\nlic aTld private keys (since both keys\nCHJ1 be used for encryption as \\'lell as\ndecryption) .\nLA message th':lt is to be encrypted is decomposed into blocks such t.hat each block can be treated\n'L.S an integer less tha.n L.\n\n..,.., ')\nt ..,,-,\n(;HAPTER 2*1\n\\Vhilewe introduced encryption in the context of authenticatioll, \\VC note that\nit is a fundaIIlental tool for enforcing seeurity.ADBNIS can use encrlJ1Jt'lon to\nprotect inforrnation in situations where the norrnal security rnechanisrns of the\nDBlVIS are not adequate. For exarnple, an intruder rnay steal tapes containing\nSOUIC data or tap a conunu.nieation line. By storing and transrnitting data in\nan encrypted forIn, the DBNlS ensures that such stolen data is not intelligible\nto the intruder.\n21.5.2\nCertifying Servers: The SSL Protocol\nSuppose \\ve associate a public key and a decryption key \"vith Alnazon. Any-\none, say, user Sa,rl1, can send Alnazon an order by encrypting the order using\nArnazon's public key. ()nly Arnazon can decrypt this secret order because the\ndecryption algorithrn requires Arnazon's private key, known only to Arnazon.\nThis hinges on 8arn's ability to reliably find out Arnazon's public key. A num-\nber of cornpanies serve as certification authorities, e.g., Verisign. Arnazon\ngenerates a public encryption key eA (and a private decryption key) and sends\nthe public key to Verisign. Verisign then issues a certificate to Arnazon that\ncontains the following inforrnation:\n(VeTi8ig'r'l\"\nArnazoin, htl;P8://w'U)w. arnazon. corn, eA )\nThe certificate is encrypted using Verisign's own (pTivate key, which is known\nto (i.e., stored in) Internet Explorer, Netscape Navigator, and other browsers.\nvVhen 8an1 carnes to the Anulzon site and wants to place an order, his browser,\nrunning the SSL protocol,2 asks the server for the Verisign certificate.\nThe\nbro\\vser then validates the certificate by decrypting it (using ·Verisign's public\nkey) and checking that the result is a certificate with the HaIne Verisign, and\nthat theURL it contains is that of the server it is talking to.\n(Note that an\natternpt to forge a certificate \\vill fail because certificates are encrypted using\nVerisign's private key,\nVilhieh is knc)\\vn only to Verisign.)\nNext, the brovvser\ngenerates\n(1, randc)ln session key, encrypt it using Arnazon's public key (\\vhieh\nit obtained frorn the validated certificate anel therefore trusts), and sends it to\nthe 1\\rn(lzon server.\nFrorn this point on, the Arnazon server and the browser can use th.c session\nkey (which both know and are confident tliat only they know) and a /3Y'fnrnetric\nencrypticHl protc)collike AES or IJES to exchangc~ securely encrypted rnessages:\nl\\Jlessages are encrypted by the sender anel decrypted by the receiver using the\nsa,HIe session key. rrhe encrypted Inessages travel over the Internet and rnay be\n._-_ _--_.._.-----\n:2 A browser uses the SSL protocol if the tclrget lJHl..i begins with https.\n\n8eC'lJ,1\"ity and .AnthoT'ization\n113\nintercepted, but they cannot be decrypted without the session key. It is useful\nto consider \\vhy \\ve need a session key; after all, the bro\\vser could sirnply have\nencrypted 8a1n's original request using Arnazon '8 public key and sent it securely\nto the Arnazon server. The reason is that, without the session key, the Arnazon\nserver has no \"'lay to securely send infonnation back to the bro\\vser. A further\nadvantage of session keys is that syrnrnetric encryption is cOlnputationally nluch\nfaster than public key encryption. The session key is discarded at the end of\nthe session.\nThus, 8aIn can be assured that only Alnazon can see the inforrnation he types\ninto the fonn shown to hirn by the AIuazon server and the inforrnation sent\nback to hiln in responses froIn the server.\nHowever, at this point, r\\rnazon\nhas no assurance that the user running the browser is actually Sanl, and not\nSOlneone who has stolen Sarn's credit card.\nl-'ypically, rnerchants accept this\nsituation, which also arises when a custoIner places an order over the phone.\nIf we want to be sure of the user's identity, this can be accoIuplished by addi-\ntionally requiring the user to login. In our exarnple, 8arn 11lUSt first establish\nan account with Alnazon and select a password. (Stun's identity is originally\nestablished by calling hiln back on the phone to verify the account inforrnation\nor by sending elnail to an elnail address; in the latter case, all we establish is\nthat the owner of the account is the individual with the given clnail address.)\nWhenever he visits the site and Anlazon needs to verify his identity, AInazon\nredirects hinl to a login fo1'1n after' using SSL to establish a session key. 'rhe\npaE;sword typed in is transrnitted securely by encrypting it with the session key.\n()ne rcrnaining drawback in this approach is that Arnazon now kno\\vs Sarn's\ncredit card nlunber, and he rnust trust Alnazon not to rnisuse it. The Secure\nElectronic Transaction protocol addresses this lirnitation. Every custolner\nrnust nnw obtain a certificate, with his or her own private and public keys,\nand every transaction involves the Alnazon server, the cust(nner's browser, and\nthe server of a trusted third party, such as Visa for credit card transactions.\nr:Che basic idea is that the bro\"vser encodes non-credit caTd inforrnation using\nAInazon's public key and the credit ca.rd infonnation using Visa's public key and\nsends these to the AJnazon\nservc:~r, which for\"vards the credit card inforrnation\n(which it cannot decrypt) to the Visa server. If theVisct server a,pproves the\ninforrnation, the transa,ction goes through.\n21.5.3\nDigital Signatures\nSuppose tllat ,Elnlcr, who works for ArnazoIl, a,nd Betsy, \\vho \\\\forks for IVlcCjnlw-\nlIill,need to COlllll1Unicate \\vith each other about inventory. Public key encryp-\ntion can be used t() create digital signatures for rnessages. rrhat is, rnessages\n\n714\nCHAPTER 2l\ncan be encoded in such a way that, if ElIneI' gets a Inessage supposedly fr(nTI\nBetsy, he can verify that it is fronl Betsy (in addition to being able to decrypt\nthe rnessage) and, further, prove that it is froIn Betsy at McGraw-lIill, even if\nthe ll1cssage is sent froIn a IIotrnail account when Betsy is traveling. Sirnilarly,\nBetsy can authenticate the originator of Inessages froln Ellner.\nIf Ellner encrypts Inessages for Betsy using her public key, and vice-versa,\nthey can exchange inforrnation securely but cannot authenticate the sender.\nSorueone who wishes to irnpersonate Betsy could use her public key to send a\nrnessage to EIrner, pretending to be Betsy.\nA clever use of the encryption sche111e, however, allows Ellner to verify whether\nthe rnessage was indeed sent by Betsy. Betsy encrypts the rnessage using her\nprivate key and then encrypts the result using Elrner's public key. When Ellner\nreceives such a 111essage, he first decrypts it using his private key and then\ndecrypts the result using Betsy's public key. rrhis step yields the original un-\nencrypted rnessage. FllrtherInore, Ehner can be certain that the rnessage was\ncomposed and encrypted by Betsy because a forger could not have known her\nprivate key, and without it the final result would have been nonsensical, rather\nthan a legible 111essage.\n1~\\lIther, because even\nE~lrner does not know Betsy's\nprivate key, Betsy cannot clairn that Ehner forged the ruessage.\nIf authenticating the sender is the objective and hiding the rnessage is not im-\nportant, we can reduce the cost of encryption by using a message signature.\nA signature is obtained by applying a one-way function (e.g., a hashing schelne)\nto the rnessagc and is considerably sInaHer. \\Ve encode the signature as in the\nbasic digital signature approach, and send the encoded signature together with\nthe full, unencoded 111cssage. rrhe recipient can verify the sender of the signa-\nture as just described, and validate the I11essage itself by applying the onc-\\vay\nfunction and cOlnparing the result with the signature.\n21.6\nADDITIONAL ISSUES REI.JATED TO SECURITY\nSecurity is a, l)road topic, and our coverage is necessarily lirnited. 'rhis section\nbriefly touches on sorne additional irnportant issues.\n21.6.1\nRole of the Database Administrator\nrrhe database (l,chninistrator (IJBA) plays an irnportant role in enfol'c:ing the\nsecurity-related aspects of i:1 dataJ>ase design. In conjunction with the o\\vners\nof the data, the I)BA aJso COlltributes to developing a security policy. The I)BA\nhas a\nsp(~cial i:1,ccount, \\vhich we call the systenl account~ (l,nd is responsible\n\nSeC1l7·o;zty and .A'ntho'rizaL·ion\nfor the overall security of the systeru. In particular ~ the DBA. deals with the\nfollo\\ving:\n1. Creating New Accounts:\nEach new user or group of users Blust be\nassigned an authorization ID and a paA'3s\\vord. Note that application pro-\ngraIns that access the database have the saIne authorization ID as the user\nexecuting the prograill.\n2. Mandatory Control Issues: If the DB~'fS supports rnandatory control·-···-\nS0111e custornized systeIns for applications \\vith very high security require-\nrnents (for exarllple, rnilitary data) provide such support~-the DBA lllUst\nassign security classes to each database object and a..'3sign security clear-\nances to each authorization ID in accordance with the chosen security pol-\nICY·\nThe DBA is also responsible for rnaintaining the audit trail, which is essen-\ntially the log of updates with the authorization ID (of the user executing the\ntransaction) added to each log entry.\nThis log is just a Ininor extension of\nthe log mechanislll used to recover from crashes. Additionally, the DBA lllay\nchoose to rnaintain a log of all actions, including reads, perfornled by a user.\nAnalyzing such histories of how the DBMS was accessed can help prevent se-\ncurity violations by identifying suspicious patterns before an intruder finally\nsucceeds in breaking in, or it can help track down an intruder after a violation\nh&\" been detected.\n21.6.2\nSecurity in Statistical Databases\nA statistical database contains specific inforrnation on individuals or events\nbut is intended to perlnit only statistical queries. For exarnple, if \\ve rnailltained\na statistical database of inforrna,tion about sailors, we would allow statistical\nqueries about average ratings, rnaxirnurn age, and so on, but not queries about\nindividua.l sailors. Security in such dataJxtses poses nevv probleurs because it is\npossible to infer protected inforrnation (such Cl\"S (1, sailor's rating) frorn ans\\vers\nto perrnitted statistical queries. Such inference opportunities represent covert\nchannels that can cornprornise the security policy of the database.\nSuppose that sailor\nSn(~aky Pete Vlants to kncrw the rating of .A.clrniral Ho1'n-\ntooter ~ the\nc~st~elned chairrnarl of the sailing clu1), and happens to kno\\v thclt\nIIorntooter is the olclest sailor in the (Jub. Pete repeat(~dly (18ks queries of thc~\nforln \"'IIow' InClny sailors aTe there whose age is greater than ..x,:?\" for vaxious\nva.1ues of ..\\'\", until the (UIS\\Ver is 1.\n(}bviously, this sa,iIor is lIorntooter,\nth(~\noldest sailor.\nNotethat each of these queries is\n(J, valicl statistical query and\nis\nperrnitt(~d.\nLet the value of ..x at this POillt be, say, fi5.\n~Pete no\\v asks the\n(lller:y, \"'vVhat is\ntll(·~ nraxirnurn rating of all sailors \\vhose age is greater than\n\n716\n(:HAPTER ·121\n65'1\"\nf\\gain~ this query is pennitted because it is a statistical query. However,\nthe ans\\ver to this query reveals J101'ntooter's rating to Pete, and the security\npolicy of the databa.se is violated.\nOne approach to preventing such violations is to require that each query rnust\ninvolve at 1e(1.'3t S01ne lnininuull nUluber, say,N, of l'()\\VS.\nvVith a rea.sonable\nchoice of N, Pete \\vould not be able to isolate the inforrnation about 1101'ntooter,\nbecause the query about the rna.xinUUll rating would fail.\nrrhis restriction,\nhc)\\vever, is easy to overCOIne. By repeatedly (1.'3king queries of the forIII, '\"How\nruany sailors are there vvhose age is greater than ..Y?\" until the systenl rejects\none such query, Pete identifies a set ofN sailors, including Florntooter. Let the\nvalue of ~Y at this point be 55. Novv, Pete can ask tvvo queries:\nIII\n\"vVhat is the SlUll of the ratings of all sailors whose age is greater than\n557\" Since N sailors have age greater than 55, this query is perrnitted.\n•\n\"What is the SUIll of the ratings of all sailors, other than l1orntooter, whose\nage is greater than 55, and sailor Pete?\" Since the set of sailors whose rat-\nings are added up now includes Pete instead of Horntooter, but is otherwise\nthe sallle, the rnunber of sailors involved is still N, and this query is also\npennitted.\n171'oln the answers to these two queries, say, Al and A 2 , Pete, who knows his\nrating, can easily calculate Horntooter's rating as Al ~ A2 + .Pete'8 rating.\nPete succeeded because he WcU3 able to ask two queries that involved Illany of\nthe sarne sailors.\n'The nurnber of rows exalnined in corllrnon by two queries\nis called their intersection.\nIf a liInit \\vere to be pla,ced on the alIlount of\nintersection perrnitted bet\\veen anyt-wo queries issued by the Si::une user, Pete\ncould be foiled. Actually, a truly fiendish (and patient) user can generally find\nout inforruation about specific individuals even if the systcrn phtces a, rniniruurn\nnUlnber of ro\\vs bound (N) and a rnaxirnurn intersection bound (1\\1) on queries,\nhut the n1.11n])er of qlleries required to do this gro\\vs in proportion to N fA!. \"Ve\ncan try to additionally lirnit the total nUlnbe1' of queries that a user is allowed\nto aslc but t\\VO users could still conspire to breach security.\nBy Il1aintaining\na log of all activity (including rectcl-on1y\nac(,~esses), such query patterns can be\ncletected, icleally before a security violation occurs. This discussion should rnake\nit\ncleaT~ however, that security in statistical databases is difficult to enforce.\n21.7\nDESIGN CASf: STUDY: THE INTERNET STORE\n\\Ve return to our case study and our friends at DBI)udes to consider security\nissues. 'There are three groups of users: custolners, ernl>loyees, a.nd the ovvner\nof the l>ook shop.\n(()f course, there is also the datal)ase adrninistrator ~ who\n\nSeC'lIT'ity aTidA'ldhorizaJion\n717\nluts universal access to all data and is responsible for regular operation of tlle\ndatabase systcrTl.)\n<:.,'\n,\nThe ()\\vner of the store has full privileges on all tables. Custorners can query the\nBooks table and place orders online, but they should not have access to other\ncustonH=~rs' records nor to other c11storne1'8' orders.DBDudes restricts access\nin t\\VO ·ways. First, it designs a silnple 'VVeb page with several fonus sirnilar to\nthe page shc}\\vn in Figure 7.1 in Chapter 7. rrhis allo\\vs custo1ners to subrnit\na s111all collection of valid requests without giving tho1n the ability to directly\naccess the underlying DB1JfS through an SQL interface. Second, I)B.Dudes uses\nthe security features of the .DB1\\·IS to li111it access to sensitive data.\nrIhe \\vebpage allows custorners to query the Books relation by ISBN nU111bcr,\nnarne of the author, and title of a boolc The webpage also has two buttons.\nThe first button retrieves a list of all of the custolIler's orders that are not\ncompletely fulfilled yet.\nl'he second button displays a list of all cornpleted\norders for that custorner.\nNote that custolllers cannot specify actual SQL\nqueries through the Web but only fill in SCHne pararneters in a forn1 to instantiate\nan autonlatically generated\nS(~L query.\nAll queries generated through fonll\ninput have a WHERE clause that includes the cid attribute value of the current\ncusto1ner, and evaluation of the queries generated by the two buttons requires\nknowledge of the custolller identification nUlnber. Since all users have to log\non to the website before browsing the catalog, the business logic (discussed\nin Section 7.7) lllust 1naintain state inforrnation about a custoDler (i.e., the\nCllstorner identification nUlnber) during the custorner's visit to the website.\n~rhe second step is to configurEl, the database to lirnit access according to each\nuser group's need to know. DBI)udes creates a special customer account that\nhas the following privileges:\nSELECT ON Books, New()rders, ()ldOrders, NewOrderlists, OldOrderlists\nINSERT ON New()rders, ()Ic:H)rders, New()rderlists, ()ld()rderlists\nErnployees should be able to acid ncvv books to the catalog, upda,te the quantity\nof (L book in stock, revise custorner orders if rlecessary, and update all custorner\ninforrnation e:ccept the credit card 'inforInation.. In fa,ct, ernployees should not\neven be able to see a custorner's credit card nurnber.\n1]lcreforc,DBDucles\ncreates the foUcnving vic\\v:\nCREATE VIEW C\\lstornerlnfo (cid,cnarnc,address)\nAS SELECT\n(~.cid, C.cnaJTlE\\ C.(l.cldress\nFROMCllstolners C\nI)BI)udes gives the employee account the follc)\\ving privileges:\n\n718\n(~HAP'TER\n~l\nSELECT ON Cllstolnerlnfo~\nBooks~\nNe\\vOrders, ()Id()rders~Ne\\v(}rclerlists~ Old()rderlists\nINSERT ON Cllstornerlnfo, Books,\nNc\\vOrders, 01dC)rders, Ne\\vOrderlists~ ()ldOrderlists\nUPDATE ON CustolnerInfo, Books,\nNew()rders, OldOrders, Nevv()rderlists, Old()rderlists\nDELETE ON Books, NewOrders, Old()rders~ NewOrderlists, ()ldOrderlists\nObserve that ernployees can rnodify Custornerlnfo and even insert tuples into\nit. This is possible because they have the necessary privileges, and further, the\nview is updatable and insertable-into. \\Vhile it seerns rea.sonable that elllployees\ncan update a custorner's address, it does sceln odd that they call insert a t1lple\ninto Cllstornerlnfo E~ven though they cannot see related infonna.tion about the\ncustorner (i.e., credit card nurnber) in the Cllstorners tahle.\nThe reason for\nthis is that the store wants to be able to take orders 1'1'0111 first-tirne custorners\nover the phone without asking for credit card inforrnation over the phone.\nErnployees can insert into Custornerlnfo, effectively creating a new Custoillers\nrecord without credit card inforluation, and custorners can subsequently provide\nthe credit card nurnber through a Web interface. (Obviously, the order is not\nshipped until they do this.)\nIn addition, there are security issues when the user first logs on to the website\nusing the cllstolner identification nUlnber.\nSending the nUlnber unencrypted\nover the Internet is a security hazard, and a secure protocol such as SSL should\nbe used.\nCornpanies such a.s CyberCash and DigiCash offer electronic conunerce pay-\nrIlcnt solutions, even inclu.ding electronic cash. Discussion of hoy\\! to incorporate\nsuch techniques into the website are outside the scope of this book.\n21.8\nREVIEW QUESTIONS\n..l\\.nS\\~lerS to the revic\\v questions can be founel in the listed sections.\n91!\n\\\\That\n(J..re tIle 1n,ain objectives in designing a secure datal)ase application?\nExplain the te1'rns ,seCT'CCY, 'integrity, availalrddy, and (lldhcn,tication. (Sec-\ntion 21.1)\nIS\nExplain\ntlH~ tenns 8ccur'ity policy ,'UHl 8cc'u'l'ily TrlcchaILisnl arid ho\\v tllCy\nare related. (Section 21.1)\nl1li\n\\\\That is\nth(~ Blain iele;,1. behind disc'rct'ion,aT,1j acces.' con!;T'Ol?\nvVhat is the\nidea behind Trl.,(1,'ruiaf,oT;ij access con,troT?\nvVhat are the\nrelativ(~ rnerits of\nthes(~ tv·/C) approaches? (Section 21.2)\n\nII\nDescribe the privileges recognized in S(~L? In particular, describe SELECT,\nINSERT, UPDATE, DELETE, and REFERENCES. For each privilege, indicate\n\\vho acquires it autornatical1y on a given table. (Section 2103)\nII\nI-Io\\v are the\nO\\VIH~rS of privileges identified? In particular, discuss atLtho-\nrizlrt;ion ID8 and '{\"ole8. (Section 21.3)\nI'l\n\\\\That is an authorizat'ion graph? Explain S(~L's GRANT and REVOKE\neOl11-\nIIHHlds in terrns of their effect on this graph. In particular, discuss \\vhat\nhappens when users pass on privileges that they receive frorn sorneone else.\n(Section 2103)\nII\nDiscuss the difference between having a privilege on a table and on a vie\\v\ndefined over the table.\nIn particular, how can a user have a privilege\n(say, SELECT) over a view 'without also having it on all underlying tables?\n\\'Tho 11111st have appropriate privileges on all underlying tables of the view?\n(Section 21.3.1)\n111\nvVhat are objects, subjects, 8eCl.Ir'ity classes, and cleaTances in rnandatory\naccess control? IJiscuss the Bell-LaPadula restrictions in tenns of these con-\ncepts. Specifically, define the sirnple seC1LTity pr'Operty and the *-pToperty.\n(Section 21.4)\nII\nWhat is a Trojan horse attack and how can it cOlnprornise discretionary\naccess control?\nExplain how lnandatory a,ccess control protects against\nTrojan horse attacks. (Section 21.4)\nJl1II\nWhat do the tenns rnult'ilevf;l table and polyinstantiation rnean? Explain\ntheir rela.tionship, a,nd ho\\,r they arise in the context of lnandatory access\ncontrol. (Section 21.4.1)\nIii\n\\'That are covert ChJlrrnels and how can they arise v\\Then both discretionary\nand luandatory access controls are in plcl,ce? (Section 2104;.2)\nII\nDiscuss the I)oD security levels for database systclns. (Section 21.4.2)\nIiIil\nExplain why (t sirnple pEtssword rnechanisrn is insufficient for authentica-\ntion of users \"\\vho <.lccess a databa..\"c renJotely, say, over the Internet. (Sec-\ntion 21.5)\nIllll\n\\Vhat is the differellC(~ between sy'tnrnctTic a,lld public-key\nen,(~r'ypti()n? C.;ive\nex:.unples of 1Nell-kncJWll encryption algoritluns of both ki.lldso v\\lhat is the\nrnain \\veakilcss of synunetric encryption and ho'Vv is this addressed in public-\nk(~:y encryption? (Section 21.5.1)\nII\n1)i8c118s the choice of encrYIltion and decryption keys in public-key en.cryp-\ntion cHId how they are llsed to c~ncrypt and decrypt dattL Explain the role\nof ()'nC-'luay !u.'ncti(Jr/,.'i. vVhat H.ssurance do\\ve h.ave that the H,SA\nSC;heln(~\n(:;:'1,nno1, be cornprornised? (Section 21.5.1)\n\n720\nC~HAPTER 21\nII\n\\:\\That are cert'ificat'ion attthorities and \\vhy are they nE~eded? Explain ho\\v\ncert'ificates are issued to sites and validated by H, bro\\vser using the SSL\nTJrotocol: discuss the role of the session key. (Section 21.5.2)\nII\nIf a user connects to a site using the SSL protocol, explain \\vhy there is still\na need to login the USCI'. Explain the use of SSL to protect pass\\vords and\nother sensitive infonnation being exchal1ged. vVhat is the secure electTolu:C\ntransaction pTOtoCO[? vVhat is the added value over SSL? (Section 21.5.2)\nII\nA d'i.qital /:rignat'UTe facilitates secure exchange of rnessages. Explain what\nit is and how it goes beyond sirnply encrypting rnessages. Discuss the use\nof rnessage signat'UTes to reduce the cost of encryption. (Section 21.5.3)\nII\n\\:Vhat is the role of the databa\"se achninistrator with respect to security?\n(Section 21.6.1)\nII\nDiscuss the additional security loopholes introduced in statistical databases.\n(Section 21.6.2)\nEXERCISES\nExercise 21.1 Briefly answer the following questions:\n1. Explain the intuition behind the two rules in the Bell-LaPadulamodel for rnandatory\naccess control.\n2. Give an exarnple of how covert channels can be used to defeat the Bell-LaPadula rnodel.\n;3. Give an exarnple of polyinstantiation.\n4. Describe a scenario in whichrnandatory access controls prevent a breach of security that\ncannot be prevented through discretionary controls.\n5. Describe a scen:'Lrio in which discretionary access controls are required to enforce a seCll-\nrity policy that cannot be tmforced using onlymancl<ttory controls.\n6. If a DBNIS already supports discretionary and Jnandatory access controls, is there a need\nfor encryption'?\n7. Explain the need for each of the following lirnits in a statistical database systern:\n(a) A maxirnurn on the munber of (pl(~ries a user ca.Jl\nPoS(~.\n(b) A rninirnUln on the munber of tuples involved in ans\\vering a query.\n(c) A maximurn on the intersection of t\\vo queries (Le.) on the number of tuples that\nboth queries exarnine).\n8. Explain the use of an audit trail, ''lith special reference to a statistical database system.\n9. \\,Vllat is the role of the DBA \\vith respect to security?\n10. Describe AES and its relationship to DES.\n11. \\?\\'hat is public-key encryption? How does it differ frorn the encryption approach taken\nin the Data Encryption Standard (DES)~ and in \\vhat ways is it better than DES?\n\nSeCllTity and \"\"4'uthorizat'ion\n721\n12. Explain hmv a company offering services on the Internet could use encryption-based\ntechniques to lllake its order-entry process secure. Discuss the role of DES, A.ES, SSL,\nSET, and digital signatures. Search the \\tVeb to find out IllOre about related techniques\nsuch as electronic cash.\nExercise 21.2 You are the DBA for the VeryFine Toy Cornpany and create a relation called\nEmployees with fields enam,e, dept, and\nSala1~1j. For authorization reasons, you also define\nviews EmployeeNallIes (with ena:rne as the only attribute) and DeptInfo with fields dept and\navgsalary. The latter lists the average salary for each departrnent.\n1. Show the view definition statements for EnlployeeNames and Deptlnfo.\n2. What privileges should be granted to a user who needs to know only average departn1ent\nsalaries for the Toy and CS departments?\n3. You want to authorize your secretary to fire people (you will probably tell hill1 whorn to\nfire, but you want to be able to delegate this task), to check on who is an elllployee, and\nto check on average department salaries. What privileges should you grant?\n4. Continuing with the preceding scenario, you do not want your secretary to be able to\nlook at the salaries of individuals. Does your answer to the previous question ensure this?\nBe specific: Can your secret~ry possibly find out salaries of some individuals (depending\non the actual set of tuples)\", or can your secretary always find out the salary of any\nindividual he wants to?\n5. You want to give your secretary the authority to allow other people to read the EUlploy-\neeNames view. Show the appropriate conI111and.\n6. Your secretary defines two new views using the EnIployeeNarnes view. The first is called\nAtoRNames and simply selects names that begin with a letter in the range A to R. The\nsecond is called HowManyNan1es and counts the number of narnes. You are so pleased\nwith this achievement that you decide to give your secretary the right to insert tuples into\nthe EnlployeeNan1es view. Show the appropriate cOllunand and describe 'what privileges\nyour secretary has after this cornrnand is executed.\n7. Your secretary allows Todd to read the Erl1ployeeNarnes relation and later quits. You\nthen revoke the secretary's privileges. \\Vhat happens to Todd's privileges?\n8. Give an exarnple of a view update on the preceding schelna that cannot be illlplernentecl\nthrough updates to Erl1ployees.\n9. You decide to go on an extended vacation, and to rnake sure that ernergencies can be\nhandled, you want to authorize your boss Joe to read and modify the Ernplo~yeesrelation\nand the ErIlployeeNalnes relation (and Joe lllust be able to (leleg(:lte authority, of course,\nsince he is too far up the managernent hierarchy to ttctually do any \\vork).\nShow the\nappropriate SQL staternents. Can Joe read the Deptlnfo view?\n10. After returning frorn your (wonderful) vacation, you see a note from Joe, indicating that\nhe authorized his secretary fvlike to rea,d the Ernployees relation.\nYou \\vant to revoke\nf\\like's SELECT privilege on Ernployees, but you do not \\vant to revoke the rights you\ngave to Joe, even teruporarily. Can you do this in SQL?\n11. Later you realize that Joe has been quite busy. He has defined a view called AllNarnes\nusing the view ErnployeeN(lIneS, defined another relation caJled StaffNarnes that he ha..s\naccess to (but you cannot access), and given his secretary f\\1ike the right to read from\nthe AllNanH:~s vicw.l\\like has passed this right on to his friend Susan. You d(~cide that,\neven at the cost of annoying .Joe 1)y revoking Borne of his privileges, you sirnply have\nto\ntakf~ away TvIike (\\,nd Susarl's rights to see your data.\\VhaJ, REVOKE staternent \\vould\nyou execute? 'VVhat rights does Joe have on Ernployees after this staternent is executed?\n\\Vha..t views are dropped as a consequence?\n\n22\nPARALLEL AND\nDISTRIBUTED DATABASES\n...\nWhat is the rnotivation for parallel and distributed DBMSs?\n...\nWhat are the alternative architectures for parallel database systellls?\n...\nHow are pipelining and data partitioning used to gain parallelism?\n...\nHow are dataflow concepts used to parallelize existingsequential code?\n...\nWhat are alternative architectures for distributed DBMSs?\n...\nHow is data distributed across sites?\n...\nHow can we evaluate and optimize queries over distributed data?\n...\nWhat are the nlerits of synchronous vs. asynchronous replication?\n...\nHow are transactions Inanaged in a distributed environment?\n..\nKey concepts:\nparallel DBNIS architectures; perfonnance, speed-\nup and scale-up; pipelined versus data-partitioned parallelism, block-\ning; partitioning strategies; dataflow operators; distributed DB11S\narchitectures; heterogeneous systernsj gateway protocols; data distri-\nbution, distributed catalogs; sernijoins, data shipping; synchronous\nversus asynchronous replication; distributed transactions, lock nlan-\nagcrnent, deadlock detection, two-phase ccnnlnit, Presurned Abort\nNo rnan IS an island,\nf~ntire of itself; every Tnan IS a pIece of the\ncontirlcnt, a part of the rnain.\n········...JohnDonne\n725\n\nC~HAPTER 2~\nIn this chapter \"ve look at the issues of p:u-allelislll arid data, distribution in a\nIJBlvIS. VVe begin by introducing parallel and distributed database systcrIls in\nSection 22.1. In Section 22.2~ we discuss aJternative hardwa,re configurations for\na parallel DBI\\lIS. In Section 22.:3,\\ve introduce the concept of data partitioning\nand consider its influence on parallel query evaluation. In Section 22.4, we sho\\v\nho\\v data partitioning can be used to parallelize several relational operations.\nIn Section 22.5, \\ve conclude our treatrnent of parallel query processing with a\ndiscussion of parallel query optirnization.\n'The rest of the chapter is devoted to distributed databases.\nvVe present an\novervievv of distributed databases in Section 22.6.\n\\Ve discuss sorne alterna-\ntive architectures for a distributed DBMS in Section 22.7 and describe options\nfor distributing data in Section 22.8.\nvVe describe distributed catalog rnan-\nagernent in Section 22.9, then in Section 22.10, we discuss query optirnization\nand evaluation for distributed databases. In Section 22.11, we discuss updating\ndistributed data, and fina.lly, in Sections 22.12 to 22.14 we describe distributed\ntransaction ruanagernent.\n22.1\nIN\"TRODUCTION\nvVe have thus far considered centralized database rnanageruent systerns in which\nall the data is luaintained at a single site and &ssluned that the processing of\nindividual transactions is essentially sequential.\nOne of the most irnportant\ntrends in data.bases is the increased use of parallel evaluation techniques and\ndata, distribution.\nA parallel database system seeks to irnprove perforruance through paral-\nlelization of various operations, such as loading data, building indexes, and\nevaluating queries.\nAlthough data, IIH1Y be stored in a distributed fa\"shion in\nsuch a systcrn, the distribution is governed. solely by perfon.nance considera-\ntions.\nIn a distributed database systenl, data, is physically stored across several\nsites, and ea,ch site is typically rnanaged by a 1)13lV18 capable of running il1-\ndependent of the 01:11e1' sites.\nrrhe location of data itenlS and the degree of\nautonorny of iJldividual sites have a significctllt irnpa,ct on aJl aspects of the\ns)'stern, including c111ery optirnization and processing, concurrency control, and\nrecovery. In contnLst to parallel datal)ases, the distribution of data is governed\nby factors such cL'3 locaJ ownership and iIlcreasccl a,vailability, in addition to\nperforlnance\nissu(~s.\n\\Vhile paraJlelisrrl is 1110tivated l)y\nperforlnculc~c~ consideratiolls,\nsev(,~ra] distinct\nissues rnotivate data, clistriblltioIl:\n\nPar-alZel and Dist'rilnl,ted IJatabascs\n727\n~\nlIB\nIncreased AvailabHity: If a site containing a relation goes dovvn, the\nrelation continues to be available if a copy is Inaintained at another site.\nM\nDistributed Access to Data: An organization Inay have branches in.\nseveral cities. Although analysts rnay need to access data corresponding to\ndifferent sites, 1NC usually find locality in the access pa,tterns (e.g., a bank\nlnanager is likely to look up the accounts of custorners at the local branch),\nand this locality can be exploited by distributing the data accordingly.\nII\nAnalysis of Distributed Data: Organizations \\vant to exa\"rnine all the\ndata available to thern, even when it is stored across rnultiple sites and\non lllultiple databc6'3c systerns. Support for such integrated access involves\nnlany issues; even enabling access to widely distributed data can be a\nchallenge.\n22.2\nARCHITEC\"rURES FOR PARALLEL DATABASES\nThe basic idea behind parallel databases is to carry out evaluation steps in par-\nallel whenever possible, and there are rnany such opportunities in a relational\nDBJ\\lIS; databases represent one of the lnost successful instances of parallel\ncornputing.\n~\nciJ\nSHARED NOTHING\nSHARED MEMOR'(\nSHARED DISK\nFigure 22.1\nPhysicaJ Architectures for Parallel Da.tabase Systems\nl~hree luain architectures have been proposed for building para.11el DBIVISs. In\na shared-IuerIlory SystCIll, Inultiple CPU·s are attached to an interconnection\nnet\\vork [Lnd c;an [l,ccess\n(1, cornrl1on region of rnain lnelilory. In a. shared-disk\ns:ysten.1, c(l,ch CPU has a private rnelnory and direct access to all disks through\nan interconnection nehvork. In a, shared-nothing systerl1: eac:h CPTJ haB local\nrnain lnelnory and disk space, but no two CP1Js can access the sarne storage\narea; all cOHununication betvveen CP1Js is tllrough a 11etwork connection.. rrhe\nthree architectures are illustrated in Figure 22.1.\n\n728\n(~HAPTER 22;\nIThe shared-rnernory architecture is closer to a conventional rnachine, and Illany\nconunercial database systerns have been ported to shared lnernory platfornlS\n\\vith relative ea\",\":ic.\nC~oInrnunication overhead is low ~ because lnain rncIIlory can\nbe used for this purpose, and operating systern services can be leveraged to\nutilize the additional CPl:Js. Although this approach is attractive for achieving\nrnoderate\nparanelisln·····~··afew tens of CPlJS can be exploited in this fashion~\nInernory contention bec01nes a bottleneck as the nurnber of CPUs increases.\nrfhe shared-disk architecture faces a sirnilar problcrn because large a1110nnts of\ndata are shipped through the interconnection network.\nThe basic problern with the shared-111Crrlory and shared-disk architectures is in-\nterference: As Inore CPUs are added, existing CPUs are slowed dovvn because\nof the increased contention for mClllory accesses and network bandwidth. It has\nbeen noted that even an average 1 percent slowdown per additional CPU 1nea11S\nthat the rnaxirnum speed-up is a factor of 37, and adding additional CPlJs ac-\ntually slows down the systern; a systenl with 1000 CPUs is only 4 percent as\neffective as a single- CPU systern! This observation has rllotivated the develop-\nrnent of the shared-nothing architecture, which is now widely considered to be\nthe best architecture for large parallel database systems.\nrrhe shared-nothing architecture requires rnore extensive n~organization of the\nDBNIS code, but it has been shown to provide linear speed-up, in that the\ntilne taken for operations decreases in proportion to the increase in the nUlnber\nof CPlJs and disks, and linear scale-up, in that perforrnance is sustained if\nthe nurnber of CPUs and disks are increased in proportion to the arnount of\ndata. Consequently, ever-rnore-powerful parallel database systcrns can be built\nby taking advantage of rapidly irnproving perforrllance for single-CPU systelns\nand connecting as rnany CPUs as desired.\nSpeed-up and scale-up are illustrated in Figure 22.2. 'The speed-up curves show\nhow, for a fixed database size, Inore transactions can be executed l)cr second\nby adding CPUs. 1118 scale-up curves shovv hovv adding Inorc resources (in the\nforln of CPlJs) enables us to process larger problerns. rrhe first scale-up graph\nIncasures the nurnber of transactions executed per second as the clatabase size is\niucTec'lsed and th(~ nurnber of CPlJs is correspondingly inCr(~(lsed. Arl alternative\n\\ivay to Ineasure scale-up is to consider the tirne Utken per transaction (kS r1101'e\nCPTJs aTe added to process an increasing nurnber of transactions per second;\nthe goal here is to sustain the response tirne per transaction.\n22.3\nPARAI~l~EL QUERY EVALUATION'\nIn this\ns(~ction, vve (liscusspH,rallel evaluation of a relational query in a I).lHvlS\n\\\\'ith\n(1, slutred-nothing architccture.\\Vhile it is I)Ossibl(\\, to consicler para11el\n\n730\nCHAPTER 22\n22.3.1\nData Partitioning\nPartitioning a large data..'Sct horizontally across several disks enables us to ex-\nploit the I/O banchvidth of the disks by reading and writing theln in parallel.\nrrhere are several \\vays to horizontally partition a relation.vVe can assign tuples\nto processors in a round-robin fashion, \\ve can use hashing, or we can a..'Ssign\ntuples to processors by ranges of field values. If there are n processors, the 'ith\ntuple is assigned to processor 'i rnodn in round-robin partitioning. Recall\nthat round-robin partitioning is used in RAID storage systelTIS (see Section 9.2).\nIn hash partitioning, a hash function is applied to (selected fields of) a tuple\nto deternline its processor.\nIn range partitioning, tuples are sorted (con-\nceptually), and n ranges are chosen for the sort key values so that each range\ncontains roughly the SalTIe nurnber of tuples; tuples in range i are assigned to\nprocessor i.\nRound-robin partitioning is suitable for efficiently evaluating queries that ac-\ncess the entire relation. If only a subset of the tuples (e.g., those that satisfy\nthe selection condition age = 20) is required, hash partitioning and range par-\ntitioning are better than round-robin partitioning because they enable us to\naccess only those disks that contain rnatching tuples.\n(Of course, this state-\nment assumes that the tuples are partitioned on the attributes in the selection\ncondition; if age = 20 is specified, the tuples must be partitioned on age.) If\nrange selections such as 15 < age < 25 are specified, range partitioning is su-\nperic)!' to ha.'3h partitioning because qualifying tuples are likely to be clustered\ntogether on a few processors. On the other hand, range partitioning can lead\nto data skew; that is, partitions with widely varying numbers of tuples across\npartitions or disks.\nSkew causes processors dealing with large partitions to\nbecorne perfonnance bottlenecks. Hash partitioning has the additional virtue\nthat it keeps data evenly distributed even if the data grows and shrinks over\ntirne.\nTo reduce ske\\v in range partitioning, the luain question is how to choose the\nranges by which tuples are distributed. ()ne effective approach is to take sarn-\npIes fronl each processor, collect and sort all sarnples, and divide the sorted set\nof samples into equally sized subsets. If tuples are to be partitioned on age,\nthe age ranges of the sarnpled subsets of tuples can be used as the ba,,5is for\nredistributing the entire relation.\n22.3.2\nParallelizing Sequential Operator Evaluation Code\nAn elegant soflvvare architectnre for parallel I)BlVISs enables us to readily par-\nallelize existing code for sequentially evaluating a relational oI>crator.\n1\"he\nbasic idea is to use parallel da.ta strearrlS.\nStrea,rIls (frorn different disks or\n\n]JaTallel arul DistTilntted Databases\n731\nthe output of other operators) are Inerged\ncl..S needed to provide the inputs\nfor a relational operator, and the output of an operator is split a.5 needed to\nparallelize subsequent processing.\nA. parallel evaluation plan consists of a dataflow network of relational, luerge,\nand split operators.\n1'he rnerge and split operators should be able to buffer\nSOlne data and should be able to halt the operators producing their input data.\n1\n1hey can then regulate the speed of the execution according to the execution\nspeed of the operator that conSUlues their output.\nAs we will see, obtaining good parallel versions of algorithllls for sequential\noperator evaluation requires careful consideration; there is no luagic formula\nfor taking sequential code and producing a parallel version. Good use of split\nand 111erge in a dataflow software architecture, however, can greatly reduce the\neffort of implementing parallel query evaluation algorithms, as we illustrate in\nSection 22.4.3.\n22.4\nPARALLELIZING INDIVIDUAL OPERATIONS\nThis section shows how various operations can be implemented in parallel in\na shared-nothing architecture.\nWe assurne that each relation is horizontally\npartitioned across several disks, although this partitioning mayor may not be\nappropriate for a given query. The evaluation of a query must take the initial\npartitioning criteria into account and repartition if necessary.\n22.4.1\nBulk Loading and Scanning\nvVe begin with two siluple operations: scanning a relation and loading a relation.\nPages can be read in parallel while scanning a relation, and the retrieved tuples\ncan then be lnerged, if the relation is partitioned across several disks.\nMore\ngenerally, the idea also c:lpplies when retrieving all tuples that Incet a selection\ncondition.\nIf ha'Shing or range partitioning is used, selection queries can be\nanswered by going to just those processors that contain relevant tuples.\nA sirnilar observation holds for bulk loading.\nFurther, if a relation ha..\" asso-\nciated indexes, any sorting of data entries required for building the indexes\nduring bulk loc:;tding can also be done in parallel (see later).\n\n732\n(;HAPTER 2;2\n22.4.2\nSorting\nA sirnple idea is to let each CPTJ sort the part of the relation that is on its local\ndisk and then rnerge these sorted sets of tuples. r:rhe degree of parallelisHl is\nlikely to be lirnited by the rnerging phase.\nA better idea is to first redistribute all tuples in the relation using range par-\ntitioning. For exarnple, if we want to sort a collection of ernploy-ee tuples by\nsalary~ salary values range froIH 10 to 210, and we have 20 processors, we could\nsend all tuples with salary values in the range 10 to 20 to the first processor,\nall in the range 21 to 30 to the second processor, and so on. (Prior to the redis-\ntribution, while tuples are distributed across the processors, \\ve cannot assurne\nthat they are distributed according to sa1ary ranges.)\nEach processor then sorts the tuples &'3signed to it, using sorne sequential sorting\nalgorithrn. For exaluple, a processor can collect tuples until its lllemory is full,\nthen sort these tuples and write out a run, until all incolning tuples have been\nwritten to such sorted runs on the local disk. rrhese runs can then be rnerged\nto create the sorted version of the set of tuples assigned to this processor. The\nentire sorted relation can be retrieved by visiting the processors in an order\ncorresponding to the ranges assigned to thenl and sirnply scanning the tuples.\nThe basic challenge in parallel sorting is to do the range partitioning so that\neach processor receives roughly the Si::Ulle runnber of tuples; otherwise, a proces-\nsor that receives a disproportionately large nurnber of tuples to sort becornes a\nbottleneck and lirnits the scalability of the parallel sort. ()ne good approach to\nrange partitioning is to obtain a sarnple of the entire relation by taking sarnples\nat each processor that initially contains part of the relation.\nThe (relatively\nsrnall) saruple is sorted and used to identify ranges with equal nUlllbers of tu-\nples. This set of range values, called a splitting vector, is then distributed to\nall processors and used to range partition the entire relation.\nA particularly irnportant application of parallel sorting is sorting the data en-\ntries in tree-structured indexes. Sorting data entries can significantly speed up\nthe process of bulk-loading an index.\n22.4.3\nJoins\nIn this section, \\ve consider ho\\v the join operation can be parallelized.\\Ve\npresent the basic idea behind the parallelization and illustrate the use of the\nrnerge and split operators described in Section 22.:3.2.\nvVe focus on parallel\nhash join, \\vhich is \\videly useel, and \"briefly\noutlin(~ how sort-rnerge join ca,n\n\njJ(Jrallel ftnd [J\"stT\"ibuted Databases\n733\n10\nbe sirnilarly parallelized.\n()ther join algorithniS can be parallelized\n(lA'S \\veIl,\nalthough not as effectively'\" as thes(~t\\vo algoritlnns.\nSuppose that 'we \\vant to join two relations. say, 1-1 and 13, on the age attribute.\nvVe aSSUIllC that they are initially distribl.lted across several disks in senne \\vay\nthat is not useful for the join operation; that is, the initial partitioning is not\nbased on the join a.ttribute. '1'he l)ctsic idea. for joining A and B in parallel is\nto decornpose the join into a collection of k srnnller joins. vVe can decornpose\nthe join by partitioning both /1 and B into a collection of k logical buckets\nor partitions. By using the sarne partitioning function for both j! and B, \\ve\nensure that the union of the k s1naller joins cOlnputes the join of A and B; this\nidea is si1nilar to intuition behind the pa..rtitioning phase of a sequential hash\njoin, described in Section 14.4.3.\nBecause A and B are initially distributed\nacross several processors, the pa,rtitioning step itself can be done in parallel at\nthese processors. At each processor, all local tuples are retrieved and hashed\ninto one of k partitions, with the salIie hash function used at all sites, of course.\nAlternatively, we can partition A and B by dividing the range of the join at-\ntribute age into k disjoint subranges and placing .A and B tuples into partitions\naccording to the subrange to which their age values belong. For exanlple, sup-\nPOs(~ that \\ve have 10 processors, the join attribute is age, with values froln 0 to\nIOO. Assurlling uniforrl1 distribution, A and B tuples with 0 < age < 10 go to\nprocessor 1, 10 < age < 20 go to processor 2, and so on. This approach is likely\nto be 1110re susceptible than hash partitioning to data skew (i.e., the number\nof tuples to be joined can vary widely across partitions), unless the subranges\nare carefully deterrnined; we do not discuss how good subrange boundaries can\nbe identified.\nI-Iaving decided on a partitioning strategy, we can assign each partition to a\nprocessor and carry out a local join, using any join algorithrl1 we want: at\neach processor. In this case, the nUIIlber of partitions k: is chosen to be equal\nto the nUlnber of processors n\navailabl(~ for carrying out the join~ and during\np;:utitioning, each processor sends tuples in the ith partition to processor ,l.\nAfter partitioning, each processor joins the A andB tuples assigned to it.\nEach join process executes\ns(~quential join code a.Jld recfdves input .4. and 13\ntuples froro several processors; a rnerge operator lnerges all incorning A tuples,\nand another rnerge operator rnerges all incorning 13 tuples. Depending on 110\\V\n\\vc\\vant tC) distribute the result of the join of ./1 and [3, the output of the join\nprocess rl1ay be split into several data strealIlS. rI'he net\\vork of operators for\nparallel join is sho\\vn in Figur(~ 22.:3. To sirnplify the figure, \\ve assurlle that the\nproc.essors doing the join are distinct frorn the processors that. initially contain\ntuples of A and [3 and sho\\v only four processors.\n\n734\ni\nC~HAprrER 42\nFigure 22.3\nDataflow Network of Operators for Parallel Join\nIf range partitioning is used, this algorithrn leads to a parallel version of a sort-\nmerge join, with the advantage that the output is available in sorted order. If\nhash partitioning is used, we obtain a parallel version of a hash join.\nImproved Parallel Hash Join\nA hash-based refinernent of the approach offers iJuproved perforruanee.\nrrhe\nruain observation is that, if A and B are very large and the nurnber of partitions\nk is chosen to be equal to the nurnber of processors n, the size of each partition\n111ay still be large, leading to a high cost for each local join at the n processors.\nAn alternative is to execute the srnaller joins Ai !Xl Hi, for i\n=\n1 ... k, one\nafter the other, but\\vith each join executed in parallel using all processors.\nThis approa,ch allows us to utilize the total available ruain rueruory at all 'n\nprocessors in eEl.ch join Ai !Xl 13i and is described in rnore detail as follcJ\\vs:\n1. At each site, apply a hash function hI to partition the A and B tuples\nat this site into partitions i = 1 ... k. Let A be the srnaller rela,tion. 1'he\nnurnber of paJtitions k is chosen such that each partition of ..4 fits into the\naggregate or cornbined rnernory of all n processors.\n2. For 'i\n=\n1 ... k, process the join of the ith partitions of A and B.\nTo\ncornpute .A.'i [Xl 13i , do the follcnving at every site:\n(a.) i\\pply a second hash function 12,2 to all Ai tuples .to detennine \\vhere\nthey should be joined and send tuple f; to site h2(t).\n(b) As A.;. tuples El.rrive to be joined, add thcln to an in-rnernory hash. tabh~.\n\n7B5\n(c) After\nall~4i tuples have been distributed, apply h2 to Hi tuples to\ndeterrnine \\\\There they should be joined and send tuple t to site h2(t).\n(d)\n..As B'i tuples HJTive to be joined, probe the in-rnernory table of ..4i\ntuples and output result tuples.\nThe lIse of the second hash function h2 ensures that tuples (L1'e (rIlore or less)\nuniforrnly distributed across all n processors participating in the join.\nThis\napproach greatly reduces the cost for each of the srnaller joins and therefore\nreduces the overall join cost.\n()bserve that all available processors are fully\nutilized, even though the srnaller joins are carried out one after the other.\nThe reader is invited to adapt the nehvork of operators shown in Figure 22.3\nto reflect the iInproved parallel join algorithrn.\n22.5\nPARALLEL QUERY OPTIMIZATION\nIn addition to pa.rallelizing individual operations, we can obviously execute dif-\nferent operations in a query in parallel and execute rnultiple queries in parallel.\nOptirnizing a single query for parallel execution has received rnore attention;\nsysterus typically optirnize queries without regard to other queries that might\nbe executing at the scuue tilne.\nrrwo kinds of interoperatioll parallelisrn can be exploited within a query:\nII\nThe result of one operator can be\npip(·~lined into another.\nFor exalnple,\nconsider a left-deep plan in which all the joins use index nested loops. The\nresult of the first (Le., the bottollunost) join is the outer relation tuples\nfor the next join node. As tuples are produced by the first join, they can\nbe used to probe the inner relation in the second join. T'he result of the\nsecond join can sirnilarly be pipelined into the next join, and so 011.\nI!III\nwiultiple independent operations ca,rl be executed concurrently. For exarn-\npIe, consider a (bush~y) plan in vilhich relations A and I3 are joined, relations\n(7 and D are joined, and the results of these two joins are finally joined.\n(;learly, the join of Jl and 13 can be executed conculTcntly with the join of\nC: anci D.\nA.n optirnizer that seeks to parallelize query evaluation ha.s to consider several\nissues, and we\nOl11~y outline the rnain points. The cost of executing individual\noperations in paraJlel (e.g., pcLrallel sorting) obviollsly differs frorn executing\nthern\ns(~quentjally, and tIle optirnizer should estirnate operation costs accord-\ningly.\n\n736\n(;HAPTER;22\nNext, the plan that returns answers quickest Inay not be the plan \\vith the\nlea,,'3t cost. For ex,unple, the cost of A.\n[)<J B plus the cost of C\nrxJ D plus the\ncost of joining their results rnay be rnore than the cost of the cheapest left-deep\nplan.\nI-Iowever, the titne taken is the titne for the 1nore expensive of .4 t><l B\nand ()\n[)<J 1) plus the titne to join their results.\nTlhis tirHe lnay be less than\nthe tirne taken by the cheapest left-deep plan. This observation suggests that\na parallelizing optirnizer should not restrict itself to left-deep trees and should\nalso consider bVJshy trees, which significantly enlarge the space of plans to be\nconsidered.\nFinally, a nurnber of pararneters, such as available buffer space and the nUl1.1-\nbel' of free processors, are known only at run-tirne. rrhis comnlent holds in a\nrnultiuser environrnent even if only sequential plans are considered; a rnultiuser\nenvironrnent is a sirnple instance of interquery parallelisrn.\n22.6\nINTRODUCTION TO DISTRIBUTED DATABASES\nAs we observed earlier, data in a distributed database systern is stored across\nseveral sites, and each site is typically rnanaged by a DBMS that can run inde-\npendent of the other sites. The classical view of a distributed database systern\nis that the systcrn should rnake the irnpact of data distribution transparent.\nIn particular, the following properties are considered desirable:\nIII\nDistributed Data Independence: lJsers should be able to ask queries\nwithout specifying where the referenced relations, or copies or fragrnents\nof the relations, are located. This principle is a natural extension of phys-\nical and logical data independence; vve discuss it in Section 22.8. Further,\nqueries that span rnultiple sites should be optirnized systcrnatically in a\ncos1,-based rnanner, taking into account COllllnunication costs and differ-\nences in local cornpntation costs. vVe discuss distributed query optirniza-\nLion in Section 22.10.\nII\nDistributed 'Iransaction Atolnicity:\nlJsers should be able to\n1,\\r1'ite\ntransactions that access and update data at several sites just as they vvould\n1,\\rrite transEl,ctions over purely local data.\nIn pa,rticular, the effects of\n(1.\n1:rans(1.cti011 across sites should continue to be atornic; that is, all changes\npersist if the tranS<lction cOllnuits and none persist if it aborts. \\Ve discuss\nthis distributed transaction processing in Sections 22.11,\n22.1:~, and 22.14.\nAJthough rnost people would agree that these properties are in general clesir-\na,blc~, in certain situ::ttions, such as when sites are\nconlH~cted by ct 810-w long-\ndistance network, these properti(~s are not efficiently achievable. Indef\\'d, it has\n1\n:\",Y'\n····0·\n··\\·i t·l··'·1··\n. r1\"\n,\\'\n,·,·t·:\"S·'\n':.1 )\"1)1··11r \"'1··,t\"·l\nt·)·l\n1\"1' :,)\":.) .. ,. ,:..·t···,s·\" ....\\\nt\n)CC11 d.l buce\n,Vl.\"\nV\\. lcn\n~I ,C,\neU C' g\n<. )d, .. J\n<. IS .1 1)11.C<',\ni lcse pIOpCl ,IC\"\nell C no ,\neven\ndcsiral)l(~. l]le argurnerlt essentiaJly is that the adrninistrative overheacl\n\n[)arnllel aTLdDistribtlted Database,s\n7~37\nt\nof supporting a systern vvith distributed data independence and transaction\natornicitY'....·..···in effect 1 coordinating all activities across all sites to support the\nview of the whole as a unified collection of data--is prohibitive, over and above\nI)B:NlS perfo1'rnanc8 considerations.\n}(eep these rerna\"rks about distributed databases in rnind a.'3 ,ve cover the topic\nin rno1'e detail in the rest of this chapter. There is no real consensus on \\vhat the\ndesign objectives of distributed databases should be, and the field is evolving\nin response to users\n1 needs.\n22.6.1\nTypes of Distributed Databases\nIf data is distributed but all servers run the sarne DBMS software,. we have a\nhomogeneous distributed database system. If different sites run under\nthe control of different DB:NISs, essentially autonorllously, and are connected\nsOlllehow to enable access to data from rnultiple sites, we have a heteroge-\nneous distributed database system, also referred to as a multidatabase\nsystem.\nThe key to building heterogeneous systelTIS is to have well-accepted standards\nfor gateway protocols. A gateway protocol is an API that exposes DBl\\1S\nfunctionality to external applications. Examples include ODBC and JDBC (see\nSection 6.2). By accessing database servers through gateway protocols, their\ndifferences (in capability, data fonnat, etc.)\nare rnasked, and the differences\nbetween the different servers in a distributed system are bridged to a large\ndegree.\nC;ateways are not a panacea, however. They add a layer of processing that can\nbe expensive, and they do not cornpletely rllcL.\"k the differences arllong servers.\nFor eXfunplc, a server Illay not be capable of providing the services required for\ndistributed transaction rnanagernent (see Sections 22.13 and 22.14), and even\nif it is capable, standardizing gateway protocols all the \\vay down to this level\nof interaction poses challenges that have not yet been resolved satisfactorily.\nDistributed data rnanagcrnent, in the final analysis, cornes at\n(1 significant cost\nin terulS of perforrna\"ncc, software cOlllplexity, and adrninistration difficulty.\nrrhis observation is especially true of heterogeneous SystCIllS.\n22.7\nDISTRIBUTED DBMS ARCHITECTURES\nThree alternative approctches are uSf~d to separat,e functionality across different\nDBIV'IS-related processes; tllese alternative distributed ])131VI8 axchitectures ar(~\nca,lled ()Zient-S'eTvcT, C/ollaborat'ing SeTver, and\n]\\;JiddZe'wo/t'(~.\n\n738\nCHAPTER t2\n22.7.1\nClient-Server Systems\nA Client-Server systelIl has one or rno1'e client processes and one or rnore\nserver processes, and a client process can send a query to anyone server process.\nClients are responsible for user-interface issues, and servers rnanage data and\nexecute transactions. Thus, a client process could run on a personal cornputer\nand send queries to a server running on a 11lainframe.\nThis architecture has becorne very popular for several reasons. First, it is rel-\natively sinlple to irnplernent due to its clean separation of functionality and\nbecause the server is centralized.\nSecond, expensive server rnachines are not\nunderutilized by dealing with lllundane user-interactions, which are now rel-\negated to inexpensive client machines. Third, users can run a graphical user\ninterface that they are familiar with, rather than the (possibly unfalniliar and\nunfriendly) user interface on the server.\nWhile writing Client-Server applications, it is inlportant to remember the\nboundary between the client and the server and keep the communication be-\ntween therll as set-oriented as possible.\nIn particular, opening a cursor and\nfetching tuples one at a time generates many rnessages and should be avoided.\n(Even if we fetch several tuples and cache them at the client, rnessages 11lUSt\nbe exchanged when the cursor is advanced to ensure that the current row is\nlocked.)\nTechniques to exploit client-side caching to reduce comlnunication\noverhead have been studied extensively, although we do not discuss them fur-\nther.\n22.7.2\nCollaborating Server Systems\nThe (;lient-Server architecture does not allow a single query to span rnultiple\nservers because the client process would have to be capable of breaking such\na query into appropriate subqueries to be executed at different sites and then\npiecing together the answers to the subqueries. The client process vvould there-\nfore be quite cOlnplex, and its capabilities would begin to overlap with the\nserver; distinguishing between clients and servers becornes harder. EliIninating\nthis distinction leads us to an alternative to the Client-Server architecture: a\nCollaborating Server systenl. \\Ve can lUl,ve a collection of datab::lse servers,\neach capable of running tra,nsactions against local data, which cooperatively\nexecute transactions spanning rnultiple servers.\n'\\Vhen a server receives a, query that requires access to data at other servers, it\ngenerates appropriate subqu(~ries to be executed by oth(~r servers and puts the\nresults together to COIllpute HJlSvVers to the original query. Ideally, the decorn-\n\nParallel and Dist'rib1lted IJatabases\n739\nposition of the query should be done using cost-based optinlization, taking into\naccount the cost of network COlnnlunication as \\vell a.s local processing costs.\n22.7.3\nMiddleware Systems\nThe Middle\\\\Tare architecture is designed to allow a single query to span rnul-\ntiple servers, without requiring all database servers to be capable of rnanaging\nsuch nlulti-site execution strategies. It is especially attractive when trying to\nintegrate several legacy systerns, whose basic capabilities cannot be extended.\nThe idea is that we need just one database server capable of rnanaging queries\nand transactions spanning nlultiple servers; the renlaining servers need to han-\ndle only local queries and transactions. We can think of this special server as\na layer of software that coordinates the execution of queries and transactions\nacross one or more independent database servers; such software is often called\nmiddleware.\nThe middleware layer is capable of executing joins and other\nrelational operations on data obtained froln the other servers but, typically,\ndoes not itself maintain any data.\n22.8\nSTORING DATA IN A DISTRIBUTED DBMS\nIn a distributed DBMS, relations are stored across several sites. Accessing a\nrelation stored at a renlote site incurs message-passing costs and, to reduce\nthis overhead, a single relation lnay be partitioned or fragrnented across several\nsites, with fragrnents stored at the sites where they are most often accessed or\nreplicated at each site where the relation is in high demand.\n22.8.1\nFragmentation\nFragrnentation consists of breaking a relation into srnaller relations or frag-\nrnents and storing the fragrnents (instead of the relation itself), possibly at\ndifferent sites.\nIn horizontal fragmentation, each fragrnent consists of a,\nsubset of TOWS of the original relation. In vertical fragluentation, each fra.g-\nrllent consists of a subset of col'Urnns of the original relation.\nIIorizontal and\nverticaJ fragrnents are illustrated in Figllre 22.4.\nTypically, the tuples that belong to a given horizontal fragrnent are identified\nby a selection query; for exarnple, crnployee tuples Blight be organized into\nfragrnents\nb~y city, \\vith all enlployees in\n(1, given city assigned to the sanie frag-\nrnent. rThe horizontal fragrnent shown in Figure 22.4 corresponds to Chicago.\n\" , storing fragrncnts in the data,l)E\\..se site at the corresponding city, \\ve a,chieve\n.:ality of referencc',Chicago data is 1n08t likely to be updated (tnel queried\n\n740\n(;HAPTEll 42\nr -\n!\n~-\n---I\nTID\neid\nDame\ncity\ni\nage\nsal\nI\nI\nI\ntl\n53666\nJones\nMadras\n!\n18\nI\n35\nI\nt2\n53688\nSnlith\nChicago\n18\n32\nI\n~.... ~.......................\nt3\n53650\nSluith\nChicago\n19\n48\nI\n.\n,.\nt4\n53831\nMadayan\nBombay\n11\n.I\n20\nI\nJ\n......... --\nr-,·_····_-\nI\nt5\n53832\nGuldu\nBOlnbay\n12\n! I J\n20\n..,................_-\n\"'.._..\n/\nf\nVertkal Fragment\nHorizontal Fragment\nFigure 22.4\nHorizontal and Vertical Fragmentation\nfronl Chicago, and storing this data in Chicago rnakes it local (and reduces\ncornrnunication costs) for nlost queries.\nSinlilarly, the tuples in a given ver-\ntical fragrnent are identified by a projection query.\nThe vertical fragrnent in\nthe figure results frorn projection on the first two columns of the ernployees\nrelation.\n\\;Vhen a relation is fragrnented, we lllust be able to recover the original relation\nfronl the fragrnents:\n•\nHorizontal Fragmentation: The union of the horizontal fragments rnust\nbe equal to the original relation. Fragrnents are usually also required to be\ndisjoint.\nIII\nVertical Fragrnentation: 'The collection of vertical fragrnents should be\na lossless-join deccnnposition,\n~lS per the definition in Chapter 19.\n1'0 ensure that\n(1, vertical fragrnentation is lossless-join, systeuls often assign a\nunique tuple ieI to each tuple in the original relation~ as shown in Figure 22.4,\nand attach this id to the projection of the tuple in each fragrnent. If we think of\nthe original relation a.s containing an addit.iC)11al tuple-id field that is a. key, this\nfield is added to each vertical fragrnent. Such\n~L decol11position is guaranteed to\nbe lossless-join.\nIn generaJ~ a relation can be (horizontally or vertically) fragrnented, a.nd each\nr(~sulting fragrnent can be further fragnlented. For sirnplicity of exposition, in\nthe rest of this chapter, ,ve\n(LSSllnH~ that fragrnents are not recursively parti-\ntioned in this rnanner.\n\nParallel and IJistT'ilndcd Databases\n22.8.2\nReplication\n741\nReplication Incaus that \"we store several copies of a relation or relation frag-\nrnent. An entire relation can be repliccltecl at one or rnore sites. Sirnila,rly, one\nor 1110re fragrncnts of a relation can be replicated at other sites. For exarnple, if\na relationR is fragrnented into 1?1,R2, and R:3, there nlight be just one copy\nof Ill, vvhereas R2 is replicated at two other sites and\nli,~>' is replica.,t:ed at all\nsites.\nThe rnotivation for replication is twofold:\n•\nIncreased Availability of Data: If a site that contains a replica goes\ndown, we can find the sarne data at other sites. Sirnilarly, if local copies of\nrerllote relations are available, we are less vulnerable to failure of COllnnu-\nnication links.\n•\nFaster Query Evaluation: Queries can execute faster by using a local\ncopy of a relation instead of going to a rernote site.\nThe two kinds of replication, called synchronous and asynchronous replication,\ndiffer prirnarily in how replicas are kept current when the relation is rnodified\n(see Section 22.11).\n22.9\nDISTRIBUTED CATALOG MANAGEMENT\nKeeping track of data distributed across several sites can get cornplicated. \\tVe\nrnust keep track of how relations are fragrnented and replicated------that is; how\nrelation fragrnents are distributed across several sites and ,vhere copies of frag-\nrnents are st()red~\"-----in addition to the lIsuaJ seherna, authorization, and statisti-\ncal inforrnation.\n22.9.1\nNaming Objects\nIf a relation is fragruented and replicated, we rnust be able to uniquely identif\\r\neach replica of ea,ch fragnlent.\nCjenerating such unique narnes requires sorne\ncare.\nIf \\ve use a global narne-server to a.ssign globally unique narnes, local\na,utonornJl is cornprornised; we 'want (users at) each site to be able to <h'3sign\nnarnes to local objects \\vithout reference to nanH:~S systernwide.\nThe usual solution. to the naTning problenl is to use narnes consisting of several\nfields.\n1;'01' excunplc, we could hELve:\n\n742\nCHAPTER ~2\nIII\nA local TULTnC field, 'which is the HaIIle assigned locally at the site\\vhere the\nrelation is crea,ted. T\\vo objects at different sites could have the saIne local\nnarne, but t\\yO objects at a given site cannot have the saIne local narne.\nIII\nA biTth s'lt:e field, vvhich identifies the site where the relation\n\\v~1..s crectted,\nand where il1fofrnation is ruaintained about all fragruents and replicas of\nthe rela.tion.\nThese two fields identify a relation uniquely; we call the cornbination a global\nrelation nanle. To identify a replica (of a relation or a relation fragnlent) ,we\ntake the global relation narne and add a 'tcplica-'id field; we call the cornbination\na global replica narrle.\n22.9.2\nCatalog Structure\nA centralized systern catalog can be used but is vulnerable to failure of the site\ncontaining the catalog. An alternative is to rnaintain a copy of a global system\ncatalog,which describes all the data at every site.\nAlthough this approach\nis not vulnerable to a single-site failure, it comprornises site autonorny, just\nlike the first solution, because every change to a local catalog rnust now be\nbroadcast to all sites.\nA better approach, which preserves local autonoruy and is not vulnerable to a\nsingle-site failure, was developed in the It* distributed database project, which\nwa..'S a successor to the Systerl1 R, project at IBlV!. Each site ruaintains a local\ncatalog that describes all copies of data stored at that site. In addition, the\ncatalog at the birth site for a relation is responsible for keeping track of where\nreplicas of the relation (in general, of fragnlents of the relation) are stored. In\nparticular, a precise description of each replica's contents\"\"\"\"''''a list of colurl1ns\nfor a vertical fragrnent or a selection condition for a horizontal fragruentis\nstored in the birth site catalog. \\Vhenever a new replica is created or a replica\nis rnoved across sites, the inforrnation in the birth site catalog for the relation\nHUlst be updated.\nTo locate a relation, the catalog; at its birth site lnust be looked up.\nrrhis\ne.atalog inforrnation can be ca.,ched at other sites for quicker access, but the\ncached inforrnation Inay becolue out of date if, for cxarnple, a fragrnent is\nrnoved.\n\\rYe vvould discover that the locally cached inforrnation is out of date\nwhen \\ve use it to access the relation, and at that point, we rll11st update the\ncache b.y looking up th(~ catalog at the birth site of the relation. (The birth site\nof a relation is recorded in each local cache that describes the relation, and the\nbirth site never changes, even if the relation is rnoved.)\n\nParallel and Dis!;r'ibll!;ed Database8\n22.9.3\nDistributed Data Independence\n743\nDistributed data independence lueans thc.tt users should be able to \\vrite queries\n\\vithout regard to ho\\v a relation is fragrnented or replicated; it is the respon-\nsibility of the DB:NIS to cornpute the relation\na~s needed (by locating suitable\ncopies of fragrnents, joining the vertical fragrnents, and taking the union of\nhorizontal fragn1cnts).\nIn particular, this property irnplies that users should not have to specify the\nfull nalne for the data objects accessed while evaluating a query. Let us see ho\\v\nusers can be enabled to access relations without considering how the relations\nare distributed. The local narne of a relation in the systeln catalog (Section\n22.9.1) is really a c01l1bination of a 'User narne and a user-defined relation narne.\n'Users can give whatever names they wish to their relations, without regard to\nthe relations created by other users. When a user writes a prograrn or SQL\nstatelnent that refers to a relation, he or she sirnply uses the relation narne.\nThe DBMS adds the user narne to the relation narne to get a local narne, then\nadds the user's site-id as the (default) birth site to obtain a global relation\nnarne.\nBy looking up the global relation narne-- -in the local catalog if it is\ncached there or in the catalog at the birth site-·the I)Bl\\;IS can locate replicas\nof the relation.\nA user Il1ay want to create objects at several sites or refer to relations created\nby other users. To do this, a user can create a, synonym for a global relation\nnarne' llsing an SQL-style cOl1unand (although such a corl1rnand is not currently\npart of the SQL:1999 standard) and subsequently refer to the relation using\nthe synonyrn. For each user known at a site, the DBl\\lS rna.intains a table of\nsynonynls as part of the systern catalog at that site and uses this table to find\nthe global relation narne. Note that a user's prograrl1 runs unchanged even if\nreplicas of the relation are rlloved, because\nth(~ global relation narne IS never\nchanged until the relation itself is destroyed.\nlJsers rnay \\vant to run queries agctinst specific replicas, especially if asyn-\nchronous replication is used.\nTo support this, the synonyrn Inechanisrn can\nbe adapted to also allo\\v users to create synon.yrl1S for global replica,\n11aJ.IH~S.\n22.10\nDISTRIBUTED QUERY PROCESSING\n\\Ve first discuss the issues involved in evaluating relational algebra operations\nin a distrilnlted datal)ase through exalnples and then outline distributed query\noptiInization. Consider the following t\\VO\nn·~lations:\n\n744\n(;lIAPTEH 22\nSailors(s'id: integer, snanu?: string, rating: integer, age: real)\nReserves(sid:\nint~~ger 1\nl)'i(~.::\n~ntege:~, day: ~~te,rnalne: string)\nr\\S in C:hapter 14, assurne that each tuple of R.eserves is 40 bytes long, tha.t a\nIH'lge can hold 100 Reserves tuples, and that ¥lC have 1000 pages of such tuples.\nSirnilarly, assurne that each tuple of Sailors is 50 bytes long, that a page can\nhold 80 Sailors tuples, and that \\ve hrrve 500 pages of such tuples.\n1'0 estiInate the cost of an evaluation strategy, in addition to counting the\nnurnber of page IjC)s, \\ve Blust count the nurnher of pages sent frorH one site\nto another because corllrIlunication costs are a significant cornponent of overall\ncost in a distributed database. \\Ve rnust also change our cost rnodel to count\nthe cost of shipping the result tuples to the site where the query is posed frOIn\nthe site where the result is assernbled!\nIn this chapter, we denote the tilne\ntaken to read one page from disk (or to write one page to disk) as 1;el and the\ntiIne taken to ship one page (from any site to another site) as 1;s.\n22.10.1\nNonjoin Queries in a Distributed DBMS\nEven sirnple operations such as scanning a relation, selection, and projection\nare affected by fragrnenta.tion and replication. Consider the following query:\nSELECT S.age\nFROM\nSailors S\nWHERE\nS.rating > ;3 AND S.rating < 7\nSuppose that the Sailors relation is horizontally fragruented, with all tuples\nhaving a rating less tha,n 5 at Shanghai and all tuples having a rating greater\nthan 5 at rrokyo.\nTheDBl\\/IS nn.lst answer this query by evaluating it at both sites and taking\nthe union of the ans\\vers.\nIf the SELECT clause contained AVG (S. age), COln-\nbining the aJ1S\\VerS could not be done by sirnply taking the union···-····the DBlV1S\nrnust cornpute the SUIn and count of age values at the two sites and use this\ninfonna,tion to cornpute the average age of all sailors.\nIf the WHERE clause contained just the condition 5'. rating > 6, on the other\nha,ud, the I)BI\\/IS should recognize thEtt this query could be ansvvered by just\n.\n.. t r[\" k\nexecutIng It a\nj ,.n yo.\nAs ::ulother exarnple, suppose that the Sailors relation, were vertically frag-\n11lented, \\vith the 8'id and rai'ing fields at Sha.nghai a,nd the\n8naUlC and age\nfields at rrokyo.\nNo field is stored at both sites. 1]lis vertical fragrnentcttion\n\nParnllel and DistTilnded [)atllbases\n.... ,4.-\n( &l:~)\nwould therefore be a lossy decornposition, except that a field containing the\nid of the corresponding Sailors tuple is included by the DB~fS in both frag-\nlnents! N'o\\v, the I)BrvIS has to reconstruct the Sailors relation by joining the\nt\\VO fraglnents on the COHllllon tuple-id field and execute the query over this\nreconstructed relation.\nFinally, suppose that the entire Sailors relation \\vere stored at both Shanghai\nand\n~rokyo.\n\\lVe could answer any of the previous queries by executing it at\neither Sha,nghai or Tokyo. vVhere should the query be executed? This depends\non the cost of shipping the answer to the query site (which rnay be Shanghai,\nTokyo, or SOllIe other site) as well as the cost of executing the query at Shanghai\nand at Tokyo...._·..··the local processing costs lnay differ depending on vvhat indexes\nare available on Sailors at the two sites, for exaluple.\n22.10.2\nJoins in a Distributed DBMS\nJoins of relations at different sites can be very (~xpensive, and we now consider\nthe evaluation options that IIlUst be considered in a distributed environrnent.\nSuppose that the Sailors relation were stored at London, and the Ileserves\nrelation were stored at Paris.\nWe consider the cost of various strategies for\ncOlnputing Sailor'S [X] Reserves.\nFetch As Needed\nWe could do a page-oriented nested loops join in Loudon with Sailors as the\nouter, and for each Sailors page, fetch all Reserves JH:tges frorn Paris.\nIf we\ncache the fetched Ileserves pages in London until the join is\ncornpk~te, pages\nare fetched only once, but aSSU111e that H,eservcs pages are not cached, just to\nsee how bad things can get.\n(The situation can get rnuch worse if vve use a\ntuple-oriented nested loops join!)\nrrhe cost is 500id to scan Sailors plus, for each Sailors page, the cost of seallning\nand shipping all of H,eserves, vvhich is 1000(td + ts). 1'he total cost is therefore\n500td + 500,OOO(td + is).\nIn additicHl, if the query was not sllbrnittccl at the London site, \\rye rnust add\nthe cost of shipping the result to the query site; this cost depends on\nth(~ size\nof the result. Because sid is a key for Sailors, the nurnber of tuples in the result\nis 100,000 (the rnunber ()f tuples in\nIlcserv(~s) and each tuple is 40 + 50 = 90\nbytes long; thus 4000/DO = 44 result tuples fit on a\npage~ and the result size\nis\n100,OOO/44=227~~ pages. rrhe cost of shipping the answer to another site, if\nnecessary, is 227:3 t,,,. In tIle rest of this section, we assurne that the query is\n\n746\nCHAPTER 22\nposed at the site where the result is computed; if not, the cost of shipping the\nresult to the query site Blust be added to the cost.\nIn this exarnple, observe that, if the query site is not London or Paris, the\ncost of shipping the result is greater than the cost of shipping both Sailors\nand Ileserves to the query site!\nTherefore, it would be cheaper to ship both\nrelations to the query site and COlllpute the join there.\nAlternatively, we could do an index nested loops join in London, fetching all\nInatching Reserves tuples for each Sailors tuple. Suppose we have an uncIus-\ntered hash index on the sid colurnn of Ileserves.\nBecause there aTe 100,000\nIleserves tuples and 40,000 Sailors tuples, each sailor has on average 2.5 reser-\nvations.\nThe cost of finding the 2.5 Ileservations tuples that lllatch a given\nSailors tuple is (1.2 + 2.5)td' assluning 1.2 l/Os to locate the appropriate\nbucket in the index.\nThe total cost is the cost of scanning Sailors plus the\ncost of finding and fetching nlatching Reserves tuples for each Sailors tuple,\n500td + 40, 000(3.7td + 2.5ts )'\nBoth algorithIns fetch required Reserves tuples from a remote site as needed.\nClearly, this is not a good idea; the cost of shipping tuples dominates the total\ncost even for a fast network.\nShip to One Site\n\\Ve can ship Sailors from London to Paris and carry out the join there, ship\nReserves to London and carry out the join there, or ship both to the site \\\\There\nthe query was posed and cornpute the join there.\nNote again that the query\ncould have been posed in London, Paris, or perhaps a third site, say, Tirnbuktu!\n1\n1he cost of scanning and shipping Sailors, saving it at Paris, then doing the\njoin at Paris is 500(2td + ts ) + 4500td, assurning that the version of the sort-\nrnerge join described in Section Itt.l0 is used and we have an adequate nurnber\nof buffer pages. In the rest of this section we aSSUInc that sort-Inerge join is\nthe join rnethod used when both relations are at the saIne site.\nThe cost of shipping Ileserves <lJld doing the join at London is 1000(2t(1 -+- t,c;) +-\n4500td·\nSenlijoins andBloomjoins\nConsider the strategy of shipping Ileserves to Londo.l1 and cornputing the join\nat London.\nSOln(~ tuples in (the current inst;-l,llCe of)H,cserves do not join ¥lith\nan.y tuple in (the current instaJ1Ce of) Sailors.\nIf Vie could sorneho\\v identify\n\nParallel and DistT'ib'tded Databases\n747\nReserves tuples that are guaranteed not to join\n~Nith any Sailors tuples, we\ncould avoid shipping thern.\nT,,·vo techniques, Se1n'ijo'ln and\nBloo1r~ioin, have been proposed for reducing\nthe number of lleserves tuples to be shipped.\nThe first technique is called\nSemijoin. The idea is to proceed in three steps:\n1. At London, cornpute the projection of Sailors onto the join colurnns (in\nthis case just the sid field) and ship this projection to Paris.\n2. At Paris, cornpute the natural join of the projection received frorn the\nfirst site with the R,eserves relation. l'he result of this join is called the\nreduction of R,eserves with respect to Sailors.\nClearly, only those Re-\nserves tuples in the reduction will join with tuples in the Sailors relation.\nTherefore, ship the reduction of Reserves to London, rather than the entire\nReserves relation.\n3. At London, cornpute the join of the reduction of R,eserves with Sailors.\nLet us compute the cost of using this technique for our example join query.\nSuppose we have a straightforward irnplernentation of projection based on first\nscanning Sailors and creating a telnporary relation with tuples that have only\nan sid field, then sorting the temporary and scanning the sorted ternporary to\neliminate duplicates.\nIf we assurne that the size of the sid field is 10 bytes,\nthe cost of projection is 500\"td for scanning Sailors, plus 100td for creating\nthe ternporary, plus 400ld for sorting it (in two passes), plus IOOtc! for the final\nscan, plus 100ld for writing the result into another tcrnporary relation; a total of\n1200f;d. (Because sid is a key, no duplicates need be elirninated; if the optiInizer\nis good enough to recognize this, the cost of projection is just (500 + 100)td.)\nThe cost of cornputing the projection and shipping it to Paris is therefore\n1200ld + 100l,';. The cost of c(nnputing the reduction of R.eserv8s is\n~~ . (100 .+\n10(0) ==\n~330(}t(j, assurning that sort-rnerge join is used.\ncrhe cost does not\nreflect that the projection of Sailors is already sorted; the cost \\vould deerca.,se\nslightly if the refined sort-Inerge join exploited this.)\n\\Vhat is the size of the reduction? If every sailor holds at lC<L5t one reservation,\nthe reduction includes every tuple of R,eserves! The effort invested in shipping\nthe projection and reducing lleserves is a total waste. Indeed, because of this\nobserva.ticHl, \\iVC note that Sernijoin is especially useful in conjunction \\vith a\nselectiol1 on one of the relations.F()r exarnplc, if W(~ \\vant to cornpute the join\nof Sailors tuples with a ral'ing greater than 8 \\vith the lleserves rela.tion, the\nsize of the projection on 8'id fen' tuples that satisfy the selection \\vould be just\n20 percent of tIle original projection, that is, 20 pages.\n\n748\nC~HAprrEIl\n~2\nLet us nuw continue the exarnple join, 'with the assurnption that \\ve have the\nadditional selection on 'r'(rt'ing. (The cost of cornputing the projection of Sailors\ngoes de)\\vn a bit, the cost of shipping it goes do\\\\rn to 20ts , and the cost of the\nreduction of F{,eser\\fPS also goes de)\\vn a little, but \\ve ignore these reductions for\nsirnplicity.) \\Ve assurne that; only 20 percent of the R,eserves tuples are included\nin the reduction, thanks to the selection.\nlIencc 1 the reduction contains 200\npages, and the cost of shipping it is 200ts .\nFinally, at London, the reduction of I{eserves is joined with Sailors, at a cost\nof ~3· (200 + 500) = 21100td. Observe that there are over 6500 page I/Os versus\nabout 200 pages shipped, using this join technique. In contrast, to ship R,eserves\nto London and do the join there costs IOOOis plus 4500td. \\Vith a high-speed.\nnetwork, the cost of Sernijoin Illay be n10re than the cost of shipping Reserves\nin its entirety, even though the shipping cost, itself is rnueh less (200t s versus\nIOOOts )·\nThe second technique, called Bloomjoin, is quite sirnilar. The luain difference\nis that a bit-vector is shipped in the first step, instead of the projection of\nSailors. A bit-vector of (SOIlIC chosen) size k is cOlnputed by hashing each tuple\nof Sailors into the range 0 to k - I and setting bit i to I if seHne tuple hashes to\ni, and 0 otherwise. In the second step, the reduction of Reserves is cOlnputed\nby hashing each tuple of lleserves (using the sid field) into the range 0 to k --1,\nusing the sanle hash function used to construct the bit-vector and discarding\ntuples whose hash value i corresponds to a 0 bit.\nBecause no Sailors tuples\nhash to such an i, no Sailors tuple can join with any R,eserves tuple that is not\nin the reduction.\n111e costs of shipping a bit-vector and reducing R,eserves using the vector are\nless than the corresponding costs in Sernijoin. ()n the other hand, the size of\nthe reduction of Iteserves is likely to be larger than in Sernijoin; so, the costs\nof shipping the reduction and joining it 'with Sailors are likely to be higher.\nLet us estirnate the cost of this approach.\nrrhe cost of cornputing the bit-\nvector is essentially the cost of scanning Sailors, \\vhich is 500td.\nrrhe cost of\nsending the bit-vector depends on the size \\ve choose for the bit-vector, 'which\nis certainly sInaJler than the size of the projection; vve take this cost to be 201: 8 ,\nfor concreteness.\nThe cost of reducing Reserves is just the cost of scanning\nH,eserves, lOOOl'd. T'he size of the reduction of R<eserves is likely to be about\nthe saIne\nEh'3 or a little larger than the size of the reduction in tIle Scrnijoin\nclpproach; instea,d of 200, \\ve will take tllis size to be 220 pages. (\\Ve aSSUIne\nthat the selection on Sailors is included, to pennit a direct cOInparison \\vith the\ncost of Scrnijoin.) 'I'he cost of shipping the reduction is therefore 220ts ' 1'he\ncost of the final join at London is :3 . (500 + 220) == 2160td.\n\nJ)arallel and IJiBtTib'lded [)tdabases\n749\nrrhus, in cornparison to Sernijoin! the shipping cost of this approach is about\nthe SaIl1t\\ although it could be higher if the bit-vector vvere not as selective\n(1.'; the projection of Sailors in tenns of reducing Reserves. 'Typically, though,\nthe reduction of lleserves is no l110re than 10 to 20 percent larger than the\nsize of the reduction in SClnijoin. In exchange for this slightly higher shipping\ncost, Bloornjoin achieves a significantly lower processing cost: less than :370Cltd\nversus rnore than 6500td for SClnijoin.\nIndeed, Bloornjoin has a lo\\ver I/C)\ncost and a lower shipping cost than the strategy of shipping all of R,eserves to\nLondon! These nurnbers inclicatewhy Bloollljoin is an attractive distributed\njoin rnethod; but the sensitivity of the rnethod to the effectiveness of bit-vector\nhashing (in reducing Reserves) should be kept in rnind.\n22.10.3\nCost-Based Query Optimization\n\\Ve have seen how data distribution can affect the inlplernentation of individual\noperations, such as selection, projection, aggregation, and join. In general, of\ncourse, a query involves several operations, and optirnizing queries in a dis-\ntributed database poses the following additional challenges:\n•\nCornrIlunication costs lllUSt be considered. If we have several copies of a\nrelation, we HlllSt also decide which copy to use.\n•\nIf individual sites are run under the control of difl'erent DBl\\JlSs, the au-\ntonolny of each site HUlst be respected while doing global query planning.\n(~uery optiInization proceeds essentially as in a centralized DBMS, as described\nin Chapter 12, with inforrnation about relations at rernote sites obtained fron1\nthe systeln catalogs. ()f course, there are nlore alternative lllethods to consider\nfor each operation (e.g., consider the new options for distributed joins), and\nthe cost rnetric ruust account for cornrnunication costs as \\vell, but the overall\nplanning process is essentially unchanged if we take the cost rnetric to be the\ntotal cost of all operations. (If \\ve consider response tilne, the fact that certain\nsubqueries can be carried out in pclrallel at different sites \\vould require us to\nchange the optirnizer ,1,8 per the discussion in Section 22.5.)\nIn the overall plan, local rnanipulatiol1 of relations at the site where they are\nstored (to corllpute an interrnediate relation to be shipped elsewhere) is encap-\nsulated into a :sugge8ted local plarl. The overall plan includes several such local\nplans, \\vhichwe can think of as subqueries executing at different sites. \\:Vhile\ngenerating the global plan, the suggested local plans provide recilistic cost es-\ntirnates for the cornputatioll of the interrnediate rehltions; the suggested local\nplans are constructed by the optirnizer rnainly to provide these local cost esti-\nIuates. A site is free to ignore the local plan suggested to it if it is able to find\na cheaper plan by llsing InorE~ current infonnation in the local catalogs. l'hus,\n\n750\nC~HAPT'ER 22\nsite autoncllny is respected in the optirnizaJion and evaluation of distributed\nquenes.\n22.11\nUPDATING DISTRIBUTED DATA\nThe classical view of a distributed DB:tvIS is that it should behave just like a\ncentralized DBNIS froul the point of vievv of a user; issues arising froln distribu-\ntion of data should be transparent to the user, although, of course, they rnu8t\nbe addressed at the irnplernentation level.\nvVith respect to queries, this vievv of a distributed DBIVIS Ineans that users\nshould be able to ask queries \\vithout worrying about how and \"\\vhere relations\nare stored; we have already seen the irnplications of this requirernent on query\nevaluation.\nvVith respect to updates, this view rneans that transactions should continue\nto be atornic actions, regardless of data fragrnentation and replication.\nIn\nparticular, all copies of a rnodified relation must be updated before the rnodi-\nfying transaction cornn1its. We refer to replication with this sernantics as syn-\nchronous replication; before an update transaction cOHllnits, it synchronizes\nall copies of rnodified data.\nAn altE:~rnativeapproach to replication, called asynchronous replication, has\ncorne to be widely usee1 in eornrnercial distributed DBlVISs. Copies of a rnodified\nrelation are updated only periodically in this approach, and a transaction that\nreads different copies of the sarne relation rnay see different values.\nT'hus,\nasynchronous replication cornprolnises distributed data independence, but it\ncan be iInplernented 1110re efficiently than synchronous replication.\n22.11.1\nSynchronous Replication\nThere are two 1)3sic techniques for ensuring that transactions see the senne vaJue\nregardless of\\vhich copy of an object they access. In the first technique, called\nvoting, a tnulsaction Inust \\vrite H\" lnajority of copies to rnodify a,ll ol)ject <:lJld\nread at lea\"st enough copies to rnake sure that one of the copies is current. For\nexanlple, if there <-:tre 10 copies and 7 copies are \\vritten by update transactions,\nthen at least /.1 copies rnust be read.\nEac:h copy has a version nurnber, and\nthe copy \\vith the highest version rllunber is current. This technique is not at-\ntra,ctive in rnost situations because reading aJl ol)ject reqllires reading rnultiple\ncopies; in rnost applications, objects are read rnuch 1n01'e frequently than\nthc~y\nare updated, and f'~fficientperfonnanceon recvls is very irnportant.\n\nI)aT'fLllel and IJistT'ib'ILted !Jai:abascs\n751\nIn the second technique, called read-any write-all, to read an object, a traJ1S-\naction can read anyone copy, but to \\vTite an object, it Inust \\vrite all copies.\nR,eads are fast, especially if we have a local copy, but 'writes are slo\\ver, relative\nto the first technique. 1\"'his technique is attractive vvhen reads are rnuch rnore\nfrequent than vvrites, and it is usually adopted for irnplernenting synchronous\nreplication.\n22.11.2\nAsynchronous Replication\nSynchronous replication COines at a significant cost. Before an update transac-\ntion can corn1'nit, it rnust obtain exclusive locks on all copies···c1.SSUlning that the\nread-any write-all technique is used· ···of rnodified data. The transaction Inay\nhave to send lock requests to rernote sites and \\vait for the locks to be granted,\nand during this potentially long period, it continues to hold all its other locks.\nIf sites or connnunication links fail, the transaction cannot cOInrnit until all the\nsites at which it has rnodified data recover and are reachable. Finally, even if\nlocks are obtained readily and there are no failures, connnitting a transaction\nrequires several additional rnessages to be sent as part of a cOTnrn'it protocol\n(Section 22.14.1).\nFor these reasons, synchronous replication is undesirable or even unachievable\nin 111any situations.\nAsynchronous replication is gaining in popularity, even\nthough it allows different copies of the saIne object to have different values for\nshort periods of tinlC. This situation violates the principle of distributed data\nindependence; users 11Ulst be aware of which copy they are accessing, recognize\nthat copies are brought up-to-date only periodically, and live with this reduced\nlevel of data consistency. Nonetheless, this seeIns to be a practical C0l11pr0l11ise\nthat is acceptable in rnany situations.\nPrimary Site versus Peer-to-Peer Replication\nA.synchronous replication C01nes in t\\VO flavors. In primary site asynchronous\nreplication, one copy of a relation is designated the primary or luaster COP)T.\nH.eplicas of the entire relation or fragrnents of the relation C(lJl be created at\nother sites; these a,re secondary copies, and unlike tIle priInary copy, they can-\nnot be updated. .l\\.. conUIlon InecllallislIl for setting up prhnary and secondary!\ncopies is that Osers first register or publish the relation at\nth(-~ priruaxy site\naXlcl subs(~quently subscribe to a fragrnent of a registered relation fron1 another\n(secondary) site.\nIn peer-to-peer as)rnchronous replication~ 111()re than one copy (although per-\nhaps rIot (11) can be designated as updatable, that is, a 1'naste1' copy. In addition\nto propagating changes, a conflict resolution\nstrat(~gy Inusi'; be used to deal\n\n752\nC~HAP'TER\n~2\nwith conflicting CIH:ulges Inade at different sites. For exarnplc, .Joe's age rnay\nbe changed to\n~i5 at one site and to 38 at another. \\Vhich value is ·correct'?\n:NIany luore subtle kinds of conflicts can arise in peer-to-peer replication, and in\ngeneral peer-to-peer replication leads to ad hoc conflict resolution. Senne spe-\ncial situations in \\vhich peer-to-peer replication does not lead to conflicts arise\nquite ()ften~ and in such situations peer-to-peer replication is best utilized. For\nexalnple:\n•\nEach 1naster is allo\\ved to update only a fragrnent (typically a horizontal\nfrag1nent) of the relation, and any two fragrnents updatable by different\n!llasters are disjoint. For excunple, it rIlay be that salaries of Gerrnan erll-\nployees are updated only in Frankfurt, and salaries of Indian ernployees are\nupdated only in 1\\1adras, even though the entire relation is stored at both\nFrankfurt and Madras.\n•\nUpdating rights are held by only one rnaster at a tillIe. For example, one\nsite is designated a backup to another site.\nChanges at the Iuaster site\nare propagated to other sites and updates are not allowed at other sites\n(including the backup). But, if the Iuaster site fails, the backup site takes\nover and updatt~s are now perrnitted at (only) the backup site.\n\\Ve will not discuss peer-to-peer replication further.\nImplementing Primary Site Asynchronous Replication\nThe Inain issue in irnpler11enting prilnary site replication is deterrnining how\nchanges to the prirnary copy are propagated to the secondary copies. Changes\nare usually propagated in two steps, called CalJtl1:re and Apply. Changes rnade\nby cOHnnitted transactions to the prirnary copy are s()Jnehow identified during\nthe Capture step and subsequently propagated to secondary copies during the\nApply step.\nIn contrast to synchronous replication~ a transacti.on that rnodifies a replicated\nrelation directly locks and changes only the prirnary copy. It is typically C0111-\nrnitted long before the Apply step is carried out. Systcrnsvary considerably\nin their ilnplernentation of these steps. \\Ve present an overvic\\v of senne of the\nalternatives.\nCapture\nrrlle Capture step is ilnplerIlented using one of two approaches. In log-based\nCaptU1'(,\\ the log luainta,inecl for recovery purposes is used to generate a record\nof updates.\nB(lsicall~Yj \\vhen the log tail is written to stable storage, all log\n\nrecords that affect replicated relations c1re also -written to a separate change\ndata table (eDT). Since the transaction that generated the update log record\nluay still he active when the record is\\vritten to the CDrT, it may subsequently\nabort.\nlJpdate log records written by transactions that subsequently abort\n1l1USt l)e rcrnoved fror11 the eDT to obtain a strearll of updates due (only) to\nconlln,itted transactions. This streanl can be obtained as part of the Capture\nstep or subsequently in the Apply step if conunit log records are added to\nthe eDT; for concreteness, vve aSSUlne that the cornruitted update strealll is\nobtained\na.~ part of the Capture step and that the CDT sent to the Apply step\ncontains only update log records of corl1ruitted transactions.\nIn procedural Capture, a procedure autornatically invoked by the DBlVlS or\nan application progra,lIl initiates the Capture process, which consists typically\nof taking a snapshot of the prirnary copy.\nA snapshot is just a copy of the\nrelation as it existed at sorne instant in tirne. (A procedure that is autoluatically\ninvoked by the DENIS, such as the one that initiates Capture, is called a trigger.\nvVe covered triggers in Chapter 5.)\nLog-based Capture has a s111aller overhead than procedural Capture and, be-\ncause it is driven by changes to the data, results in a slua11er delay between the\ntirne the prirnary copy is changed and the ti111e that the change is propagated\nto the secondary copies. (Of course, this delay also depends on ho\\v the Apply\nstep is implelnented.) In particular, only changes are propagated, and related\nchanges (e.g., updates to two tables with a referential integrity constraint be-\ntween thern) are propagated together. The disadvantage is that ilnpleluenting\nlog-based Capture requires a detailed understanding of the structure of the log,\nwhich is quite systern specific.\nTherefore, a· vendor cannot easily iInplernent\na log-based Capture rnechanisrn that ,viII capture clulnges rnade to data. in\nanother vendor's DB1\\1S.\nApply\nfI'he Apply\nstep takes the changes collected by the Capture step, vvhich are\nin the CDT table or a snapshot, and propagates 1,h81n to the secondary copies.\nThis c:an be done b:y having the prirnary site continuously send the CDT or\nperiodically requesting (the latest portion of) the crrr or a, snctpshot frorH\nthe prirnary site.\nTypically, each secondary site runs a copy of the J\\pply\nprocess and 'pulls' the changes in the eDT fronl the prirnary site using periodic\nrequests. The interval\nl)(~t\\veen such requests can be controlled by a tilner or\na user ~s appliccl,tion prograrl1. ()nce the changes are avail(1)le at the secondary\nsite, they can be applied directly to the replica.\n\n754\nCHAPTER 12\nIn sorne systerns, the replica Heed not be just a frag1Ilent of the original relation~\nit can be a view defined using SQL, and the replication rnechanisrn is sufficiently\nsophisticated to 11laintain such a view at a reillote site incrementally (by reeval-\nuating only the part of the vie\\v affected by changes recorded in the CI)T).\nLog-ba..f.3ed Capture in conjunction with continuous Apply rninirnizes the delay\nin propagating changes.\nIt is the best cor11bination in situations where the\nprimary and secondary copies are both used as part of an operational DBlVIS\nand replicas must be as closely synchronized with the prinlary copy as possi-\nble. Log-based Capture with continuous Apply is essentially a less expensive\nsubstitute for synchronous replication.\nProcedural Capture and application-\ndriven Apply offer the 11l0St flexibility in processing source data and changes\nbefore altering the replica; this flexibility is often useful in data warehousing\napplications where the ability to 'clean' and filter the retrieved data is 1110re\nimportant than the currency of the replica.\nData Warehousing: An Example of Replication\nCornplex decision support queries that look at data from Illultiple sites are be-\ncoming very inlportant. The paradigrn of executing queries that span r11ultiple\nsites is sirnply inadequate for perfornlance reasons. One way to provide such\ncomplex query support over data froln rllultiple sources is to create a copy of\nall the data at SaIne one location and use the copy rather than going to the in-\ndividual sources. Such a copied collection of data is called a data warehouse.\nSpecialized systelIls for building, rnaintaining, and querying data warehouses\nhave becolne irnportant tools in the rnarketplace.\nData vvarehouses can be seen as one instance of asynchronous replication, in\n'which copies are updated relatively infrequently.\n'\\Vhen we talk of replica-\ntion, \\ve typically rIlCal1 copies Inaintained under the control of a single DBlVIS,\n\\vhereaswith data \\varehousing, the original data rnay be on different sofhvare\nplatforrns (including databa\",'Sc systerns and as file systerIls) and even l)clong to\ndifferent organizations. This distinction, 110\\VeVer, is likely to becoine blurred\na.'3 vendors adopt luore 'open' strategies to replication.\nFor exarnple, sorne\nproducts already support the IJlaintenance of replicas of relations stored in one\nvendor's DB~·1S in al10ther vendor's I)BlVIS.\nvVe 110te that data warehousing involves rnore than just replication. vVe discuss\nother aspects of data warehousing in Chapter 2.5.\n\nParallel aTLd Dii8b~ib1LtedDataba8e8\n22.12\nDISTRIBUTED TRANSACTIONS\n7~5\nIn a distributed DBlvIS, a given transaction is subrnitted at SOIne one site, but\nit can access data at other sites &') well. In this chapter we refer to the activity\nof a transaction at a given site as a subtransaction.\nVVhen a transaction\nis subrnitted at S0111e site, the transaction rnanager at that site breaks it up\ninto a collection of one or rnoro subtransactions that execute at different sites,\nsubrnits theln to transaction rnanagers at the other sites, and coordinates their\nactivity.\n\\\\1e now consider ~lSpects of concurrency control and recovery that require ad-\nditional attention because of data distribution. As we saw in Chapter 16, there\nare many concurrency control protocols; in this chapter, for concreteness, we\nassurne that Strict 2PL 'with deadlock detection is used. We discuss the follow-\ning issues in subsequent sections:\n•\nDistributed Concurrency Control: How can locks for objects stored\nacross several sites be managed?\nHow can deadlocks be detected in a\ndistributed database?\n•\nDistributed Recovery: Transaction atomicity lllUSt be ensured-·---when a\ntransaction commits, all its actions, across all the sites at which it executes,\nrnust persist. Si111ilarly, when a transaction aborts, none of its actions must\nbe allowed to persist.\n22.13\nDIS\"fRIBUTED CONCURRENCY CONTROL\nIn Section 22.11.1, we described t\\VO techniques for irnplernenting synchronous\nreplication, and in Section 22.11.2, \"vo discussed various techniques for irllple-\nrnenting asynchronous replication.\nrrhe choice of technique deterrnines which\nobjects are to be locked.\nWhen locks are obtained and released is deterrnined\nby the concurrency control protocol.vVe now consider how lock and unlock\nrequests are irnplcrnented in a distributed envirorllnent.\nLock rnanagernent can be distributed across sites in rnanyways:\nII\nCentraliz,ed: A single site is in charge of handling lock and unlock requests\nfor all objects.\n!IIIl\nPriulary Copy: ()ne copy of each object is designated the prirnclry copy.\n.i\\.ll requests to lock or unlock a copy of this object are handled by the lock\nrnanager at the site \\vhere the prirnary copy is stored, regardless of where\nthe copy itself is stored.\n\n75G\n(;HAPTER 22\nII\nFully Distributed: R,equests to lock or unlock a copy of an object stored\nat a site are handled by the lock lnanager at the site \"where the copy is\nstored.\nThe centralized schelne is vulnerable to failure of the single site that controls\nlocking. The prirnary copy scherne avoids this problern, but in general, reading\na,n object requires cornrnunicatiollwith t\\VO sites: the site vvhere the prirnary\ncopy resides and the site \"where the copy to be read resides.\nThis problern\nis avoided in the fully distributed 8che1nc, because locking is done at the site\nwhere the copy to be read resides. However, \\vhile writing, locks rnust be set\nat all sites where copies are rnoclified in the fully distributed schclne, whereas\nlocks need be set only at one site in the other two schernes.\nClearly, the fully distributed locking scherne is the 1110st attractive schelne if\nreads are much more frequent than writes, as is usually the case.\n22.13.1\nDistributed Deadlock\nOne issue that requires special attention when using either priluary copy or fully\ndistributed locking is deadlock detection.\n(Of course, a deadlock prevention\nscherne can be used instead, but we focus on deadlock detection, which is widely\nused.) As in a centralized DBMS, deadlocks rnust be detected and resolved (by\naborting sorne deadlocked transaction).\nEach site rnaintains a local waits-for graph, and a cycle in a local graph indicates\na, deadlock. lIowever,\nthen~ can be a deadlock even if no local graph contains\na cycle.\nFor exarnple, suppose that two sites, A and B, both contain copies\nof objects 01 and 02, and that the read-any write-all technique is used.\nI~l,\nwhich wants to read ()1 and write 02, obtains an S lock on 01 and an X lock\non 02 at Site A, then requests an ..X lock on 02 at Site B. T2, which \\vants\nto read 02 and write 01, rneanwhilc, obtains an S lock on 02 and an ..x lock\non 01 at Site B, then requests an X lock on ()1 at Site A..A.s\nI~'igure 22.5\nillustrates,\n7~2 is waiting for Tl aJ, Site A. and Tl is waiting for T2 at Site 13;\nthus, \\ve have a deadlock, \\vhich neither site can detect based solely on its local\nwaits-for graph.\nTo detect such deadlocks, a distributed deadlock detection algoritlun rnust\nbe used. \\Ve descTil)e three such algoritluns.\nThe first algorithrn,\\vhich is centralized, consists of periodically sending all 10-\ncal waits-for graphs to one site that is responsible for global deadlock detection.\nAt this site, the globaJ \\va-its-for graph is generated by cOlubinin.g all the local\ngraphs; the set of nodes is the union of nodes in the local graphs, and there is\n\n]JaTallel and Di8trilnded Databases\n7fJ7\nGlobal Waits-for Graph\nFigure 22.5\nDistributed Deadlock\nan edge frorn one node to another if there is such an edge in any of the local\ngraphs.\nThe second algorithrn, which is hierarchical, groups sites into a hierarchy. For\ninstance, sites r11ight be grouped by state, then by country, and finally into a\nsingle group that contains all sites.\nEvery node in this hierarchy constructs\na waits-for graph that reveals deadlocks involving only sites contained in (the\nsubtree rooted at) this node. All sites periodically (e.g., every 10 seconds) send\ntheir local waits-for graph to the site responsible for constructing the waits-\nfor graph for their state. The sites constructing waits-for graphs at the state\nlevel periodically (e.g., every minute) send the state waits-for graph to the\nsite constructing the waits-for graph for their country. The sites constructing\nwaits-for graphs at the country level periodically (e.g., every 10 rninutes) send\nthe country waits-for graph to the site constructing the global waits-for graph.\nThis scheme is based on the observation that l110re deadlocks are likely across\nclosely related sites than across unrelated sites, and it puts 1110re effort into\ndetecting deadlocks across related sites. All deadlocks are eventually detected,\nbut a deadlock involving two different countries J.nay take a while to detect.\nThe third algorithrn is sirllple: If a transaction waits longer than SOIne chosen\ntinle-out interval, it is aborted.\nAlthough this algorithrll rnay cause rnany\nunnecessary restarts, the overhead of deadlock detection is (obviously!)\nlow,\nand in a heterogeneous distributed database, if the participating sites cannot\ncooperate to the extent of sha,ring their \\va,its-for graphs, it rnay be the only\noption.\nA subtle point to note with respect to distributed deadlock detection is that\ndelays in proI5agating local inforrnation rnight cause the deadlock detection\nalgorithr11 to identify 'deadlocks' that do not really exist.\nSuch\nsituations~\ncalled phantoln deadlocks~ lead to unnecessary aborts. For concreteness, we\ncliscuss the centralized algorithrn, although the hierarchical algorithrn suffers\nfr0111 the Se1Ine problern.\n\n758\n(;HAPI'ER 2'2\nConsider a rnodificatioll of the previous exarnple. As before, the two transac-\ntions \\vait for each other, generating the local \\vaits-for graphs shown in Figure\n22.5, and the local vvaits-for graphs are sent to the global deadlock-detection\nsite.\nIIo\\vever, 7'2 is now aborted for 1'ea..,;on8 other than deadlock.\n(For ex-\narnple, T2 rnay also be executing at a third site, 'where it reads an unexpected\ndata value and decides to abort.) .At this point, the local waits-for graphs have\nchanged so that there is no cycle in the 'true' global \\vaits-for graph. l-Io\\vever,\nthe constructed globaJ waits-for graph \\vill contain a cycle, and 7'1 Inay well be\npicked as the victirn!\n22.14\nDISTRIBUTED RECOVERY\nRecovery in a distributed DBJVIS is rnore cornplicated than In a centralized\nDBMS for the following reasons:\nl1li\nNew kinds of failure can arise: failure of COlnnlunication links and failure\nof a remote site at which a subtransaction is executing.\nl1li\nEither all subtransactions of a giv(~n transaction Iuust ccnnlnit or none HUlst\nconlnlit, and this property IIlust be guaranteed despite any cOIllbination of\nsite and link failures. T'his guarantee is achieved using a commit proto-\ncol.\nAs in a centralized DBMS, certain actions are carried out as part of norrnal\nexecution to provide the necessary infonnation to recover fro111 failures. A log is\nrnaintained at each site, and in addition to the kinds of inforrnation rnaintained\nin a centralized DB11S, actions taken as part of the cOlInnit protocol are also\nlogged. The Inost widely used conunit protocol is called TUJO-Phase Cornmit\n(2PC). A variant caned 21J C with Prcsurncd Abort, which we discuss next, hc'k'3\nbeen adopted as an industry standard.\nIn this section, we first describe the steps taken during nonnal execution, con-\ncentrating on the cOHnnit protocol, and tJlen discuss recovery fron1 failures.\n22.14.1\nNormal Execution and Commit I>rotocols\nI)uring Donnal execution, each site rnaintains a log, and the actions of a sub-\ntransaction are ~.ogged at the site \\vhere it executes. The regular logging activity\ndescribed in Chapter 18 is carried out and, in addition, a eornnlit protocol is\nfollc)\\ved to ensure that all subtra,nsa.ctions of a given transaction either cOIrnnit\nor H,bort uniforrnly. 1'he transacticHl rnanager at the site vvhcl'e the transaction\n()riginat(~cl is called the coordinator for the transaction; transaction Inanagers\nat sites\n\\vh(~l'e its subtraJ1Sactiol1S execute are\nca.11(~d subordinates (\\vith re-\nspect to the coordinatioll of this transaction).\n\nParallel and DistTilnlted ]Jatabases\n7~9\n\\Ve no\\v describe the Two-:Pha'5e Cornmit (2PC) protocol~ in tenns of the\nrnessa,ges exchanged and the lc)g records vlritten.\n\\Vhen the user decides to\ncornrnit a\ntransactioll~ the conunit cOIrunand is sent to the coordinator for the\ntransaction. This initiates the 2PC; protocol:\n1. The coordinator sends a prepare rnessage to each subordinate.\n2. \\'Then a subordinate receives a prepare rnessage~ it decides \\vhether to abort\nor cornrnit its subtransaction.\nIt force-writes an abort or prepare log\nrecord, and then sends a. no or yes rnessage to the coordinator. Note that\na prepare log record is not used in a centralized DB.l\\;lS; it is unique to the\ndistributed cornrnit protocol.\n3. If the coordinator receives yes lnessages fr0 III all subordinates, it force-\nwrites a cornmit log record and then sends a cornrnit rnessage to all sub-\nordinates. If it receives even one no rnessage or receives no response fronl\nSOHle subordinate for a specified titne-out interval, it force-writes an abort\nlog record, and then sends an abort Inessage to all subordinates. 1\n4. vVhen a subordinate receives an abort Inessage, it force-writes an abort log\nrecord, sends an ack Inessage to the coordinator, and aborts the subtrans-\naction. When a subordinate receives a cornrnit rnessage, it force-writes a\ncOl1nnit log record, sends an ack rnessage to the coordinator, and corrunits\nthe subtransaction.\n5. After the coordinator has received ack rnessages frorn all subordinates, it\nwrites an end log record for the transaction.\n1:'he narne\nT'llJO- ]J}ULSC (7ornTnit reflects the fact that two rounds of rnessages\nare exchanged: first a voting phase, then a tennination pha..se, both initiated\nby the coordinator.\nffhe basic principle is that any of the transaction tnan-\nagel'S involved (including the coordinator) can unilaterally a,bort a transaction,\n\\vhereas therernust be unanirnity to conuuit a transaction, vVhen a rnessage\nis serlt in 2PC, it signals a decision by the sender. To ensure that this decision\nsurvives a crash at the sender's site, the log\nr(~cord describing the decision is\nahvays forced to stable storage before the rnessage is sent.\n;\\ transaction is ofIicially cornrnitted at the tirne the coordiIlator'8 cOllnnit log\nrecord reaches stable storage. Subsequent failures cannot affect the outcorne of\nthe transaction; it is irrevocaJ)ly corrunitted. Log records\\vritten to record the\nconnnit protocol actions contain the type of the record,\ntlH~ tn.lllsaction iel, and\nthe identity of the coordiu;:ltor.\ni\\ coordinator's conunit or abort log record\nalso contains the identities of the subordinates.\n1As a,n optilnization J the coordinator need not send abort meSS;:lges {;o subordinates who voted no,\n\n760\n(';IIAPTER, ~2\n22.14.2\nRestart after a Failure\nvVhen a site COUles back up after a erao.sh, \\ve invoke a recovery process that\nreads the log and processes all transactions executing the conunit protocol at\nthe tirne of the cnlsh. The transaction rnanager at this site could have been the\ncoordinator for SOUle of these transactions and a subordinate for others. We do\nthe follo¥ling in the recovery process:\n•\nIf vve have a cornInit or abort log record for transaction T, its status is clear;\nwe redo or undo 1\"\\ respectively. If this site is the coordinator, which can\nbe deterrnined froru\ntl1(~ cOlTllnit or abort log record, we rnust periodically\nresend·-,·-because there rnay be other link or site failures in the\nsystem~~~\"-a\ncom/n7,it or abort rnessage to each subordinate until we receive an ack. After\nwe have received acks frorn all subordinates, we write an end log record for\nT.\n•\nIf we have a prepare log record for T but no conunit or abort log record,\nthis site is a subordinate, and the coordinator can be detennined froIn\nthe prepare record.\nWe rllust repeatedly contact the coordinator site to\ndetermine the status of T.\nOnce the coordinator responds with either\ncOlurnit or abort, we write a corresponding log record, redo or undo the\ntransaction, and then write an end log record for T.\n•\nIf we have no prepare, cOllunit, or abort log record for transaction T,\nT certainly could not have voted to connuit before the crash; so we can\nunilaterally abort and undo T and vvrite an end log record. In this case,\nwe have no way to detennine whether the current site is the coordinator\nor a subordinate for 1'1. flowever, if this site is the coordinator, it rnight\nhave sent a prepare rnessage prior to the crEL.'3h, and if so, other sites rnay\nhave voted yes. If such a subordinate site contacts the recovery process at\nthe current site, we now know that the current site is the coordinator for\nT, and given that there is no cOllnnit or abort log record, the response to\nthe subordinate should be to abort 7\n1\n•\n()bserve that, if the coordinator site for a transaction I' fails, subordinates who\nvoted yes cannot decide \\vhether to conunit or abort\n~r until the coordinator\nsite recovers; \\ve say that l' is blocked. In principle, the active subordinate\nsites could cOl1nnunicate arnong thelIlselves, and if at lccL.'3t one of thelIl contains\nan abort or coinrnit log record for T, its status becornes globally known. 1\"0\nconununicate arnong thernselves, all subordinates nlust be told the identity of\nth(~ other subordinates at the titne th(~y are sent the ]Jrcpa:re Inessage. llowever,\n2PC is still vlllnerable to coordinator failure durirlg recovery because even if all\nsubordinates voted yes, the coordinator (-who also ha,s a vote!) rnay have de-\ncided to aJ)ort rr, and this decision cannot be detennined until the coordinator\nsi1,e recovers.\n\nParallel a1ul [Jistrilnded Database8\n791\n\\Ve covered how a site recovers fro111 a crash, but\\vhat should a site that is\ninvolved in the cOIIunit protocol do if a site that it is cornrl1unicating with fails?\nIf the current site is the coordinator, it should shnply abort the transaction.\nIf the current site is a\nsllbordina,te~ and it has not yet responded to the coor-\ndinator's prepaT(; lnessage, it can (and should) abort the transaction. If it is a\nsubordinate and has voted yes, then it cannot unilaterally abort the transac-\ntion, and it cannot cOIUlnit either; it is blocked. It lnust periodically contact\nthe coordinator until it receives a reply.\nFailures of COffilnunication links are seen by active sites as failure of other sites\nthat they are comnlunicating with, and therefore the solutions just outlined\napply to this case as \\-vell.\n22.14.3\nTwo-Phase Commit Revisited\nNow that we examined how a site recovers frolll a failure, and saw the inter-\naction between the 2PC protocol and the recovery process, it is instructive to\nconsider how 2PC can be refined further. In doing so, we arrive at a more ef-\nficient version of 2PC, but equally irnportant perhaps, we understand the role\nof the various steps of 2PC ruore clearly. Consider three basic observations:\n1. l'he ack rnessages in 2PC\nan~ used to detennine when a coordinator (or\nthe recovery process at a coordinator site following a crash) can 'forget'\nabout a transaction T. lJntil the coordinator knows that all subordinates\nare aware of the cornrnit or abort decision for T, it IIlust keep inforrnation\nabout T in the transaction table.\n2. If the coordinator site fails aJter sending out ]J'repoxe rnessages but before\nwriting a cornrnit or abort log record, when it cornes back up, it 1Uts no\ninf'orruatiol1 abollt the transaction's connnit status prior to the crash. IInv.l-\never, it is still free to abort the transaction unilaterally (beca,use it has not\n\\vrittcn a conunit record, it can still cast a no vote itself). If another site\ninquires about the status of the transaction, the recovery process, as we\nhave seen, responds \\vith an abort rnessage.\nTherefore, in the absence of\ninforrnation, a transaction is pres'luned to h..ave aborted.\n:3. If a subtnlnsaction does no llIHlates, it has no changes to either redo or\nundo: in other vvords. its cornrnit or abort status is irrelevant.\n'l'he first tvvo ol)servations suggest several refinernents:\nm\n\\Vhen a coordirlCltor (tborts a tnl,nsa,cticHl T', it can undo ~r and rerl10ve it\nfronl the transaction table irnrrlediately.\nAfter all \\ rernoving\n~r frorn the\ntable results in a 'no inforrnatioI1' state with respect to T, and the default\n\n762\nCHAPTER 62\nresponse (to an enquiry about T) in this state, \\vhich is abort, is the correct\nresponse for an aborted transaction.\n•\nBy the same token, if a subordinate receives an abort Inessagc, it need not\nsend an ack lnessage. rrhe coordinator is not waiting to hear frorn subor-\ndinates after sending an abor't 111essage! If, for SOlne rea...\")on, a subordinate\nthat receives a prepflrc message (and voted yes) does not receive an abort\nor cornm,it Inessage for a specified tirne-out interval, it contacts the coordi-\nnator again. If the coordinator decided to abort, there Inay no longer be\nan entry in the transaction table for this transaction, but the subordinate\nreceives the default abort nlessage, whicll is the correct response.\n•\nBecause the coordinator is not waiting to hear froul subordinates after\ndeciding to abort a transaction, the names of subordinates need not be\nrecorded in the abort log record for the coordinator.\n•\nAll abort log records (for the coordinator as well as subordinates) can\nsimply be appended to the log tail, instead of doing a force-write.\nAfter\nall, if they are not written to stable storage before a crash, the default\ndecision is to abort the transaction.\nThe third basic observation suggests SOlne additional refinements:\n•\nIf a subtransaction does no updates (which can be easily detected by keep-\ning a count of update log records), the subordinate can respond to a prepare\n111essage from the coordinator with a r'eader message, instead of yes or no.\nThe subordinate writes no log records in this case.\n•\nvVhen a coordinator receives a reader lnessage, it treats the Inessage as a yes\nvote, but with the optiInization that it does not send any lnore messages\nto the subordinate, because the subordinate's cornlnit or abort status is\nirrelevant.\n•\nIf all subtransactions, including the sllbtransaction at the coordinator site~\nsend a reader luessagc, we do not need the second phase of the conunit pro-\ntocol. Indeed, \\\\'e can sirnply rernove the transaction frolH the transaction\ntable: \\vithout \\vriting any log records at any site for this transaction.\n1lH~ T'wo-Phasc Cornrnit protocol with the refinernents discussed in this section\nis called Two-Phase Commit with Presurned Abort.\n22.14.4\nThree...Phase Commit\nA cornlnitprotocol caIled Three-Phase Conlrnit (3PC) can avoid blocking\neven if the coordinator\nsit(~ fails during recovery. T'he basic idea is that, \\vhen\n\nParallel and Distrilnded Databases\n763\nthe coordinator sends out prel)arc rnessages and receives yes votes £1'0111 all sub-\nordinates, it sends all sites a pTccomrrl-it message, rather than a cornrnit rnessage.\n\\\\Then a sufficient l1Ulllber..··....··....more than the lIlaxinlulll nUlnber of failures that\nnlust be handled········of acks have been received, the coordinator force-writes a\ncornrn·it log record and sends a cornmit lnessage to all subordinates. In 3PC,\nthe coordinator effectively postpones the decision to cornrnit until it is sure\nthat enough sites know about the decision to corn111it; if the coordinator sub-\nsequently fails, these sites can C0l11111Unicate with each other and detect that\nthe transaction rnust be corllrnitted-conversely, aborted, if none of thern has\nreceived a precomrnit rnessage-'-without waiting for the coordinator to recover.\nrrhe 3PC protocol ilnposes a significant additional cost during normal execution\nand requires that COlnrIlunication link failures do not lead to a network partition\n(wherein sorne sites cannot reach some other sites through any path) to ensure\nfreedo111 fronl blocking. For these reasons, it is not used in practice.\n22.15\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\n•\nDiscuss the different rnotivations behind parallel and distributed databases.\n(Section 22.1)\n•\nDescribe the three lllain architectures for parallel DBMSs.\nExplain why\nthe shared-memory and\nshaT'(~d-disk approaches suffer frOlll interference.\nWhat can you say about the speed-up and scale-up of the shared-nothing\narchitecture? (Section 22.2)\n•\nDescribe and differentiate pipelined parallelism and data-partitioned paral-\nlelism. (Section 22.3)\n•\nDiscuss the following techniques for partitioning data: round-Tobin, hash,\nand range. (Section 22.3.1)\nII\nExplain how existing code can be parallelized by introducing split and\nrnerrJC operators. (Section 22.3.2)\n11II\nDiscuss huw cadI of the following operators can be parallized using data\npartitionipg:\nscanning, sorting, joiTL.\nCornparc the use of sorting versus\nhashing for partitioning. (Section 22.4)\n11III\nvVhat do \\ve need to consider in optilllizing queries for parallel execution?\nDiscuss interoperation parallelislll,\nleft-dcc~p trees versus bushy trees, and\n(~ost estirnation. (Section 22.5)\n\n-(\"4\nI '):'\n•\n..\n..\n..\n..\n..\n..\n•\n..\n..\n•\n•\nCIIAPTER 2~\nDefine the tenns disiribtded data\nitbde1JendeTU~eand distribttted transaction\natryrnicitll.\n.ATe these concepts sllpported in current eornrnercial systerns?\n\\Vhy not? \\\\That is the difference bet\\veen hornogeneoclls and heteTogeneOtls\ndistributed databa\",'3es? (Section 22.6)\nDescribe the three lllain architectures for distributed DB~'lSs. (Section 22.7)\n1\\ relation can be distributed by jraglnent'ing it or rcplicat'ing it across\nseveral sites. Explain these concepts and ho\\v they differ. Also, distinguish\nbetween }un'1;zontal and vertical fragrnentation. (Section 22.8)\nIf a relation is fraglnented and replicated, each partition needs a globally\nunique nalne called the Tclat'ion narnc.\nExplain how such global naInes\nare created and the Inotivation behind the described approach to narning.\n(Section 22.9.1)\nExplain how rnetadata about such distributed data is rnaintained in a dis-\ntr'ibuted catalog. (Section 22.9.2)\nDescribe a nauling scherne that supports distributed data independence.\n(Section 22.9.3)\nWhen processing queries in a distributed DBlVlS, the location of partitions\nof the relation needs to be taken into account.\nDiscuss the alternatives\nwhen joining two two relations that reside on different sites. In particular,\nexplain and describe the rnotivation behind the Sernijoin and Bloornjoin\ntechniques. (Section 22.10.2)\nWhat issues rnust be considered in optirnizing queries over distributed data,\nin addition to where the data is located? (Section 22.10.3)\n\\\\7hat is the difference bet\\veen synchronous asynchronous replication? Why\nhas asynchronous replication gained in popularity? (Section 22.11)\nDescribe the 'ooting and Tead-a'ny 'l1J'rite-all approaches to synchronous repli-\ncation. (Section 22.11.1)\nSurnruarize the peer-lo-peer and ]JTinul,Ty site approaches to asynchronolls\nrepliccLtion. (Section 22.11.2)\nIn prirnary site replication, changes to the prirnary copy Inust be propa-\ngated to secondary copies. vVhat is done in the Caph.lT'c and Apply steps?\nDescribe log-based and proceri'ttrnl approaches to Capture and cornpare\ntheln. VVhat are the variations in scheduling the Apply step? Illustrate the\nuse of asynchronolls replication in a data vi.larehouse. (Section 22.11.2)\n\\Vhat is a 8'ubtrans(u.:t'io'n? (Section 22.12)\n\njJo,rallel and D'i.stributed Database8\n765\nit\nII\n\\'That are the choices for rnanaglng locks in a distributed DBrvIS? (Sec-\ntion 22.13)\nIi\nDiscuss deadlock detection in a distributed datab<:1Se. Contr&')t the ce1lLrnl-\n'ized, hierarchical, and tirne-o'ut approaches. (Section 22.13.1)\nIII\n\\\\Thy is recovery in a distributed l.)BlVIS rIlore cornplicated tha.n III a cen-\ntralized systern? (Section 22.14)\nIII\n\\\\That is a connnit protocol and \\vhy is it required in a distributed database?\nDescribe and C01npa.1\"e T'wo-fJhasc a11d Three-Phase Cornn1it.\n\\Vhat is\nblocking, and how does the Three-Pha,,')e protocol prevent it?\nvVhy is it\nnonetheless not used in practice? (Section 22.14)\nEXERCISES\nExercise 22.1 Give brief answers to the following questions:\n1. What are the siruilarities and differences between parallel and distributed database rnan-·\nagement systerns?\n2. Would you expect to see a parallel database built using a wide-area network?\n\\Vould\nyou expect to see a distributed database built using a wide-area network? Explain.\n3. Define the terms 8cale--'Up and speed-up.\n4. Why is a shared-nothing architecture attractive for parallel database systerns?\n5. The idea of building specialized hardware to run parallel database applications received\nconsiderable a+\nt.ion but has fallen out of favor. Cornrnent on this trend.\n6. \\\\That are th,\n(Lntages of a distributed D131\\1[8 over a centralized DBNIS?\n7. Briefly descr\nnnd cornpare the Client-Server and Collaborating Servers architectures.\ntting Servers architecture, \\vhen a transaction is subrnitted to the DBIVIS,\nhow its activities at various sites are coordinated. In particular, describe\n.saction managers at the different sites, the concept of 8ubtransacf:ions,\n_cept of d'i.5tributed tTan.saction atO'lnicity.\n8. In the Collal\nbriefly dew'\nthe role\n(l\nand the,\nExercise 22.2 Give brief a,nswers to the follmving questions:\n1. I)efine the tenus fragrnentat'ion and rcpl'icah 0 T/\" in tenns of where data is stored.\n2.\n\\\\'11(1' is the difference behveen synclrrorwu8 and a,synchTo'TWU8 replication?\na.Define the tern1 distrilndcd data independence.\n\\Vha.t does this Inean'with respect to\nquer:ying ane! updating data in the presence of data fragrnentation and n::plication?\n4.\nC:onsieh:~r the 'uot:ing BJ1d n::ad-anywriic-all techniques for irnplementing synchronous\nreplication.\\\\rhat are their respectiv(~ l>ros and cons?\n5. C;ive an c)Verview of henv asynchronous replication can\n1)(' irnplernented.\nIn particular:\nexplain the tenns Capt un:' and Apply.\n6. \\VIHtt is the difference between log-based and procedural irnplernentatiOlls of capture?\n7. \\VllY is giving database objcc:ts unique names rnore cennplicated in a distributed DB~IS?\n\n766\nCHAP'TER 42\n8. Describe a catalog organization that pennits any replica (of an entire relation or a frag-\nrnent) to be given a unique nam,e and provides the nanling infrastructure required for\nensuring distributed data independence.\n9. If infonuation fi'Olll renlote catalogs is cached at other sites, what happens if the cached\ninfoflllation becOInes outdated? How can this condition be detected and resolved?\nExercise 22.3 Consider a parallel DB,lVIS in \\vhich each relation is stored by horizontally\npartitioning its tuples across all disks:\nErnployees(eid: integer, did: integer, .sal.' real)\nDepartlnents(~'id: integer, Tngrid: integer, budget: integer)\nThe rngT\"id field of DepartInents is the e'id of the manager.\nEach relation contains 20-byte\ntuples, and the sal and budget fields both contain unifonnly distributed values in the range\no to 1 rnillion.\nThe Enlployees relation contains 100,000 pages, the Departrnents relation\ncontains 5,000 pages, and each processor has 100 buffer pages of 4,000 bytes each. The cost of\none page I/O is tel, and the cost of shipping one page is t s ; tuples are shipped in units of one\npage by waiting for a page to be filled before sending a rnessage frmn processor 'i to processor\nj.\n'There are no indexes, and all joins that are local to a processor are carried out using\na sort-rnerge join.\nAssurne that the relations are initially partitioned using a round-robin\nalgorithlll and that there are 10 processors.\nFor each of the following queries, describe the evaluation plan briefly and give its cost in tenns\nof tel and t s . You should cornpute the total cost across all sites as well as the 'elapsed time'\ncost (i.e., if several operations are carried out concurrently, the tirne taken is the rnaxilnurn\nover these operations).\n1. Find the highest paid ernployee.\n2. Find the highest paid employee in the departrnent with d'id 55.\n3. Find the highest paid ernployee over all departnHmts with lJ'ndget less than 100,000.\n4. Find the highest paid enlployee over all departlnents with budget less than\n~300,000.\n5. Find the a;verage salary over all departnrents with budget less than ~300,OOO.\n6. Find the salaries of all rnanagers.\n7. Find the salaries of all rnanagers who rn::mage a departrnent with a budget less than\n300,000 and eaTll rnore than 100,000.\n8. Print the eids of all elnployees, ordered by increasing salaries. Each processor is connected\nto a separate printer, and the answer can appear as severaJ sorted lists, cadI printer] by\na different processor, as long as we can ol)tain a fully sorted list by concatenating the\nprinted lists (in sorne order).\nExercise 22.4 Consider the saIne scenario as in Exercise 22.:3, except 1.h;':1t the relations are\noriginally partitioned using range partitionirlg on the sal and fnulget fields.\nExercise 22.5 Repeat Exercises\n22.~) and 22.4 \\vith (i) 1 processor, ,1nd (ii) lelO processors.\nExercise 22.6 COllsicler the Ernployees (-uHIDepartments relations descril)cd in\nEx.(~rcise\n22.~3.\nrIhey are now stored in a distributed DBl\\!lS with all of Eluployees stored at Naples\n<'lnd all of Departlnents stored at Berlin. There arc no indexes on these relations. 'rhe cost of\nvarious operations is as describecl in Exercise 22.:3. Consider the query:\n\nParallel and Di8trib1Lted Databases\nSELECT *\nFROM\nEInployees E) Dcpartrncnts D\nWHEREE.eid = I).Ingrid\n7t)7\n'The query is posed at Delhi, and you are told that only 1 percent of ernployees are IIlanagers.\nFind the cost of answering this query using each of the following plans:\n1. Ship Departrnents to Naples, cornpute the query at Naples, then ship the result to Delhi.\n2. Ship Ernployees to Berlin, cornpute the query at Berlin, then ship the result to Delhi.\n3. COInpute the query at Delhi by shipping both relations to Delhi.\n4. COlnpute the query at Naples using BloOlnjoin; then ship the result to Delhi.\n5. Compute the query at Berlin using Bloornjoin; then ship the result to Delhi.\n6. Cornpute the query at Naples using Sernijoin; then ship the result to Delhi.\n7. COInpute the query at Berlin using Sernijoin; then ship the result to Delhi.\nExercise 22.7 Consider your answers in Exercise 22.6.\nWhich plan ll1inin1izes shipping\ncosts? Is it necessarily the cheapest plan? Which do you expect to be the cheapest?\nExercise 22.8 Consider the Ernployees and Departments relations described in Exercise\n22.3. They are now stored in a distributed DBMS with 10 sites. The DepartInents tuples are\nhorizontally partitioned across the 10 sites by did, with the same nUInber of tuples assigned\nto each site and no particular order to how tuples are assigned to sites. The Employees tuples\nare sirnilarly partitioned, by sal ranges, with sal S; 100,000 assigned to the first site, 100,000 <\nseLL::; 200,000 assigned to the second site, and so OIl. In addition, the partition sal :S 100,000\nis frequently accessed and infrequently updated, and it is therefore replicated at every site.\nNo other EU1ployees partition is replicated.\n1. Describe the best plan (unless a plan is specified) and give its cost:\n(a) Cornpute the natural join of Enlployees and Departlnents by shipping all fragrnents\nof the slImller relation to every site containing tuples of the larger relation.\n(b) Find the highest paid ernployee.\n(c) Find the highest paid clnployee with salary less than 100,000.\n(d) Find the highest rn1id ernployee with sala,1'y between 400,000 and 500,000.\n(e) 'Find the highest paid clnployee with salary between 4fjO,OOO and 550,000.\n(f) Find the highest paid rnanager for those departnwnts stored at the query site.\n(g) Find the highest pajd lnanager.\n2. ASSU111ing the sarne (taUl distribution, describe the sites visited and the locks obtained\nfor the foll()\\ving update transactions, a\",:;surnillg that 8ynchTono'u8 replication is used for\nthe replication of Ernployees tuples \\vith sal ::s 100, (}(}{):\n(a) Give einployees with salary less than 100,000 a 10 percent raise, with a Inaxirnurn\nsaJary of 100,000 (i.e., the raise cannot incre;:lse the salary to rnore than 100,(00).\n(b) Give all ernployees <1 10 percent raise. The conditions of the original partitioning\nof Elnployees IIlust still be satisfied after the update.\na. AssuIning the saIne data distribution, describe the sites visited and the locks obtained\nfor the following update transactions, a..ssurning that rL8ynchTOn01./.,8 replication is used for\nthe replication of Ernployees tuples with sal :S 100,000.\n\n768\nCHAPTER:22\nFor all ernployees \\vith salary less than lOOlOOO give them 11 10 percent raisc l \\vith\na rnaxirmun salary of 100/)00.\nGive an ernployees a 10 percent raise. After the update is conlpleted~ the conditions\nof the original partitioning of Eluployecs rnust still be satisfied.\nExercise 22.9 Consider the EInployees cUll:1 .DepartInents tahles frOluExercise 22.:3. You arc\n;;1 DBA and you need to decide ho\\v to distribute these t\\VO tables across t\\VO sites, IVlanila and\nNairobi. Your D131\\:18 supports only unclustered 13+ tree indexes. You have a choice between\nsynchronous and asynchronous replication. 1\"'01' each of the following scenarios, describe hmN\nyou would distribute thenl and what indexes you would build at each site.\nIf you feel that\nyou have insufficient inforrna.tion to 1nake a decision, explain briefly.\n1. Half the departInents are located in IVlanila (l,lld the other half aTe in Nairobi. Departrnent\ninformation, including that for ernployees in the depart1nent, is changed only at the site\nwhere the departrnent is located, but such changes are quite frequent.\n(Although the\nlocation of a depart1nent is not included in the Departrnents schclna, this inforrnation\ncan be obtained frorn another table.)\n2. Half the departrnents are located in 1Vlanila and the other half are in Nairobi. Departrnent\ninformation, including that for errlployees in the departrnent, is changed only at the site\nwhere the departrnent is located, but such changes are infrequent. F'inding the average\nsalary for each departrnent is a frequently asked query.\n~3. Half the departlnents are located in Ivlanila and the other half are in Nairobi. Ernployees\ntuples are frequently changed (only) at the site where the corresponding departrrlent is lo-\ncated, but the Depart1nents relation is aJulOst never changed. Finding a given ernployee's\nrnanager is a frequently asked query.\n4. Half the e1nployees work in l'vlanila and the other half \\vork in Nairobi. E1nployees tuples\nare frequently changed (only) at the site where they work.\nExercise 22.10 Suppose that the Ernployees relation is stored in l\\1adison and the tuples\nwith sal ~ 1.00,000 are replicated at Ne\\v York. Consider the following three options for lock\nrnanagernent: all locks Huulaged at a\ns'inglf.~ site, say, 1VIilwaukee; prvirnaTy copy with l'vladison\nbeing the primary for Ernployees; and fully di8tTilnded.\nFor each of the lock rnanagernent\noptions, explain what locks are set (and at which site) for the following queries. Also state\nfrorn which site the page is reac1.\n1. A query at Austin wants to read a page of Erllployees tuples \\vith sal s: 50,()OO.\n2.\nA qttery at I\\lfadison wants to read a page of E1nployees tuples \\vith sal s: 50,000.\n3. A query at Ne\\v ',/'ork wants to re;:ld a page of Enlployees tuples \"vith sal:::; 50,000.\nExercise 22.11 Briefly answer the follcnving questions:\n1. C\\Hnpare the relative rnerits of centralized and hierarchic;:tl deadlock detection in a dis-\ntril)lIted I)BI\\/IS.\n2. \\iVhat is a pha:ntorn de(ullock? Give an exarnple.\n:3.\n(.;iv(~ an example of a distributed D131\\-'18 \\vith three sites such that no hvo loc;::d \\vaits-for\ngraphs reveal a deadlock, :vet there is a global deadlock.\n4. C;onsider the following rnoclification to ;:I, local waits-for gn1ph: Add a neVil node '1:':1:1, and\nfor ever.v trans;:lc:tion 7:/ that is waiting for a lock at ;:ulOther sib~, add the edge 'Ii\n7~';I:t.\nA.lso ;:lch.l HJl edge\nT~':d --+ Ti if a tr;::lHSi:lction executing at another site is waiting for Ti\nto release\n;:1 lock at this site.\n\nJ)arallel and DislI'ib'ulc:d Dato,ba8C8\n7tiqf\nIf there is ,1 cycle in the 11lodiHed local WEtits-for gra.ph that does not involve 7:~xt ~\nwhat call you conclude? If every cycle involves\nT~~:rt., what can you conclude?\nSuppose that every site is assigned a unique integer\n\\Vhenever the lOC!:l}\nwaits-for graph suggests that there Blight be a global deadlock, send the local waits-\nfor graph to the site with tIle next higher site\"\"id.At that site, combine the received\ngraph \"vith the local \\vaits-fc)[ grap,h. If this cornbined graph does not indictl.t:e a\ndeadlock, ship it on to the next site, awl so on, until either a cleadlock is detected\nor we are back at the site that originated this round of deadlock detection. Is this\nscheuw guaranteed to find a global deadlock if one exists?\nExercise 22.12 Tirnestarnp-based concurrency control schernes can be used in a distributed\nDBivIS, but we rllust be able to generate globally unique, rllonotonicaJly increasing tirnestarnps\nwithout a bias in favor of anyone site. One approach is to a,...':\\sign timestrunps at a single site.\nAnother is to use the local clock tiTne and to append the site-iei. A third scherne is to use a\ncounter at each site. COIllpare these three approaches.\nExercise 22.13 Consieler the rIlultiple-granlllarity locking protocol described in Chapter 18.\nIn a distributed DB?vIS, the site containing the root object in the hierarchy can becmne a,\nbottleneck. You hire a database consultant who tells you to rnodify your protocol to allow\nonly intention locks OIl the root and irnplicitly grant all possible intention locks to every\ntransaction.\n1. Explain why this rnodification \\vorks correctly, in that transactions continue to be able\nto set locks on desired parts of the hierarchy.\n2. Explain how it reduces the demand on the root.\n3. Why is this idea not included as part of the standard rllultiple-granularity locking protocol\nfor a centralized DBTvlS?\nExercise 22.14 Briefly answer the following questions:\n1. Explain the need for a cornmit protocol in a distributeclDIJf'vIS.\n2. Describe 2PC. Be sure to explain the need for force-writes.\n;3. vVhy are nch: HlCssages required in 2PC?\n4. vVhat are the differences between 2PC; and 2PC with PresulTled Abort?\n5. Give an exarnple execution sequence such that 2PC cHId 2PC 'with Presurned Abort:\ngenerate an identical sequence of actions.\n6. Give\n('UI exarIlple execution sequence such that 2PC; and 2PC,; with PresuIl1ed Abort\ngenerate different sequences of actions.\n7. \\Vhat is the intuition behirHI :3PC? \\:Vhat are its fH'08 and cons relative to 2PC?\n8. Suppose th.<1t a site gets no response frorn iJ.nother site for <'1 long tiITle. C;an the first site\ntell whether .the connecting link has failed or the other site has failed?\nHow is such a\nfailure handled?\n9. Suppose that the coordinator inclucles a list of aU subordinates in the In'f'-]UJ,'T'C Inessage. If\nthc~ coordinator fails aJter sending out either an abcrd or COTT1Jnit rnessage, call you suggest\na\\va,Y for active sites to terrninate this tra.nsaction without wajting f<)I' the coordinator\nto recover?\nAssurnf~ that sonle but not all of the abort or cornrt1:it rnessages frOln the\ncocn'clinator are lost.\n\n770\nC~HAPTER 2&2\nIII Suppose that 2PC with PresuIl1ed Abort is used as the cOInmit protocol. Explain how\nthe systmll recovers froIn failure and deals with a particular transaction T in each of the\nfollowing cases:\n(a) A subordinate site for T fails before receiving a prepare rnessage.\n(b) A subordinate site for T fails after receiving a pTcparc rnessage but before rnaking\na decision.\n(c) A subordinate site for T fails after receiving a prepare lnessage and force-writing\nan abort log record but before responding to the pl'eparernessage.\n(d) A subordinate site for T fails after receiving a prepare message and force-writing a\nprepare log record but before responding to the prepare lnessage.\n(e) A subordinate site for T fails after receiving a prepare rnessage, force-writing an\nabort log record, and sending a no vote.\n(f) The coordinator site for T fails before sending a prepare lnessage.\n(g) The coordinator site for T fails after sending a prepare lllCssage but before collecting\nall votes.\n(h) The coordinator site for T fails after writing an abort log record but before sending\nany further rnessages to its subordinates.\n(i) The coordinator site for T fails after writing a comrnit log record but before sending\nany further rnessages to its subordinates.\n(j) The coordinator site for T fails after writing an end log record. Is it possible for the\nrecovery process to receive an inquiry about the status of T frolll a subordinate?\nExercise 22.15 Consider a heterogeneous distributed DBMS.\n1. Define the terms multidatabase system and gateway.\n2. Describe how queries that span multiple sites are executed in a rnultidatabase systern.\nExplain the role of the gateway with respect to catalog interfaces, query optirnizatiOll,\nand query execution.\n3. Describe how transactions that update data at rnultiple sites are executed in a lllulti-\ndatabase systern.\nExplain the role of the gateway with respect to lock rnanagernent,\ndistributed deadlock detection, Two-Phase COllnnit, and recovery.\n4. SChell1aS at different sites in a llnI1tidatabase systern are probably designed independently.\nT'his situation can lead to semantic heterogeneity; that is, units of rneasure rnay differ\nacross sites (e.g., inches versus centirneters), relatiolls containing essentially the SaIne\nkind of infonnation (e.g., eIllployee salaries and ages) rnay have slightly different schernas,\nand so on. vVhat ilnpact does this heterogeneity have on the end user? In particular,\nCOllunent on the concept of distributed data, independence in such a systcrIl.\nBIBLIOGRAPHIC NOTES\n\\Vork on parallel algorithrns for sorting and various relational operations is discussed in the\nbibliographies for Chapters 1:3 and 14. Our discussion of parallel joins follows [220], and our\ndiscussion of parallel sorting follows\n[22~3]. De\\Vitt and C,ray make the case that for future\nhigh perfornuHlce database systeuls, parallelisnl will be the key [221]. Scheduling ill parallel\nclata,b;:lse systenls is discusserl in [522).\n[49G] contains ,1 good collection of papers on query\nprocessing in ptlrallel daUthase systems.\n\n})aTallel and Distributed Databases\n771\nt\nTextbook discussions of distributed databa..C)€s include [78, 144, 580]. Good survey articles in-\nclude [85], which focuses OIl concurrency control; [637], which is about distributed databases\nin general; and [785], which concentrates on distributed query processing. Two InajaI' projects\nin the area were 8DD-1 [636] and R* [777]. Fragmentation in distributed databases is consid-\nered in [157, 207]. Replication is considered in [11, 14, 137,239,238, 388, ~385, :~35, 552, 600].\nFor good overviews of current trends in asynchronous replicatioll, see [234, 709, 772]. Papers\non view maintenance mentioned in the bibliographic notes of Chapter 21 are also relevant\nin this context. Olston considers techniques for trading of performance versus precision in a\nreplicated environment [571, 572, 573].\nQuery processing in the 8DD-1 distributed database is described in [88]. One of the notable\naspects of 8DD-1 query processing was the extensive use of 8emijoins. Theoretical studies\nof Semijoins are presented in [83, 86, 414].\nQuery processing in R* is described in [667].\nThe R* query optimizer is validated in [500]; much of our discussion of distributed query\nprocessing is drawn from the results reported in this paper. Query processing in Distributed\nIngres is described in [247].\nOptimization of queries for parallel execution is discussed in\n[297,323,383]. Franklin, Jonsson, and Kossman discuss the trade-offs between query shipping,\nthe more traditional approach in relational databases, and data shipping, which consists of\nshipping data to the client for processing and is widely used in object-oriented systerns [284].\nA good recent survey of distributed query processing techniques can be found in [450].\nConcurrency control in the 8DD-1 distributed database is described in [91]. Transaction man-\nagement in R* is described in [547]. Concurrency control in Distributed Ingres is described in\n[714]. [740] provides an introduction to distributed transaction rrlanagement and various no-\ntions of distributed data independence. Optimizations for read-only transactions are discussed\nin [306]. Multiversion concurrency control algorithrlls based on timestamps were proposed in\n[620]. Tirrlestamp-ba.;;ed concurrency control is discussed in [84, 356]. Concurrency control\nalgorithms based on voting are discussed in [303, 318, 408, 452, 732]. The rotating prirnary\ncopy scheme is described in [538]. Optimistic concurrency control in distributed databases is\ndiscussed in [660], and adaptive concurrency control is discussed in [488].\nTwo-Phase Commit was introduced in [466, 331]. 2PC with Presumed Abort is described in\n[546], along with an alternative called 2PC with Presum.ed Cornmit. A variation of Presumed\nCornrrlit is proposed in [465].\nThree-Phase COlnrrlit is described in [692].\nThe deadlock\ndetection algorithnls in R* are described in [567]. Many papers discuss deadlocks, for exaInple,\n[156, 243, 526, 632].\n[441] is a survey of several algoritluns in this area.\nDistributed clock\nsynchronization is discussed by [464].\n[3~33] argues that distributed clata independence is not\nalways a good idea, clue to processing and adlninistrative overheads. The ARIES algorithrl1\nis applicable for distributed recovery, but the details of how rnessages should be handled are\nnot discussecl in [544]. The approach taken to recovery in SDD-1 is described in\n[4~3]. [114]\nalso addresses distributed recovery. [444] is a survey article that discusses concurrency control\nand recovery in distributed systerIls. [95) contains several articles on these topics.\nIVlultidatabase systerns are discussed in [10, IV3, 230, 2:31, 242,476, 485, 519, 520, 599, 641,\n765, 797]; sec [112, 486, 684] for surveys.\n\n23\nOBJECT-DATABASE\nSYSTEMS\n..\nWhat are\nobject-databa,,~'3e systerlls and what new features do they\nsupport?\n..\nvVhat kinds of applications do they benefit?\n(..\n\\Vhat kinds of data types can users de.fine?\n(..\n\"Vhat are abstract data types and their benefits?\n..\n\\\\That is type inheritance and why is it useful?\n..\nWhat is the irnpact of introducing object ids in a database?\n...\nHow can we utilize the new features in database design?\ni\"'\"\nWhat are the new implelncntation challenges?\n..\n\\Vhat difFerentiates object-relational and object-oriented DBIvISs?\n...\nKey concepts: user-defined data types, structured types, collection\ntypes; data abstraction, rnethocls, encapsulation; inheritance, early\nand late binding of rnethods, collection hierarchies; object identity,\nreference types, shallow and deep equality\nwith Joseph M. HeHerstein\n[!n,'l'ucT'sily of C:fal!~foTT1,'iaBcTkeley\n--YOll knovv Iny Inethods,\n\\~l(ttson. A.pply theIn.\nArthur Conan ])oyle, The A1CTl1.0'iT8 of She'dock 1lolro,c;8\n772\n\n()bject-Database Systc1Tl'/3\n77~3\nH.{-~lational datalx:hse systeros support a sruaU, fixed collection of data types\n(e.g., integers, dates, strings),\\vhich h&9 proven adequate for trcLClitional appli-\ncation dOHHlins such as adruinistrative data processing.\nIn tHany applic<ltion\ndornains, hO'wever, rnuch 1nore eornplex kinds of data Blust be handled. I\"rypi_\ncally this cornplex data has been stored in OS file systerns or specialized data\nstructures, rather than in a DB.tvIS.Exanlples of dornains vvith cOJ.uplex data\ninclude cornputer-aided design and rnodeling (CA.D/CAlvf.), rnultilnedia repos-\nitories, and docurnent Hl8.Jlagernent.\nAs the arnount of data grows, the luany features offered by a DBIvISfor exarIl-\npIe, reduced application developnlent tilne, concurrency control and recovery,\nindexing support, and query capabilities·······becorue increasingly attractive and,\nultirnately, necessary.\nTo support such applications, a DBNIS HUlst support\ncornplex data types.\n()bject-oriented concepts strongly influenced efforts to\nenhance database support for cornplex data and led to the developrnent of\nobject-database systelus, \\vhich we discuss in this chapter.\nObject-database systerlls have developed along two distinct paths:\nII\nObject-Oriented Database Systems:\nObject-oriented database sys-\nterns are proposed as an alternative to relational systerlls and are ainled\nat application dornains where cODlplex objects playa centra,} role.\n1'he\napproach is heavily influenced by object-oriented prograrllrlling languages\nand can be understood as an atternpt to add DBMS functionality to a\nprograunning language environrnent. The ()bject Database :M:anagenlcnt\nGroup (()DMG) has developed a standard Object Data Model (ODM)\nand Object Query Language (OQL), which are the equivalent of the\nS(~I..I standard for relational database systerns.\nl1li\nObject-Relational ])atabase Systenls: ()bject-relational database s.ys-\nterns ca,n be thought of as an atternpt to extend relational databa...sc systerns\n\"lith the functionality necessary to support a broader class of applications\nand, in\nnlEUl~Y '\\THY-S, provide a bridge between the relational and object-\noriented paTadiguls. 1'he\nSC~I.I:1999 standard extends S(~L to incorporate\nsupport for the ol)ject-relationaJ rnode1 of data.\n\\Ve use clcronyuls for relational, object-oriented, and object-relational datrtbase\nrnanagernent systerns (RDBMS, OODBMS, ORJDBMS). In this chapter,\nvve focus 011\n()HI)B~ilSs and ernphasize ho\\v they can be vie\\ved CbS a develop-\nrnent of HJ)B1\\18s, rather than CbS an entjrely different paradigrn, as exernplified\nl)y the evolution of SCJL:1999.\nvVe concentrate on developing the fUlldarnental concepts rather than\npresc~nt­\ning S(~L:1999;\nsorn(~ of the features \\ve discuss axe not inc.luded in SC}L:1999.\n\n774\nCjHAPTER 2a t\nNonetheless, \\;\\le have chosen to ernpha\",<:;ize concepts relevant to SQL:1999 and\nits likely future extensions.\nvVe also try to be consistent with SCJL:1999 for\nnotation, although we occasionally diverge slightly for clarity. It is hnportant\nto recognize that the rnain concepts discussed are COIlllTIOn to both ()llDBJ\\;ISs\nand ()()DBNISs; we discuss how they are supported in the ODLjOQL standard\nproposed for ()ODB)\\t[Ss in Section 23.9.\nRDB1\\JIS vendors, including IBIVI, Inforrnix, and ()racle, are adding OIl-DBMS\nfunctionality (to varying degrees) in their products, and it is inlportant to\nrecognize how the existing body of knowledge about the design and inlple-\nrnentation of relational databa'3es can be leveraged to deal with the OrtDBMS\nextensions. It is also ilnportant to understand the challenges and opportunities\nthese extensions present to database users, designers, and irnplernentors.\nIn this chapter, Sections 23.1 through 23.6 introduce object-oriented concepts.\nThe concepts discussed in these sections are COlunlon to both OODBMSs and\nORDBJVISs. We begin by presenting an example in Section 23.1 that illustrates\nwhy extensions to the relational rnodel are needed to cope with some new\napplication dornains. 'This is used as a running exarnple throughout the chapter.\nWe discuss the use of type constructors to support user-defined structured data\ntypes in Section 23.2. We consider what operations are supported on these new\ntypes of data in Section 23.3. Next, we discuss data encapsulation and abstract\ndata types in Section 23.4.\nWe cover inheritance and related issues, such as\nrnethod binding and collection hierarchies, in Section 23.5. We then consider\nobjects and object identity in Section\n2~3.6.\nvVe consider how to take advantage of the new object-oriented concepts to do\nOI{DBMS database design in Section 23.7. In Section 23.8, we discuss SOHle\nof the new irnplernentation challenges posed by object-relational systerns. We\ndiscuss ()I)L and OQL, the standards for OODBMSs, in Section 23.9, and then\npresent a brief cornparison of ()R,DBMSs and OC)DBwISs in Section 2;t10.\n23.1\nMOTIVATING EXAMPLE\nAs a specific exarnple of the need for object-relational systcrlls, we focus on a\nnew business data processing probler.n that is both harder and (in our view)\nrnorc entertaining than the dollars and cents bookkeeping of previous decades.\nToday, cornpanies in industries such as entertainruent are in the business of\nselling bits; their basic corporate assets are not tangible products, but rather\nsoftwa.1'c artifacts such as video (l,nd audio.\n\\Ve consider the fictional Dinky Entertaiurnent Corupa,ny, a laxgc IIollywood\nconglornerate whose rllctin (\\'ssets are a collection of cartoon characters, espe-\n\n()b.ject-IJatabasc Systern8\n775\nj}\ncially the cuddly and internationally beloved IIerbert the \\VarIll.\nDinky ha..s\nseveral IIerbert the \\Vornlfihns, rnany of which are shown in theaters around\nthe world at any given tiTne. Dinky also rnakes a good deal of rnoney licensing\nHerbert's irnage, voice, and video footage for various purposes: action figures,\nvideo gaInes, product endOrSelllents, and so on.\nI)inky's database is used to\nlnanage the sales and leasing records for the various IIerbert-related products,\n&l) well a..s the video and audio data that rnake up IIerbert's lllany filIns.\n23.1.1\nNew Data Types\nThe basic problern confronting Dinky's database designers is that they need\nsupport for considerably richer data types than is available in a relational\nDBMS:\nII\nUser-defined data types: Dinky's assets include Herbert's iIllage, voice,\nand video footage, and these rnust be stored in the database. To handle\nthese new types, we need to be able to represent richer structure. (See Sec-\ntion 23.2.) Further, we need special functions to rnanipulate these objects.\nJior example, we may want to write functions that produce a cOlnpressed\nversion of an irnage or a lower-resolution image. By hiding the details of the\ndata structure through the functions that capture the behavior, we achieve\ndata abstract'ion, leading to cleaner code design. (See Section 23.4.)\n..\nInheritance: As the nurnber of data types grows, it is irnportant to take\nadvantage of the cornrnonality between different types. :For exarnple, both\ncOInpressed irnages and lower-resolution irnages are, at SOlne level, just\nilnages.\nIt is therefore desirable to inherit some features of iluage ob-\njects \\vhile defining (and later Inanipulating) cOInpressed irnage objects\nand lower-resolution irnage objects. (See Section\n2~j.5.)\n!Ill\nObject Identity: Given that seHne of the new data types contain very\nlarge instances (e.g., videos), it is iInportant not to store copies of objects;\ninstead, we IllUSt store Tejerence8, or po'inleTs, to such objects.\nIn turn,\nthis underscores the need for giving objects a unique object identity, vvhich\ncan be used to refer or 'point' to theln frorn elsewhere in the data.\n(See\nSection 2:3.6.)\nFlow lnight \\ve address these issues in an IlI)BNIS? \\Ve could store ilnages,\nvideos, and so on\nCh') BLC)Bs in current relational\nsyst(~lns. A binary large\nobject (BLOB) is just a long 8trea1n of bytes, and the DBNIS's support\nconsists of storing and retrieving BLC)Bs in such a rnanner that a user does not\nhave to worry about the size of the BLC)B; a 13LC}B can span several pages,\nunlike a tnulitional attribute.\nAll further processing of the BLC)B has to be\ndone by the user's clpplication progranl, in the host languclge in \\vhich the\n\n776\n(;lIAPTER ~3\nThe SQL/MM Standard: SQL/Nl:Nl is an eillerging standard that builds\nupon SQL:1999's new data types to define extensions of SQL:1999 that\nfacilitate handling of coruplex InultiInedia data types. SQL/lv!NI is a rnul-\ntipart standard.\nPart 1, SClL/rvUvfFl'arne\\vork, identifies the SQL;1999\nconcepts that are the foundation for SQLj1VllVI extensions.\nEach of the\nrelnaining parts addresses a specific type of ccnnplex data:\nFull Text,\nSpatial, Still Image, and Data Mining.\nSQL/lVllVi anticipates that\nthese l1e\\v coruplex types can be used in colurnns of tables ck') field values.\n.\n.\n..\n1\".....\nLarge Objects: SQL:1999 includes a new data type called LARGE OBJECT\nor LOB, with two variaJ1ts called BLOB (binary large object) and CLOB (char-\nacter large object).\nThis standardizes the large object support found in\nlnany current relational DBMSs.\nLOBs cannot be included in priruary\nkeys, GROUP BY, or ORDER BY clauses. rrhey can be cornpared llsing equa.l-\nity, inequality, and substring operations.\nA LOB has a locator that is\nessentially a unique id and allows LOBs to be rnanipulated without exten-\n.\n.\nsIve copYIng.\nL()Bs are typically stored separately froIn the data records in whose fields\nthey appear.\n1BlY1 DB2, InforInix, Microsoft SQL Server, Oracle 8, and\nSybase ASE all support LOBs.\nSQL code is ernbedded. This solution is not efficient because we are forced to\nretrieve all BLOBs in a collection even if rnost of the111 could be\nfiltt~red out\nof the ansvver by applying user-defined functions (within the I)B1118). It is not\nsatisfactory frorn a data consistency standpoint either, because the selnantics\nof the data now depends heaNily on the host la,nguage application code and\ncannot be enforced by the I)BlVfS.\nAs for structured types and inheritance, there is siInply no support in the\nrelational Dlodel.\nVVe are forced to Ina.p data 'with such cOlnplex structure\ninto a collection of flat tables.\n(vVe saw exarnples of such rnappings \"vhen \\ve\ndiscussed the tnlJlS1ation frorH Ell diagrarns vvith illheritance to relations in\nC:hapter 2.)\nrrhis application clearly requires features not available in the relational Inode1.\nAs an illustration of these features, Figure 2:3.1 presents S(~L:1999 :DDL state-\nrnents for a l)Ortion of Dinky's ()HJ)I31VIS sehelna used in subsequent excunples.\nAlthough the 1)})L is very sirnilar to that of a traditional relational systeru,\nSOln(~ irnportant distinctions highlight thene\\v data rnodeling capabilities of\n,Ul ()JII)B1VlS. J\\ quick glance at the l)DL staternents is sufficient for now; we\nstudy thenl in detail in the next section, after presenting S0111e of the basic\n\n()b.ject-1Jatabase 8ystenM)\nconcepts that our sanlple application suggests are needed in a next-generation\nl)BrvlS.\n1. CREATE TABLE FralnC\\'3\n(}rarneno integer, 'iTrulge jpeg_image, categoT'Y integer)~\n2. CREATE TABLE Categories\n(c'id integer, narne text 1 If:aSL.pricc float, c01n:rncnts text);\n:3. CREATE TYPE theater_t AS\nROW( tno integer, n,arne text, address text, phone text)\nREF IS SYSTEM GENERATED;\n4. CREATE TABLE Theaters OF theater_t REF is tid SYSTEM GENERATED;\n5. CREATE TABLE Nowshowing\n(jilnL integer, theater REF(theater.,t) SCOPE rrheaters, start date,\nend date);\n6. CREATE TABLE FilIns\n(filrnno integer, l/if;le text, staTs VARCHAR(25)\nARRAY [10]),\ndirector text, budget float);\n7. CREATE TABLE Countries\n(narnc text, boundary polygon, population integer, language text);\nFigure 23.1\nSQL:1999 DDL Staternents for Dinky Schema\n23.1.2\nManipulating the New Data\nThus far, we described the new kinds of data that rnust be stored in the Dinky\ndatabase.\nWe have not yet said anything about how to use these nevv types\nin queries, so let us study two queries that I)inky's database needs to support.\nThe syntax of th(~ queries is not critical; it is sufficient to understand what they\nexpress. v\\le return to the specifics of the queries' syntax later.\n()ur first challenge COIn8S frorn the\nC~log breakfast cereal cornpany. Clog pro-\nduces a cereal called I)elirios and it vvants to lease an irnage of IIerbert\nthE:~\n\\\\TorIn in front of a sunrise to incorporate in the I)elirios box design. A query\nto present (1, collection of possible irnages (uld their le<ls8 prices can be ex.pressed\nin SC~L-like syntax as in\nI4'iglln~ 2:3.2. I)inky has a nUJn1>er of rnethodsvvritten\nin an irnperative language like .Jnva a,nd registered \\vith the datal)Clse systern.\nThese\nlIH~thods can be used in queries in the sallIe way as built-ill Hletllods,\nsuch as =.\n,--~, <, >, are used in a relational language like S(~L. 1'he thwrnb-\nnail IJlethod in the Select clausf~ produces a srnaU version of its full-size input:\nbnnge.\nrrhe i:L8'il'nri.se rnethod is a boolean function that analyzes an irnage\nand returns tr'ue if the inHtge contains a sun.rise; the is_h(Tbcrt Inethod returns\nh\"u.c if the\nirnag(~ contains a picture ()f l1erbert. rrhe query produces the frarl1c\n\n778\n(jHAPTER 23\ncode nUlnber~ irnage thurnbnail, and price for all frames that contain Herbert\nand a sunrise.\nSELECT F.fnuneno, thulnbnail(F.irnage), C.lease_price\nFROM\nFralnes F, Categories C\nWHERE\nF.category = C.cid AND is.Bllnrise(F.irnage) AND isJlerbert(F.inlage)\nFigure 23.2\nExtended SQL to Find Pictures of Herbert at Sunrise\nThe second challenge carnes froIn Dinky's executives. They know that Delirios\nis exceedingly popular in the tiny country of A.ndorra, so they want to lIlake\nsure that a number of Herbert filIns are playing at theaters near Andorra when\nthe cereal hits the shelves. To check on the current state of affairs, the execu-\ntives want to find the 11aInes of all theaters showing Herbert fihns within 100\nkilorneters of Andorra. Figure 23.3 shows this query in an SQL-like syntax.\nSELECT\nFROM\nWHERE\nN.theater··-->na1ne, N.theater-> address, F.title\nNowshowing N, Filrns F, Countries C\nN.film = F.filrnno AND\noverlaps(C.bollndary, radius(N.theater->address, 100)) AND\nC.narne::::: 'Andorra' AND 'Herbert the Worm' = F.stars[l]\nFigure 23.3\nExtended SQL to Find Herbert Films Playing near Andorra\nThe theater attribute of the Nowshowing table is a reference to an object in\nanother table, which has attributes narne,\naddr'(~88, and location. This object\nreferencing allows for the notation N. theater-> narne and N. theater..···> address,\neach of which refers to attributes of the theater_t object referenced in the\nNowshowing row N. The stars attribute of the tUrns table is a set of narnes of\neach [ibn's stars. The r'O,(1'i'u,8 nlethod returns a circle centered at its first argu-\nlllent with radius equal to its second argurnent.\n~rhe overlaps rnethod tests\nfor spatial overlap. Nowshowing and Filrns are joined by the equijoin clause,\n\\vhile Nowshowing and Countries are joined by the spatial overlap clause. The\nselections to 'Andorra' and fiInls containing 'Herbert the vVorrn' cornplete the\nquery.\nrrhcse two object-relational queries are sirnilar to SQL-92 queries but heLve SOlllC\nunusual features:\nl1li\nUser-Defined Methods:\nUser-defined abstract types are rnanipulated\nvia their 1nethods, for exalnple, i.'Lhcrbert (Section 23.2).\nII\nOperators for Structured Types:\nA.long with the structured types\navailable in the deLta rnodel, ()R,DBMSs provide the natural Inethods for\nthose types.\nFor exarnple, the ARRAY type supports the standard array\n\n()bject-Database SYStC1718\n779\nIi>\noperation of accessing an array elenlent by specifying the index;\nI?~stars[l]\nreturns the first elernent of the array in the staTs cohllnn of film F (Sec-\ntion 23.:3).\n11III\nOperators for Reference Types: Reference types are dereferenced via.\nan arrow (---»\nnotation (Section 23.6.2).\nIb suuullarize the points highlighted by our 1110tivating exanlple, traditional\nrelational systenls offer liInited flexibility in the data types available. Data is\nstored in tables and the type of each field value is lirnited to a siulple atornic type\n(e.g., integer or string), with a sl11all, fixed set of such types to choose frarn.\nThis lirnited type systern can be extended in three Inain ways:\nuser-defined\nabstract data types, structured types, and reference types.\nCollectively, we\nrefer to these new types &'S complex types. In the rest of this chapter, we\nconsider how a DBl\\!IS can be extended to provide support for defining new\ncomplex types and rnanipulating objects of these new types.\n23.2\nSTRUCTURED DATA TYPES\nSQL:1999 allows users to define new data types, in addition to the built-in types\n(e.g., integers).\nIn Section 5.7.2, we discussed the definition of new distinct\ntypes. Distinct types stay within the standard relational model, since values of\nthese types rnust be atornic.\nSQL:1999 also introduced two type constructors that allow us to define new\ntypes with internaJ structure.\n~rypes defined using type constructors are called\nstructured types.\nThis ta.kes us beyond the relational model, since field\nvalues need no longer be atornic:\n11III\nRDW(n1 Il, ... , nn t,n): A type representing a row, or tuple, of n fields \\vith\nfields 11,1, ... , Tl'n of types Ll, ... ,\"tn respectively.\nII\nbase ARRAY [iJ):\nA type representing an array of (up to) i base-type\niterns.\nThe theater_t type in Figure 23.1 illustrates the\nIH~\\V ROW data type.\nIn\nSQL:1999, the ROW type hetS ()., special role because every table is a collection of\nro\\vs ...·..·every table is a set of 1'o\\vs or a. rnultiset of rc)\\vs. Values of other types\ncan appear only a..s field values.\nThe staT/., field of table Filrns illustrates the ne,v ARRAY type. It is an array of\nupto 10 elernents, ea.ch of \\vhich is of type VARCHAR(25). Note that 10 is the\nrnaxirnurn nurnber of el(~rnents in the array; a.t <lny tiTne, the array\n(unlik(~, say,\n\n780\n(\n-~\n.. ;-\n.\n?')\n.JHAP fER\n~)\nSQL:1999 Structured Data Types: Several conunercial\nsystenls~ in-\ncluding IBw! DB2, Infonnix tTDS, and Oracle 9i support the ROWand ARRAY\nconstructors. rrhe listof, bagof, and setof type constructors are :not in-\ncluded in\nS(~L:1999.\nNonetheless, cOIlunereiaJ systerIls support sorne of\nthese constructors to varying degrees.\n()racle supports nested relations\nand arrays, but does not support fully cornposing these constructors. In-\nfOI'rnix supports the setof, hagof, and Ustof constructors and allows thern\nto be cornposed. Support in this area varies \\videly across vendors.\nin C) can contain fewer elenlcnts. Since SQL:1999 does not support rnultidi-\nInensional arrays, vector rnight ha,ve been a rnore accura,te narne for the array\nconstructor.\nThe power of tyP(~ constructors cornes froIn the fact that they can be cornposed.\nThe following row type conta,ins a field that is an array of at Inost 10 strings:\nROW(filrnno: integer, staT's: VARCHAR(25) ARRAY [10])\nThe row type in\nSC~L:1999 is quite general; its fields can be of any SQL:1999\ndata type.\nUnfortunately, the arra.y type is restricted; elernents of an array\ncannot be arrays thcrnselves. Therefore, the following definition is illegal:\n(integer ARRAY [5]) ARRAY [10]\n23.2.1\nCollection Types\nS(~L: 1999 supports only the ROW a,nel ARRAY type constructors. Other COUUllon\ntype constructors include\nIII\nlistof(base): A. type representing\n<:1, sequence of base-t~ype itcrllS.\nIi\nsetof (base): .l\\ type\nrer)l'(~senting a set of base-type HeIns.\nSets cannot\ncontain duplicate elen1cn1,8.\nII\nbagof(base): j\\ type representin.g a, bag or rnv,ltisct of base-type iterns.\nTypes llsing listof, ARRAY, bagof, or setof as the outennost type constructor\na.re sornetirnes referred to a,s collection types or bulk data types.\n\n()b)ect- Databa,se Sy,steIns\n781\nt\n~rhe lack of support for these collection types is recognized as a weakness of\nSQL:1999's support for cornplex objects and it is quite possible that SODle of\nthese collection types \"'lill be added in future revisions of the SQL standard. 1\n23.3\nOPERATIONS ON STRUCTURED DATA\nrIhe I)131V18 provides built-in Inethods for the types defined using type con-\nstructors. These lnethods are analogous to built-in operations such as addition\nand rIlultiplication for atcnnic types such as integers. In this section we present\nthe Illethods for various type constructors and illustrate ho\\v SC~L queries can\ncreate and rnanipulate values \\vith structured types.\n23.3.1\nOperations on Rows\nGiven an iteul ri whose type is ROW(n1t1 , ... , TLn t n ), the field extraction rnethod\nallo\\vs us to\n~l,ccess an individuaJ field nk llsing the traditional clot notation\n'i.nk. If ro\\v constructors are nested in a type definition, dots rnay be nested to\naccess the fields of the nestE\"d row; for exarnple i.'fLk.n1/. If we have a collection\nof rows, the dot notation gives us a collection as a result. :For exarnple, if i is\na list of rows, i.nk gives us a list of itcrns of type tTl; if i is a set of rows, i.nk\ngives us a set of iterns of type tn.\n[This\nrH~ste(l-dot notation is often called a path expression, because it de-\nscribes a path through the nested structure.\n23.3.2\n()peratiolls on Arrays\nArray types support an 'alT<lY index' rnethod to allow 11sers to access array\niterns at a, particular offset. A. postfix 'square bracket' syntax is usually used.\nSince the nuruber of elernents can v<try, there is an operator (CARDINALITY) that\nreturns the nU1nbe1' of elerIlents ill\ntl1E~ array. '1'he varia,hle nurnl)er of elernents\nalso rn.otivates an operator to C:Ollcatenate t\\VO arrays.\nrrh(~ following exanlple\nillustrates these operations on S(~L:1999 arr(\\ys.\nSELECT F.fillnIlo, (F.staTs II ['Brando', 'Pacino'])\nFROM\nFilrnsF\nWHERE\nCARDINALITY(F.stars) < :3 AND F.stars[l]::::::'Iledford'\n1According to Jinl tvlelton, the editor of the SQL:19DD standard, these collection types 'l}/ere con-\nsidered for inclusion but omitted because sorne problt·mls with their specifications were discovered too\nlate for correction in the SQL: 1999 timc-fr;lrne.\n\n782\nCHAPTER 33\nFor each fibn with Redford ctS the first star2 and fe\"wer than three stars, the\nresult of the query contains the fihn'8 array of stars concatenated \\vith the\narray containing the two elcrnents 'Brando' and 'Pacino'. Observe ho\\\\r a value\nof type array (containing Brando and Pacino) is constructed through the use\nof square brackets in the SELECT clause.\n23.3.3\nOperations on Other Collection Types\nAlthough only arrays are supported in SQL:1999, future versions of SQL are\nexpected to support other collection types, and we consider what operations are\nappropriate over these types of data. provide such operations. Our discussion\nis illustrative and not Ineant to be cOlnprehensive.\nFor exarnple, one could\nadditionally allow aggregate operators count, surn, avg, rna.T, and rnin to be\napplied to any object of a collection type with an appropriate base type (e.g.,\nINTEGER). ()ne could also support operators for type conversions. For exaInple,\none could provide operators to convert a rnultiset object to a set object by\nelirninating duplicates.\nSets and Multisets\nSet objects can be cornpared using the traditional set methods c,~, =,:2, ~.\nAn iteln of type setof (faa) can be cornpared with an iteln of type faa using\nthe E rnethod, as illustrated in Figure 23.3, which contains the cornparison\n'.fferbert the W O'T\"rn' E F. stars. T\\vo set objects (having elernents of the saIne\ntype) can be cornbined to forIlI a new object using the u, n, and --- operators.\nEach of the Inethods for sets can be defined for Inultisets, taking the nUlnber of\ncopies of elernents into account. The U operation silnply adds up the nurnber\nof copies of an elernent, the n operation counts the lesser nU1nbel' of tirnes a\ngiven elernent appears in the two input rnultisets, Etuel _. subtracts the nurnber\nof ti1nes a given e1ernent appears in the second lnultiset frorn the nUlnber of\ntinles it appears in the first Inultiset.\nFor exarnplc, using rnultiset scrnantics\nU ({ 1 2 2· 2}\n{2 2 ')} ') --' {I ') 2 2 2 2 ,-)}. r1 ({\"1 2 ') '2'}\n{') ') ')}') -\n{2 2}' , . I\n\"\n,\n\"\n,\n.\n,\n,t)\n..-\n, .....\"\",d,\nI\n., ,.....,\n,\n.....,..... ,t.J\n--'.'\n,d,11C\n({1,2,2,2L\n{2,2,~3}) == {1,2}.\nLists\nl'raditiona.l list operations include head, \\vhich returns the first ele1nent; tail,\nvvhich returns the list obtained by rCIIloving the first elernent; prepend, which\n--_._\n.\n2Note that the first e1crnent in an SQL arra.y has index vaJue 1 (not 0,\n<:lAoS in sorne IanguH,ges).\n\n()b.iect-Database 8ysterns\ntakes an elcrnent and inserts it (1.S the first elernent in a list; and append, which\nappends one list to another.\n23.3.4\nQueries Over Nested Collections\n\\Ve no\\v present SOHle exar11ples to illustrate ho\\v relcltions that contain nested\ncollections can be queried, using SQL syntax. In particular, extensions of the\nrelational rnodel with nested sets and rnultisets have been \\videly studied and\n\\ve focus on these collection types.\n\\Ve consider a variant of the FihIIS relation from Figure 23.1 in this section,\nwith the staTs field defined as a setof\n(VARCHAR [25] ), rather than an array.\nEach tuple describes a filrn, uniquely identified by filrnno, and contains a set\n(of stars in the filrn) a..'3 a field value.\nOur first exarnple illustrates how we can apply an aggregate operator to such a\nnested set. It identifies filrns with r11or8 than two stars by counting the nurnber\nof stars; the CARDINALITY operator is applied once per FilnIs tuple.\n~~\nSELECT F.filmno\nFROM\nFilrns F\nWHERE\nCARDINALITY(F.stars) > 2\nOur second query illustrates an operation called unnesting.\nConsider the\ninstance of Filrns shown in Figure 23.4; we have olnitted the direcloT and budget\nfields (included in the Filnls schenHl in Figure 23.1) for simplicity. A flat version\nof the saIne inforrna.tion is shown in Figure 23.5; for each filrn and star in the\n£ibn, we have a tuple in Filrns_flat.\n--r-----·-·.._--.--:..-- -\n;]\nstar's\n.\n.\n.\n~~\n,-\n98\nCasablanca\n{Bogart, Bergluan}\n54\nEaTth vVorrns Are Juicy\n{Herbert, vVanda}\n['··---··--···--······---·-1--·-·--·---·-------\nfil'mno\ntitle\nFigure 23.4\nA Nested Relation, Films\n1\"11e follc)\\ving quer,r generates the instance of Fihns~flat frcnn Fihns:\nSELECT F Jilrnno, F.title, S AS star\nFROM\nFilrnsF,F.stars AS S\n3SQL: 1999 does not support set or rnultiset values, as we noted earlier. If it did) it would be natural\nto allow the CARDINALITY operator to be applied to a set-vaJuc to count the nUluber of elernents; we\nhave used the operator in this spirit.\n\n784\n\"Figure 23.5\nA Plat\nVersion,Films~fla,t\nThe variable F is successively bound to tUI>les in Filrns. and for each value\nv\n.\nJ\nof I?, the vaTiable S is successively bound to the set in the staTs field of .F'.\nConversely, we Inay \"vant to generate the instance of F'ilrns frorn FilIns_fiat. We\ncan generate the Filrns instance using a, generalized fonn of SQL '8 GROUP BY\nC011struct, as the following query illustrates:\nSELECT\nF.fihnno, F.title, seLgen(F.star)\nFROM\nFihns.Jlat F\nGROUP BY F.fihnno, F.title\nThis oxaluplc\nintroducE~s (1, ne'w operator seLgen, to be used with GROUP BY,\nthat requires sorne explanation. The GROUP BY clause partitions the FihnsJlat\ntable by sorting on the .filTn:no attribute; all tuples in a given partition have the\nsanle filrnno (and therefore the sarne title). Consider the set of values in the star\ncohunn of (1, given partition. In an SC~L-92 query, this set rnust be surnluarized\nby applying an aggregate operator such as COUNT. Now that we allow relations\nto contain sets as field values, however, \\ve can return the set of staT values as\na field value in a single anSWE~r tuple; the ans\\ver tuple also contains the fihnno\nof the corresponding partition.\nrrhe set-gen operator collects the set of star\nvalues in a paTtition and creates a set-valued object. This operation is called\nnesting.\nvVe can irnagine shnihtr generator functions for creating Inuitise1's,\nlists. and so on. IIo\\vever, such generators are not included in SQL:1999.\n23..4\nENCAPSULATION AND ADTS\nConsicler the Fnunes table of Figure 2:3.1.\nIt lUls a colun111 ,zrnage of type\njpeg~image, vlhich stores a, cOlnpressed iUHlge representing a single frarne of a\nfihn.I'he jpeg_image tYI)C is not one of the DBlVIS's l>uilt-in types and \\VEtS\nd(~fined 1)y a user for the I)inky application to store ilna,ge data cornpressed\nllsingth(~ JPEC; stanclard. As another exarllple, the Countries table defined in\nLine 7 of Figure 2:3.1 h;:lS a colurnn boundaT'Y of t,ype polygon, \\v111ch contains\nr(~presentationsof the shapes of countries' outlines on a vvorld rnap.\n\n()b,ject:-Database Syst:en18\n.Allowing users to define arbitrary nc\\v data types is a key feature of ()RDBf\\,1Ss.\n'1\"he DBrvlS alh:)\\vs users to store and retrieve objects of type jpeg_image, just\nlike an object of any other type, such as integer.\nNo\\v atornic data types\nusually need to have\nt~ype-specific operations defined by the user 'who creates\nthern. For\nexanlple~ one rnight define operations on an irnage data type such\na\"s compress, rotate, shrink, and crop. rrhe ccnnbination of an atolIlic data\ntype and its associated rnethods is called an abstract data type, or ,A,DT.\nTraditional S(~L COlnes with built-in ..l\\DTs, such as integers (-with the a\",ssoci-\nated arithnletic rnethods) or strings (with the equality~ cornparison, and LIKE\nlllethods). Object-relational systerns include these ADT's and also allow users\nto define their o\\vn ADTs.\nThe label abstract is applied to these data types because the database systerIl\ndoes not need to InlOW how anAD1~'s data is stored nor ho\\v the ADT's rneth-\nods work. It rnerely needs to know \\vhat rnethods are availa,ble and the input\nand output types for the rnethods. I-Eding ADT internals is called encapsu-\nlation.4 Note that even in a. relational systern, atolnic types such as integers\nhave associated rnethods that encapsulate the1n. In the case of integers, the\nstandard Inethods for the ADT are the usual aritlunetic operators and COll1-\nparators. To evaluate the addition operator on integers, the database systenl\nneed not understand the laws of addition it l11erely needs to know how to\ninvoke the addition operator's code and what type of data to expect in return.\nIn an object-relational systenl, the Silllplification due to encapsulation is critical\nbecause it hides any substantive distinctions between data types and allows an\nOR,DB1VIS to be iInplernented \\vithout anticipating the types and rnethods that\nusers Inight want to add. For exarnple, (l,dding integers and overlaying irnages\ncan be treated unifonnly by the systern, vvith the only significant distinctions\nbeing that different code is invoked for the t\\VO operations and differently typed\nobjects are expected to be returned froIll that code.\n23.4.1\nDefining Methods\nrro register a\nrH~\\V rnethod for a user-defined data type, users rnust \\vrite the\ncode for the nlcthod and then infor1n the datalHlse systcrI1 about the Inethod.\n'rhe cod.e to be \\i\\rritten depends on the languages supported by the DBlVIS\nand, possibly, the operating systerH in question. For eXC1ruple, the OHI)Bl\\;IS\nInay handle J a\\ta, co(h,~ in the Linux operating systern. In this case, the lnet,hod\ncode nlu,st be \\vritten iII Java and cOlnpiled into a .Java bytecode file stored in.\na Linux file s~vsteln. 'Then an SC~L-st~yle luethod registration eOllunand is given\nto the ()I:ll)Bl\\/lS so that it recognizes the\nnc~\\v rnethod:\n4SOlue OIlIJBivISs actually refer to Aryl's as opaque types beca,use they are enci::tpsula.ted a,nd\nhence one cannot see their details.\n\n786\nCHAPTER\n~:3\nr:\n-'\"- _._-\"\".,.-\n-._-\n,-~\n-\n-----.\nPackaged ORDBMS Extensions:\nDeveloping a set of user-defined\n.\ntypes and rnethods for a particular application·······-say, iInage management·,·,·,,·\nI\ncan involve a significant aIIlount of work and dornain-speeific expertise. As\na result, most ORDBMS vendors partner with third parties to sell prepack-\naged sets of ADrrs for particular domains. Inforn1ix calls these extensions\nDataBlades, Oracle calls theln Data Cartridges, IBNI calls thern DB2 Ex-\ntenders, and so on. These packages include the ADT 11lethod code, DDL\nscripts to automate loading the ADTs into the system, and in some cases\nspecialized access methods for the data type. Packaged ADT extensions are\nanalogous to the class libraries available for object-oriented programIning\nlanguages: They provide a set of objects that together address a COlnnlon\ntask.\nSQL:1999 has an extension called SQL/MIVI that consists of several inde-\npendent parts, each of which specifies a type library for a particular kind\nof data. The SQL/MM parts for Full-Text, Spatial, Still Iillage, and Data\nMining are available, or nearing publication.\nCREATE FUNCTION is_sunrise(jpeg_image) RETURNS boolean\nAS EXTERNAL NAME '/a/b/c/dinky.class' LANGUAGE 'java';\nThis statenlent defines the salient aspects of the lllethod: the type of the asso-\nciated ADT, the return type, and the location of the code. Once the method is\nregistered, the DBNIS uses a Java, virtual lnachine to execute the code5 . Fig-\nure 23.6 presents a nUlnber of rnethod registration cOllllnands for our Dinky\ndatabase.\n1. CREATE FUNCTION thumbnail(jpeg_image) RETURNS jpeg_image\nAS EXTERNAL NAME '/a/b/c/dinky.class' LANGUAGE 'java';\n2. CREATE FUNCTION is_sunrise(jpeg_image) RETURNS boolean\nAS EXTERNAL NAME '/a/b/e/dinky.class' LANGUAGE 'java';\n3. CREATE FUNCTION isJnerbert(jpeg_image) RETURNS boolean\nAS EXTERNAL NAME '/a/b/c/dinky.class' LANGUAGE 'java';\n4. CREATE FUNCTION radius (polygon, float) RETURNS polygon\nAS EXTERNAL NAME '/a/b/c/dinky.class' LANGUAGE 'java';\n5. CREATE FUNCTION overlaps (polygon, polygon) RETURNS boolean\nAS EXTERNAL NAME '/a/b/c/dinky.class' LANGUAGE 'java';\nFigure 2:1.6\nIVlethod H.,egistration Conunands for the Dinky Da.taha..se\n._--_.\n51n the case of non-portable cOIl1piled code\nwritten, for example, in a, language like C++\" .....···the\nD131v18 uses the operating; system's dynamic linking facility to link the method code into the databa..se\nsystem so that it can be invoked.\n\n()bject-Database Systerns\n787\n~\nrrype definition statelnents for the user-defined atornic data types in the Dinky\nscherna are given in Figure 23.7.\n1. CREATE ABSTRACT DATA TYPE jpeg_image\n('inte'rnallength = VARIABLE, input =\njpeg~n, output = jpeg_out);\n2. CREATE ABSTRACT DATA TYPE polygon\n(internallength = VARIABLE, input = polyjn, 01LtP'Ut == poly_out);\nFigure 23.7\nAtomic Type Declaration Commands for Dinky Database\n23.5\nINHERITANCE\nWe considered the concept of inheritance in the context of the ER, model in\nChapter 2 and discussed how ER diagrarns with inheritance 'were translated\ninto tables. In object-database systems, unlike relational systerns, inheritance\nis supported directly and allows type definitions to be reused and refined very\neasily. It can be very helpful when modeling similar but slightly different classes\nof objects. In object-database systerns, inheritance can be used in two ways: for\nreusing and refining types and for creating hierarchies of collections of sirnilar\nbut not identical objects.\n23.5.1\nDefining Types with Inheritance\nIn the Dinky database, we rnodel rnovie theaters with the type theater .._t.\nDinky also wants their database to represent a new rnarketing technique in the\ntheater business: the theater-cafe, which serves pizza and other rneals while\nscreening movies.\nrrheater-cafes require additional inforrnation to be repre-\nsented in the database. In particular, a theater-cafe is just like a theater, but\nhas an additional attribute representing the theater's IIlenu. Inheritance allows\nus to capture this 'specialization' explicitly in the database design with the\nfollowiIlg DDL staternent:\nCREATE TYPE theatercafe_t UNDER theater_t (rn,enu text);\nThis staternent creates a new type, theatercafe_t, which has the sarne at-\ntributes and rnethods <:l.S theater_t, plus one Etdditional attribute rnenu of type\ntext. lVlethocl,s defined on theater_..t apply to objeets of type theatercafe_t,\nbut not viee versa.\nvVe\nsa~y that theatercaf e_t inherits the attributes and\nrnethods of theater_t.\nNote that the illherita,nce rnechanisrll is not rnerely a rnacro to shorten CREATE\nstaternents.\nIt creates an explicit relationship in the databa..se between\nth(~\nsubtype (theatercafe_t) and the supertype (theater_t ):An object of the\n\n788\nCJHA.prrElt 23\ns'ublypc is also considered to be an object of the B'Upert:lJpe.\nThis treatlnent\nIneans that an.y operations thi:tt apply to the supertype (nlcthods as \\ven as\nquery operators, such as projection or join) also apply to the subtype. 1:'his is\ngenerall.y· expressed in the follo\\ving principle:\nThe Substitution Principle:\nC~iven a supertype A. and a subtype\nJj, it is always possible to substitute an object of type B into a legal\nexpression vvritten for objects of type A, without producing type errors.\nThis principle enables easy code reuse because queries and Inethodswritten for\nthe supert)rpe can be applied to the subtype vlithout I1lodification.\nNote that inheritance can also be used for atc)luic types, in addition to ro\\v\ntypes.\nGiven a supertype image_t with rnethods title(), nurnber._of_colors(),\nand d'isplay(), we can define a subtype thumbnail_image_t for slllall irnages\nthat inherits the rnethods of image_to\n23.5.2\nBinding Methods\nIn defining a subtype, it is sornetiInes useful to replace a rnethod for the Sll-\npertype with a new version that operates differently on the subtype. Consider\nthe image_t type and the subtype jpeg_image_t frorH the Dinky database.\nlJnfortunately, the display() rnethod for standard .iInages does not work for\nJPEG irnages, which are specially cOlnpressed.\nTherefore, in creating type\njpeg_image_t, we write a special display() rnethod for JPEG iruages and reg-\nister it with the database systern using the CREATE FUNCTION cOIIuuand:\nCREATE FUNCTION di8play(jpeg~image)\nRETURNS jpeg_image\nAS EXTERNAL NAME '/a/b/c/jpeg.class' LANGUAGE 'java.';\nIlegistering a ne\\v rnethod \\vith the sarne HeHne as an aIel rnethod is called\noverloading the luethod narne.\nBecause of over1oading~ the systern Inust understand 'which rnethod is intended\nin a, particular expression. For exarnple, when the systern needs to invoke the\ndisplay() rnethod on an object of type j peg ....image _t, it uses the specialized\ndisplay rnethocL 'VIlcn it needs to invoke display on an object of type image __t\nthat is not otherwise subtyped, it invokes the standard display Inethod. The\nprocess of d(~ciding which rnethod to invoke is called binding the rnethod to\ntJIC ol)ject. In certa.in situations, this binding CHJl be done \\vhen (tIl expn~ssiorlis\npaTsed (early binding), but in other\ncct.~es the 1n08t specific type of HJl object\nC(1,nnot be known until rl.ln-tinle, so the rnethod cannot be l)ound until then\n(late binding) . Late birlding fa,cilties acId flexibility but can rnake it harder\n\n()bject-Dat;aba8c8y8terIL8\n7~9\nfor the user to rea)f:;on about the Inethods that get invoked for a given query\nexpreSSIon.\n23.5.3\nCollection Hierarchies\nType inheritance was invented for object-oriented progranuning languages, and\nour discussion of inheritance up to this point differs little £roln the discussion\none Inight find in a book on an object-oriented language such as C++ or Java.\nI-I()\\vever, because database systerns provide query languages over tabular data\nsets, the lnechanisnls fronl progrannning languages are enhanced in object\ndatabases to deal with tables and queries as well.\nIn particular, in objeet-\nrelational systelIls, we can define a table containing objects of a particular\ntype, such &'-; the Theaters table in the Dinky sehenla. Given a new subtype,\nsuch as theatercafe_t, we would like to create another table Theater_cafes to\nstore the inforrnation about theater cafes. But, when writing a query over the\nTheaters table, it is sornetirnes desirable to ar;;k the saIne query over the rrhe-\nater_cafes table; after all, if we project out the additional C01UlllI1S, an instance\nof the Theater_cafes table can be regarded as an instance of the Theaters table.\nR,ather than requiring the user to specify a separate query for each such table,\nwe can infonn the systern that a new table of the subtype is to be treated as\npart of a table of the supertype, with respect to queries over the latter table.\nIn our exalnple, we can say\nCREATE TABLE Thea,ter_Cafes OF TYPE theatercafe ....t\nUNDER Theaters;\nThis staternent tells the systern that\nqu(~ries over the Theaters table should\nactually be run over all tuples in both the rrheaters and rrheater_Cafes tables. In\nsuch cases, if the subtype definition involves rnethod overloading, late-binding\nis used to ensure that the appropriate rnethods are called for each tuple.\nIn general, t11.e UNDER chl11se can be used to genera,te an arbitrary tree of ta-\nbles, called\n(1, collection hierarchy.\n(~ueries over a particular tal)le T in the\nhierarchy are run over all tuples in rr and its descendants. Sornetirnes, a user\nrnaywant the query to nUl ollly on rr and not on the descencl<tnts; additiona1\nsyntax, for exaInple, the key\\vord ONLY, can be used in the query's FROM clause\nto fl,chieve this effect.\n23~6\nOBJJ1:Cl-'S, OIDS, AND REFERENCE TYPES\nIn object-elatabase systerns, data objects can be given an object identifier\n(aid), \\vhich is sotne value that is unique in the database across tirne.\nThe\n\n790\nCHAP1'ER 243\nr---- -\n~-················ -_ -.-................\n- ------.....\n.-----...\nI\nI\nOIDs: IBNl DB2, Inforul..ix lJDS, and Oracle 9i support REF types.\nI\nL--._.\n.__._..__..\n.----_~ __ __\n_\n__\n.\nDBl\\iIS is responsible for generating aids and ensuring that an oid identifies an\nobject uniquely over its entire lifetillle. In SOHle systenls, all tuples stored in\nany table are objects and autornatically assigned unique oids; in other systenls,\na user can specify the tables for 'which the tuples are to be c1ssigned aids. Often,\nthere are also facilities for generating oids for larger structures (e.g., tables) as\nwell as slnaller structures (e.g., instances of data values such as a copy of the\ninteger 5 or a .TPEG ilnage).\nAn object's aid can be used to refer to it from elsewhere in the data. An oid\nhas a type similar to the type of a pointer in a progralnll1ing language.\nIn SQL:1999 every tuple in a table can be given an aid by defining the table\nin ternlS of a structured type and declaring that a REF type is associated with\nit, a,'3 in the definition of the Theaters table in Line 4 of Figure 23.1. Contrast\nthis with the definition of the Countries table in Line 7; Countries tuples do\nnot have associated aids. (SQL:1999 also assigns 'oids' to large objects: This\nis the locator for the object.)\nREF types have values that are unique identifiers or aids.\nSQL:1999 requires\nthat a given REF type must be associated with a specific table. For exalnple,\nLine 5 of Figure 23.1 defines a cohllnn theater of type REF(theater_t).\nThe\nSCOPE clause specifies that iterns in this colurnn are references to rows in the\nrrheaters table, which is defined in Line 4.\n23.6.1\nNotions of Equality\nThe distinction between reference types and reference-free structured types\nraises another issue: the definition of equality. Two objects having the saIne\ntype are defined to be deep equal if and only if\n1. The objects <1,1'e of atolnic type and have the saIne value.\n2. The objects are of reference type and the deep eq'l.lals operator is true for\nthe two reforenced objects.\n;3. rrhe objects are of structured type and the deep eqtlal.s operator is true for\nall the corresponding subparts of the two objects.\nTwo objects that have the SaJllC reference type are defined to be shallow equal\nif both refer to the saIne object (i.e., both references use the saUle aid). T'he\n\n()b.ject-Database Systen18\n791\ndefinition of shallow equality can be extended to objects of arbitrary type by\ntaking the definition of deep equality and replacing deep eq1J,als by shallo'w elj'uals\nin parts (2) and\n(~~).\nAs an exarnple, consider the cornplex objects ROW (538, tS9, 6-3-97, 8-7-97)\nand ROW(538, i33, 6-3-97, 8-7-97), whose type is the type of rows in the table\nNowshowing (Line 5 of Figure 23.1). 1'hese t\\VO objects are not shallow equal\nbecause they differ in the second attribute value.\nNonetheless, they rnight\nbe deep equal, if, for instance, the oids t89 and t33 refer to objects of type\ntheater_t that have the saIne value; for exarnple, tuple (54, ':Nlajestic', '115\nKing', '2556698').\nWhile two deep equal objects Inay not be shallow equal, a._\" the exarnple illus-\ntrates, two shallow equal objects are always deep equal, of course. 'The default\nchoice of deep versus shallow equality for reference types is different across\nsystenls, although typically we are given syntax to specify either semantics.\n23.6.2\nDereferencing Reference Types\nAn item of reference type REF (basetype) is not the sarne as the basetype itenl\nto which it points. To access the referenced basetype itenl, a built-in deref ()\nrnethod is provided along with the REF type constructor. For example, given\na tuple from the Nowshowing table, one can access the name field of the ref-\nerenced theater_t object with the syntax Nowshowing.deref (theater). narne.\nSince references to tuple types are comInon, SQL:1999 uses a Java-style arrow\noperator, which cOD.lbines a postfix version of the dereference operator with a\ntuple-type dot operator. The narne of the referenced theater can be\naccess(~d\nwith the equivalent syntax Nowshowing.theater-> narne, as in Figure 23.3.\nAt this point we have covered all the basic type extensions used in the Dinky\nscherna in Figure 23.1. The reader is invited to revisit the scherna and exarnine\nthe structure and content of each table and how the new features are used in\nthe various sarnple queries.\n23.6.3\nURLs and DIDs in SQL:1999\nIt is instructive to note the differences between Internet lJRIJs Etnel the oids\nin object systerns.\nFirst, oids uniquely identify a single object over all tirne\n(at least, until the object is deleted, when the oid is undefined), vvherea,s the\n'\\Veb resource pointed at by an lJHJ-J can change over tirue.\nSecond, oids are\nsirnply identifiers and carry no physical infonnation about the objects they\nidentify\nthis rnakes it possible to change the storage location of an object\nwithout rnodifying pointers to the object. In contra,st, lJH.I.ls include net\\'lork\n\n792\n(jHAprI'ER 2*3\naddresses and often file-syst;enl narnes\n(;1,,-:;; \\veU, lIle<.:tning that if the resource\nidentified by the {JIlL has to Inove to another file or network a,ddress. then all\nlinks to that resource are either incorrect or r(~quire a\n~forwardiIlg' rnechanisH1.\nrrhird, oids are aut(Hnatically generated by the I)B1\\;18 for each object, \\vhereas\nlJIlLs are user-generated.\nSince users generate lJR,Ls, they often ernbed sc-\nrnantic inforlllation into the {JR.L via rnachine, directory, or file names; this\ncan becoine confusing if the object's properties change over tilne.\nFor lJIlLs, deletions can be troublesorne:\nT'his leads to the notorious '404\nPage Not Found' error. For oids, SQL:1999 allows us to say REFERENCES ARE\nCHECKED as part of the SCOPE clause and choose one of several actiol1swhen a\nreferenced object is deleted. This is a direct extension of referential integrity\nthat covers oids.\n23$7\nDATABASE DESIGN FOR AN ORDBMS\nThe rich variety of data types in an OH,DBTv1S offers a database designer Inany\nopportunities for a rnore natural or lllore efficient design. In this section we illus-\ntrate the differences between IlDBl\\!lS and ()RI)BMS database design through\nseveral exarnples.\n23.7.1\nCollection Types and ADTs\n()ur first exarnple involves several space probes, each of which continuously\nrecords a video. A single video strearll is associated with each probe, and while\nthis strearn \\Vck'3 conected over a certail1 tiule period, vve assurne that it is now\na cOlllplete object associated Vilith the probe.\nDuring the tirne period over\nwhich the video \\vas collected, the probe's locatiol1\\vas periodieaJly recorded\n(such infonnation (;an ea.-.sily be pigg~y-backedonto the header portion of a video\nstreanl conforrning to the TvIPEC; sta,ndard). 1'he inforrnation associated \\vith\na probe has three parts: (1) a probe ID that identifies a probe uniquely, (2) a\nvideo s/;'r'carn, and 03) a location 8cqucn.ce of (t;inLe 1 location) pairs. \\iVhat kind\nof a database scherna should we use to store this infonnation?\nAn RDBMS Database Design\nIn H,11 HIJBlVJ:S, \\ve rnust store each video strcanl as a BIJ)13 Etnd each location\nsequellce as tuples in a\ntabh.~. A possible HJ.JBIVJ.S\ndata,b~J,,'3e design follo\\vs:\nProbes(.E~.~~~_ integer,\nUTnc~:~..._~imestamp, lat::\n!:_~..~.~_,__!!:)ng:\nre~.!,\ncarnCTa: string, 'v'ideo: BLOB)\n\n()bject-lJatabase Syslelf);8\n-n, ':3\n{ ;;~\nThere is a single table caned Probes and it lu1.'3 severa,} rows for each probe. Each\nof these rovvs has the saIne pid, carneT(J\" and1rideo values, but different t'irne,\ntat, and lcntg values. (vVe have used latitude and longitude to denote location.)\nThe key for this table can be represented as a functional dependency: IJTLN\n-:. C\nYll , '~lhere N stands for longitude. There is another dependency: p--;.\n(;V~\n'rhis relation is therefore not in BCNT;\"; indeed, it is not even in :INF. vVe ca:n\ndecolupose Probes to obtain a BCNF scherna:\nProbeS_bLoc(pid: integer, lirne: timestamp, tat: real, long: real)\nProbes_Video(p'id: integer, carn,era:\"-;'t'i='ing, 'v'ideo: BLOB)\nThis design is about the best we can achieve in an RDBl'v1S.Ho\\vever, it suffers\nfrorn several dn1\\vbacks.\nFirst, representing videos\naA.'3 BLOBs lI1eanS that we have to write application\ncode in an external language to lnanipulate a video object in the database.\nConsider this query:\n\"For probe 10, display the video recorded between 1:10\nP.M. and 1:15 P.M. on lVlay 10 1996.\" We lnust retrieve the entire video object\nassociated ''lith probe 10, recorded over several hours, to display a segrnent\nrecorded over five rninutes.\nNext, the fact that each probe has an associated sequence of location readings\nis obscured, and the sequence inforrnatiol1 associated with a probe is dispersed\nacross several tuples. A third drawback is that vve are forced to separate the\nvideo infonnation froTn the sequence inforrnation for a probe. rrhese lirnitations\nare exposed by queries that require us to consider all the infonnation associated\nv'lith each probe; for excunple, ;'Fhr each probe, print the earliest tirne at which\nit recorded, and the CEtrnen.l type.\" T'his query 110v'l involves a join of Probes.\".Loc\nand Probes_Video on the p'id field.\nAn ORDBMS Database Design\n;\\n ()HJ)BlVIS supports a lnuch better solution. FirsL \\ve can store the video\nas an A.DT object and \\vrite rnethods that c':1I)ture a.ny special rna,nipulation\n\\ve \\vish to perforrr1. Second, 1)eCflT1Se \\ve aTe allovved to store structured types\nsuch as lists,\\ve (:a,n stc)re tIl(:; loca,tion sequerlce for\n(1. probe in a single tuple,\nalong\\vith thE: video infonnation. '['his layout elirnina,t;c;s the need for joins in\ncIlH;ries that involve both the sequence ancl video inforrnation. An ()HI)BNIS\ndesign for our exarrlpl(~ consists of ,1 single relation called Probes_i\\llInfo:\nProbes ...AIUnfo(pid: integer, loc8eq: location...seq, Ca'fTl·e'ra: string;\n'video: mpeg_.stream)\n\n794\nCHAPTER 2~\nThis definition involves t,vo new\" types, location...seq and mpeg_stream. The\nmpeg_stream type is defined as an ADT, with a Inethod display() that takes\na start tiIne and an end tirne and displays the portion of the video recorded\nduring that interval. This rnethod can be irnplernented efficiently by looking at\nthe total recording duration and the total length of the video and interpolating\nto extract the segnlcnt recorded during the interval specified in the query.\nOur first query in extended SQL using this display lnethod follows.\nWe now\nretrieve only the required segment of the video rather than the entire video.\nSELECT display(P.video, 1:10 Po M0 May 10 1996, 1:15 Po M0 May 10 1996)\nFROM\nProbes-.Alllnfo P\nWHERE\nPopid = 10\nNow consider the location_seq type.\nWe could define it as a list type,\ncontaining a list of ROW type objects:\nCREATE TYPE location_seq listof\n(row (time: timestamp, lat: real, long: real))\nConsider the locseq field in a row for a given probe. This field contains a list\nof rows, each of which has three fields. If the ORDBMS implements collection\ntypes in their full generality, we should be able to extract the time colurnn\nfrom this list to obtain a list of timestamp values and apply the MIN aggregate\noperator to this list to find the earliest tiIne at which the given probe recorded.\nSuch support for collection types would enable us to express our second query\nthus:\nSELECT\nFROM\nP.piel, MIN(P.locseq.tirne)\nProbes._AllInfo P\nCurrent ()ItDBI\\1Ss are not &'3 general and clean a..s this exalnple query suggests.\nFor instance, the systern rnay not recognize that projecting the tirne colurnn\nfrorn a list of rows gives us a list of tirnestarnp values; or the systcru rnay allovv\nus to apply an aggregate operator only to a table and not to a nested list value.\nContinuing \"'lith our exa.rnple, we lnay want to do specialized operations on\nour location sequences that go beyond the standard aggregate operators. For\ninstance, we rnay want to define a lnethod that takes a tirne interval and COIIl-\n,\n.'\nputes the distaIlce traveled by the probe during this interval. The code for this\nrnethod rnust understaJnd details of a probe's trajectory and geospatial coordi-\nnate systenls.\nFbI' these reasons, \\ve rnight choose to define location_seq as\nan ADT\\.\n\nObject-Database Systerns\n79~\nClearly, an (ideal) ORDBIvIS gives us IIlaIlY useful design options that are not\navailable in an RDBMS.\n23.7.2\nObject Identity\n\\Ve now discuss S0l11e of the consequences of using reference types or aids. The\nuse of aids is especially significant when the size of the object is large, either\nbecause it is a structured data type or because it is a big object such &s an\nimage.\nAlthough reference types and structured types seem sirnilar, they are actually\nquite different. For example, consider a structured type my_theater tuple (ina\ninteger, name text, address text, phone text) and the reference type theater\nref (theater_t) of Figure 23.1. rrhere are irnportant differences in the way that\ndatabase updates affect these two types:\n•\nDeletion: Objects with references can be affected by the deletion of ob-\njects that they reference, while reference-free structured objects are not\naffected by deletion of other objects.\nFor exaluple, if the Theaters table\nwere dropped from the database, an object of type theater might change\nvalue to null, because the theater_t object it refers to has been deleted,\nwhile a similar object of type my_theater would not change value.\n•\nUpdate: Objects of reference types change value if the referenced object\nis updated. Objects of reference-free structured types change value only if\nupdated directly.\n•\nSharing versus Copying:\nAn identified object can be referenced by\nllluitiple reference-type iterIls, so that each update to the object is reflected\nin IYlany places.\n~ro get a sirnilar effect in reference-free types requires\nupdating all 'copies' of an object.\nThere are also irnportant storage distinctions between reference types and non-\nreference types, \\vhich rnight affect perfoI'rnance:\nIII\nStorage Overhead: Storing copies of a large value in rnultiple structured\ntype objects IYlay use lnnch rno1'e space than stori.ng the value once and\nreferring to' it elsewhere through reference type objects.\nThis additional\nstorage requirelnent can affect both disk usage and buffer lnanagcrncnt (if\nIIlal1Y copies are accessed at once).\nIII\nClustering: The subparts of a structured object are typically stored to-\ngether on disk. Objects with references rna,Y point to other objects that a,1'e\nfar a:way on the disk, and the disk ann Inay require significant rnOVClnent\n\n-oc-\nt ,10\n(~;HApcrER ~3\nOIDs and Referential Integrity: In SQL:1999, aU the oids that ap-\npear in a cohunn of a relation are required to reference the srtrne target\nrf~lation. This ~scoping' ulakes it possil)]e to check oid refere:nces for \"1'e£e1'-\nentiaJ. integrity' just like foreign key references are ehecked.vVhile current\nOI{DB1JlS products supporting oids do not support such checks, it: is likely\nthat they will in future releases. This will nlake it rnnch safer to use aids.\nto asserrlble the object and its references together. Structured objects can\nthus be l'nor8 efficient than reference types if they are typically accessed in\ntheir entirety.\nMany of these issues also arise in traditional prograunuing languages such as C\nor Pascal, which distinguish between the notions of referring to objects by value\nand by refer-ence.\nIn database design, the choice between using a structured\ntype or a reference type typically includes consideration of the storage costs,\nclustering issues, and the effect of updates.\nObject Identity versus Foreign Keys\nlJsing an oid to refer to an object is silnila,r to using a foreign key to refer\nto a tuple in another relation but not quite the seune:\nAn oid can point to\nan object of theater_t that is stored any'whcr-c in the database, even in a\nfield, whereas a foreign key reference is constrained to point to an object in a,\nparticular referenced relation. This restriction rnakes it possible for the DBlV1S\nto provide lnuch greater support for\nrefer(~ntial integrity than for arbitra,ry aid\npointers.\nIn general, if an object is deleted while there\nan~ still oid-pointers\nto it, the best the DBl\\IIS can do is to recognize the situation by rnaintajning\na reference count.\n(Even this lirnited support becornes irnpossible if oids can\nbe copied freely.) Thereforc 1 the responsibility for avoiding dangling n~ferences\nrests largely \\'lith the user if oids are llsed to refer to objects. This burdensoIllc\nresponsibility suggests that\nvVE~ should use oids \\vith great ca.ution and use\nforeign keys instead \\vhenever possible.\n23.7.3\nExtending the ER Model\nThe Ell rnodel, cLS described in Chapter 2, is not adequate for ()ItDB1\\tlS design.\n\\Ve have to use an extendedEH, rnodel that supports structured attributes\n(i.e., sets, lists, arra,Ys a,s attribute values) I distinguishes \\vhethc~r entities have\nol)ject ids, and allc)\\vs us to Inodel entities \\vhose attributes include rnethods.\n\\Ve illustrate these connnents using an extended Ell diagrarn to describe the\n\n()b,icct- Database Sy\"teTn,'3\n797\nli\nspace probe data in Figure 2:.3.8; our notational conventions are ad hoc and\nonly for illustr;::ttivc purposes.\n~ -(,~isPlay(start! end) ,._--:)\n---'_-'-_.~,..__.....~\n....-----,..-\"._.......~......--\nFigure 23.8\nThe Space Probe Entity Set\nThe definition of Probes in Figure 23.8 has t\\VO\nrH~\\V &,;pects.\nFirst, it has a\nstructured-type attrilnlte listof (row (ti'lnc, lat, lo'ng)); each value assigned to\nthis attribute in a Probes entity is a list of tuples with three fields.\nSecond,\nProbes has an attribute called video that is an abstract data type object, \\vhich\nis iIldicatecl by a dark oval for this attribute \\vith a dark line connecting it to\nProbes. Further, this ctttribute has an 'attribute' of its own, \\vhich is a rnethod\nof the J\\DT.\nAlternatively, we could rnodel each video as an entity by using an\nt~ntity set\ncalled Videos.\nThe association between Probes entities and Videos entities\ncould then be captured by defining a relationship set that links thenl.\nSince\neach video is collected by precisely one probe and every video is collected by\nse)lne probe, this relationship can be rna,intained by siInply storing a reference to\na probe object with each Videos entity; this technique is essentially the second\ntranslation approach frorn ER, diagnuns to tables discussed in Section 2.4.1.\nIf we also rnake Videos a \\iVeak entity set in this alternative design, we can add\na referential integrity constraint that causes a Videos entity to be deleted \\vhen\nthe corresponding Probes entity is deleted.\n1\\/101'8 generally, this alternative\ndesign illustrates a strong sirnila1'ity bet\\veen storing references to objects and\nforeign keys; the foreign key rnechanisT11 achieves the saIne effect as storing oids~\nbut in\n(1, controlled lUanneI'. If oids are used. the user rnusi ensure that there\nare no dangling references when an object is deleted, \\iVith very little support\nfroIll the DBlvlS.\nFinally~we nofe tllat a significant extension. to the Ell rHodel is required to\nsupport the design of nested collections.\nFor\nexa.rnple~ if a location sequence\nis rnodeled\n(J\",'3 an entity, and \\ve \\vant to clefine an attribute of Probes that\ncontftins a set of such entities, there is no \"vay to do this\\vithont extending the\nEll rrlodeL\\Vc do not disC1ISS this I)oint furtller at the level of Ell diagraIlls,\nbut consider an exaJnple\n11E~xt that\nillustrat(~swhento use a nest,ed\n(~ollection.\n\n798\nC~HAPTER~3\n23.7.4\nUsing Nested Collections\nNested collections offer great rnodeling power but also raise difficult design\ndecisions.\nConsider the following \\vay to rnodel location sequences (other in-\nfonnation about probes is ornitted here to sirnplify the discussion):\nProbesl(pid: integer, locseq: location_seq)\nrrhis is a good choice if the irnportant queries in the workload require us to look\nat the location sequence for a particular probe, as in the query \"For each probe,\nprint the earliest tirne at which it recorded and the caluera type.\" On the other\nhand, consider a. query that requires us to look at all location sequences: \"Find\nthe earliest tiIne at which a recording exists for laf;=5, long=90.\" This query\ncan be answered 1110re efficiently if the following scherna is used:\nProbes2(pi(~: integ~r, tim~:_.timestamp, tat: real, long: real)\nThe choice of scherna Blust therefore be guided by the expected workload (as\nalways). As another example, consider the following scherna:\nCan_TeachI (cid: integer, teacheTs: setof (ssn: string), sal: integer)\nIf tuples in this table are to be interpreted as \"Course cid can be taught by any\nof the teachers in the teacheTs field, at a cost sal.\" then we have the option of\nusing the following schenla. instead:\nCarLTeach2( cid: integer, teachCT_8sn:\nst~_~~.~, sal: integer)\nA choice between these two alternatives can be Inade based on hovv we expect\nto query this table.\nOn the other hand, suppose that tuples in CalL.Teachl\nare to be interpreted as \"Course cid can be taught by the tearnteacheT8, at\na cornbined cost of sal.\"\nCarLTeach2 is no longer a viable alternative. If we\n,vanted to flatten CarLleachl, ~we would have to use (1, separate table to encode\ntearns:\nCan_TCcl,ch2 (.~:'~i..~~.:'..._\n...~.12~.~..~!!!_~_,_..!~~~(J;rrL'id:\n0 i ~,\n,,(l,l: int eger)\n1'earns(tid: oid, .'iBn: string)\nAs these exarnples illustrate, nested collections are appropriate in certain situa-\ntions, but this fea,ture can ea,,'3ily be rnisused; nested collections should therefore\nbe used ·with care.\n\n800\nCHAPTER 23\ni\\n additional COlllplication arises \\'lith array types.\n'fraditionally, array ele-\nlnents are ston-xl sequentially on disk in a ro\\'l-by-ro\\v fashion; for exarnple\n1Io\\v8ver, queries rnay often request suba.rrays that are not stored contiguously\non disk (e.g., .i411 , ,;121, ... ,Arn1). Such requests can result in a very high I/O\ncost for retrieving the subarray. rro reduce the nurnber of l/Os required, arrays\nare often broken into contiguous chunks, vvhich are then stored in senne order\non disk. Although each chunk is sorne contiguous region of the array, chunks\nneed not be rovv-by-ro\\v or colurnn-by-colurllll. For exalnple, a chunk of size 4\n111ight be All, A12 , A21 , /122 , 'which is a square region if we think of the array\nas being arranged row-by-row in two dimensions.\nIndexing New Types\nOne ilnportant reason for users to place their data in a database is to allow\nfor efficient access via indexes.\nUnfortunately, the standard RDB11S index\nstructures support only equality conditions (B+ trees and hash indexes) and\nrange conditions (B+ trees). An irnportant issue for OR,DB1\\IISs is to provide\nefficient indexes for AD'I' rnethods and operators on structured objects.\nMany specialized index structures have been proposed by researchers for par-\nticular applications such as cartography, genorne research, 11lultirnedia reposito-\nries, \\Veb search, and so on. An OR,DBlVIS cornpany Cctnnot possibly inlplernent\nevery index that has been invented. Instead, the set of index structures in an\n()R,DBlVfS should be user-extensible.\nExtensibility would allow an expert in\ncartography, for exanlple, to not only register an AD1' for points on a rnap\n(i.e., latitude··longitude pairs) but also irnplernent an index structure that sup-\nports natural rnap queries (e.g., the R,-tree, \\vhich lnatches cOllclitions such as\n\"Find rne all theaters within 100 Iniles of Andorra\"). (See Chapter 28 for 1110re\non Il-trees and other spatial indexes.)\nOne vvay to rnake the set. of index structures extensible is to publish\n;:1,11 ac-\nccs.s nu~thod 'interface that lets users irnplcrnent an index structure o'llts'ide the\nDEl\\;IS. 'I'he index and data can be stored in a file systeIll and the DEl\\;IS sirnply\nissues the open, ne:l.:f, and\nCl08(:~ iterator requests to the user's external index\ncode.\nSuch functionality rnakes it possible for a user to connect a I)Bl\\1S to\na\\Neb search engine, for exauII>le. A rnain dravvback of this approach is that\ndata in an external index is lI0t protected l)y tIle ])B1V1S'8 support for concur-\nrency and rec:over:y.\n1\\n\nalterrlativf~ is for the ()llI)B1ilS to provide a generic\n'tenlphtte' irHlex structure that is sufficientl:y general to encornpass rnost index\nstructures that usersrn.ight irlvent.\nBec<luse snell a structure is\nil11plc~Jnented\nwithin theDBrvIS, it can support high concurrency and recovery. The\nG1C't'1,(:T-\n\nObject-Database8ystenLS\n801\nalized SleaTch Tree (GiS'r) is suell a structure. It is a ternplate index structure\nba.':'ed on B+ trees, \\vhich aJlo\\vs IllOSt of the tree index structures invented so\nfar to be irnplernented'with only a few lines of user-definedAD'T' code.\n23.8.2\nQuery Processing\n.A..DTs and structured types call for l1e\\v functionality in processing queries\nin ()RDI-3I\\!ISs.\nrrhey also change a nurnber of a..ssuruptions that affect the\nefficiency of queries. In this section we look at two functionality issues (u8er-\ndefined aggregates and security) and two efficiency issues (rnethod caching and\npointer swizzling).\nUser-Defined Aggregation Functions\nSince users are allowed to define new rnethods for their ADTs, it is not unrea-\nsonable to expect thern to want to define new aggregation fUllctions for their\nADTs as well.\nFor example, the usual SQL aggregates····---CDUNT,\nSUM, MIN,\nMAX, AVG--are not particularly appropriate for the image type in the Dinky\nschema.\nMost ORDBMSs allow users to register new aggregation functions \\vith the\nsystern.\nTo register an aggregation function, a user lnust iruplenlent three\nrnethods~ which we call 'initiaIize, iterate, and terrninate. The in'it'ial'ize rnethod\ninitializes the internal state for the aggregation. The iterate rnethod updates\nthat state for every tuple\nseen~ \"vhile the terrninate rnethod C0111putes the ag-\ngregation result based on the final state and then cleans up. As an exarnple,\nconsider an aggregation function to cornpute the second-highest value in a, field.\n1'he init'ialize call would allocate storage for the top two values~ the 'iterate call\nwould corupare the current tuple's value with the top two and update the top\ntwo as necessary, and theterrn'inate call \\vould delete the storage for the top\ntwo values~ returning a copy of the second-highest value.\nMethod Security\nAIYTs give users the pO\\\\'8r to a,(1d code to the DBl'vlS; this power can be\nabused.\nA buggy or rnalicious ADT rnethod can bring do\\vn the database\nserver or eveil corrupt the databcL'Sc.\nThe DBNIS lnust have rnechanisrns to\nprevent buggy or rnalicious user code frcHn causing probleIlls.\nIt 1nay rnake\nsense to overricle these rnechanislIls for efficiency in production environrnents\nwith vendor-supplied rnethods. I-Io\\vever, it is irnportant for the rnechanisrns to\nexist, if only to support delJugging of J\\DT rnethocls; othervvise rnethod \\vriters\n\n802\nCHAPTER ~3\n\\vould have to \\vrite bug-free code before registering their rnethods with the\nDBMS--not a very forgiving progralIuning environlnent.\n()ne rnechanisrn to prevent problerns is to have the user rnethods be intc11Jreted\nrather than cornp'iled. The DBIv1S can check that the rnethod is well behaved\neither by restricting the power of the interpreted language or by ensuring that\neach step taken by a rnethod is safe before executing it. Typical interpreted la.n-\nguages for this purpose include Java and the procedural portions of SQL:1999.\nAn alternative rnechanislll is to allow user methods to be cOlnpiled frorn a\ngeneral-purpose progranuning langllage~ such as C++, but to run those rneth-\nods in a different address space than the DBMS. In this case, the DBMS sends\nexplicit interprocess cOl1uIlunications (IPCs) to the user rnethod~ which sends\nIPCs back in return. This approach prevents bugs in the user methods (e.g.,\nstray pointers) frorn corrupting the state of the DBNIS or database and prevents\nrnalicious methods frorn reading or Inodifying the DBMS state or database as\nwell. Note that the user writing the method need not know that the DBMS is\nrunning the method in a separate process: The user code can be linked with a\n'wrapper' that turns method invocations and return values into IPCs.\nMethod Caching\nUser-defined ADT methods can be very expensive to execute and can account\nfor the bulk of the time spent in processing a query. During query processing,\nit may 11lake sense to cache the results of methods, in case they are invoked\nlllultiple times with the same arglunent. Within the scope of a single query,\none can avoid calling a Inethod twice on duplicate values in a colurnn by either\nsorting the table on that colullln or using a ha,..'3h-ba'3ed scherne ruuch like that\nused for aggregation (see Section 14.6). An alternative is to rnaintain a cache\nof rnethod inputs and rnatching outputs as a table in the database. Then, to\nfind the value of a rnethod on particular inputs, we essentially join the input\ntuples with the cache table. rrhese two approaches can also be cornbined.\nPointer Swizzling\nIn sorne applications, objects are retrieved into rnernory and accessed frequently\nthrough their oids; dereferencin.g rnust be irnplcrnented very efficiently. 801ne\nsysterns rnaintain a ta,hle of oids of objects that are (currently) in InenlOI'.Y.\n\\Vhen an object () is In'ought into nlCnlOl'jr, they check each oid\nc~ontained\nin 0 and replace oids of\nin-rrH~rnory objects by in-rncrl10ry\npoint(~rs to those\nobjects.\n1'his techrlique, caJled pointer swizzling, nUl-kes\nreferenc(~s to in-\nrnernory objects ver~y .fast. rfhe dO\\7vnsicle is tlHtt vvhen an object is paged out,\n\n()b.iect-Database 8ys/;clns\n2389\nOODBMS\n805\nt\nIn the introduction of this chapter, \\ve defined an O()DB:NlS as a progranlIning\nlanguage ''lith support for persistent objects.vVhile this definition reflects the\norigins of OODB~1Ss accurately, and to a certain extent the irnplernentation\nfocus of OODBl\\'1Ss, the fact that OODBl\\!ISs support collecl:'ion, type\" (see\nSection 2:3.2.1) rnakes it possible to provide a query language over collections.\nIndeed, a standard has been developed by the Object Database l\\'lanagernent\nGroup and is called Object Query Language.\nOQL is sirnilar to SQL, with a SELECT----FROM--HWHERE---style syntax (even GROUP\nBY, HAVING, and ORDER BY are supported) and rnany of the proposed SC~L:1999\nextensions.\nNotably, OQL supports structured types, including sets, bags,\narrays, and lists.\n1~he OQL treatrnent of collections is rnore uniforlll than\nSQL:1999 in that it does not give special treatrnent to collections of rows;\nfor exalnple, ()QL allows the aggregate operation COUNT to be applied to a\nlist to C0111pute the length of the list.\nO(~L also supports reference types,\npath expressions,ADrrs and inheritance, type extents, and SQL-style nested\nqueries.\n1'here is also a standard Data Definition Language for OODB1\\1Ss\n(Object Data Language, or ODL) that is sirnilar to the DDL subset of\nSQL but supports the additional features found in OODBMSs, such as ADT\ndefinitions.\n23.9.1\nThe ODMG· Data Model and ODL\n'The ODl\\iIG data rnodel is the basis for an OODBl\\iIS, just like the relational\ndata 1nodel is the basis for an IlDB1\\1S. A database contains a collection of ob-\njects, which are sirnilar to entities in the Ell rnode!. Every object ha.s a unique\naid, and a database contains collections of objects with Silllilar properties; such\na collection is called a class.\nThe properties of a class arc specified using ()l)L and are of three kinds: at-\ntributes, relationships, and rnethod8.\n_Attributes have an atolnic type or a\nstructured\nt~ype. ODl.l supports the set, bag, list, array, and struct t,ype\nconstructors; these are just setof, bagof, listof, ARRAY, and ROW in the ter-\nrninology of Section 2:3.2.1.\nR,elationships have a type that is either a reference to an object or a collection\nof such references.\nA relationship captures ho'v an object is related to one\nor r1101'e\nobj(~cts of the\n8e1111(' class or of\n;.1 different clctss.\nj\\ relationship in\nthe ()IJIvIG- rnodel is really just\n(1 bincl,ry relationship in the sense of theEI{\nInodel. A rela.tionship has\n~l corresponding inverse relationship; intuitively,\nit is the relationship 'in the other clirection.' For exarnple, if a Inovie is being\n\n806\n(jHAPTER 23\nClass == Interface + Implenlentation: Properly speaking, a cla...,s con-\nsists of an interface together\\vith an irnpleluentation of the interface. An\nODL interface definition is irnpleInel~ted in an OODBlYIS by translating it\ninto declarations of the object-oriented language (e.g., C·+\"+, Snlalltalk or\nJava) supported by the OODBMS. If V\\Te consider C++, for instance, there\nis a library of cl<:1Sses that irnplcrnent the ODL constructs. There is also an\nObject Manipulation Language (OML) specific to the programlning\nlanguage (in our exanlple, C++), which specifies how database objects j\nare manipulated in the progralnnl.ing .language.\nrr.he .goal is to seamlessly\nintegrate the prograrnrning language and the database features.\nshown at several theaters and each theater shows several rnovies, we have two\nrelationships that are inverses of each other: shownAt is associated with the\nclass of movies and is the set of theaters at which the given movie is being\nshown, and nowShowing is associated with the class of theaters and is the set\nof rnovies being shown at that theater.\nMethods are functions that can be applied to objects of the class.\nl~here is\nno analog to methods in the ER or relational models.\nThe keyword interface is used to define a class. For each interface, we can\ndeclare an extent, which is the narne for the current set of objects of that\nclass. The extent is analogous to the instance of a relation and the interface\nis analogous to the scherna. If the user does not anticipate the need to work\nwith the set of objects of a given class-it is sufficient to manipulate individual\nobjects--···the extent declaration can be ornitted.\nThe following ()DL definitions of the lViovie and Theater cla,,'3ses illustrate these\nconcepts.\n(While these classes bear S(Hne resernblance to the Dinky databa\",sc\nscherna, the reader should not look for an exact parallel, since we have rnodified\nthe exarnple to highlight ()DL features.)\ninterface Iv/lovie\n(extent IVlovies key rnovieNarne)\n{ attribute date start;\nattribute date end;\nattribute string rnovienarne;\nrelationship Set('fheater) shownAt inverse Theater::nowSho\\ving;\n}\n1~he collection of databa...'3c objects whose cla\"ss is lVlovie is called lVIovies.\nNo\ntwo objects in lVIovies have the sarne rnovieNarne value, as the key declaration\n\n()bject-.Database Systelns\n8Q7\nindicates. Each lnovie is shov,rn at a set of theaters and is shown during the\nspecified period. (It \\vould be rnore realistic to c'h'Ssoeiate a different period with\neach theater, since a 1110vie is typically played at different theaters over different\nperiods. While we can define a class that captures this detail, \\ve have chosen\na sirnpler definition for our discussion.) A theater is an object of cla.o;;;s Theater,\ndefined a.s:\ninterface Theater\n(extent Theaters key theaterNarne)\n{ attribute string theaterName;\nattribute string address;\nattribute integer ticketPrice;\nrelationship Set (Movie) nowShowing inverse .lVlovie::shownAt;\nfloat numshowingO raises(errorConntingMovies);\n}\nEach theater shows several movies and charges the same ticket price for every\nmovie. Observe that the shownAt relationship of Movie and the nowShowing\nrelationship of Theater are declared to be inverses of each other. Theater also\nhas a lllethod numshowing() that can be applied to a theater object to find the\nnumber of movies being shown at that theater.\nODL also allows us to specify inheritance hierarchies, as the following class\ndefinition illustrates:\ninterface SpecialShow extends lVlovie\n(extent SpecialShows)\n{ attribute integer I11axinnunAttendees;\nattribute string benefitCharity;\n}\nAn object of class SpecialShow is an object of class l\\1ovie, with SOI11e additional\nproperties, as discussed in Section 23.5.\n23.9.2\nOQL\nrIhe ODlV1G query language\nO(~L was deliberately designed to have syntax\nsirnilar to S(~L to rnake it easy for users falniliar with S(~L to learn ()QL. Let\nus begin 'with a query that finds pairs of Inovies and theaters such that the\nrnovie is sho\\vn at the theater and the theater is showing lHore than one rnovie:\nSELECT Innarne: lVLrnovieNarn(\\ tnaIne: I'.theaterNarne\n\n808\nFROM\nWHERE\nlViovies lV1, lVLsho\\vnAt T\nT .nurllshowing() > 1\n(~IIAPTER\n~:3\nThe SELECT clause indicates how \\ve can give Ilallles to fields in the result:\nThe t\\VO result fields are called rnnarne and\ntnarr~e. The part of this query that\ndiffers frorn\nS(~L is the FROM clause. The variable 1\\/[ is bound in turn to each\nrnovie in the extent l\\lovies. For a given rnovie Ai, we bind the variable T in\nturn to each theater in the collection lv1. shownAt. Thus, the use of the path\nexpression lvi. shownAt allows us to easily express a nested query. The follo'Vving\nquery illustrates the grouping construct in OQL:\nSELECT\nFROM\nGROUP BY\nT.ticketPrice,\navgNurn: AVG(SELECT P.T.nurnshowingO FROM partition P)\nl'heaters l'\nT .ticketPrice\nFor each ticket price, we create a group of theaters with that ticket price.\nThis group of theaters is the partition for that ticket price, referred to using\nthe OQL keyword partition.\nIn the SELECT clause, for each ticket price,\nwe cornpute the average nunlber of rnovies shown at theaters in the partition\nfor that ticketPrice.\nOQL supports an interesting variation of the grouping\noperation that is missing in SQL:\nSELECT\nFROM\nGROUP BY\nlow, high,\navgNllln: AVG(SELECT P.T.nurnshowingO FROM partition P)\nTheaters T\nlow: T.ticketPrice < 5, high: rr.ticketPrice >::::: 5\nThe GROUP BY clause now creates just two partitions called low and high. Each\ntheater object T is placed in one of these partitions bEksed on its ticket price. In\nthe SELECT clause, lo'wand high are boolean variables, exactly one of which is\ntrue in any given output tuple; partition is instantiated to the corresponding\npartition of theater objects. In our exarnple, 'Vve get t\\VO result tuples. ()ne of\nthern has lOll) equal to true and avgNuTn equal to the average nurnber of rnovies\nshown at theaters \\vith a low ticket price. The second tuple hEks high equal to\ntrue a\"nd avgNuTn equal to the average nurnber of Inovies shown at theaters\nwith a high ticket price.\nThe next query illustrates ()(~L support for queries that return collections other\nthan set and rnultiset:\n(SELECT\nrr.theaterNarne\nFROM\nTheaters 'T\nORDER BY T.ticketPrice DESC) [0:4]\n\n()bject-.lJatabasc SYS'tCTflS\n8U9\nThe ORDER BY clause nU1,kes the result a list of theater naInes ordered by ticket\nprice.\nl'\"1he clcrnents of a list can be referred to by position, starting \\vith\nposition O. Therefore, the expression [0:4] extracts a list containing the naInes\nof the five theaters \\vith the highest ticket prices.\nOC~L also supports DISTINCT, HAVING, explicit nesting of subqueries, vie'\\.\\T def-\ninitions, and other SQL features.\n23.10\nCOMPARING RDBMS, OODBMS, AND ORDBMS\nNow that we have covered the lnain object-oriented DBMS extensions, it is\ntirne to consider the two lnain variants of object-datab&'Ses, OODBlVlSs and\nORDBJVISs, and cornpare thern with RDBMSs. Although we presented the con-\ncepts underlying object-databases, we still need to define the tenns OODBMS\nand OR,DBMS.\nAn ORDBMS is a relational DBl\\1S with the extensions discussed in this\nchapter.\n(Not all ORDBMS systerlls support all the extensions in the gen-\neral forrn that we have discussed theIn, but our concern in this section is the\nparadigrll itself rather than specific systenls.)\nAn OODBMS is a progranl-\nrning language with a type systern tha..t supports the ft~atures discussed in this\nchapter and allows any data object to be persistent; that is, to survive across\ndifferent prograrn ex(~cutions.Manycurrent systerns conform to neither defi-\nnition entirely but are lIluch closer to one or the other and can be classified\naccordingly.\n23.10.1\nRDBMS versus ORDBMS\nCOlllparing anllDBlvlS with an OI{DBMS is straightforward. An R,DBl\\1S does\nnot support the extensions discussed in this chapter. rrhe resulting sirnplicity\nof the data rnodel rnakes it easier to optirnize queries for eHicient\nexecution~\nfor exanlple. A relational systern is also easier to use because there are fe-weI'\nfeatures to Illa,ster. ()n the other hand, it is less versatile than an ()HJ)BiviS.\n23.10.2\nOODBMS versus ORDBMS: Similarities\nOODB1VlSs and ()HI)BJVISs both support user-defined ADTs, structured types,\nol)ject identity and reference types, and inheritance.\nBoth support a cpler,)l\nlanguage for rnanipulating collection types.\n()RDBlVlSs support an extended\nfonn of S(~L, and 001)131\\118s support\nC)I}L/O(~L. The sirnilarities are by no\nrneans accidental ()llDBl\\1Ss consciously try to add ()()DBlVfS features to an\nRI)B1\\118 , and\n()()DB~'1Ss in turn have developed query languages based on\n\n810\nCHAPTER\n2~3\nrelational query languages. Both OODBIvISs and OR,DBIvlSs provide DBNIS\nfunctionality such as concurrency control and recoverv.\nii,\nto,-\n~$\n23.10.3\nOODBMS versus ORDBMS: Differences\nThe fundaulental difference is really a philosophy that is carried all the way\nthrough: OODBMSs try to add DBIvlS functionality to a progranllning lan-\nguage, wherca'3 ORDBIvlSs try to add richer data types to a relational DBlVIS.\nAlthough the two kinds of object-databases are converging in terrns of func-\ntionality, this difference in their underlying philosophy (and for most systeIns,\ntheir irnplementation approach) has iInportant consequences in terIllS of the\nissues emphasized in the design of these DBJVISs and the efficiency with which\nvarious features are supported, as the following comparison indicates:\nII\nOODBMSs airn to achieve seamless integration with a programrning lan-\nguage such as C++, Java, or Smalltalk.\nSuch integration is not an im-\nportant goal for an ORDBMS. SQL:1999, like SQL-92, allows us to embed\nSQL commands in a host language, but the interface is very evident to the\nSQL programer. (SQL:1999 also provides extended prograrnming language\nconstructs of its own, as we saw in Chapter 6.)\nII\nAn OODBMS is aimed at applications where an object-centric viewpoint\nis appropriate; that is, typical user sessions consist of retrieving a few\nobjects and working on theHl for long periods, with related objects (e.g.,\nobjects referenced by the original objects) fetched occasionally.\nObjects\nrnay be extrelnely large and rnay have to be fetched in pieces; therefore,\nattention Inust be paid to buffering parts of objects. It is expected that\nrnost applications can cache the objects they require in rnemory, once the\nobjects are retrieved froIn disk. rrherefore, considerable attention is paid to\nrnaking references to ill-lnernory objects efficient. Tl'ansactions are likely to\nbe of very long duration and holding locks until the end of a transaction Inay\nlead to poor perfonnance; therefore, alternatives to Two-Phase Locking\nHUlst be used.\nAn OR,DBlVfS is optirnized for c1pplications in which large data collections\nare the focus, even though objects rnay have rich structure and be fairly\nlarge.\nIt is expected that applications will retrieve data frorn disk ex-\ntensively and optirnizing disk access is still the rnain concern for efficient\nexecution. Tl'a,nsactions are assurned to be relatively short and traditional\nR.DB1VIS techniques are typically used for concurrency control and recovery.\niii\nT'he query facilities of ()(lL are not supported efficiently in rnost O()DBlVfSs,\n\\vhereas the query facilities are the centerpiece of an ()HI)Bl\\1S. To scnne\nextent, this situation is the result of different concentrations of effort in\nthe cleveloprnent of these systerns.\n'1'0 a sigrlificant\n(~xtenti it is also a\n\nObject-Database SY8terns\n81).\nconsequence of the systerlls' being optirnized. for very different kinds of\napplications.\n23.11\nREVIEW QUESTIONS\nAnsvvers to the review questions can be found in the listed sections.\n..\nConsider the extended Dinky exarnple frorn Section 23.1.\nExplain how\nit lllotivates the need for each of the following object-database features:\n'User-defined struct'Ured types, abstract data types (AD Ts), inheritance, and\nobject identity. (Section 23.1)\n..\nWhat are structured data types? What are collection types, in particular?\nDiscuss the extent to which these concepts are supported in SQL:1999.\nWhat irnportant type constructors are lllissing? What are the limitations\non the ROWand ARRAY constructors? (Section 23.2)\n..\nWhat kinds of operations should be provided for each of the structured\ndata types? To what extent is such support included in SQL:1999? (Sec-\ntion 23.3)\n..\nWhat is an abstract data type? How are nlethods of an abstract data type\ndefined in an external programnling language? (Section 23.4)\n..\nExplain inheritance and how new types (called subtypes) extend existing\ntypes (called supertypes). What are rnethod overloading and late b'inding?\nWhat is a collect'ion hieruTchy? Contrast this with inheritance in prograrn-\nIning languages. (Section 23.5)\nI!II\nI-Iow is an object identifier (aid) different froln a record id in a relational\nDElVIS? How is it different froIn a URI./? \\iVhat is a reference type?\nDe-\nfine deep and shallow equalit,y and illustrate thern through an exarnple.\n(Section 23.6)\nII\n'The rnultitude of data types in an (}H.DBlVIS allcnvs us to design a rnore nat-\nural and efficient databa\"se schcrna but introduces S(Hne nev.,r design choices.\nI)iscuss OHJ)BJ\\.{S database design issues and illustrate your discussion us-\ning an exalnple application. (Section 23.7)\n11III\nIrnplernenting an ()11I)B1rfS brings new challenges. The systcrll rnust store\nlarge ADTs and structured types that rnight be very large. Efficient and\nextensible index rnechanisrns lIlUSt be provided.\nExarnples of nc\\v func-\ntionality include 'u,8cr-def£ned aggregation ./1lnct'ions (we can define nc\\v\naggrega,tion, functions for our AI)Ts) and rnethod security (the systcrIl\nhas to prevent user-defined rnethods fronl cornprornising the security of\nthe\n,DB~'IS). ExarIlples of nc\\v techniques to increase perfonnance include\n\n812\nCl:IAPTER ~3\n1nethod cachi'llg and pointeT 871J'izzling.\n'The optirnizer must know about\nthe l1e\\v functionality and use it appropriately.\nIllustrate each of these\nchallenges through an exarnple. (Section 23.8)\nII\nCornp<u'e OODBIvISs Vvith\n()R,DB~1Ss.\nIn particular, cornpare OQL and\nSQL:1999 and discuss the underlying data rnode!.\n(Sections 23.9 and\n23.10)\nEXERCISES\nExercise 23.1 Briefly answer the following questions:\n1. What are the new kinds of data types supported in object-database systcrIls? Give an\nexarnple of each and discuss how the exanlple situation would be handled if only an\nRDBl'v1S were available.\n2. What rIlust a user do to define a new ADT?\n3. Allowing users to define rIlethods can lead to efficiency gains. Give an exarnple.\n4. \\\\That is late binding of nlCthods?\nGive an exarnple of inheritance that illustrates the\nneed for dynamic binding.\n5. What are collection hierarchies? Give an exalIlple that illustrates how collection hierar-\nchies facilitate querying.\n6. Discuss how a DBNIS exploits encapsulation in ilnplernenting support for ADTs.\n7. Give an exarnple illustrating the nesting and unnesting operations.\n8. Describe two objects that are deep equal but not shallow equal or explain why this is\nnot possible.\n9. Describe two objects that are shallow equal but not deep equal or explain why this is\nnot possible.\n10. COlnpare RDBNISs with ORDBlVISs.\nDescribe an application scenario for which you\nwould choose anRDB11S and explain why. Silnilarly, describe an application scenario\nfor which you would choose an ORDBl\\t1S and explain why.\nExercise 23.2 Consider the Dinky schclna shown in Figure 23.1 and all related lncthocls\ndefined in the chapter. \\\\Trite the following queries in SQI...:1999:\n1. How luany filrns were shm,vn at theater tno = 5 between January 1 and February 1 of\n2002'1\n2. \\Vhat is the lowest budget for a filnl with at leaBt t\\vo stars?\n:3. Consider theaters (It which a fihu directed by Steven Spielberg started showing on Jan-\nuary 1, 2002. For each such theater, print the narnes of all countries within a 100-ruile\nradius. (You can use the o'ucrlap and nulius rnethods illustrated in Figure 2:3.2.)\nExercise 23.3 In a cornpany database, you need to store inforrnation about clnployees, de-\npartrnents, and children of erIlployees. For each ernployec, identified by ssn, you rnust record\nyears (the number of years that the ernployee h(;1.\" worked for the cornpany), phone, and photo\ninforrnation.\nThere are two subclasses of ernployees: contract and regular.\nSalary is coru-\nputed by invoking a rnethod that takes year8 as a pararneter; this rnethod ha.s a different\n\n()b:ject-Database SY8te'tns\n813\n~\nirnplernentation for each subclass. Further, for each regular enlployee, you Il1USt record the\nIHune and age of every child.\nl'he rnost conunon queries involving children are sirnilar to\n\"Find the average a.ge of 1301/5 children\" and:'Print the narnes of all of Bob's children.\"\nA photo is a large inmge object and call be stored in one of several irnage fonnats (e.g.,\ngif, jpeg). You want to define a display rnethod for iUlage objects; display IIlust be defined\ndifferently for each irnage fonnat.\n'For each departlllcnt, identified by dno, you rnust record\ndnmne, budget, and\nWOTkET8 infonnation.\nHf(wkc'T'8 is the set of crllployees who work in a\ngiven departrnent. Typical querie.s involving workers include, \"Find the average salary of all\nworkers (across all departrnents).\"\n1. Using extended SQL, design an ORDBIVIS scherna for the cornpany databa\"se. Show all\ntype definitions, including rnethod definitions.\n2. If you have to store this infonnation in an RDBl\\'1S, what is the best possible design?\n3. Cornpare the ORDBrvIS and RDBIVIS designs.\n4. If you are told that a COlllInon request is to display the irnages of all employees in a given\ndepartruent, how would you use this inforulation for physical database design?\n5. If you are told that an ernployee's ilnage rnust be displayed whenever any information\nabout the employee is retrieved, would this affect your scherna design?\n6. If you are told that a COU1Inon query is to find all erTlployees who look sirnilar to a given\nimage and given code that lets you create an index over all irnages to support retrieval\nof sinlilar iuulges, what would you do to utilize this code in an OR.DBMS?\nExercise 23.4 ORDBMSs need to support efficient access over collection hierarchies. Con-\nsider the collection hierarchy of Theaters and Theater-cafes presented in the Dinky exanlple.\nIn your role as a DBMS illlplernentor (not a DBA), you rnust evaluate three storage alterna-\ntives for these tuples:\nII\nAll tuples for all kinds of theaters are stored together all disk in an arbitrary order.\nII\nAll tuples for all kinds of theaters are stored together on disk, with the tuples that are\nfrOIIl TheateLcafes stored directly after the last of the non-cafe tuples.\nIII\nT'uples froIll Theater_cafes are stored separately froIll the rest of the (non-cafe) theater\ntuples.\n1. F'or each storage option, describe a rnechanisrn for distinguishing plain theater tuples\nfrorn Theater_cafe tuples.\n2. For each storage option, describe hmv to handle the insertion of a new non-cafe tuple.\n~i. \\\\Thich storage option is 1110St efficient for queries over all theaters?\nOver just r1'he-\nateLcafes? In terrns of the nurnber of 1/Os, how rnuch rnore efficient is the best technique\nfor each type of query cornpared to the other two techniques?\nExercise 23.5 Different ORDBl\\!ISs use different techniques for building indexes to evaluate\nqueries over collection hierarchies. For our Dink:y' exarnple, to index theaters by name there\nare two COIIlIIlon options:\nIII\nBuild one 13+ tree index over Theaters.narne and another 13+ tree index over '1'he··\nater_caJes. narne.\nII\nBuild one B+ tree index over the union of I'heaters. Twrne and Theater __cafes. nayne.\n\n814\nCHAPTER 243\n1. Describe how to effic.iently evaluate the following query using each indexing option (this\nquery is over aU kinds of theater tuples):\nSELECT * FROM Theaters T WHERE T.narne= 'tvlajestic'\nGive an estiInate of the nurnber of l/Os required in the two different scenarios, assurning\nthere are 1 rnillion standard theaters and 1000 theater-cafes.\n\\V\"hich option is Inore\nefficient?\n2. Perforrn the saIlIe analysis over the following query:\nSELECT * FROM Theater-cafes 'I' WHERE T.nalne = '1vIajestic'\n3. For clustered indexes, does the choice of indexing technique interact with the choice of\nstorage options? For unclustered indexes?\nExercise 23.6 Consider the following query:\nSELECT thurnbnail(Lirnage)\nFROM\nlInages I\nGiven that the 1.image colurnn 111ay contain duplicate values, describe how to use hashing to\navoid conlputing the thum,bnail function rnore than once per distinct value in processing this\nquery.\nExercise 23.7 You are given a two-dimensional, n x n array of objects. Assume that you\ncan fit 100 objects on a disk page. Describe a way to layout (chunk) the array onto pages so\nthat retrievals of square m x m subregions of the array are efficient. (Different queries request\nsubregions of different sizes, i.e., different m values, and your arrangement of the array onto\npages should provide good perforrnance, on average, for all such queries.)\nExercise 23.8 An ORDBJ\\;IS optiruizer is given a single-table query with n expensive selec-\ntion conditions,\n(Tn ( ... ((71 (T))). For each condition (7i, the optirnizer can estinlate the cost C\\\nof evaluating the condition on a tuple and the reduction factor of the condition Ti. Assurne\nthat there are t tuples in T.\n1. How many tuples appear in the output of this query?\n2. Assurning that the query is evaluated a,s shown (without reordering selections), what\nis the total cost of the query?\nBe sure to include the cost of scanning the table and\napplying the selections.\n:3. In Section\n2~3.8.2, it was asserted that the optiruizer should reorder selections so that\nthey are applied to the table ill order of increasing rank, where ranki =\n(Ti\n~... 1)/Ci.\nProve that this assertion is optirual. 'TIHlt is, 8ho\\'/ that no other ordering could result in\na query of lower cost. (Hint: It may be ea..-.,iest to consider the speciaJ ca.se where n = 2\nfirst and generalize from there.)\nExercise 23.9 ORDBIVlSs support references as a data type. It is often clailnecl that using\nreferences instead of ke)lforeign key relationships will give rnuch higher perfonnance for joins.\n'T'his Cllwstion asks you to explore this issue.\nIII\nConsider the follmving SQL: 1999 DDL \\vhich only uses straight relational constructs:\nCREATE TABLE R(rkey integer, r'data text);\nCREATE TABLE S(skey integer, rfkey integer);\n\n()~ject-DatabasE Syste'rns\nAssurne that we have the following straightforward join query:\nSELECT S.skey, H..relata\nFRO?-1\nS, R\nWHERE\nS.rfkey = R.rkey\nII\nNow consider the following SQL:1999 ORDBIvIS schelna:\nCREATE TYPE r_t AS ROW(1'key integer, rdata text);\nCREATE TABLE R OF r _t REF is SYSTEM GENERATED;\nCREATE TABLE S (skey integer, r REF (r_t) SCOPE R);\nAssurne we have the following query:\nSELECT S.skey, S.r.rkey\nFROM\nS\n81~\nWhat algorithrll would you suggest to evaluate the pointer join in the ORDBMS scherna?\nHow do you think it will perform versus a relational join on the previous scherna?\nExercise 23.1.0 Ivlany object-relational systerns support set-valued attributes using some\nvariant of the setof constructor. For eXaInple, assurning we have a type person_t, we could\nhave created the table Filrns in the Dinky Schema in Figure 23.1 as follows:\nCREATE TABLE Films(filrnno integer, title text, star's setof Person);\n1. Describe two ways of irnpleIIlenting set-valued attributes.\nOne way requires variable-\nlength records, even if the set elements are all fixed-length.\n2. Discuss the irnpact of the two strategies on optimizing queries with set-valued attributes.\n3. Suppose you would like to create an index on the column stars in order to look up filrns\nby the narne of the star that has starred in the filIIl. For both irnplenlentation strategies,\ndiscuss alternative index structures that could help speed up this query.\n4. What types of statistics should the query optirnizer rnaintain for set-valued attributes?\nHow do we obtain these statistics'?\nBIBLIOGRAPHIC NOTES\nA nurnber of the object-oriented features described here are based in part on fairly old idea\")\nin the prograrnrning langui:tges cornrnunity.\n[42] provides a good overview of these ideas in\na database context.\nStonebraker's book [719J describes the vision of OHDB:NISs ernbodied\nby his company's early product, Illustra (now a product of Inforrnix). Current connnercial\nDBJ\\lSs ,vith object-relational support include Infonnix Universc.l! Server, U3I\"v'l D13/2 CS V2,\nand UniSQL. An new version of Oracle is scheduled to include OHJ)BrvlS features a,,'3 well.\nIvL:rny of the idc..ls in current object-relational systerlls carne out of i:l, few prototypes built in\nthe 19808, especially POS'I'(jRES [72:3], Starburst (::351], and 02 [218].\nThe iclea of an object-oriented dataJ)c1\".se wa.s first articulated in [197], \\vhich described the\nGernStone prototype system. Other prototypes includeDASDBS [G57], EXODlTS [1:30], nus\n[27:,3], Ol:>jectStore [4G:3], ODE, [18] ORION [4::'32),\nSHOH.,E [1291,\nand 'rl-IOH [482]. 02 is\nactually an early exarnple of a, systenl that ,vas beginning to rnerge the thcrnes of ORDBrvISs\n\n816\n(;HAPTER ·~3\nand OODBivISs ~it could fit in this list as well.\n[41] lists a collectioll of features that aTe\ngenerally considered to belong in an 00DB1\\18. Current cornrnercially available OODBIVISs\ninclude GelllStone, Itasca, 02, Objectivity, ObjectStore, Ontos, Poet, and Versant.\n[4:31J\ncornpares OODBIvISs and RDBivISs.\nDatabase support for ADTs was first explored in the INGRES and POSTGRES projects\nat U.C. Berkeley.\nrrhe basic ideas are described in [716]' including Inechanisllls for query\nprocessing and optilnization with ADTs as well as extensible indexing. Support for ADTs\n\\vas also investigated in the Dannstadt database systern, [480]. Using the POSTGRES index\nextensibility correctly required intiInate knowledge of DBTvIS-internal transaction ruechanisllls.\nGeneralized search trees were proposed to solve this problern; they are described in (376], with\nconcurrency and ARIES-based recovery details presented in [L147].\n[672] proposes that users\nlnust be allowed to define operators over ADT objects and properties of these operators that\ncan be utilized for query optiInization, rather than just a collection of lnethods.\nArray chunking is described in (653]. Techniques for luethod caching and optimizing queries\nwith expensive lnethods are presented in [37:3, 165]. Client-side data caching in a client-server\n00D131\\I1S is studied in [283].\nClustering of objects on disk is studied in [741].\nWork on\nnested relations was an early precursor of recent research on complex objects in OOD13rvlSs\nand ORD13IvISs. One of the first nested relation proposals is (504]. rvlVDs play an inlportant\nrole in reasoning about reduncancy in nested relations; see, for exalnple, [579].\nStorage\nstructures for nested relations were studied in (215].\nFonnal rnodels and query languages for object-oriented databases have been widely studied;\npapers include [4, 56, 75, 125, 391, 392, 428, 578, 724].\n[427] proposes SQL extensions for\nquerying object-oriented databases. An early and elegant extension of SQL with path expres-\nsions and inheritance was developed in GElY! [791]. There has been ITluch interest in cornbining\ndeductive and object-oriented features. Papers in this area include (44, 288, ,195, 556, 706, 793].\nSee [3] for a thorough textbook discussion of fonnal aspects of object-orientation and query\nlanguages.\n[4~i2~,\n4~)5, 721, 796] include papers on D13l'vISs that \\vould now be tenned object-relational\na\"ndjor object-oriented. [794] contains a detailed overview of scherna and database evolution\nin object-oriented database systenls. A thorough presentation of SQL: 1999 can be found in\n[525), and advanced features, including the object extensions, are covered in [523]. A short\nsurvey of new SQL:1999 features can be found in [2:37]. The incorporation of several SQL:1999\nfeatures into I131v1 D132 is described in [128J. OQL is described in [141]. It is based to a large\nextent on the 02 query language, which is described, together with other a..'3pects of 02, in\nthe collection of papers [55].\n\n24\nDEDUCTIVE DATABASES\n..\nWhat is the nlotivation for extending SQL with recursive queries?\n..\nWhat important properties must recursive programs satisfy to be\npractical?\n..\nWhat are least lnodels and least fixpoints and how do they provide a\ntheoretical foundation for recursive queries?\n..\nWhat cOlnplications are introduced by negation and aggregate opera-\ntions? How are they addressed?\n..\nWhat are the challenges in efficient evaluation of recursive queries?\n..\nKey concepts: Datalog, deductive databases, recursion, rules, in-\nfeI'ences, safety, range-restriction;\nleast model, declarative seman-\ntics; least fixpoint, operational semantics, fixpoint operator; negation,\nstratified program.s; aggregate operators, rnultiset generation, group-\ning; efficient evaluation, avoiding repeated inferences, Seminaive fix-\npoint evaluation; pushing query selections, lVlagic Sets rewriting\nFor 'Is' and 'Is-Not' though with Rule and Line,\nAnd 'lJp-and-l)own' by Logic I define,\nOf all that one should care to fathorn, I\nvVa.s never deep in anything but-------\\Vine.\n.. -Rubaiyat of ()rnar !(hayyarn, Translated by Edward Fitzgerald\nllelational database rnanagernent systenls have been enonnously successful for\nC),chninistrative da,ta processing. In recent years, ho-wever, as people have tried to\n817\n\n818\nCl:IAPTER 24\nuse database systerus in increasingly cornplex applications, some irnportant linl-\nitations of these systellls have been exposed. For sonle applications, the query\nlanguage and constraint definition capabilities have been found inadequate. As\nan exarnple, sonle cornpanies ulaintain a huge parts inventory database and\nfrequently want to ask questions such as, \"Are 'we running Iowan any parts\nneeded to build a ZX600 sports car?\"\nor \"What is the total cornponent and\nassernbly cost to build a ZX600 at today's part prices?\" These queries cannot\nbe expressed in SQL-92.\nvVe begin this chapter by discussing queries that cannot be expressed in rela-\ntional algebra or SQL and present a rnore powerful relational language called\nDatalog.\nQueries and views in SQL can be understood as\nif~then rules: \"If\nsome tuples exist in tables mentioned in the FROM clause that satisfy the condi-\ntions listed in the WHERE clause, then the tuple described in the SELECT clause\nis included in the answer.\" Datalog definitions retain this if-then reading, with\nthe significant new feature that definitions can be recursive, that is, a table\ncan be defined in terms of itself.\nThe SQL:1999 standard, the successor to\nthe SQL-92 standard, requires support for recursive queries, and a large subset\nS0111e systerlls, notably IBM's DB2 DBMS, already support thelu.\nEvaluating Datalog queries poses some additional challenges, beyond those en-\ncountered in evaluating relational algebra queries, and we discuss sonle iUlpor-\ntant ilnplernentation and optimization techniques developed to address these\nchallenges. Interestingly, some of these techniques have been found to irnprove\nperforrnance of even nonrecursive SQL queries and have therefore been imple-\nrnented in several current relational DBMS products.\nIn Section 24.1, we introduce recursive queries and Datalog notation through\nan exaruple. We present the theoretical foundations for recursive queries, lea..'St\nfixpoints and least rnodels, in Section 24.2. We discuss queries that involve the\nuse of negation or set-difference in Section 24.3. Finally, we consider techniques\nfor evaluating recursive queries\nefficientl~y in Section 24.5.\n24.1\nINTR()DUCTION TO RECURSIVE QUERIES\n\\i\\re begin with a sinlple exaJnple that illustrates the li111its of S(~L-92 queries\ncUld the power of recursive definitions. Let Assernbly be a relation \\vith three\nfields part,\n8'ubpart, and qty.\nAn excunple instance of Assernbly is shc)\\vn in\nFigure 24.1. Each tuple in Assernbly indicates IH}w Inany copies of a particular\nsubpart are COlltained in a given part. The first tuple indicates, for exarnple,\nthat\n(1, trike contains three \"wheels. '}'he Assclnbly relation can be visuaJized a,s\na tree, as sho\\vn in Figure 24.2.\nA. tuple is shovvn as an edge going frorn the\npart to the subpaJ\"t, with the qty value as the edge label.\n\nDed'Uct'iveDatabases\n819\nJ;\npedal\ntire\nseat\n~\ntube\nnm\ntrike\n~\n#-\n•\nwheel\nframe\nA\nA\nspoke\ntrike\n\\vheel\n3\ntrike\nfralne\n1\nfrarne\nseat\n1\nfranle\npedal\n1\nwheel\nspoke\n2\n._..__.\n.__.._--\n\\vheel\ntire\n1\ntire\nrun\n1\ntire\ntube\n1\nFigure 24.1\nAn Instance of Assembly\nFigure 24.2\nAssembly Instance Seen as a Tree\nA natural question to ask is, \"What are the cornponents of a trike?\" Rather\nsurprisingly, this query is inlpossible to write in SQL-92.\nOf course, if we\nlook at a given instance of the Assernbly relation, we can write a 'query' that\ntakes the union of the parts that are used in a trike.\nBut such a query is\nnot interesting---we want a query that identifies all components of a trike for\nany instance of Assembly, and such a query cannot be written in relational\nalgebra or in SQL-92. Intuitively, the problem is that we are forced to join the\nAsselnbly relation with itself to recognize that trike contains spoke and tire,\nthat is, to go one level down the Assenlbly tree. For each additional level, we\nneed an additional join; two joins are needed to recognize that trike contains\nrim, which is a subpart of tire. Thus, the ntullber of joins needed to identify\nall subparts of trike depends on the height of the Assen1bly tree, that is, on\nthe given instance of the Assembly relation. No relational algebra query works\nfor all instances; given any query, we can construct an instance whose height is\ngreater than the nurnber of joins in the query.\n24.1.1\nDatalog\nWe now define a relation called Cornponents that identifies the cOlnponents of\nevery part. Consider the following program, or collection of rules:\nComponents (Part , SUbpart)\n\"-\nComponents (Part , Subpart) .-\nAssembly (Part , SUbpart, Qty) \"\nAssembly (Part , Part2, Qty) ,\nComponents (Part2 , Subpart)\"\nThese axe rules in Datalog, a relational query language inspired by Prolog, the\n\\vell-known logic progranuning language; indeed, the notation follows Prolog.\nThe first rule should be read as follo\\vs:\nFor all values of Part, Subpart, and\n(~ty,\n\n820\nCHAPTER 24\n$\nif there is a tuple (Part, Subpart, (~ty) in Assclnbly,\nthen there IJlUSt be a tuple (Part, Subpart) in (;olnponents.\nrThe second rule should be read as follovols:\nFor all values of Part, Part2, Subpart, a.nd Qty,\nif there is a tuple (Part, Part2, Qty) in Assernbly and\na tuple (Part2, Subpart) in Components,\nthen there HUlst be a, tuple (Part, Subpart) in C()lnponents.\nThe part to the right of the :- sYlnbol is called the body of the rule, and\nthe part to the left is called the head of the rule.\nThe syrnbol :- denotes\nlogical irnplication; if the tuples lIlentioned in the body exist in the database,\nit is irnplied that the tuple rnentioned in the head of the rule rnust also be\nin the database. (Note that the body could be ernpty; in this case, the tuple\nrnentioned in the head of the rule rnust be included in the database.) 1'herefore,\nif we are given a set of Assenlbly and Cornponents tuples, each rule can be\nused to infer, or deduce, sorne new tuples that belong in COlnponents. This\nis why database systerns that support Datalog rules are often called deductive\ndatabase systems.\nBy assigning constants to the variables that appear in a rule, we can infer a spe-\ncific Coruponents tuple. For example, by setting Part::::::trike, Subpart::::::wheel,\nand Qty=S, we can infer that (tTike, wheel) is in eoulponents. Each rule is\nreally a ternplate for Inaking inferences: An inference is the use of a rule to\ngenerate a new tuple (for the relation in the head of the rule) by substituting\nconstants for varia,bles in such a way that every tuple in the rule body (after\nthe substitution) is in the corresponding relation instance.\nBy considering each tuple in Asselnbly in turn, the first rule allows us to infer\nthat the set of tuples obtained by taking the projection of Assernbly onto its\nfirst two fields is in CCHnponents.\nThe seco11d rule then allo\\vs us to cOlnbine previously discovered Cornponents\ntuples with Assernbly tuples to infer new Cornponents tuples. \\Ve can apply\nthe second rule by considering the cross-product of Assernbly and (the current\ninstance of) Cornponents and assigning values to the variables in the rule for\neach rO¥l of the cl'oss-product, one row at a titne. ()bserve ho\\v the repeated\nuse of the varial)le Part2 prevents certain ro\\vs of the cross-product fronl con-\ntributing any ne\\v tuples; in effect, it specifies an equality join condition on\nAssenIbly and Cornpouents.\nThe tuples obtained by one application of this\nrule are shown in Figure 24.:t (In addition, COlnponents contains the tuples\nobtained l)y applying the first rule; these are not shown.)\n\nDerluct~iveDatabase8\ntrike\nspoke\nf----trike\ntire\n;\n_.__.\ntrike\nseat\n..__......_.._-\ntrike\npedal\nwheel\nrhn\nwheel\ntube\n'''_\"n\nFigure 24.3\nComponents Tuples Obtained\nby Applying the Second Rule Once\n821\ntrike\nspoke\n·~·t\"l~n{e···-·-·\ntire\ntrike\nseat\ntrike\npedal\nwheel\nrun\nwhe81- '''Tllb-e---l\ntrike\nrirn\ntrike\ntube\nFigure 24.4\nComponents Tuples Obtained by\nApplying the Second Rule Twice\nThe tuples obtained by a second application of this rule are shown in Figure\n24.4.\nNote that each tuple shown in Figure\n24.~~ is reinferred. Only the last\ntwo tuples are new.\nApplying the second rule a third time does not generate additional tuples. rrhe\nset of Components tuples shown in Figure 24.4 includes all the tuples that can\nbe inferred using the two Datalog rules defining Cornponents and the given\ninstance of Assembly.\nrrhe components of a trike can now be obtained by\nselecting all Cornponents tuples with the value trike in the first field.\nEach application of a Datalog rule can be understood in ternlS of relational\nalgebra. The first rule in our exarnple program simply applies projection to the\nAssernbly relation and adds the resulting tuples to the Cornponents relation,\nwhich is initially ernpty. The second rule joins Assernbly with COlllponents and\nthen does a projection. The result of each rule application is cornbined with\nthe existing set of Cornponents tuples using union.\nThe only Datalog operation that goes beyond relational algebra is the repeated\napplication of the rules defining CCHnponents until no new tuples are generated.\nThis repeated application of a set of rules is called the jiJ.:point operation, and\n\\ve develop this idea further in the next section.\nvVe conclude this section by rewriting the Datalog definition of Cornponents\nusing S(~L:1999 syntax:\nWITH RECURSIVE Cornponents(Part, Subpart) AS\n(SELECT A1.Part, AJ.Subpart FROM Assernbly .,A.I)\nUNION\n(SELECT A2.Part, Cl.Subpart\nFROM\nAssernbly A2; Cornponents C1\n\n822\nC~HAPTER 24\nWHERE\nA2.Subpart = Cl.PaTt)\nSELECT * FROM COlllponents C2\nThe WITH clause introduces a relation that is part of a query definition; this\nrelation is shnilar to a view, but the scope of a relation introduced using WITH\nis local to the query definition. The RECURSIVE key\\vord signals that the table\n(in our exarnple, Cornponents) is recursively defined.\nThe structure of the\ndefinition closely parallels the Datalog rules. Incidentally, if we wanted to find\nthe cornponents of a particular part, for exanlple, tTikc, we can sirnply replace\nthe last line ¥lith the following:\nSELECT * FROM Cornponents C2\nWHERE\nC2.Part = 'trike'\n24.2\nTHEORETICAL FOUNDATIONS\nWe classify the relations in a Datalog prograln as either output relations or in-\nput relations. Output relations are defined by rules (e.g., COluponents), and\ninput relations have a set of tuples explicitly listed (e.g., Assembly). Given\ninstances of the input relations, we Inust compute instances for the output re-\nlations. The meaning of a Datalog prograrIl is usually defined in two different\nways, both of which essentially describe the relation instances for the output\nrelations. Technically, a query is a selection over one of the output relations\n(e.g., all Components tuples C with C.paTt = tTike). However, the lueaning of\na query is clear once we understand how relation instances are associated with\nthe output relations in a Datalog progranl.\nrrhe first approach to defining the sernantics of a Datalog progralll, called the\nleast '{nodel sC'lnantics, gives users a way to understand the prograrn without\nthinking about how the prograrn is to be executed. That is, the sernanties is\ndeclarative, like the sernantics of relational calculus, and not o]JfTo,tional like\nrelational algebra sClnantics. This is irnportant becC111se recursive rules lnake it\ndifficult to understand a· prograIll in tcrrns of an evaluation strategy.\nThe second approach, called the least\nfi~rpoint 8crnantic8, gives a conceptu<tl\nevaluation strategy to COlnpute the desired relation insta.nces. This serves a..'3\nthe basis for recursive query evaluation in a I)Brv:rS. 1I10re efficient evaluation\nstrategies are used in an actual iInplernentation, but their correctness is sho\\vI1\nby dernonstntting their equivalence to the lea\"st fixpoint approach. rrhe fixpoint\nsClnantics is thus operational and. plays a. role analogous to that of relational\nalgebra sernalltics for nonrecursive queries.\n\nDeductive Databases\n24.2.1\nLeast Model Semantics\n823\nvVe \\¥ant users to be able to understand a Datalog progTarn by understanding\neach rule independent of other rules, vvith the lneaning: If the body 'istT'ue, the\nhead is alsotT'ue. This intuitive reading of a rule suggests that, given certain\nrelation instances for the relation naines that appear in the body of a rule,\nthe relation instance for the relation rnentioned in the head of the rule 111USt\ncontain a certain set of tuples. If a relation Harne R. appears in the heads of\nseveral rules, the relation instance for R IIlUSt satisfy the intuitive reading of\nall these rules. However, we do not want tuples to be included in the instance\nfor R, unless they are necessary to satisfy one of the rules defining R,. That is,\nwe want to cornpute only tuples for R that are supported by SaIne rule for R.\nTo lnake these ideas precise, we need to introduce the concepts of rnodels and\nleast models. A model is a collection of relation instances, one instance for each\nrelation in the prograrn, that satisfies the following condition. For every rule in\nthe prograrll, whenever we replace each variable in the rule by a corresponding\nconstant, the following holds:\nfr every tuple in the body (obtained by our replaceUlent of variables\nwith constants) is in the corresponding relation instance,\nI\n1hen the tuple generated for the head (by the assignrnent of constants\nto variables that appear in the head) is also in the corresponding rela-\ntion instance.\nObserve that the instances for\nthE~ input relations are given, and the definition\nof a rnodel essentially restricts the instances for the output relations.\nConsider the rule\nComponents (Part , Subpart) '-\nAssembly (Part , Part2, Qty) ,\nComponents (Part2, Subpart).\nSuppose we replace the variable Part by the constant wheel, Part2 by 'tin:,\n(~ty\nby 1, and Subpart by rirn:\nComponents (wheel , rim) '- Assembly(wheel, tire, 1),\nComponents (tire , rim).\nLet A be an instance of Assernbly and C be an instance of COlnpouents. If A\ncontains the tuple\n(udu~el, Lire, 1) and C contains the tuple (UTe, Tim,), then\nC rrulst also contain the tuple ('wheel, rirn) for the pajr of instancc~s A. and C\n\n824\nCHAPTER 211\nto be a rnodel.\n()f course, the instances A and (; rnust satisfy the inclusion\nrequirenlent just illustrated for every assignruent of constants to the variables\nin the rule: If the tuples in the rule body are in.A. and C, the tuple in the head\nInus1; be in C.\nAs an exarnple, the instances of Asscrnbly shown in Figure 24.1 and Cornpo-\nnents shovvn in F'igure 24.4 together fornl a rnodel for the Conlponcnts prograll1.\nC;iven the instance of Assernbly shown in Figure 24.1, there is no justification\nfor including the tuple\n(spok~e, pedal) to the COlnponents instance.\nIndeed,\nif we add this tuple to the cornponents instance in Figure 24.4, '-'Te no longer\nhave a lllodel for our program, a.s the following instance of the recursive rule\nderllonstrates, since (wheel, pedal) is not in the Cornponents instance:\nComponents (wheel , pedal) :-\nAssembly(wheel, spoke, 2),\nComponents(spoke, pedal).\nHowever, by also adding the tuple (wheel, pedal) to the Cornponents instance,\nwe obtain another rnodel of the Components prograrll. Intuitively, this is un-\nsatisfactory since there is no justification for adding the tuple (spoke, pedal)\nin the first place, given the tuples in the Assembly instance and the rules in\nthe prograln.\nWe address this problern by using the concept of a least rllodel. A least model\nof a prograrn is a rnodel M such that for every other model M2 of the sarne\nprogranl, for each relation Il in the progranl~ the instance for R in ]\\II is contained\nin the instance of R in 1\\12. The 1nodel forIned by the instances of Assernbly\nand COlnponents shown in Figures 24.1 and 24.4 is the least rHodel for the\nCC)lnponents progralll vvith the given Assernbly instance.\n241&2,,2\nThe Fixpoint Operator\nA fixpoint of a function f is a value v such that the function applied to the\nvalue returns the saIIle value, that is, f(v)\n=\n'U. Consider a function applied\nto a set of values that also returns a set of values. For eXH,rnple, we carl define\ndouble to l)e a function tllat Illuitiplies every elenlcnt of the input set by two\nand d(YlJ,blc+ tobe double U idenhty. T'hus, d()'ublc( {1,2,5} ) == {2,4,lO}, and\ndouble+( {1,2,5} ) ::::::: {1,2,4.,5,lO}.The set of all even integers\nwhich happens\nto be an infinite set-is a fixpoint of the function double-+. Another fixpoint\nof the function dO'lLble-f- is the set of all integers. l'he first fixpoint (the set of\nall\n(~ven integers) is .';'(nailer than the second fixpoint (the set of all integers)\nbecause it is contained in the latter.\n\nDed'uct'lvC Database,s\n825\nThe least fixpoint of a function is the fixpoint that is slnaller than every other\nfixpoint of that function. In general, it is not glHlranteed that a function has\na loa.\",t fixpoint. For exaIl1ple, there lnay be t'\\¥o fixpoints, neither of \\vhich is\n81na11er than the other. (Does double have a least fixpoint? What is it?)\nNo\\V let us turn to functions over sets of tuples, in particular, functions defined\nusing relational algebra expressions. The Cornponents relation can be defined\nby an equation of the fonn\nC!orrLponents\n=\nJrl,5 (Assernbly [)<J2=1 Cornponents) U\n7Tl,2 (Assernbly)\n1'his equation has the forn1\nCornponents = f(Cornponents,Assembly)\nwhere the function f is defined using a relational aJgebra expression.\nFor a\ngiven instance of the input relation Assernbly, this can be sirnplified to\nC7ornponents\n=\nf(C\nfo1nponents)\nThe least fixpoint of f is an instance of Cornponents that satisfies this equa-\ntion.\nClearly the projection of the first two fields of the tuples in the given\ninstance of the input relation Assernbly rnust be included in the (instance that\nis the) least fixpoint of Cornponents. In addition, any tuple obtained by joining\nComponents with Assernbly and projecting the appropriate fields IllUst also be\nin Components.\nA little thought shows that the instance of Components that is the least fixpoint\nof f can be ccnnputed using repeated applications of the Datalog rules sho\\vn\nin the previous section. Indeed, applying the two Datalog rules is\nid(~ntical to\nevaluating the relational expression used in defining COlnponcnts. If an appli-\ncation generates Cornponents tuples that are not in the current instance of the\nCornponents relation, the current instance cannot be the fixpoint. Therefore,\nwe add the new tuples to Cornponents <tnd evalu<.tte the relational expression\n(equivalently, the two Datalog rules) again. T'his process is repeated until ev-\nery tuple generated is already in the current instance of Cornponents. \\\\Then\napplying the rules to i,he currerlt set of tuples does not produce any rl(~\\V tuples,\n\\ve have reached a fixpoint.\nIf CC)lnponents is initialized to the erupty set of\ntuples. intuitively vve infer only tuples that (1,1'e necessary b:y the definition of a\nfixpoint, and the fixpoint cornputed is the least fixpoint.\n24.2.3\nSafe Datalog Programs\n(~onsider the follovving p1'ograrn:\nComplexYarts (Part)\n: - Assembly(Part, Subpart, Qty) , Qty > 2.\n\n826\nCHAPTER 21\nAccording to this rule, a cOlnplex part is defined to be any part that ha\" Inore\nthan t\\VO copies of anyone subpart. :1:01' each part lnentioned in the Asselnbly\nrelatioIl, \\ve can cac.;ily check \\vhether it is a cOll1plex part. In contrast, consider\nthe following prograrn:\nPriceYarts (Part, Price) '-\nAssembly (Part , Subpart, Qty) , Qty> 2.\nThis variation seeks to associate a price with each cornplex part. IIowever, the\nvariable Price does not appear in the body of the rule.\nThis Ineans that an\ninfinite number of tuples must be included in any model of this progralll. To\nsee this, suppose we replace the variable Part by the constant trike, SubPart by\nwheel, and Qty by 3. This gives us a version of the rule with the only remaining\nvariable being Price:\nPriceYarts(trike,Price) :- Assembly (trike , wheel, 3), 3 > 2.\nNow, any assignment of a constant to Price gives us a tuple to be included in\nthe output relation Price__.Parts. For example, replacing Price by 100 gives us\nthe tuple Price_Parts(trike,lOO). If the least Inodel of a progralIl is not finite,\nfor even one instance of its input relations, then we say the program is unsafe.\nDatabase systems disallow unsafe programs by requiring that every variable\nin the head of a rule also appear in the body.\nSuch progralns are said to\nbe range-restricted, and every range-restricted Datalog prograln has a finite\nleast model if the input relation instances are finite. In the rest of this chapter,\nwe &'3SUllle that prograrns are range-restricted.\n24.2.4\nLeast Model =Least Fixpoint\nDoes a Datalog prograln always have a least rnodel?\n()r is it possible that\nthere are two rnodels, neither of which is contained in the other'?\nSirnilarly,\ndoes every Datalog progranl have a least fixpoint?\nVvThat is the relationship\nbetween the least rnodel and the least fixpoint of <t Datalog prograln?\nAs we noted earlier, not every function has a lea\"st fixpoint. Fortunately, every\nfunction defined in tenns of relational algebra expressions tl1at do not contain\nset-difference is ,guaranteed to have a least fixpoint, and the least fixpoint can\nbe cornputed by repeatedly evaluating the functic)ll.\nl'his tells us that every\nl)atalog prograrn has a le~t.st fixpoint and that it can l)e cOlnputed by repeatedly\napplyi11g the rules of th(~ l)rogranl on the given instances of the input relations.\nF\\u·ther, every I)atcl.log progr(un is glla.r<lnteed to have a least rnodel <lud the\nleast rnodel is equal to the least fixpoint of the I>l'ograln. These results (whose\n\nDeducti've Databases\n827\nproofs we do not discuss) provide the bclSis for Datalog query processing. 'Users\ncan understand a progTarn in terms of 'If the body is true, the head is also true,'\nthanks to the least l1lodel sClnantics. rrhe DBNIS can COlllpute the answer by\nrepeatedly applying the prograrn rules, thanks to the least fixpoint sernantics\nand the fact that the least nlodel and the least fixpoint are identical.\n24.3\nRECURSIVE QUERIES WITH NEGATION\nUnfortunately, once set-difference is allo\\ved in the body of a rule, there r11ay\nbe no least rnodel or least fixpoint for a program. Consider the following rules:\nBig(Part):-\nAssembly (Part , Subpart, Qty) , Qty> 2,\nNOT Small (Part) .\nSmall (Part) :-\nAssembly (Part , Subpart, Qty) , NOT Big(Part).\nThese two rules can be thought of as an attenlpt to divide parts (those that\nare mentioned in the first colulnn of the Asselubly table) into two classes, Big\nand Small. The first rule defines Big to be the set of parts that use at least\nthree copies of some subpart and are not classified as small parts. The second\nrule defines Small as the set of parts not classified as big parts.\nIf we apply these rules to the instance of Assembly shown in Figure 24.1, trike is\nthe only part that uses at least three copies of senne subpart. Should the tuple\n(trike) be in Big or SUlall? If we apply the first rule and then the second rule,\nthis tuple is in Big. To apply the first rule, we consider the tuples in Asselubly,\nchoose those with Qty > 2 (which is just (trike)), discard those in the current\ninstance of Srnal1 (both Big and Small are initially elnpty), and add the tuples\nthat are left to Big. 1'herefore, an application of the first rule adds (trike) to\nBig. Proceeding siInilarly, \\ve can see that if the second rule is applied before\nthe first, (tTike) is added to Srnall instead of Big.\nrrhis prognun has hvo fixpoints, neither of 'which is srnaller than the other, as\nshown in Figure 24.5. (rhe first fixpoint 11&\\) a Big tuple that does not appear in\nthe second fixpoint; therefore, it is not sInaBer than the second fixpoint. 1'he\nsecond fixpoint ha,s a 81na11 tuple that does not appear in the first fixpoint;\ntherefor(\\ it is D.ot sr11a11e1' than the first fixpoint.\nThe order ill \\vhich \\ve\napply the\nrul(~s detennines \\vhich fixpoint is cOlnputed; this situation is very\nunsatisfactory.\\Ve want users to be able to understand their queries vvithout\nthinking (1)out exactly\" ho\\v the evaJuation proceeds.\n]'he root of the problerH is the use of NOT. \\Vhen \\ve apply the first rule, senne\nirlferences (1re\ndisallc:)\\~red because of the presence of tuples in 8rna11.\nPcLrts\n\n828\nBig\nSmall\ntrike\nFixpoint 1\nBig\nSmall\nC;HAPTER 24\n\"\nFixpolnt 2\nFigure 24.5\nTwo Fixpoints for the Big/Small Program\nthat satisfy the other conditions in the body of the rule are candidates for\naddition to Big; we remove the parts in 8rna11 frorn this set of candidates.\nThus, sorne inferences that are possible if 8ruall is ernpty (as it is before the\nsecond rule is applied) are disallowed if SInall contains tuples (generated by\napplying the second rule before the first rule).\nHere is the difficulty: If NOT\nis used, the addition of tuples to a relation can disallow the inference of other\ntuples. Without NOT, this situation can never arise; the addition of tuples to a\nrelation can never disallow the inference of other tuples.\nRange-Restriction and Negation\nIf rules are allowed to contain NOT in the bodYl the definition of range-restriction\nrnust be extended ensure that all range-restricted prograrJlS are safe. If a re-\nlation appears in the body of a, rule preceded by NOT 1 we call this a negated\noccurrence. Relation occurrences in the body that are not negated are called\npositive occurrences.\nA prograrn is range-restricted if every variable in\nthe head of the rule appears in sorne positive relation occurrence in the body.\n24.3.1\nStratification\nA widely used solution to the problern caused by negation, or the use of NOT,\nis to irnpose certain syntactic restrictions on prograrlls. rrhese restrictions can\nbe ea~sily checkecl and progrcuns that satisfy thern have a natural lneaning.\n\\Ve say that a tableT depends on a table 8 if sorne rule with T in the head\ncontains 5\", or (recursively) contains a predicate that depends on 8 ~ in the\nbod:y. A recursively defined predicate always depends on itself. For exarnple,\nBig depends on Sruall (and on itself).\nIndeed, the tables Big and Srnall (l,re\n\nDedv\"ct'l'tle Databa,scs\n8' ')9,\n_04:.1\nnlutually recursive, that is, the definition of 'Big depends on SrnaU and vice\nversa. ,\"Ve say that a table 'T depends negatively on a tal)le 8 if SCHne rule\n\\vith 'T in the head contains NOT S, or (recursively) contains a predicate that\ndepends negatively on S, in the body.\nSuppose \\ve classify the tables in a prograrll into strata or layers as follows.\nThe tables that do not depend on any other tables aTe in straturll O.\nIn our\nBig/S1nall exarnple, ASSCIIlbly is the only table in stratu1Il O. Next, \\ve identify\ntables in straturll 1; these are tables that depend only on tables in stratuln 0\nor straturn 1 and depend negatively only on tables in straturn O. Higher strata\nare sirnilarly defined: '}'he tables in straturni are those that do not belong to\nlOVvTer strata, depend only on tables in stratuIll i O[ lower strata, and depend\nnegatively only on tables in lo\\;ver strata. A stratified program is one whose\ntables can be classified into strata according to the above algoritlull.\nrrhe Big/Sruall progralIl is not stratified. Since Big and Snlall depend on each\nother, they 1nust be in the sarne straturn.\nHo\\vever, they depend negatively\non each other, violating the requirc1Ilent that a table can depend negatively\nonly on tables in lower strata. Consider the following variant of the Big/Srnall\nprogra1Il, in which the first rule has been rnodified:\nBig2(Part) :- Assembly (Part , Subpart, Qty) , Qty> 2.\nSmal12(Part) :- Assembly (Part , Subpart, Qty) , NOT Big2(Part).\nThis prograrn is stratified. Slnall2 depends on Big2 but Big2 does not depend\non 8111a1l2.\nAssernbly is in stratu111 0, Big is in straturn 1, and Srna1l2 is in\nstraturn 2.\nA stratified prograrn is evaluated stratu1n-by-straturn, starting with stratunl\nO. 'To evaluate a straturn, we cornpllte the fixpoint of all rules defining tables\nin this straturn. \"Fhen evaluating a straturn, any occurrence of NOT involves\na table frorH a lower straturn, \\\\'hich has therefore been corupletely evaluated\nby no\\\\'. The tuples in the negated table still disallow sorne inferences, but the\neffect is cornpletely deterrninistic, given the straturn-by-straturn evaJuation. In\nthe\nc~xa.Inple, Big2 is C01uput8(1 before 81na1l2 because it is in\n('t loviler straturIl\nthan 8rna112:\n(triA,~e) is added to Big2.\nNext, 'when we cornpute 81na112, \\ve\nrecognize that (trike) is not in 8rna112 l)ecause it is already in Big2.\nIncidentally, note that the stratified Big/Srnall progranl is not even recursive. If\n\\ve repla,ce ..Assernbl.y b)l\" Cornponents, \\ve obtain a recursive, stratified prograrn:\n}\\,sscrnbly is in straturn 0, Cornponents is in stratlull 1, Big2 is also in straturn\n1~ and 81na112 is in straturn 2.\n\n830\nIntuition behind Stratification\n(;IIAPTgR 24\n®\nConsider the stratified version of the Big/Slnall prograrll.\nThe rule defining\nBig2 forces us to add (l:T'ike) to Big2 and it is natural to a\"C;;SUlne that (tirike) is\nthe only tuple in Big2, because \\ve have no supporting evidence for any other\ntuple being in Big2.\nThe rninirnal fixpoint conrputecl by stratified fixpoint\nevaluation is consistent \\vith this intuition. However, there is another rninhnal\nfixpoint: \\Ve can place every part in Big2 and rnake Srna1l2 be ernpty. \\\\Thile\nthis assignrllent of tuples to relations seeIns unintuitive, it is nonetheless a\nrninimal fixpoint.\nrrhe requirernent that prograrns be stratified gives lIS a natural order for eval-\nuating rules. When the rules are evaluated in this order, the result is a unique\nfixpoint that is one of the minirnal fixpoints of the prograrll.\nThe fixpoint\nC0111puted by the stratified fixpoint evaluation usually corresponds well to our\nintuitive reading of a stratified prograrll, even if the program has rnore than\none rllininlal fixpoint.\nFor nonstratified Da.talog progranls, it is harder to identify a natural model\nfrorn arnong the alternative rninirnal rnodels, especially when we consider that\nthe Ineaning of a prograrll must be clear even to users who lack expertise in\nDlathelnatical logic. Although considerable research has been done on identi-\nfying natural rnodels for nonstratified prograrns, practical irnplernentations of\nDatalog have concentratt~d on stratified prograrns.\nRelational Algebra and Stratified Datalog\nEvery relational algebra query can be written as a range-restricted, stratified\nDatalog progra.rn.\n(Of course, not all Datalog progranls can be expressed in\nrelational algebra; for exarnple, the Cornponents prograrn.)\n'We sketch the\ntranslation frorn algebra to stratified Datalog by writing a Datalog progra.rn for\neach of the b::l..sic algebra operations, in terrns of two eXC1rnple tables R, and S,\neach with t¥lO fields:\nSelection:\nProjection:\nCross-product:\nSet-difference:\nlJnion:\nIlesult(Y) :- Il(X,Y), X=c.\nItesult(Y) :- H(X,Y).\nIlesult(X:,Y,lJ,V) :- Il(X,YL S(lJ,V).\n11esult(X,Y) :- Il(X,yT), NOT S(U,V).\nH.esult(X,Y) :- R,(X,Y).\nResult(X,Y) :- S(X,Y).\n\\Ve conclude ()ur discussion of stratification l>y noting that S(~L:1999 requires\nprograrns to be stratified. rrhe stratified Big/Sruall prograrn is shovvn belovl in\nSCJL: 1999 notation, vvith a final additional selection on Big2:\n\nDed'uct'ive Databases\n831\nSQL:1999 and Datalog Queries: A Datalog rule is linear recursive\nif the body contains at Illost one occurrence of any table that depends on\nthe table in the head of the rule. A linear recursive program contains\nonly linear recursive rules. All linear recursive Datalog progranls can be\nexpressed using the recursive features of SC~L:1999. IIowever, these features\nare not in Core SQL.\nWITH\nBig2(Part) AS\n(SELECT\nA1.Part FROM Assernbly Al WHERE Qty > 2)\nSrnall2(Part) AS\n((SELECT- A2.Part FROM Assernbly A2)\nEXCEPT\n(SELECT\nBl.Part fron1 Big2 Bl))\nSELECT * FROM Big2 B2\n24.4\nFROM DATALOG TO SQL\nTo support recursive queries in SQL,we lllust take into account the features\nof SQL that are not found in Datalog.\nTwo central SQL features rnissing in\nDatalog are (1) SQL treats tables as Tntlltisets of tuples, rather than sets, and\n(2) SQL pennits grouping and aggregate operations.\nThe rnultiset selnantics of SQL queries can be preserved if we do not check for\nduplicates after applying rules. Every relation instance, including instances of\nthe recursively defined tables, is a lllultiset.\nrrhe nurnber of occurrences of a\ntuple in a relation is equal to the nurnber of distinct inferences that generate\nthis tuple.\nThe second point can be addressed by extending Data.logwith grouping and\naggregation operations. Tlhis rnust be done\\vith rnultiset sernantics in rnind,\nas \\ve no\\v illustrate. Consider the following prograrn:\nNumPartsCPart, SUM((Qty))) :- AssemblyCPart, Subpart, Qty).\n'fhis prograrn is equivalent to the SC~L query\nSELECT\nA.Part, SUM (A.Qty)\nFROM\nAssernbly r'\\\nGROUP BY A.Part\n\n832\n(JHAPTER 24\nThe angular brackets (...) notation \\va\"s introduced in the LDL deductive sys-\nteln~ one of the pioneering deductive database prototypes developed at IVICC\nin the late 19808. VVe use it to dell0te TlH1ULsct\ngeneTat'ion~ or the creation of\nrnultiset-values. In principle, the rule definil1gNurnParts is evaluated by first\ncreating the telnporary relation ShO\\Vll in Figure 24.(3. \\Ve create the ternporary\nrelation by sorting on the part attribute (which appears on the left side of the\nrule, along with the (...) terrn) and collecting the I11ultiset of qtU values for\neach po,Tt value. vVe then apply the SUM aggregate to each lllultiset-value in the\nsecond colu111n to obtain the ans\\ver ~ \\vhich is sho\\vn in Figure 24.7.\n-.......\"\"\"\"\"\"\"\"........\ntrike\n4\nf--._.,_.•\nfran1e\n2\n---\nwheel\n3\ntire\n2\n_..\nl part\n[SUM ((qty) 'I\ntrike\n{3,l}\n1--.\nfrarne\n{l,l}\n~•..__.\nwheel\n{2,l}\n...._.__.\ntire\n-'{1,l}\n'~_.\"\"-',\"-\"-\nI part\nI (qty) I\nFigure 24.6\nTemporary Relation\nFigure 24.7\nThe Tuples in NumParts\nThe telnporary relation shown in Figure 24.6 need not be nlC1terialized to corn-\npute NurnParts; for exalllplc, SUM can be applied on-the-fly or Assenlbly can\nsirnply be sorted and aggregated as described in Section 14.6.\nThe use of grouping and aggregation, like negation, causes cOlnplicatiol1s when\napplied to a partially cOlnputed relation. rrhe difficulty is overcorne by adopt-\ning the sarne solution used for\nn(~gation, stratification. Consider the following\nprograrn: 1\nTotParts(Part, Subpart, SUM«(Qty))) :- BOM(Part, Subpart, Qty).\nBOM(Part, Subpart, Qty) :- Assembly (Part , Subpart, Qty).\nBOM(Part, Subpart, Qty) :- Assembly(Part, Part2, Qty2) ,\nBOM(Part2, Subpart, Qty3) , Qty=Qty2*Qty3.\nThe idea is to count the l111rnber of copies of Subpart for each Part. By ~1ggre­\ngating over B()l\\II rather than Assernbly, we count subparts at any level in the\nhierarchy instead of just irnrnediate subparts. This prograrn is a version of a\nvvell-known problcrl1 called Bill-of-1Vlo,terials and variants of it are probably the\nlnost \\vide1)\" used recursive queries in practice.\n'rhe irnportant point to note in this exarnple is that we Inust vvait until the\nrelation BC)]VI has been cornpletely evaluated l)(·Jore \\ve apply the rrotParts\n~\"ule. ()thervvis8, \\ve obta.in incornplete counts. T\n1his situation is analogous to\ntheprobler11 we faced 'with negation; we have to\n(~valuate the negated rel[\\,tion\n1The reader should write this in SQL: 1999 syntax, as a sirnple exercise.\n\nDed'ltctive Databases\n833\ni\\\n~~:1999Cycle D~~:tion:-~~e Da;~~~~ qU;~~~-thatd~~ot l~::ith-l\n111etie operations have finite answers and the fixpoint evaluation is guaran-\n!\nteed to halt. Unfortunately, recursive SQL queries Ina)' have infinite answer\nsets and query eva1ua,tion rnay not halt. l'here are tvvo independent rea,..\nI\nsons for this: (1) the use of aritlnnetie operations to generate data values\nI\nthat are not stored in input tables of a query, and (2) rTIultiset scrnantics\n1\n1\n\"'1\nfor rule applications; intuitively, problems arise from cycles in the data.\n,\n(To see this, consider the Cornponents prograrn on the Assenlbly instance\nI\nshown in Figure 24.1 plus the tuple (tube, 'wheel, 1).) SQL:1999 provides\nI_special constructs to check for such cycles.\n_~\ncornpletely before applying a rule that involves the use of NOT. If a prograrn is\nstratified with respect to uses of (...) as well as NOT, stratified fixpoillt evalua-\ntion gives us 111eaningful results.\nThere are two further aspects to this exarnple. First, we rnust understand the\ncardinality of each tuple in BOlVI, based on the rnultiset sernantics for rule\napplication. Second, we rnust understand the cardinality of the multiset of Qty\nvalues for each (Part, Subpart) group in TotParts.\nI part\n.._[ subpart]···qtyl\n0----\n.\nfrarue\n_..._.\ntrike\n1\n...\n._~\n-·'\"'trike\nseat\n1\nfra.rne\nseat\n1\nframe\npedal\n2\n.~.~...\nseat\ncover\n1\n~-\"....\n_........\n\".'\"-\nFigure 24.8\nAnother Instance of Assembly\ntrike\ny~,,-\nwheel\nframe\nA\nA\nspoke\ntire\nseat\npedal\n~\nnm\ntube\nFigure 24.9\nAssembly Instance Seen a,..s a Graph\n\\Ve illustrate these t\\VO points using the instance of Assernbly shown in Figures\n24.8 and 24.9. f\\pplying the first BONI rule, we add (one copy of) every tuple in\nAssernbly to BOl\\J1. Applying the second BOIVI rule, \\ve ctdd the follo\\ving four\ntuples to B()l\\JI: (trike, scat, 1), (trike, pedal, 2) \\ (trike, coveT, 1), and (frarne,\ncoveT, 1). ()bserve that the tuple (trike, seat, 1) \\vas\n(11r(~ady in BOl\\/f because\nit \\vas generated by ctpplying the first rule; therefore, rnultiset sernantics for\nrule application gives us two copies of this tuple. Applying the second BC)IVI\nrule on the new tuples, we generate the tuple (b'ike, cover, 1) (using the tuple\n(fran~e, cover, 1) for BaNI in t.he body of the rule): this is our second copy of\nthe tuple. i\\pplying the second rule again on this\ntupl(~ does not generate any\n\n834\nCHAPTER 24\n,\ntuples, and the cOInputation of the BO:NI relation is now cOlnplete. The BaM\ninstance at this stage is sho\\vn in Figure 24.10.\ntrike\nfrarne\n1\ntrike\nseat\n1\nframe\nseat\n1\nfraIne\npedal\n2\nseat\ncover\n1\ntrike\nseat\n1\ntrike\npedal\n2\ntrike\ncover\n1\nframe\ncover\n1\ntrike\ncover\n1\nFigure 24.10\nInstance of BON! Table\ntrike\nfraIlle\n{1}\ntrike\nseat\n{1,1}\ntrike\ncover\n{1,1}\ntrike\npedal\n{2}\nfram.e\nseat\n{1}\nf---fr-aI-n-e-l--p-e-d-a-1--+-·T2T···_·\n-+-----+-~:;,-----l\nseat\ncover\n{I}\nframe\ncover\n{I}\nFigure 24.11\nTemporary Relation\nMultiset grouping on this instance yields the temporary relation instance shown\nin Figure 24.11. (This step is only conceptual; the aggregation can be done on\nthe fly without materializing this terllporary relation.)\nApplying SUM to the\nrllultisets in the third column of this temporary relation gives us the instance\nfor TotParts.\n24.5\nEVALUATING RECURSIVE QUERIES\nrrhe evaluation of recursive queries has been widely studied.\nWhile all the\nproblems of evaluating nonrecursive queries continue to be present, the newly\nintroduced fixpoint operation creates additional difficulties. A straightforward\napproach to evaluating recursive queries is to cornpute the fixpoint by repeat-\nedly applying the rules as illustrated in Section 24.1.1. One application of all\nthe prograrn rules is caIled an iteration; we perfonn as rnany iterations as nec-\nessary to reach the le&'3t fixpoint. This approach has two rnain disadvantages:\nII\nRepeated Inferences: As Figures 24:.:3 and 24.4 illustrate, inferences are\nrepeated across iterations. That is, the sarne tuple is inferred repeatedly\nin the \",arne way, using the ScHne rule and the seune tuples for tables in the\nbody of the rule.\n11II\nUnnecessary Inferences: Suppose we want to find the cornponents of\nonly a wheel. Cornputing the entire Cornponents table is \\ve:lsteful and does\nnot take advantage of inforrnation in the query.\n\nDeductive Databases\n835\n$\nIn this section, we discuss how each of these difficulties can be overcorne. \\Ve\nconsider only Datalog progralns without negation.\n24.5.1\nFixpoint Evaluation without Repeated Inferences\nCOlnputing the fixpoint by repeatedly applying all rules is called Naive fix-\npoint evaluation. Naive evaluation is guaranteed to cornpute the least fix-\npoint, but every application of a rule repeats all inferences lllade by earlier\napplications of this rule. We illustrate this point using the following rule:\nComponents (Part , Subpart) :-\nAssembly (Part , Part2, Qty) ,\nComponents (Part2, Subpart).\nWhen this rule is applied for the first time, after applying the first rule defining\nComponents, the Components table contains the projection of Assembly on\nthe first two fields. Using these Components tuples in the body of the rule, we\ngenerate the tuples shown in Figure 24.3. For example, the tuple (wheel, rim)\nis generated through the following inference:\nComponents (wheel , rim) :- Assembly(wheel, tire, 1),\nComponents (tire, rim).\nWhen this rule is applied a second tilne, the Components table contains the\ntuples shown in Figure 24.3 in addition to the tuples that it contained before\nthe first application. Using the Components tuples shown in Figure 24.3 leads\nto new inferences; for example,\nComponents(trike, rim) :- Assembly(trike, wheel, 3),\nComponents (wheel, rim).\nHowever, every inference carried out in the first application of this rule is also\nrepeated in the second application of the rule, since all the Assernbly and\nCornponents tuples used in the first rule application are considered again. For\nexarnple, the inference of (wheel,\nTiTr~) shown above is repeated in the second\napplication of this rule.\n1~he solution to this repetition of inferences consists of rernelnbering which\ninferences were carried out in earlier rule applications and not carrying theln\nout again.\nvVe can 'relnclnber' previously executed inferences efficiently by\nsirnply keeping track of which COlnponents tuples were generated for the first\ntiIne in the rnost recent applica.,tion of the recursive rule.\nSuppose \\ve keep\ntrack by introducing\n(1, new relation called delta._Clornponcnts and storing just\nthe newly generated Cornponents tuples in it. Now, we can use only tlH~ tuples\n\n(~HAPTER ~4\nin deUeL G1(nnponents in the next application of the recursive rule; any inference\nusing other COIuponents tuples should have been carried out in earlier rule\nEtpplications.\nThis refincrllcnt of fixpoint evaluation is called Seminaive fixpoint evalua-\ntion.Let us trace Serninaive fixpoint evaluation on our exarllple prognllTI. The\nfirst application of the recursive rule produces the Cornponents tuples shown in\nFigure 24.3, just like Naive fixpoint evaluation, and these tuples are placed in\ndelta_ C:on~ponents. In the second application, however, only delta_ C;()'{nponents\ntuples are considered, which rneans that only the following inferences are carried\nout in the second application of the recursive rule:\nComponents (trike , rim) :- Assembly(trike, wheel, 3),\ndelta_Components(wheel, rim).\nComponents (trike , tube)\n:-Assembly(trike, wheel, 3),\ndelta_Components(wheel, tube).\nNext, the bookkeeping relation delta_Cornponents is updated to contain just\nthese two Cornponents tuples. In the third application of the recursive rule, only\nthese two delta_ Cornponents tuples are considered and therefore no additional\ninferences can be nlade. The fixpoint of Cornponents has been reached.\nTo irnplernent Serninaive fixpoint evaluation for general Datalog prograrns, we\napply all the recursive rules in a prograrll together in an iteration. Iterative\napplication of all recursive rules is repeated until no new tuples are generated in\nSOHle iteration. 10 surnrnarize how Serninaive fixpoint evaluation is carried out,\nthere are two irnportant differences with respect to Naive fixpoint evaluation:\niIII\nWTe rnaintain a delta version of every recursive predicate to keep track of the\ntuples generated for this predicate in the Inost recent iteration; for excunple,\ndelta_ Cornponents for COHlponents. rrhe delta versions are updated at the\nend of each iteration.\nII\n1'he original prograrn rules are re\\vritten to ensure that every inference uses\nat least one delta tuple; that is, one tuple that\\vas not kno\\vn before the\nprevious iteration. This property guarantees that the inference could not\nhave been caxriccl out in earlier iterations.\n\\JVe do lI0t discuss details of Serninaive fixpoint evaluation (such fiB the a.lgo-\nritlun for rc\\vriting progranl rules to ensure the use of a delta tuple in each\ninference) .\n\nDed'uctive Databases\n24.5.2\nPushing Selections to Avoid Irrelevant Inferences\n837\nConsider a nonrecursive vievv definition.\nIf \\ve \\vant only those tuples in the\nviC\\\\T that satisfy an additional selection condition, the selection can be aJlded\nto the plan as a final operation, and the relational algebra transforlnations\nfor conunuting selections with other relational operators all<)\\v us to 'push'\nthe selection ahead of rnore expensive operations such as cross-products (;l,nd\njoins. In effect, \\ve restrict the cornputation by utilizing selections in the query\nspecification. 1'he problerIl is rnore cOlnplicated for recursively defined queries.\n\\Ve use the following progranl as an exarnple in this section:\n8ameLevel(81 , 82)\n8ameLevel(81 , 82)\nAssembly(P1, 81, Q1),\nAssembly(Pl, 82, Q2),\nAssembly(Pl, 81, Qi),\n8ameLevel(Pl, P2), Assembly(P2, 82, Q2).\nConsider the tree representation of Assernbly tuples illustrated in Figure 24.2.\n1\"here is a tuple (81,82) in SarneLevel if there is a path froln 81 to 82 that\ngoes up a certain nUlnber of edges in the tree and then CaInes down the saIne\nnurnber of edges.\nSuppose we want to find all SalneLevel tuples with the fIrst fIeld equal to\nspoke. Since SalneLevel tuples can be used to COlupute other SarneLevel tuples,\nwe cannot just cornpute those tuples with spoke in the first field. For exa.rnple,\nthe tuple (1uheel, frarne) in SarneLevel allows us to infer a SarneLevel tuple\nwith spoke in the first field:\n8ameLevel(spoke, seat) '-\nAssembly(wheel, spoke, 2),\n8ameLevel (wheel , frame),\nAssembly(frame, seat, i),\nIntuitively, we have to conlpute all SarneLevel tllpleswhose first field conta,ins\na. value on the path froln .spoke to the root in Figure 24.2. Each such tuple has\nthe potentia1 to contribute to (lnS\\Vers for the given query. On the other hand,\ncornputing the entire SarneLevel table is wasteful; for exarnple, the SalneLevel\ntuple (l'ir'e, 8(:'0,1:) cannot be used to infer (lIly (1118\\Ver to the given query (or,\nindeed, to infer any tuple that can in turn be used to infer an ans\\ver tuple).\n\\iVe define\n(1, new table, \\vhich \\ve call l\\1agic_SaIneLevel, such that each t11ple\nin this table identifies a value Tn for \\vhich\"ve have to cornpute all SarneLevel\ntuples with Tn in the first colulun to ansvver the given query:\nMagic_SameLevel (Pi)\n:- Magic_.SameLevel (81), Assembly(P1, 81, Ql).\nMagic ..8ameLevel (spoke) '-\n\n838\n(;IIAPTER 44\nConsider the tuples in lVlagic_SanleLevel.\nObviously we have (spoke).\nlJs-\ning this lVlagic_SalneLevel tuple and the Assernbly tuple ('LVheel, spoke, 2), we\ncan infer that the tuple (wheel) is in J\\1agic_SarneLevel. lJsing this tuple aJld\nthe Assernbly tuple (tT'ike, 'wheel, a), \\ve can infer that the tuple (tT'ike) is in\nNlagic_SarneLevel. Thus, J\\tIagic_SarneLevel contains each node that is on the\npath frorn spoke to the root in Figure 24.2. The Magic_SarneLcvel table can be\nllsed as a filter to restrict the computation:\n5ameLevel(51 , 52) :- Magic_5ameLevel(51) ,\nAssembly(P1, 51, Q1), Assembly(P2, 52, Q2).\n5ameLevel(51 , 52) :- Magic,._5ameLevel(51) , Assembly(Pl, 51, Ql),\n5ameLevel(Pl, P2), Assembly(P2, 52, Q2).\nThese rules together with the rules defining rvlagic_SarneLevel give us a pro-\ngranl for cornputing all SanleLevel tuples with spoke in the first column. Notice\nthat the new progranl depends on the query constant spoke only in the sec-\nond rule defining lVlagic_SameLevel. Therefore, the program for cornputing all\nSameLevel tuples with seat in the first column, for instance, is identical except\nthat the second Magic_SarneLevel rule is\nMagic_5ameLevel(seat) :- .\n~rhe nurnber of inferences rnade llsing the Magic program can be far fewer than\nthe nurnber of inferences nlade using the original progranl, depending on just\nhow rnuch the selection in the query restricts the cornputation.\n24.5.3\nThe Magic Sets Algorithm\nWe illustrated the intuition behind the Magic Sets algorithrn on the SarneLevel\nprograrn, which contains just one output relation and one recursive rule.\nThe intuition behind the rewriting is that the rows in the Magic tables cor-\nrespond to the subqueries whose answers are relevant to the original query.\nBy evaluating the rewritten prograrn instead of the original prograrn, \\ve can\nrestrict cornputation by intuitively pushing the selection condition in the query\ninto the recursion.\nrIhe algorithrn, however, ca.Il be applied to any Datalog prograrn. T'he input to\nthe algorithrn consists of the prograrn and a query pattern, which is a relation\nwe want to query plus the fields for which a query will provide constants. The\noutput of the algorithrn is a rewritten prograrn.\nThe l\\1a,gic Sets prognun rewriting algorithrn can be surnrnarized a..'3 follows:\n\n]Jeductive IJatabases\n8~9\n1. Generate the Adorned Prograln: In this step~ the progranl is re\\vritten\nto l1lake the pattern of queries and subqueries explicit.\n2. Add Magic Filters: IVlodify each rule in the Adorned Prograrn by adding\na IVlagic condition to the body that acts a'S a filter on the set of tuples\ngenerated by this rule.\n~~. Define the Magic Tables:\nWe create nc\\v rules to define the l\\tlagic\ntables.\nIntuitively, froIll each occurrence of a table R in the body of an\nAdorned PrograIu rule, we obtain a rule defining the table\n~1.agi(>_I{.\nvVhen a query is\np()sed~ we add the corresponding Iv1agic tuple to the rewrit-\nten prograrl1 and evaluate the least fixpoint of the prograrIl (using Serninaive\nevaluation).\nWe rernark that the Magic Sets algorithrll has turned out to be quite effective\nfor cornputing correlated nested SQL queries, even if there is no recursion~ and\nis used for this purpose in rnany cornrnercial DBlVISs, even systenls that do not\ncurrently support recursive queries.\nWe now describe the three steps in the Magic Sets algorithrIl using the SarneLevel\nprogram as a running exalllple.\nAdorned Program\nWe consider the query pattern 8 ameLevelbf . Thus, given a value\nc~ we want\nto cornpute all rows in 8arneLevel in which c appears in the first eolurnn. \\Ve\ngenerate the Adorned Prograrn pad frorn the given prograrn P by repeatedly\ngenerating adorned versions of rules in [J for every reachable query pattern,\nwith the given query pattern as the only reachable pattern to begin with;\nadditional reachable patterns are identified during the course of generating the\nA,dorned Prograrll as described next.\nConsider a rule in? whose head contains the sarne table as sorne reachable\npattern.\nrrhe adorned version of the rule depends on the order in \\vhichwe\nconsider the predicates in the body of the rule. l'b sirnplify our discussion, ,ve\nassurne that this is ahvays left-to-right.\nI~-'irst~ we replace the head of the rule\n,vith the rnatching query pattern. After this step, the recursive SarneLevel rule\nlooks like this:\n8arncLeveZbf (S1, 82)\n:- Assembly(P1, 81, Q1),\n8ameLevel(P1, P2), Assembly(P2, 82, Q2).\nNext, we proceed left-to-right in the l)ody of the\nrul(~ until 'we encounter the\nfirst recursive predicate. .A.11 cohullns that contain a consUl,ut or a variable that\n\n840\n(;HAPTER ~4\nappears to the left are rnarked b (for bound) and the rest are rnar,ked f (for free)\nin the query pattern for this occurrence of the predicate. vVe add this pattern\nto the set of reachal)le patterns and Inodify the rule accordingly:\nSaTrLeLevelbf (Sl,82)\n:- Assembly(Pl, 81, Ql),\nSarneLeveZbf (Pi, P2), Assembly CP2, 82, Q2).\nIf there are additional occurrences of recursive predicates in the body of the\nrecursive rule, we continue (adding the query patterns to the reachable set and\nrllodifying the rule). (()f course, in linear recursive progralns, there is at illOSt\none occurrence of a recursive predicate in a rule body.)\n\\Ve repeat this until we have generated the adorned version of every rule in P\nfor every reachable query pattern that contains the same table as the head of\nthe rule. The result is the Adorned Program pad, which, in our example, is\nSameLeveZbf C81, 82)\n: -\nAssemblyCP1, 81, Ql),\nAssemblyCP1, 82, Q2).\nSameLeveZbf (81, 82)\n: - AssemblyCP1, 81, Q1),\nSameLeveZbf CP1, P2), Assembly CP2, 82, Q2).\nIn our exarnple, there is only one reachable query pattern. In general, there\ncan be several. 2\nAdding Magic Filters\nEvery rule in the Adorned Prograrn is rllodified by adding a 'nlagic filter' pred-\nicate to obtain the rewritten prograrn:\nSarne-Levelbf (81, 82)\n: -\n1\\dag'ic~k9arneLeveZbf (81) ,\nAssembly(Pl, 81,\nQ1), Assembly(P2, 82, Q2).\nSarneLeveZbf (S1, S2)\n: -\nJ\\;lag'ic...,San1eLevelbf CS1),\nAssembly(Pl, 81, Ql), 8arneLevelbf (P1, P2),\nAssembly(P2, 82,\nQ2).\nThe filter predicate is\n(:l, copy of the head of the rule, 'with 'IVlagic' a..s a prefix\nfor the table nhrne and the variables in colllrnns corresponding to free deleted,\nas illllstrc:ited in these two rules.\n2 As an exarnple: consider a variant of the SameLevel program in which the variables PI and 1\"'J2\nare interchanged in the body of the recursive rule (Exercise 24.5)\n\nDe(l1u:tive ]Jatabascs\nDefining Magic Filter Tables\n841\nConsider the Adorned Prograrl1 after every rule has been rnodified a,,') described.\nFrorH each occurrence 0 of a recursive predicate in the body of a rule in this\nrIlodified prograrl1, ,ve generate a rule that defines a .NIagic predicate.\nT'he\nalgorithrl1 for generating this rule is as fo11o\\:vs:\n(1) Delete everything to the\nright of occurrence () in the body of the rllodified rule.\n(2) i\\dd the prefix\n'1I1agic' and delete the free colulnns of ().\n(~)) Move 0, \"with these changes, into\nthe head of the rule.\nFroIn\nth~~ recursive rule in our example, after steps (1) and (2) we get:\nSam,eLevelbf (81, 82)\n: -\n]vIagic_SarneLevelbf (S1) ,\nAssembly(P1, 81, Q1), AIag'ic_SameLevelbf (P1) .\nAfter step (3), we get:\nMagic.,_SameLevel bf (P1)\n: -\nA1ag'ic_Sam,eLevelbf (S1) ,\nAssembly(Pl, 81, Q1).\nThe query itself generates a row in the corresponding Magic table, for exarnple,\nl\\Iagic.._SarneLevelbf (seat).\n24.6\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nIII\nDescribe Datalog prograrIlS. lJse an exarnple Datalog prograrn to explain\nwhy it is not possible to write recursive rules in SQL-92. (Section 24.1)\nl1li\nDefine the terrns rnodel and least '(node!.\nvVhat can you say about least\nrnodels for Datalog prograrns? \\\\Thy is this approach to defining the rnean-\ning of a Datalog prograrll called declarative? (Section 24.2.1)\nIII\nDefine the tenns .fi:rpoint and least ji:Epoint. \\iVhat can you say about 1e&5t\nfixpoints for IJatalog prograrlls?\nvVhy is this approach to defining the\nrneaning o{ a Datalog prograrIl said to be operational? (Section 24.2.2)\nl1li\n\\Vhat is a safe prograrn? \\\\lllY is this property irIlportant? \\\\That is range-\nrestriction and how does it ensure safety'? (Section 24.2.3)\nl1li\n\\Vhat is the connection between lccl.':lt lnodels and lea..st fixpoints for I)atalog\nprograrIls? (Section 24.2.4)\n\n842\nCHAPTER 24\n•\nExplain ~why prograIIls with negation rnay not have a lea.'3t model or least\nfixpoint. Extend the definition of Tange-Testriction to prograrns with nega-\ntion. (Section 24.3)\n•\nvVhat is a stratified prograIn? lIow does stratification address the probleln\nof identifying a desired fixpoint? Show how every relational algebra query\ncan be \\vritten as a stratified Datalog prograrIl. (Section 24.3.1)\n•\nTwo ilnportant aspects of SQL, rnultiset table8 and aggr'egation 'with group-\ning, are rnissing in Datalog. Hovv can we extend Datalog to support these\nfeatures? Discuss the interaction of these two new features and the need\nfor stratification of aggregation. (Section 24.4)\n•\nDefine the terms infeTence and iteration. What are the two main challenges\nin efficient evaluation of recursive Datalog programs? (Section 24.5)\n•\nDescribe Sem,inaive fixpoint evaluation and explain how it avoids repeated\ninferences. (Section 24.5.1)\n•\nDescribe the Magic Sets program transformation and explain how it avoids\nunnecessary inferences. (Sections 24.5.2 and 24.5.3)\nEXERCISES\nExercise 24.1 Consider the Flights relation:\nFlights(fino: _~~teger, from: string, to: string, distance: integer,\ndeparts: time, arrives: time)\nWrite the following queries in Datalog and SQL:1999 syntax:\n1. Find the fino of all flights that depart from Madison.\n2. Find the .flrw of all flights that leave Chicago after Flight 101 arrives in Chicago and no\nlater than 1 hour after.\n:3. Find the fino of all flights that do not depart from NIadison.\n4. Find aJI cities reachable frOlll l\\iladison through a series of one or 1I10re connecting flights.\n5. Find all cities reachable from IVladison through a chain of one or rnore connecting flights,\nwith no 1I1Ore than 1 hour spent on any connection.\n(That is, every connecting flight\nmust depart\n<within an hour of the arrival of the previous flight in the chain.)\n6. Find the shortest tilne to fly frOl11\n~IIadison to i\\dadras, using a chain of one or 1nore\nconnecting flights.\n7. Find the Jlno of all flights that do not depart [1'0111\n~!Iadison or a city that is reacha.ble\nfrolIlrvladison through a chain of flights.\nExercise 24.2 Consider the definition of Cornponents in Section 24.1.1. Suppose that the\nsecond rule is replaced by\n\nDeduct'ive Database.s\nComponents (Part, Subpart)\n:-Components (Part, Part2),\nComponents (Part2 , Subpart).\nU-:I;v\n1. If the rnodified prograIn is evaluated on the ASS€lnbly relation in Figure 24.1~ how Inany\niterations does Naive fixpoint evaluation take and what COlllponents facts are generated\nin each iteration?\n2. Extend the given instance of Asselllbly so that Naive fixpoint iteration takes two rnore\niterations.\n3. 'i\\lrite this prograrn in SQL:1999 syntax, using the WITH clause.\n4. vVrite a progranl in Datalog syntax to find the part with the Inost distinct subparts; if\nseveral parts have the saIne Inaxinlllm number of subparts, your query should return all\nthese parts.\n5. How would your answer to the previous part be changed if you also wanted to list the\nnumber of subparts for the part with the Inost distinct subparts?\n6. Rewrite your answers to the previous two parts in SQL:1999 syntax.\n7. Suppose that you want to find the part with the rnost subparts, taking into account\nthe quantity of each subpart used in a part, how would you rllodify the COlnponents\nprogram?\n(Hint: To write such a query you reason about the nuruber of inferences of\na fact. For this, you have to rely on SQL's nlaintaining as many copies of each fact as\nthe nurnber of inferences of that fact and take into account the properties of Seulinaive\nevaluation. )\nExercise 24.3 Consider the definition of Components in Exercise 24.2.\nSuppose that the\nrecursive rule is rewritten as follows for Seminaive fixpoint evaluation:\nComponents (Part , Subpart) :-\ndeLta__Components(Part, Part2,\nQty) ,\ndeLta_Components (Part2 , Subpart).\n1. At the end of an iteration, what steps Illust be taken to update delta_Cornponents to\ncontain just the new tuples generated in this iteration? Can you suggest an index on\nCornponents that Inight help to lIHlke this faster?\n2. Even if the delta relation is correctly updated, fixpoint evaluation using the preceding\nrule does not always produce all answers. Show an instance of Assembly that illustrates\nthe probleru.\n3. Can you suggest a way to rewrite the recursive rule in tenns of delt(LC;Olnponents so\nthat Scrninaive fixpoint evaluation ahvays produces all answers and no inferences are\nrepeated across iterations?\n4. Show how your version of the rewritten prograrn perfonns on the exa,rnple instaJICe of\nAssernbly that you used to illustnlte the problern with the gi\"ven rewriting of the recursive\nrule.\nExercise 24.4 Consider the definition of SarneLevel In Section 24.5.2 and the Assernbly\ninstance shown in Figure 24.1.\n1. Re\\vrite the recursive rule for\nSelninaivf~ fixpoint evaluation and shc)\\,v ho\\v Serninaive\nevaluation proceeds.\n2. Consider the rules defining the relation Nlagic, \\vith spoke as the query constant.\nFor\nSernil1aive evaluation of the 'Nlagic' version of the SarneLevel prognllu, all tuples in Ivlagic\nare cornputed first. Show how 8erninaive evaluation of the I\\rlagic relation proceeds.\n\n844\nCJHAPTER 24\n~3. After the fvI;:lgic relation is cmnputed, it can be treated Cl\"S a fixed database relation, just\nlike AsseInbly, in the Senlinaive fixpoint evaluation of the rules defining SarneLeveI in\nthe 'IVIagic' version of the prograrn. Rewrite the recursive rule for Selninaive evaluation\nand show how Scrninaive evaluation of these rules proceeds.\nExercise 24.5 Consider the definition of SanleLevel in Section 24.5.2 and a query in which\nthe first argulnent is bound. Suppose that the recursive rule is rc\\vritten as folIovls, leading\nto rnultiple binding patterns in the adorned prognnn:\n8ameLevel(81 , S2) :- Assembly(Pl, 81, Ql),\nAssembly(P1, 82, Q2).\n8ameLevel(81 , S2) :- Assembly(Pl, S1, Ql),\nSameLevel(P2, P1), Assembly(P2, S2, Q2).\n1. Show the adorned progranl.\n2. Show the J\\1agic program.\n3. Show the Magic program after applying Seminaive rewriting.\n4. Construct an example instance of Assenlbly such that the evaluating the optirnized pro-\ngrarn generates less than 1% of the facts generated by evaluating the original prograrn\n(and finally selecting the query result).\nExercise 24.6 Again, consider the definition of SameLevel in Section 24.5.2 and a query in\nwhich the first argurnent is bound. Suppose that the recursive rule is rewritten as follows:\nSameLevel(Sl, 82) :- Assembly(Pl, 81, Ql),\nAssembly(Pl, S2, Q2).\nSameLevel(Sl, 82) :- Assembly(P1, S1, Ql),\nSameLevel(P1, Rl), SameLevel(Rl, P2), Assembly(P2, S2, Q2).\n1. Show the adorned program.\n2. Show the l\\!Iagic prograln.\n~3. Show the rv1agic prograul after applying Serninaive rewriting.\n4. Construct an exarnple instance of Asselnbly such that the evaluating the optimized pro-\ngranl generates less than 1% of the facts generated by evaluating the original progranl\n(and finally selecting the query result).\nBIBLIOGRAPHIC NOTES\n'1'he use of logic as a query language is discussed in several papers [296,\n5~:W], \"which arose out\nof influential workshops. (jood textbook discussions of deductive databases can be found in\n[747, :-3, 14:-\"3, 794, '50~3]. [614] is a recent survey article that provides an overview and covers\nthe rnajor prototypes in the area, including LI)L [177], Glue-Nail! [214, 549]\nEKS-Vl [758],\nAditi [615], Coral [612], LOLA [804], and XSB [644].\nl'he fixpoint sernantics of logic programs (and deductive databases a.., a special case) is pre-\nsented in [751], which also shows\nequivalenc(~ of the fixpoint seInantics to a lea.st-rnodd se-\nIrul,ntics. The use of stratification to give a natural sernantics to prograrns with negation \\va..s\ndeveloped independently in [:37, 154, 559,752].\n\nDeduct'i'ue Databases\n845\nEfficient evaluation of deductive database queries has been widely studied, and [58J is a\nsurvey and cOlnparison of several early techniques; [611] is a more recent survey_ Serninaive\nfixpoint evaluation wa..\" independently proposed several tiInes; a good treatuwut appears in\n[54].\nr1'he Ivlagic Sets technique is proposed in [57] and generalized to cover all deductive\ndatabase queries without negation in [77]. 'The Alexander rnethod\n[G~nJ was independently\ndeveloped and is equivalent to a variant of l'vIagic Sets called Supplernentary Atagic Sets in [77].\n[553] shows how lVIagic Sets offers significant perfonnance benefits even for nonrecursive SQL\nqueries. [ti73] describes a version of l'vlagic Sets designed for SQL queries with correlation, and\nits irnplernentation in the Starbufst systern (which led to its iInplenlentation in IBNI's DB2\nDBNIS). [670] discusses ho\\v lVlagic Sets can be incorporated into a Systenl R style cost-based\noptimization framework. The\n~lagic Sets technique is extended to prograIIls with stratified\nnegation in [53, 76] _ [121] cOlnpares tvlagic Sets with top-do\\vn evaluation strategies derived\nfroIn Prolog.\n[642] develops a prograrn rewriting technique related to lVlagic Sets called lVlagic Counting.\nOther related methods that are not based on progranl rewriting but rather on fun-tirne control\nstrategies for evaluation include [226, 429, 756, 757]. The ideas in 1.226] have been developed\nfurther to design an abstract rnachine for logic prograll1 evaluation using tabling in [609, 727];\nthis is the basis for the XSB systell1 [644].\n\n25\nDATA WAREHOUSING AND\nDECISION SUPPORT\n..\nWhy are traditional DBIvISs inadequate for decision support?\n..\nWhat is the multidimensional data nlOdel and what kinds of analysis\ndoes it facilitate?\n..\nWhat SQL:1999 features support rnultidiInensional queries?\n..\nHow does SQL:1999 support analysis of sequences and trends?\n..\nHow are DBMSs being optimized to deliver early answers for interac-\ntive analysis?\n..\nWhat kinds of index and file organizations do OLAP systerlls require?\n..\nVVhat is data warehousing and why is it irnportant for decision sup-\nport?\n..\nWhy have rnaterialized views becorne iInportant?\n..\nHow can we efficiently Inaintain rnaterialized views?\n..\nKey concepts: OLAP, rnultirnensional rnodel, dinlellsions, nleasures;\nroll-up, drill-clown, pivoting, cross-tabulation, CUBE; WINDOW queries,\n[rallleS, order; top N queries, online aggregation; bitllli:tp indexes, join\nindexes; data warehouses, extract, refresh, purge; rnaterialized views,\nincrernental rnaintenancc, rnaintaining warehouse views\nNotlling is lnore difficult, and therefore rnore precious, than to be\n(1)1<\" to decide.\n. NCtI)oleon Bonaparte\n846\n\n[)ata\n~;VaTeho1L8ing and Decis'io'n Support\ni47\nDcl.tabase luanagclncnt systerIls are widely used by organizations for rnaintain-\ning data that docurnents their everyday operations. In applications that update\nsuch operational data 7 transactions typically rnake srna11 changes (for exalnple,\nadding a reservation or depositing a check) and a large nU111ber of transactions\nH1USt be reliably and efficiently processed. Such online transaction process-\ning (OLTP) applications have driven the gruwth of the DBlVIS industry in the\npast three decades and \"vill doubtless continue to be irnportant. DB1VISs have\ntraditionally been optirnized extensively to perforn1 vvell in such applications.\nH,ecently, ho\\vever, organizations have incrc&':lingly crnphasized applications in\nwhich current and historical data is coruprehensively analyzed and explored,\nidentifying useful trends and creating sununaries of the data, in order to support\nhigh-level decision rnaking. Such applications are referred to clS decision sup-\nport.\n~1ainstrearn relational DBlVlS vendors have recognized the irnportance\nof this rnarket segment and are adding features to their products to support it.\nIn particular, SQL has been extended with new constructs and novel indexing\nand query optirl1ization techniques are being added to support cornplex queries.\nThe use of views has gained rapidly in popularity because of their utility in\napplications involving cornplex data analysis. While queries on views can be\nanswered by evaluating the view definition when the query is subrnitted, pre-\ncornputing the view definition can rnake queries run Inuch faster.\nCarrying\nthe r11otivation for preconlputed views one step further, organizations can con-\nsolidate inforrnation from seven:tl databases into a data warehouse by copying\ntables fror11 rnany sources into one location or rnaterializing a view defined over\ntables fr01n several sources.\nData '\\varehousing has becorne widespread, and\nIl1any specialized products are no\\v available to create and rnanage warehouses\nof data frorH 1l1ultiple databases.\nvVe begin this chapter with an overview of decision support in Section 25.1.\n\\Ve introduce the rnultirnensional rnodel of data in Section 25.2 and consider\ndatabase design issues in 25.2.1.\nvVe discuss the rich cla..ss of queries that it\nnaturally supports in Section 25.;3. \\Ve discuss how new SQL:1999 constructs\nallc)vl us tel express rnultidilnensional queries in 25.3.1.\nIn Section 25.4, vve\ndiscuss\nS(~L:1999 extensions that support queries over relations\ntLS ordered\ncollections.\\\\'\"8 consider hOVl to optirnize for fa,st generation of initial ansvvers\nin Sectioll 25.5.\n1'11C rnany query language extensions required in the ()LA.P\nenvirolllnentprornpted the developrnent of llC\\V irnplcrncntation techniques; we\ndiscuss these in Section 25.6. In Section 25.7, \\ve\nexarnirl(~ the issues involved\nin creating and rnaintaining a data \\varehouse. FraIn a technical sta,ndpoint, a\nkey issue is how to Ilutintain \\vctrehouse inforrnation (replicated tables or views)\n·when the llnderl.ying source infonnation changes. After covering tlH~ iUlportcl,nt\nrole played byvic\\vs in OLAP and \\vaxehousing irl Section 25.8, we consider\nIIuLintenance of rnaterialized vievvs in Sections 25.9 and 25.10.\n\n848\nC;lIAPTER 25\n25.1\nINTRODUCTION TO DECISION SUPPORT\n()rganizational decisioll rnaking requires a cOlnprehensive vievv of all aspects of\nan enterprise, so 111any organizations created consolidated data warehouses\nthat contain data drawn frcHn several databa..,;es ll1atntained by different busi-\nness units together vvith historical and SUlnlna,ry inforInation.\nThe trend toward data warehousing is c0I11plelnented by an increa.sed ernphasis\non po\"Vverful analysis tools.\nlVlany characteristics of decision support queries\nmake traditional SQL systenls inadequate:\n•\n1\n1he WHERE clause often contains rnany AND and OR conditions. As we saw\nin Section 14.2.3, OR conditions, in particular, are poorly handled in rnany\nrelational DBJV1Ss.\n•\nApplications require extensive use of statistical functions, such as standard\ndeviation, that are not supported in SQL-92. Therefore, SQL queries rnust\nfrequently be ernbedded in a host language progrcun.\n•\nMany queries involve conditions over time or require aggregating over time\nperiods. SQL-92 provides poor support for such time-series analysis.\n•\nUsers often need to pose several related queries. Since there is no conve-\nnient way to express these cOlnnlonly occurring families of queries, users\nhave to write thern as a collection of independent queries, \\vhich can be\ntedious. Further, the DBlVIS has no way to recognize and exploit optimiza-\ntion opportunities arising froln executing nlany related queries together.\nThree broad classes of analysis tools are available. First, SOIne systerIls support\na elc-1SS of stylized queries that typically involve group-by and aggregation oper-\nators and provide excellent support for cOlnplex boolean conditions, statistical\nfunctions, and features for tilne-series analysis.\nApplications dominated by\nsuch queries are called online analytic processing (OLAP). 'These systerns\nsupport a querying style in which the data is best thought of &9 a rnultidi-\nlnensional array and are influenced by end-user tools, such as spreadsheets, in\naddition to database query languages.\nSecond, sorne DBwISs support traditional\nS(~L-style queries but are designed\nto also support OLAP queries efficiently.\nSuch systenls can be regarded\n(1\",,,\nrelational DB1'v1Ss optirnized for decision support applications. 'Nlany vendors of\nrelational DBIVISs are currently enhancing their products in this direction and,\nover tilne, the distinction bet\\veen specialized OLAP systerns and relational\nDBIVISs enhEtnced to support ()LAP queries is likely to dirninish.\nThe third class of analysis tools is rllotivated by the desire to find interesting\nor unexpected trends and patterns in large data sets rather than the conlplex\n\nData 1-1l a:reho'U/3'ing and Decision Support\n~49\n___ •__• __ •••••• _._ ••__ • __••__•\n._._.__ri·_ri\n•\nSQL:1999 and OLAP: In this chapter, ,,'Ie discuss a nUInber of features\nintroduced in SQL:1999 to support OLAP. In order not to delay publica-\ntion of the SQL:1999 standard, these features \\vere actually added to the\nstandard through an amendment called SQL/OLAP.\nquery characteristics just listed. In exploratory data analysis, although an\nanalyst can recognize an :interesting pattern' \"vhen shown such a pattern, it is\nvery difficult to fannulate a query that captures the essence of an interesting\npattern.\nFor exalnple, an analyst looking at credit-card usage histories Illay\nwant to detect unusual activity indicating Inisuse of a lost or stolen card. A\ncatalog lllerchant lnay want to look at custolner records to identify pro111ising\ncustoiners for a new proillotion; this identification would depend on inccnIle\nlevel, buying patterns, delllonstrated interest areas, and so all.\nThe alllount\nof data in Inany applications is too large to perrnit rnanual analysis or even\ntraditional statistical analysis, and the goal of data mining is to support\nexploratory analysis over very large data sets. We discuss data rnining further\nin Chapter 26.\nClearly, evaluating OLAP or data rnining queries over globally distributed data\nis likely to be excruciatingly slow.\nFurther, for such cOlnplex analysis, often\nstatistical in nature, it is not essential that the IllOSt current version of the data\nbe used. The natural solution is to create a centralized repository of all the\ndata; that is, a data warehouse. Thus, the availability of a warehouse facilitates\nthe application of ()LAP and data rnining tools and, conversely, the desire to\napply such analysis tools is a strong 1Ilotivation for building a data warehouse.\n25.2\nOLAP: MULTIDIMENSIONAL DATA MODEL\naLAI' applications are dOlninated by ad hoc, cOlnplex queries. In SQL terllls,\nthese are queries that involve group-by and aggregation operators. l'he natural\n\\vay to think about typical ()LAP queries, ho\\vever, is in tenns of a rnultidilnen-\nsinnal data rllodel. In this section, \\ve present the rnultidirnensional data Illodel\nand corupare it with a relational representation of data.\nIn subsequent sec-\ntions, we describe ()LAP queries in. terrns of the rllultidirnensional data rnodel\nand consider .. scnne ne,v irnplernentation techniques designed to support such\nqueries.\nIn the rnultidirnensional data rnodel, the focus is on a collection of nurneric\nmeasures. Each Inea..sure depends on a set of dirnensions. vVe use a running\nexarnple based on sales data.. The rncc'tsure attribute in our exarnple is sales.\nThe dirnensions are Product, Location, and Tirne. (jiven a product, a location;\n\n850\nCHAPTER 25&\nand a tinle, 'ATe have at I110st one associated sales value. If we identify a product\nby a unique identifier pid and, sirnilarly, identify location by locid and tiIne\nby tirrLeid, we can think of sales inforrnation\nCt.') being arranged in a three-\ndirnensional array Sales.\nThis array is shown in Figure 25.1; for clarity, we\nshow only the values for a single loeid value, locid- 1, which can be thought of\nas a slice orthogonal to the lacid axis.\nrr.\n......\n\"0\nN\n.....\n0..\n......\n......\nlocid/'/~-\"'-\n/\n2\ntimeid\n3\nFigure 25.1\nSales: A rvlulticlimensional Dataset\n'This view of data as a multiclhnensional array is readily generalized to rnore\nthan three dirnensions.\nIn OLAP applications, the bulk of the data can be\nrepresented in such a rnultidiInensional array.\nIndeed, some OLAP systerns\nactually store data in a rnultidiInensional array (of course, irnplenlented with-\nout the usual prograrnrning language asslunption that the entire array fits in\nrnelnory). 0 LAP systerns that use arrays to store rnultidirnensiona.l datasets\nare called nlultidimensional OLAP (MOLAP) systcrl1s.\nThe data in a 11lultidirnensional array can also be represented\nc1As a relation,\nas illustrated in Figure 25.2, which shows the SeHne data as in Figure 25.1,\nwith additional rovvs corresponding to the 'slice' locid==- 2. Tlhis relation, which\nrelates the dirnensions to the rneasure of inten~st, is called the fact table.\nNO\\~l let us tun1 to dirnensions.Each dirnension can have a set of associated\nattributes. For exarnple, the Location dilTlension is identified by the loc'id at-\ntribute, \\vhich \\ve used to identify a location in the Sales table. \"'Fe aSSUlnc\nthat it also has\n~),ttributes cau'ntry, state, and city.\n\\Ve further assurne that\nthe Product dirnension has ctttributes pnanu:, category, and pTice in additi(Jn\nto the identifier pid.\n'rhe catcgoTy of a product indicates its general nature;\nfor exarnple, a product pant could have category value appaTcl.\n\\Ve assurne\nthat the rI'inlc dirnension has attributes date, 71JCek, Tnonth, quar-fcT, ycaT, and\nholiday.)lag in addition to the identifier tirneid.\n\nData Warcho11sing and Dec-is'ion 8'lLppOTt\n251\nlocid\n[\\'ales\n-_....\n\"\".........\"'......,m\"\" _.\n...\n--\nII\n1\n1\n25\n11\n2\n1\n8\n11\n3\n1\n15\n12\n1\n1\n30\n_.\n12\n2\n1\n20\nr--------\"...\n,,~·.'\"\"\"\" ....nfflfn,'V<+..._\n...................• ....\n12\n3\n1\n50\n.................\"\"'......... \".~\".«\"....\"'.-.,\n13\n1\n1\n8\n-.-f-.\n13\n2\n1\n10\n13\n3\n1\n10\n--\nII\n1\n2\n35\n11\n2\n2\n22\n11\n3\n2\n10\n.__.,_._---\n12\n1\n2\n26\n.......,..........,.........._....._._..__.f-----\n12\n2\n2\n45\n. _._.._...................... ------_.._._-\n]2\n3\n2\n20\n..._----.......\n13\n1\n2\n20\n13\n2\n2\n40--\n13\n3\n2\n5\n[\n-------·-I--=~~~:-·\npid\nL.!..!!!!eid\nsta~\ncountry I\nProducts\nLocations\ncity\nlocid\n-\n.. -\n--=\n..- - .\n11\nLee Jeans\nApparel\n25\n.._._...\n12\nZord\nToys\n18\n~,~--\n13\nBiro Pen\nStationery\n2\n..\"\"\"'''\"'.........''\"'........ .,..~\n.•\n..............\n- ..._-_.........\n... -\n1\nMadison\nWI\nUSA\n._-\n2\nFresno\nCA\nUSA\n--\n5\nChennai\nTN\nIndia\n'-----\nSales\nFigure 25.2\nLocations, Products, and Sales\nH.eprf~sent:ed as Helations\n\n852\n( '1.\n.\n.\n2r:::\njHAPTER\naft\nFor each dinlension, the set of associated values can be structured a\" a hierar-\nchy. For exarnple, cities belong to states, and states belong to countries. Dates\nbelong to weeks and rIlonths, both 'weeks and 1110nths are contained in quaT-\ntel's, and quarters are contained in years.\n(Note that a vveek could span two\nrnonths; therefore, weeks are not contained in rnonths.) SCHne of the attributes\nof a diruension describe the position of a dirnensioll valuevvith respect to this\nunderlying hierarchy of dirnensioll values. The hierarchies for the Product, Lo-\ncation, and ~rirne hierarchies in our exarnple are sho\\vn at the attribute level in\nFigure 25.3.\nPRODUCT\ncategory\npname\nTIl\\tIE\nyear\nI\nquarter\nweek\nmonth\n~/\ndate\nFigure 25.3\nDimension Hierarchies\nLOCATION\ncountry\nstate\ncity\nInfonnation about dirnensions can also be represented &s a collection of rela-\ntions:\nLocations( lo~..id: intege:!:> city: string, state: string, country: string)\nProducts(pid:\nint.~..~er, pnam,e: string, category: string, price: real)\nTirnes(t'irnei~: integer, date: string, week: integer, rnonth: integer,\nquarter: integer, year: integer,\nholiday~..fiag: boolean)\nThese relations arc luuch srnalIer than the fact table in a typical 0 I..lAP appli-\ncation; they are called the diInension tables. OLAP systcrl1s that store all\ninforrnation, including fact tables, as relations are called relational OLAP\n(ROI.JAP) systcrns.\n1'he Tinlcs table illustrates the attention paid to the T'irne dirnension in typical\nOLAP applications.\nSC~L's date and tirnestaulp data types are not adequctte;\nto support slunrnarizations that reflect business operations, infonnation such\nas fiscal quarters, holiday status, and so on is rnaintained for ea,ch tirne value.\n\nData WarehOtlSing and DecL5'ion ~(hL1JPOTt\n25.2.1\nMultidimensional Database Design\nFigure 25.4 shows the tables in our running sales exarnple. It suggests a star,\ncentered at the fact table Sales; such a cornbination of a fact table and di-\nrncnsion tables is called a star schema. This schelna pattern is very COIIUIlon\nin databc\"kses designed for 0 LAP. IThe bulk of the data is typically in the fact\ntable, which ha..'3 no redundancy; it is usually in BCNF. In fact, to Ininimize\nthe size of the fact table, dirnension identifiers (such as p'id and t'irneid) are\nsystcrn-generated identifiers.\nPRODUCTS\nLOCATIONS\nSALES\nFigure 25.4\nAn Example of a Star Schema\nholiday_flag\nInforrnation about dinlension values is rnaintained in the dirnension tables. Di-\n111ension tables are usually not nonnalized. The rationale is that the dimension\ntables in a database used for OL,AP are static and update, insertion, and dele-\ntion anoillalies are not irnportant. Further, because the size of the database is\ndorninated by the fact table, the space saVE-xi by norrnalizing dilnension tables\nis negligible. Therefore, rnini111izing the cornputation tilllC for cOlllbining facts\nin the fact table with dirnension inforrnation is the rnain design criterion, which\nsuggests that we avoid breaking a dirnension table into srnaller tables (which\nrnight lead to additional joins).\nSnlall response tirnes for interactive querying are irnportant in OLAP, and rnost\nsysterns support the Hlaterialization of SUrl1Inary tables (typically generated\nthrough queries using grouping). Ad hoc\nqueri(~s posed by users are answered\nusing the original ta,bles along with precornputed surnrnaries. A very irnportant\ndesign issue is which sunnnary tables should be rnaterialized to achieve the\nbest use of available rnerllory and answer cOHnI1only a.sked ad hoc queries with\ninteractive response tirnes. In current OLAP systerns, deciding \"vhich surnnlary\ntables to rnaterialize rnay \\vell be the Inost irnportant design decision.\nFinally, new storage structures and indexing techniques have been developed to\nsupport ()LAP and they present the database designer \\'lith additional physical\n\n854\nCHAPTER 2[)f\ndesign choices. \\Vc cover BOIHe of these hnplclnentatiol1 techniques in Section\n')!:\"\" t'\n....d.t>.\n25.3\nMULTIDIMENSIONAL AGGREGATION QUERIES\nNow that \\ve have seen the rnulticliInensiol1alluoclel of data, let us consider how\nsuch data can be queried and rnanipulatecl. The operations supported by this\nInodel are strongly influenced by end user tools such as spreadsheets. The goal\nis to give end users v.rho are not SQL experts an intuitive and po\\verful interface\nfor cornnlon business-oriented analysis tasks. Users are expected to pose ad hoc\nqueries directly, without relying on database application prograrrnners.\nIn this section, we asslllne that the user is working with a multidirnensional\ndataset and that each operation returns either a different presentation or a\nsunllnary; the underlying dataset is always available for the user to 1nanipulate,\nregardless of the level of detail at which it is currently viewed. In Section 25.3.1,\nwe discuss how SQL:1999 provides constructs to express the kinds of queries\npresented in this section over tabular, relational data.\nA very C01111non operation is aggregating a rneasure over one or 1nore dimen-\nsions. The following queries are typical:\n..\nFind the total sales.\nII\nFind total sales for each city.\nII\nFind total sales for each state.\n'These queries can be expressed as\nS(~L queries over the fact and dirnension\ntables. When we aggregate a rnea.'3ure OIl one or rnore di1nensions, the aggre-\ngated 1118'1.SUre depends on fewer diInensioIls than the original Ineasure.\nFor\nexanlple, when we cornpute the total sales by city, the aggregated rneasure is\ntotal sales and it depends only on the Location di1nension,whereas the original\nsales rneasure depended on the Locatioll,Tirne, a,nd Product dirnensions.\nAnother use of aggregation is to SU1Ilrnarize at different levels of a dirnension\nhierarchy. If \\ve are given total sales per city, we can aggregate 011 the Location\ndinlension to obtain sales per state.\nThis operation is called roll-up in the\nOLAI' literature. 1\n1he inverse of roll-up is drill-down: Given total sales by\nstate, \\ve can Etsk for a 1Ilore detailed presentation by drilling down on Location.\n\\\\\"T\nk f'\nI I·\n.\nt\nI\nI\n·t\n£\nI tIt t (·tl\nI\n(\n.' ..:1\n'f '-',\n-\n,-\n1\n1\"\"\n.'\n\"\n'\"'j ,\"\n~'i\n.,~\n. - r\n\"J\n\".''''\n-\".\n,.,~\n~, i'\n::}\"1\n.\n.,.\n'.,.\n0.\n'.\n..,.\nI{\n\",~\n.\".1\n.)'\"\n-\n:~.\n-\", I.\"\n\"\n:.\\\n,.,...., \"\n:.\\-~\nC Cdn <1S01 t;d. LS J)\n(,1tJ 01 .J US, SeL CS J)\nC1 Y\n01\nd, t;C CC iCC\nS ,<:1,c\nWI, 1 set cs\npresented on a per-state basis for the rernaining states,\nriS before).\nWe can\nalso drill dowll on a diluension other than Location. For exarnple, \\ve can ask\n\njJata WaTeho1.M3'ing and Decision SUppO'Tt\nfor total sales for each product for each state, drilling do\\vn OIl the Prodnet\ndiInension.\nAnother C0111ll10n operation is pivoting.\nConsider a, tabulax presentation of\nthe Sales table. If Vle pivot it on the Location and Titne dirnensions, we obtain\na table of total sales for each location for each tillle value. This infoI\"luation\ncan be presented\n<:1..'; a tvvo-dirnensional chart in which the axes are labeled\n'with location and titne values; the entries in the chart correspond to the total\nsales for that location and\ntirnt~.\nTherefore, values that appear in colurnns\nof the original presentation becoIne labels of axes in the result presentation.\nThe result of pivoting, called a cross-tabulation, is illustrated in Figure 25.5.\nObserve that in spreadsheet style, in addition to the total sales by year and\nstate (taken together), we also have additional sunlillaries of sales by year and\nsales by state.\nWI\nCA\nTotal\n1995\n1996\n1997\nTotal\n63\n81\n144\n~,-,._-\n38\n107\n145\n.~.~.~_..._.\n75\n35\n110\n176\n223\n399\n----\nFigure 25.5\nCross-Tabulation of Sales by Year and State\nPivoting can also be used to change the dirnensions of the cross-tabulation;\nfroIn a presentation of sales by year and state, we can obtain a presentation of\nsales by produet and year.\nClearly, the OLAP frarnework rnakes it convenient to pose a broad class of\nqueries.\nIt also gives catchy naInes to sorne farniliar operations:\nSlicing a\ndataset arnonnts to an equality selection on one or rIlore dirnensions, possibly\nalso with SC)lne dirnensions projected out. Dicing a dataset arl10unts to a range\nselection. These terrllS corne frcnl1 visuaJizing the effect of these operations on\na cube or cross-tabulated representation of the data.\nA Note on Statistical Databases\nlVlany ()LAP concepts c],re present in earlier work on statistical databases\n(SDBs), which are databaBe systerl1s designed to support statistical applica-\ntions, although this connection has not been sufficiently recognized because\nof differences in application dornains and tern.linology. The rnultidirnensional\ndata rllodel, 'with the notions of a rneasure associated with dirnensions (lond\n\n856\n(\n'1\n.\n2r..:\nJHAPTER\n··0$\nclassification hierarchies for dirncIlsion vahles, is also used in SDBs.\nOLAP\noperations such as roll-up and drill-dovlJl have counterparts in SDBs. Indeed,\nsorne irnplcrnentation techniques developed for OLAP are also applied to SDBs.\nNonetheless~ senne differences arise frorn the different dOlnains ()L.LLlP and SDBs\n\\vere developed to support. For exarnple, SnBs are used in socioeconornic appli-\ncations, where classification hierarchies and privacy issues are very ilnportant.\nThis is reflected in the greater cornplexity of classification hierarchies in SDBs,\nalong with issues such as potential breaches of privacy.\n(The privacy issue\nconcerns whether a user with access to sUllunarized data can reconstruct the\noriginal, unsununarized data.) In contrast, OLAP has been ailned at business\napplications with large volulnes of data and efficient handling of very large\ndatasets has received lnore attention than in the SDB literature.\n25.3.1\nROLLUP and CUBE in SQL:1999\nIn this section, we discuss how lnany of the query capabilities of the rnultidi-\n111ensionalrlloclel are supported in SQL:1999. Typically, a single OLAP opera-\ntion leads to several closely related SQL queries with aggregation and grouping.\nFor exarnple, consider the cross-tabulation shown in Figure 25.5, which was ob-\ntained by pivoting the Sales table. To obtain the saIne inforrnation, we would\nissue the following queries:\nSELECT\nFROM\nWHERE\nGROUP BY\nrr.year, 1.state, SUM (S.sales)\nSales S, T'irnes T, Locations L\nS.tirneid=T.tiIneid AND S.locid=L.locid\nT.year, 1.state\nThis query generates the entries in the body of the chart (outlined by the dark\nlines). The surllluary cohunn on the right is generated by the query:\nSELECT\nFROM\nWHERE\nGROUP BY\n]\".year, SUM (S.saJes)\nSales S1 ,]~ilncs T\nS ·\n·1 T'\n·1\n'\" .tuneu =\n.tunclC\nT.year\nl'hc~ sunnnary ro\\v at the bottorl1 is generated l)y the query:\nSELECT\nFROM\nWHERE\nGROUP BY\nL.state, SUM (S.sales)\nSales S, Locations L\nS.locid=L.locicl\nI\".state\nl'he C>UI11111ative SUITl in the bottonl-right corner of the ch;:ut is produced by the\nquery:\n\n857\n$\nSELECT\nFROM\nWHERE\nSUM (S.sales)\nSales S ~ Locations L\nS.loc:id=L.locid\nThe exarnple cross-tabulation can be thought of as roll-up on the entire dataset\n(Le., treating everything as one big group), on the Location dirnension, on the\nrrirne dirnensioIl, and on the Location and Tinle dinlensions together.\nEach\nroll-up corresponds to a single SQL query with grouping. In general, given a\nrneEtSUre with k a..ssociated dirnensions, we can roll up on any subset of these k\ndiInensions; so \\ve have a total of 2k such SQL queries.\nThrough high-level operations such as pivoting, users can generate lTlany of\nthese\n2k~ SQL queries.\nR,ecognizing the cornrnonalities between these queries\nenables r110re efficient, coordinated COlTlputation of the set of queries.\nSQL: 1999 extends the GROUP BY construct to provide better support for roll-up\nand cross-tabulation queries. The GROUP BY clause with the CUBE keyword is\nequivalent to a collection of GROUP BY statenlents, with one GROUP BY state-\nnlE~nt for each subset of the k dirnensions.\nConsider the following query:\nSELECT\nFROM\nWHERE\nGROUP BY\nrr.year, L.state, SUM (S.sales)\nSales S, Tirnes T, Locations L\nS.tirneid=T'.tirneid AND S.1ocid=L.locid\nCUBE (T.year, L.state)\nThe result of this query, shown in Figure 25.6, is just a tabular representation\nof the cross-tabulation in Figure 25.5.\nSQL: 1999 also provides variants of GROUP BY that enable cornputatioll of sub-\nsets of the cross-tabulation cornputed using GROUP BY CUBE. For exarnple, \\VC\ncall replace the grouping clause in the previous query \\ivith\nGROUP BY ROLLUP\nCr.y(~ar, L.state)\nIn contrast to GROUP BY CUBE, Vile aggregate by an pairs of year ltnd state values\netnel by each\n~year, and. cornpute an overall SlIHl for the entire dataset (the la.st\nrCNl in Figure 25.6), but \\VC do not aggregate for 8(1,ch state value. 1'11e result\nis identical to that sho\\vn inF'igure 25.6, except that the rows with 'nl1,ll ill the\nT. ycru' COhUIlll and non-nvJl valuc~s in\ntlH~ L.,'itatc colurnn are not cornputed.\nCUBE pid, locid, tirneid BY SUM Sales\n\n858\nCHAPTER 2fJ\n1995\n63\n81\n144\nCA\nnull\n1995 -r--------t---,\n1995\n1996\n\\V1\n38\n1996\nCA\n107\n1996\n1997\n1H,dl\nWI\n145\n75\n1997\nCA\n35\n110\n176\n223\nnull,--+---\nWI\nCA\nnull\n--+--~---t---\nnull\n1997\nnull\nnull\n399\nFigure 25.6\nThe Result of GROUP BY CUBE on Sales\nrrhis query rolls up the table Sales on all eight subsets of the set {pid, locid,\ntirneid} (including the empty subset). It is equivalent to eight queries of the\nfonn\nSELECT\nSUM (S.sales)\nFROM\nSales S\nGROUP BY grouping-list\nThe queries differ only in the grouping-list, which is sorne subset of the set {pid,\nlocid, tirneid}. We can think of these eight queries a'3 being arranged in a lattice,\nas shown in Figure 25.7. The result tuples at a node can be aggregated further\nto cornpute the result for any child of the node. This relationship between the\nqueries arising in a CUBE can be exploited for efficient evaluation.\n{pid. locid, timeid}\n~I~\n{pid, locid}\n{pid, timeid}\n{Iocid, timeid}\n\\><1><1\n{pid}\n{Iocid}\n{timeid}\n~I~\n{ }\nFigure 25.7\n'l'he Lattice of GROUP BY Queries ill a CUBE Query\n\nData vf,!aTc}UJ1U3ing and Decision SllPP07't\n25.4\nWINDOW QUERIES IN SQL:1999\n859\nThe tiIne dirnension is very important in decision support and queries involving\ntrend analysis have traditionally been difficult to express in SQL. To address\nthis, SQL:1999 introduced a fundamental extension called a query window.\nExamples of queries that can be written using this extension, but are either\ndifficult or iInpossible to write in SQL without it, include\n1. Find total sales by rnonth.\n2. Find total sales by rnonth for each city.\n3. Find the percentage change in the total monthly sales for each product.\n4. Find the top five products ranked by total sales.\n5. Find the trailing n day moving average of sales.\n(fbI' each day, we must\ncompute the average daily sales over the preceding n days.)\n6. Find the top five products ranked by cumulative sales, for every month\nover the past year.\n7. Rank all products by total sales over the past year, and, for each product,\nprint the difference in total sales relative to the product ranked behind it.\nThe first two queries can be expressed as SQL queries using GROUP BY over the\nfact and dinlension tables. The next two queries can be expressed too, but are\nquite complicated in SQL-92. The fifth query cannot be expressed in SQL-92\nif n is to be a pararneter of the query. The last query cannot be expressed in\nSQL-92.\nIn this section, we discuss the features of SQL:1999 that allow us to express all\nthese queries and, obviously, a rich class of sirnilar queries.\nThe rnain extension is the WINDOW clause, which intuitively identifies an ordered\n'window' of rows 'around' each tuple in a table. Tihis allows us to apply a rich\ncollection of aggregate functions to the windovv of a row and extend the row\nwith the results. F'or exarnple, we can associate the average sales over the past\n3 days with every Sales tuple (each of which records 1 day~s sales). This gives\nus a 3-day Illoving average of sales.\n\\Vhile there is sorne sirnilarity to the GROUP BY and CUBE clauses, there are\nilnportant differences as vvell.\nFor exarnple, like the WINDOW operator, GROUP\nBY\nall()\\~ls us to create partitions of rows and flT)ply aggregate functions such as\nSUM to the rows in a pa.,rtition. lIo\\vever, unlike WINDOW, there is a single output\nrow per pa.rtition, rather than one output row for each ro\\v, and E~ach partition\nis an unorder(~d collection of 1'O\\\\7's.\n\n860\n\\Ve now illustrate the \"window concept through an exalnple:\nCHAPTER 25\nt\nSELECT L.state, rr.IIlonth, AVG (S.sales) OVER \"vV AS Inovavg\nFROM\nSales S, Tinles rr, Locations L\nWHERE\nS.tirneid=T.tirIleid AND S.1ocid=L.locid\nWINDOW VV~ AS (PARTITION BY L.state\nORDER BY 'f.lnonth\nRANGE BETWEEN INTERVAL '1'\nMONTH PRECEDING\nAND INTERVAL '1'\nMONTH FOLLOWING)\nThe FROM and WHERE clauses are processed as usual to (conceptually) generate\nan interrnediate table, which we refer to a.'3 Ternp. vVindows are created over\nthe TeHIp relation.\nThere are three steps in defining a window. First 1 we define part'it'ions of the\ntable, using the PARTITION BY clause. In the exarnple, partitions are based on\nthe L.8tate colurnn. Partitions are sitnilar to groups created with GROUP BY, but\nthere is a very important difference in how they are processed. To understand\nthe difference, observe that the SELECT clause contains a column, T. month,\nwhich is not used to define the partitions; different rows in a given partition\ncould have different values in this colulun. Such a colurnn cannot appear in the\nSELECT clause in conjunction with grouping, but it is allowed for partitions.\n'The reason is that there is one answer row for each row in a partition of Ternp,\nrather than just one answer row per partition. The window around a given row\nis used to COlnpute the aggregate functions in the corresponding answer row.\nThe second step in defining a \\vindow is to specify the\nordeTir~g of rows within\na partition. We do this using the ORDER BY clause; in the exarnple, the rows\nwithin each partition are ordered by T. 'Tnonth.\nThe third step in window definition is to !Ta'Tne windo\\vs; that is, to establish\nthe boundaries of the window associated with each row in terrns of the ordering\nof rows within partitions. In the exalnple, the window for a row includes the\nro\\v itself plus all rows whose rnonth value is within a Inonth before or after;\ntherefore~ a row \\,those Tnonth value is .Jllne 2002 has a window containing all\nrows with Tnonth equal to !\\Ilay, June, or July 2002.\n1'he answer ro\\v corresponding to a given 1'0\\\\,' is constructed by first identifying\nits \\vindo\\v.\nThen~ for each ansvver colurun defined using a window aggregate\nfunction, we cornpute the a,ggregate llsing the ro\\vs in the V\\Tindo\\v.\nIn our\nexarnple~ each ro\\v of l\"elnp is essentially a ro\\v of Sales, tagged with\nextra details (about the location and tirne dirnensions). There is one partition\nfor ea.ch state (tnd every ro\\v of Ternp belongs to exactly one partition. Consider\n\n861\na ro\\v for a store in \\Visconsin.\n~rhe row states the sales for a given product, in\nthat\nstore~ at a certain tirHe. The \\'lindc)\\1\\;' for this ro\\.\\! includes all ro\\vs that\ndescribe sales in \\Visconsin vvithin the previous or next Inonth and 1novavg is\nthe average of sales (over all products) in \\Visconsin \\vithin this period.\n\\\\To note that the ordering of ro\\vs 'ivithin a partition for the purposes of windoVvT\ndefinition does not extend to the table of answer ro\\vs. The ordering of ansvver\nro\\vs is nondeterlninistic, unless, of course, \\ve fetch therIl through (1, cursor and\nuse ORDER BY to order the cursor's output.\n25.4.1\nFraming a Window\nThere are two distinct ways to fra1118 a window in SQL:1999.\nl'he exarnple\nquery illustrated the RANGE construct, which defines a window based on the\nvalues in SOllle cohulln (rnonth in our exarnple). The ordering colu111n has to\nbe a nU111eric type, a datetillle type, or an interval type since these are the only\ntypes for which addition and subtraction are defined.\nThe second approach is based on using the ordering directly and specifying how\nIllany rows before and after the given row are in its window. Thus, we could\nsay\nSELECT L.state, T.rnonth, AVG (S.sales) OVER \\V AS Inovavg\nFROM\nSales S,\nTilll(~S T, Locations L\nWHERE\nS.tirneid=T'.tinlE~id\nAND S.locid=L.locid\nWINDOW W AS (PARTITION BY L.state\nORDER BY T.IIlonth\nROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING)\nIf there is exactly one row in Tenlp for each IIlonth, this is equivalent to the\nprevious query. IIo\\vever ~ if c\\, given lnonth has no rows or lnultiple l\"(nvs, the\nt\\VO queries produce different results. In this case, the result of the second query\nis hard to understand because the \\vindc)\\vs for different rows do not align in a,\nIlfttllralway.\nThe second approach is appropriate if, in tcnns of our exarnple~ there is exactly\none 1'o\\v per lllonth.\nC·eneralizing frOIrI this, it is also appropriate if there is\nexactly one i·o\\v for every vahle in the sequence of ordering COhll1Ul values.\n·UnJike the first approach, 'where the ordering has to be specified over a single\n(rullneric, datetinl€-\\ or interval type) colurnn, the ordering can be based on a\ncornposite key.\n\n862\n(;HAPTER 25\n\\rYe can also define \\vindows that include all rcnvs that are before B, given row\n(UNBOUNDED PRECEDING) or all r0\\VS after a given row (UNBOUNDED FOLLOWING)\n'within the row~s partition.\n25.4.2\nNew Aggregate Functions\n\\:Vhile the standard aggregate functions that apply to rnultisets of values (e.g.,\nSUM,\nAVG) can be used in conjunction \\vith Willdo\\ving, there is a lleed for a\nnew class of functions that operate on a !'ist of values.\nThe RANK function returns the position of a row within its partition.\nIf a\npartition ha..'3 15 rows, the first rovv (according to the ordering of rows in the\nwindow definition over this partition) ha.s rank 1 and the last row has rank 15.\nThe rank of intermediate rows depends on whether there are multiple (or no)\nrows for a given value of the order.ing colurnn.\nConsider our running example. If the first row in the Wisconsin partition has\nthe lllonth January 2002, and the second and third rows both have the rnonth\nFebruary 2002, then their ranks are 1, 2, and 2, respectively. If the next row\nhas rllonth March 2002 its rank is 4.\nIn contrast, the DENSE_.RANK function generates ranks without gaps.\nIn our\nexalnple, the four rows are given ranks 1, 2, 2, and 3. The only change is in\nthe fourth row, whose rank is now 3 rather than 4.\nThe PERCENT...RANK function gives a lneasure of the relative position of a row\nwithin a partition.\nIt is defined as (RANK-1) divided by the Innnber of rows\nin the partition. CUME-DIST is sirnilar but based on actual position within the\nordered partition rather than rank.\n25.5\nFINDING ANSWERS QUICKLY\nA recent trend, fueled in part by the popularity of the Internet, is an ernphasis\n011 queries for which a user vvants only the first fevill, or the 'best' few, ansvvers\nquickly.\n\\Vhcn users pose queries to a search engine such as AltaVista, they\nrarely look beyond the first or second page of results.\nIf they do not find\nwhat they are looking for, they refine their query and resubrnit it. '.rhe senne\nphen()lneuon occurs in decision support applications and scnne DBl\\;1S products\n(e.g., DB2) already support extended SQL con.structs to specify sueh queries. A\nrelated trend is that, for cornplex queries, users would like to ~ee an approxirnat(~\nanswer quickly and then have it 1Je continually refined, rather than \\vait until\nthe exact ansvver is\navailablc~. \\Ve now discuss these 1,\"\"0 trends l)riefly.\n\nData WaTcho'u8'ing and Decis'io71 S'UPJ)OTt\n25.5.1\nTop N Queries\n&63\nAn analyst often wants to identify the top-selling handful of products, for ex-\nalnple. \\Ve can sort by sales for each product and return answers in this order.\nIf \\ve have a Inillion products and the analyst is interested only in the top 10,\nthis straightforward evaluation strategy is clearly \\vasteful. It is desirable for\nusers to be able to explicitly indicate how rnany answers they want, rnaking\nit possible for the DB1VlS to optirnize execution. l-'he follo\\ving exarnple query\nasks for the top 10 products ordered by sales in a given location and tiIne:\nSELECT\nP.pid, P.pnarne, S.sales\nFROM\nSales S, Products P\nWHERE\nS.pid=P.pid AND S.locid==l AND S.tiIneid=3\nORDER BY S.sales DESC\nOPTIMIZE FOR 10 ROWS\nThe OPTIMIZE FOR N ROWS construct is not in SQL-92 (or even SQL:1999), but\nit is supported in IBM's DB2 product, and other products (e.g., Oracle 9i) have\nsirnilar constructs. In the absence of a cue such as OPTIMIZE FOR 10 ROWS, the\nDBMS computes sales for all products and returns thenl in descending order\nby sales. The application can close the result cursor (i.e., tenninate the query\nexecution) after consulning 10 rows, but considerable effort has already been\nexpended in cornputing sales for all products and sorting them.\nNow let us consider how a DBMS can use the OPTIMIZE FOR cue to execute the\nquery efficiently. The key is to sOlnehow cornpute sales only for products that\nare likely to be in the top 10 by sales. Suppose that we know the distribution\nof sales values because we rnaintain a histogran1 on the sales cohuun of the\nSales relation.\nWe can then choose a value of sales, say, c, such that only\n10 products have a larger sales value.\nFor those Sales tuples that rneet this\ncondition, we can apply the location and tirne conditions as well and sort the\nresult..Evaluating the following query is equivalent to this approach:\nSELECT\nFROM\nWHERE\nORDER BY\nP.pid, P.pnarne, S.sales\nSales S, Products P\nS.pid=P.picl AND S.locid=1 AND\nS.sales DESC\nS.tirneid::::::~3 AND S.sales > c\nThis approach is, of course, ruuch faster than the alternative of cornputing all\nproduct sales and sorting thern, but there are SOIne in1portant problerns to\nresolve:\n1. flow do 'we choose the sales cntoff value c? EIistograrns and other systeln\nstatistics can be used for this rn1rI)()SC, but this can be a tricky issue. For\n\n864\nC;HAPTER 25\n@\none\nthing~ the statistics rnaintained by a DBtv.IS are only approxirnate.\nFor another, even if \\ve choose the cutoff to reflect the top 10 sales values\naccurately, other conditions in the query Inay elirninate SOHle of the selected\ntuples, leaving us with fewer than 10 tuples in the result.\n2.\n~'Vhat 'if we have\n'fnon~ than 10 t'll]Jlesin the 'result? Since the choice of\nthe cutoff c is approxirnate, \\'Ie could get 1nore than the desired nurnber\nof tuples in the result.\nrrhis is easily handled by returning just the top\n10 to the user.\n\\Ve still save considerably with respect to the approach\nof cornputing sales for all products, thanks to the conservative pruning of\nirrelevant sales infonnation, using the cutoff c.\n3. What 'if we have fewer' than 10 tuples in the. resv,lt? Even if \\ve choose the\nsales cutoff c conservatively, we could still cOlnpute fe\\ver than 10 result\ntuples. In this case, we can re-execute the query with a srnaller cutofF value\nC2 or sirnply re-execute the original query \\vith no cutoff.\nThe effectiveness of the approach depends on how well we can estirnate the\ncutoff and, in particular, on rninimizing the nurnber of tiules we obtain fewer\nthan the desired nurnber of result tuples.\n25.5.2\nOnline Aggregation\nConsider the following query, which asks for the average sales arIlount by state:\nSELECT\nFROM\nWHERE\nGROUP BY\nL.state, AVG (S.sales)\nSales S, Locations L\nS.locid=L.locid\nL.state\nThis can be an expensive query if Sales and Locations are large relations. \\Ve\ncannot a.chieve fast response tirnes with the traditional approach of cornputing\nthe anwer in its entirety when the query is presented. One alternative, as we\nhave seen, is to use precornputation.\nAnother alternative is to cornpute the\nans\\ver to the query when the query is presented l)ut return an approxirnate\nansvver to the user as soon as possible.\nA.s the cornputation progresses, the\nans\\ver quality ,is continually refined. This approach is called online aggrega-\ntion. It is very attra,ctive for queries involving aggregation, beca,use efficient\ntechniques for cornputing and refining approxirnate ans\\\\rers are available.\nChllinf: aggregation is illustrated in Figure 25.8: For CeLeb statc\" \"\"the grouping\ncriterion for our exarnple query . the current value for average sales is displayed,\ntogether with a confidence interval\n1\n1he entry for Alaska tells us that the\n\nData YlaTcho'lLs'ing and Decis'ion, 8'UPIJOTt:\nSTAfUS\nPRJORrrU£\nStare\n,VGtsmtSJ r -.\nlnt.enal\n_:::~J\nf.Iii~\\\n\\~)\nAlabama\n5,232.5\n97%\n103.4\n-=~==]\nAlaska\n2,832.5\n93%\n132.2\nJ\n~.-~'\nArizona\n6,432.5\n98%\n52.3\n\\~)\nWyoming\n4,243.5\nFigure 25.8\nOnline Aggregation\ncurrent estiInate of average per-store sales in Alaska is $2,8~32.50, and that this\nis within the range $2,700.30 to $2,964.70 with 93% probability.\nrrhe status\nbar in the first column indicates how close we are to arriving at an exact value\nfor the average sales and the second cohllnn indicates 'whether calculating the\naverage sales for this state is a priority.\nEstimating average sales for Alaska\nis not a priority, but estimating it for Arizona is a priority.\nAs the figure\nindicates, the DBlVIS devotes Inore systern resources to estiInating the average\nsales for high-priority states; the estirnate for Arizona is Inucll tighter than that\nfor Alaska and holds with a higher probability. Users can set the priority for\na state by clicking on the Prioritize button at any tilne during the execution.\nThis degree of interactivity, together with the continuous feedback provided by\nthe visual display, rnakes online aggregation an attractive technique.\nTo irnplernent online aggregation, a DEl\\!IS lIlust incorporate statistical tech-\nniques to provide confidence intervals for approxiInate answers and use non-\nblocking algorithms for the relational operators.\nAn algorithnl is said to\nblock if it does not produce output tuples until it has consurned all its input\ntuples. For exarnple, the sort-Illerge join algoritlun blocks because sorting re-\nquires all input tuples before detennining the first output tuple. Nested loops\njoin and hash join are therefore preferable to sort-rnerge join for online aggrega-\ntion. Sirnilarly, hash-based aggregation is better than sort-based aggregation.\n25.6\nIMPLEMENTATION TECHNIQUES FOR OLAP\nIn this section we survey 80r11e irnplernentatioll techniques rllotivated by the\n()LAP envirornnent. rrhe goal is to provide a feel for how ()LAP systerIls differ\nfron1 1nore traditional S(~L systerns; our discussion is faT frorn cornprehensive.\n\n866\nCHAPTER 2q\nr------.-----.-- ---\n-\n- -\n_\n_\n_ __\n__.._._.-_.,._-_._--\n••- ••--.....-\n•• \" ••\" ....m\"..'.....- ••••-.---....----:.l\nBeyond B+\nTl~ees: Complex queries have rnotivated the addition of\npowerful indexing techniques to DBMSs. In addition to I3:+ tree indexes,\nOracle 9i supports bitlnap and join indexes and Inaintains these dynalni-\ni\ncally as the indexed relations are updated. Oracle 9i also supports indexes\non expressions over attribute values, such as 10 * sal + bonus. Microsoft\nSQL Server uses bitrnap indexes.\nSybase IQ supports several kinds of\nbitrnap indexes, and rnay shortly add support for a linear h&'3hing based\nindex. Informix UDS supports R trees and Inforrnix XPS supports bitlIlap\nindexes.\nl---\n~__.\n..\nThe rIlostly-read environruent of OLAP systerns rnakes the CPU overhead of\nrnaintaining indexes negligible and the requireruent of interactive response tinles\nfor queries over very large datasets rnakes the availability of suitable indexes\nvery important. This combination of factors has led to the developrnent of new\nindexing techniques. We discuss several of these techniques. We then consider\nfile organizations and other OLAP implenlentation issues briefly.\nWe note that the ernphasis on query processing and decision support appli-\ncations in OLAP systems is being cornplemented by a greater erllphasis on\nevaluating cOlnplex SQL queries in traditional SQL systerIls. Traditional SQL\nsysterns are evolving to support OLAP-style queries more efficiently, supporting\nconstructs (e.g., CUBE and window functions) and incorporating irnpleruentation\ntechniques previously found only in specialized 0 LAP systems.\n25.6.1\nBitmap Indexes\nConsider a table that describes custorners:\nCustoruers( custid: integer, narne: string, gender': boolean, rating: integer)\nThe rating value is an integer in the range 1. to 5, and only two values are\nrecorded for gender. Cohllnns with few possible values are called sparse. vVe\ncan exploit sparsity to construct a new kind of index that greatly speeds up\nqueries 011 these cobulins.\nTh(~ idea is to r.i'ecord values for sparse colurnns as a sequence of bits, one for\neach possible value.\nFbI' exarnple,\na, gender value is either 10 or en; a\n1. in\nthe first position denotes ruale, and 1. in the second position denotes fe1nale.\nSimilarly, 10000 denotes the rai'ing value 1, and 00001 denotes the rating value\n5.\n\nData Wareho,lt,Crtng and Decision SUPPO'lt\ngf)7\nIf we consider the gender values for all rows in the Custorners table, vve can\ntreat this as a collection of two bit vectors, OIle of which has the a.,')sociated\nvalue ~/I(ale) and the other the associated value F(ernale). Each bit vector has\none bit per row in the Custorners table, indicating vvhether the value in that\nrow is the value associated with the bit vector. The collection of bit vectors for\na COhUllIl is called a bitrnap index for that colurnn.\nAn exaInple instance of the Customers table, together with the bitlnap indexes\nfor gender and rating, is shown in Figure 25.9.\n.----\"\n0\n0\n1.\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1.\n0\n0\n0\n1.\n0\nI ·;11\n..•1J··.·.].·.···.·.·.·.···ld·······I··········;4·····.....[!J\n112\nJoe\nM\n3\n...\n,,-\n115\nRaIn\nM\n5\n...\n119\nSue\nF\n5\n112\nWoo\nM\n4\n.:::.:=\n1.\n0\n1\n0\n0\n1.\n1\n0\n!M!Fj\nFigure 25.9\nBitmap Indexes on the Customers Relation\nBitmap indexes offer two important advantages over conventional hash and tree\nindexes. First, they allow the use of efficient bit operations to answer queries.\nFor example, consider the query, \"How Inany Inale custolllers have a rating\nof 5?\" We can take the first bit vector for gender and do a bitwise AND with\nthe fifth bit vector for rating to obtain a bit vector that has 1. for every male\ncustoIner with rating 5. We can then count the number of Is in this bit vector\nto answer the query. Second, bitmap indexes can be much luore cOInpact than\na traditional B+ tree index and are very cunenable to the use of cornpression\ntechniques.\nBit vectors correspond closely to the rid-lists used to represent data entries in\nAlternative (3) for a traditional B+ tree index (see Section 8.2). In fact, we can\nthink of a bit vector for a given age value, say, as an alternative representation\nof the rid-list for that value.\nThis suggests away to combine bit vectors (and their advantages of bitwise\nprocessing) with B+ tree indexes: We can use Alternative (3) for data entries,\nusing a bit vector representation of rid-lists. A caveat is that, if an rid-list is\nvery slnall, the bit vector representation rnay be Illuch larger than a list of rid\nvalues, even if the bit vector is cornpressed. Further, the use of corupression\nleads to decornprcssion costs, offsetting sorne of the C0I11putational advantages\nof the bit vector representation.\nA Inore flexible approach is to usc a standard list representation of the rid-list\nfor S01ne key values (intuitively, those that contain few clernents) and a bit\n\n868\nCHAPTER 2~\nvector representation for other key values (those that contain rnany elenlents,\nand therefore lend themselves to a cOInpact bit vector representation).\nThis hybrid approach, 'which can easily be adapted to work \\\\lith hash indexes\na,,~ well as B+ tree indexes, haa.') both advantages and disadvantages relative to\na standard list of rids approach:\n1. It can be applied even to cohllnns that are not sparse; that is, in ,vhich are\nTnany possible values can appear. The index levels (or the hashing scheIue)\nallow us to quickly find the 'list' of rids, in a standard list or bit vector\nrepresentation, for a given key value.\n2. Overall, the index is Tnore cornpact because we can use a bit vector rep-\nresentation for long rid lists. \\Ve also have the benefits of f&'3t bit vector\nprocessIng.\n3. On the other hand, the bit vector representation of an rid list relies on\na Inapping fron1 a position in the vector to an rid.\n(This is true of any\nbit vector representation, not just the hybrid approach.)\nIf the set of\nrows is static, and we do not worry about inserts and deletes of rows, it\nis straightforward to ensure this by assigning contiguous rids for rows in\na table.\nIf inserts and deletes Inust be supported, additional steps are\nrequired.\nFor exanlple, we can continue to assign rids contiguously on a\nper-table basis and sirnply keep track of which rids correspond to deleted\nrows. Bit vectors can now be longer than the current nUlnber of rows, and\nperiodic reorganization is required to cOlllpact the 'holes' in the assignrnent\nof rids.\n25.6.2\nJoin Indexes\nCornputing joins with sIllall response tirnes is extrernely hard for very large\nrelations. One approach to this problern is to create an index designed to speed\nup specific join queries. Suppose that the Custorners table is to be joined ~with\n(1, table called Purchases (recording purchases Inacle by custorners) on the c,ltsUd\nfield.vVe can create a collection of (c, p) pairs, where p is the rid of a Purchases\nrecord that joins \\vith a Custc)lners recol'c! with cusUd c.\nThis idea can be generalized to support joins over ruore than t\\VO relations. \\Ve\ndiscuss the special case of a star scherna., in \\vhich the fact table is likely to\nbe joined with several dirnension tables. Consider a join query that joins fact\ntable F vvith dilnension tables D1 and D2 and includes selection conditions on\ncohunn [:1 of tal)le 1)1 Etnd colurnn (:12 of table D2. \\Ve store a tuple ('tl' ('2, r)\nirl the join index if T1 is the rid of a tuple in table 1)1 with value ('1 in cohunn\nC\n1\n1 , '1'2 is the rid of a tuple in table D2 ,vith value C2 in colllrnn (:12, and T is the\nrid of a tllple in the fact ta,ble F, (uHl tJlcsethree tUl)les join with each other.\n\nData Vvareh07l8\"ing arul DeC\"lS'lCyn S'llppOTt\n~:~p~ex Queries: The IBM DB2 o;;:izer recognizes star join~:::l\nand perfOfIns rid-b&ged sernijoins (using BIoarn filters) to filter the fact\nIII\ntable. 1'hen fact table rO\\V8 are rejoined to the dimension tables. Cornplex\n.\n(rnnltitable) dirnension queries (called snowflake qucrvlcs) are supported.\nDB2 also supports CUBE using SlnclJ't algorithrns that rninhnize sorts.~1i­\ncrosoft SQL Server optiInizes star join queries extensively.\nIt considers\ntaking the cross-product of srnall dirnension tables before joining with the\nfact table, the use of join indexes, and rid-basedserniJoins. Oracle 9i also\nallows users to create diInensions to declare hierarchies and functional de-\npendencies. It supports the CUBE operator and optirnizes star join queries\nby elinlinating joins when no colunlll of a dirnension table is part of the\nquery result.\nDBMS products have also been developed specifically for\ndecision support applications, such as Sybase IQ.\n_ ~__~\n•__ <,.,••_\"•.\" .•_ •••••\n•\n---.J\nThe drawback of a join index is that the nurnber of indexes can grow rapidly\nif several colurnns in each dirnension table are involved in selections and joins\nwith the fact table.\nAn alternative kind of join index avoids this problem.\nConsider our exarnple involving fact table F and dirnension tables Dl and D2.\nLet G1 be a column of Dl on which a selection is expressed in some query that\njoins Dl with F. Conceptually, we now join F with Dl to extend the fields of F\nwith the fields of Dl, and index F on the 'virtual field' G1: If a tuple of Dl with\nvalue Cl in colurnn C\\ joins with a tuple of F with rid r, we add a tuple (C1' r)\nto the join index. We create one such join index for each colurnn of either Dl\nor D2 that involves a selection in SOHle join with F; C1 is an exarnple of such a\nCOIUllUl.\nThe price paid with respect to the previous version of join indexes is that join\nindexes created in this way have to be cornbined (rid intersection) to deal with\nthe join queries of interest to us. This can be done efficiently if \\ve rnake the\nne\\v indexes bitrnap indexes; the result is called a, bitrnapped join index.\nThe idea works especiaJly\nW(~ll if cohunns such a\"s Cil are sparse, and therefore\nwell suited to bitrnap indexing.\n25.6.3\nFile Organizations\nSince rllFtny OLAP queries involve just a fev\\! colurnns of a large relation, vertical\npartitioning becornes attractive. IIcrwever, storing a. relation colurnn-\\vise can\ndegrade perfoI\"rnance for queries that involve several colurnns. An alternative\nin a, rl1ostly-read envirollrnent is to store the relation rOvv-vvise, but also store\neach COlUH1Il separatel:y.\n\n870\nCHAPTER 26\nA rnore radical file organization is to regard the fact table as a large Illuitidi-\nrnensional array and store it and index it as such. This approach is taken in\nNI0LAP systerns. Since the array is lIluch larger than available lnain lnelnory,\nit is broken up into contiguous chunks, as discussed in Section 23.8. In addition,\ntraditional B+- tree indexes axe created to enable quick retrieval of chunks that\ncontain tuples \"'lith values in a given range for one or rnore diInensions.\n25.7\nDATA WAREHOUSING\nData warehouses contain consolidated data from many sources, augrnented with\nsunnnary inforrnation and covering a long time period. Warehouses are lnuch\nlarger than other kinds of databases; sizes ranging frorn several gigabytes to ter-\nabytes are cornman. Typical workloads involve ad hoc, fairly cOlllplex queries\nand fast response tilnes are important. These characteristics differentiate ware-\nhouse applications from OL'TP applications, and different DBMS design and\nirnplerrlentation techniques nUlst be used to achieve satisfactory results. A dis-\ntributed DBMS with good scalability and high availability (achieved by storing\ntables redundantly at more than one site) is required for very large warehouses.\nExternal Data Sources\nFJ\nr=J\nOperational Databases\nEXTRACT\nCLEAN\nTRANSFORM\nLOAD\nREFRESH\n----·-.--------------------------1\nMetadata Repository\n---_.-------\nData Warehouse\nSERVES\nVisualization\nOLAP\nFigure 25.10\nA rrypical Data \\Varehousing Architecture\nA typical data warehousing architecture is illustrated in Figure 25.10. An orga-\nnization's daily operations access and rnodify operational databases. Data\nfror11 these oIlfSrational databases and other external sources (e.g., custorner\nprofiles supplied by external consultants) are extracted by using interfaces\nsuch as JI)BC (see Section 6.2).\n\nData\n~JVaTeh(nl8'ing and Decision 8UPP01\nTt\n25.7.1\nCreating and Maintaining a Warehouse\n871\nl\\!Iany challenges rnust be Inet in creating and Inaintaining a large data ware-\nhouse.A good datab&'3e scherua nlust be designed to hold an integrated collec-\ntion of data copied froIn diverse sources. For exarnple, a cornpany warehouse\nrnight include the inventory and personnel departrnents' databa.'3es, together\nwith sales databases rnaintained by offices in different countries.\nSince the\nsource databases are often created and rnaintained by different groups, there\nare a nUlnber of selnantic Inisrnatches across these databases, such as different\ncurrency units, different narnes for the saIne attribute, and differences in how\ntables are nornlalized or structured; these differences Inust be reconciled when\ndata is brought into the warehouse. After the warehouse schenla is designed,\nthe warehouse must be populated, and over tirne, it Inust be kept consistent\nwith the source databases.\nData is extracted from operational databases and external sources, cleaned\nto Inininlize errors and fill in Inissing information when possible, and trans-\nformed to reconcile semantic Inismatches. Transforlning data is typically ac-\ncOlnplished by defining a relational view over the tables in the data sources\n(the operational databases and other external sources). Loading data consists\nof ruaterializing such views and storing therll in the warehouse. Unlike a stan-\ndard view in a relational DBMS, therefore, the view is stored in a database\n(the warehouse) that is different frorn the database(s) containing the tables it\nis defined over.\nThe cleaned and transfonned data is finally loaded into the warehouse. Ad-\nditional preprocessing such &'3 sorting and generation of surnrnary infornuttion\nis carried out at this stage. Data is partitioned and indexes are built for effi-\nciency. Due to the large vohllue of elata, loading is a slow process. Loading a\nterabyte of data sequentially can take 'Y\"eeks, and loading even a gigabyte can\ntake hours. Parallelisul is therefore iInportant for loading warehouses.\nAJter data is loaded into a warehouse, additional rneasures rnust be taken to\nensure that the data in the vvarehouse is periodically refreshed to reflect\nupdates to the data sources and periodically purge old data (perhaps onto\narchival rnedia).\nObserve the connection between the problern of refreshing\nwarehouse tables and a,synchronously rnaintaining replica..\" of tables in a dis-\ntributed DBMS. Maintaining replicas of source relations is an essential part of\nwarehousing, and this application clornain is an iInportant factor in the popu-\nlarity of a.synchronous replication (Section 22.11.2), even though asynchronous\nreplication violates the principle of distributed data independence. The prob-\nlern of refreshing warehouse tables (\\vhich are rnaterialized views over tables in\n\n872\nthe source databEkses) has also rene\\ved interest in inerernental Illaintenance of\nrna.terialized vic\\vs. (\"VVe discuss rnaterialized vie\\\\Ts in Section 25.8.)\nAn irnportant tC4sk in Inainta,ining a warehouse is keeping track of the data\ncurrently stored in it; this bookkeeping is done by storing infofrnation about\nthe \\varehouse data in the systenl catalogs. rrhe systerIl catalogs associated 'with\na \\varehouse are very large and often stored and 111anaged in a separate database\ncalled a metadata repository. The size and cornplexity of the catalogs is in\npart due to the size and cOlnplexity of the warehouse itself and in part because\na lot of adrninistrative inforrnation rnust be Inaintained. For excunple, we HlllSt\nkeep track of the source of each warehouse table and when it was last refreshed,\nin addition to describing its fields.\n1'he value of a warehO\"use is ultin1ately in the analysis it enables. The data in a\nwarehouse is typically accessed and analyzed using a variety of tools, including\nOLAP query engines, data. mining algorithrns, inforrnation visualization tools,\nstatistical packages, and report generators.\n25.8\nVIEWS AND DECISION SUPPORT\nViews are widely used in decision support applications.\nDifferent groups of\nanalysts within an organization are typically concerned with different aspects\nof the business, and it is convenient to define views that give each group insight\ninto the business details that concern it. Once a view is defined, we can write\nqueries or new view definitions that use it, as we saw in Section 3.6; in this\nrespect a view is just like a base table. Evaluating queries posed against views\nis very ilnportant for decision support applications. In this section, we consider\nhow such queries can be evaluated efficiently after placing views within the\ncontext of decision support applications.\n25.8.1\nViews, OLAP, and Warehousing\nViews are closely related to OLAP and data warehousing.\nOLAP queries are typically aggregate queries. Analysts want fa.st answers to\nthese queries over very large datasets, and it is natural to consider precoluputing\nvievvs (see SectiorlS 25.9 and 25.10). In particular, the CUBE operator~ 'discussed\nin Section 25.3\"\"'gives rise to several aggregate queries that are closely related.\nThe relationships that exist betvveen the Inany aggregate queries that arise froln\na single CUBE operation can be exploited to develop very effective precornpu-\ntation strategies. The idea is to choose a subset of the aggregate queries for\nInaterialization in such a. vvay that typical CUBE queries can be quickly answered\nby using the Inaterialized views arld doing S(Hne additional cornplltation. The\n\nData HlaTcho'zMring a'nd Deci,sion 5'UPPOTt\n~73\nchoice of views to 111ateria1ize is influenced by ho\\v lllany queries they can po-\ntentially speed up and by the aillount of space required to store the Inaterialized\nview (since we have to \\vork with a given alnount of storage space).\nA data \\varehouse is just a collection of &csynchrollously replicated tables and\nperiodically synchronized views. A Wareh(HIS(~ is characterized by its size, the\nnuruber of tables involved, and the fact that IllOSt of the underlying tables\nare froln external, independently lnaintained databases. Nonetheless, the fun-\ndaluental probleln in warehouse lnaintenance is asynchronous rnaintenance of\nreplicated tables and materialized views (see Section 25.10).\n25.8.2\nQueries over Views\nConsider the following view, RegionalSales, which cornputes sales of products\nby category and state:\nCREATE VIEW RegionalSales (category, sales, state)\nAS SELECT P.category, S.sales, L.state\nFROM\nProducts P, Sales S, Locations L\nWHERE\nP.pid = S.pid AND S.locid = L.locid\nThe following query computes the total sales for each category by state:\nSELECT\nH,.category, It.state, SUM (R.sales)\nFROM\nRegionalSales H,\nGROUP BY R.category, R,.state\n\\\\1hile the SQL standard does not specify how to evaluate queries on views, it\nis useful to think in ternlS of a process called query modification. rrhe idea is\nto replace the occurrence of RegionalSales in the query by the view definition.\nThe result on this query is\nSELECT\nFROM\nGROUP BY\nH,.category, R.state, SUM (R.sales)\n( SELECT P.category, S.sales, L.state\nFROM\nProducts P, Sales S, Locations L\nWHERE\nP.piel = S.pid AND S.locid == L.locid ) AS R,\nR,.category, H,.state\n25.9\nVIEW MATERIALIZATION\nvVe can ansvver a query on a view by using the query rnodification technique\njust described. Often, however, queries against cornplex view definitions Illust\n\n874\nCHAPTER 25\nbe answered very fast because users engaged in decision support activities re-\nquire interactive response tirIles.\nEven with sophisticated optilnization and\nevaluation techniques, there is a lirnit to how fa..\"t we can answer such queries.\nAlso, if the underlying tables are in a rernote database, the query rIlodifica-\ntion approach rnay not even be feasible because of issues like connectivity and\navailability.\nAn alternative to query rnodification is to precornpute the view definition and\nstore the result. When a query is posed on the view, the (unrllodified) query is\nexecuted directly on the precornputed result. This approach, called view ma-\nterialization, is likely to be rnuch fa,;ter than the query modification approach\nbecause the complex view need not be evaluated when the query is computed.\nMaterialized views can be used during query processing in the sarne way a'S\nregular relations; for exarnple, we can create indexes on nlaterialized views to\nfurther speed up query processing. The drawback, of course, is that we must\nmaintain the consistency of the precomputed (or m,aterialized) view whenever\nthe underlying tables are updated.\n25.9.1\nIssues in View Materialization\nThree questions must be considered with regard to view nlaterialization:\n1. What views should we rnaterialize and what indexes should we build on\nthe rnaterialized views?\n2. Given a query on a view and a set of materialized views, can we exploit\nthe rnaterialized views to answer the query?\n3. I-Iow should we synchronize rnaterialized views with changes to the under-\nlying tables? The choice of synchronization technique depends on several\nfactors, such a.c; whether the underlying tables are in a rernote database.\nWe discuss this issue in Section 25.10.\n'rhe answers to the first two questions are related.\n'fhe choice of vievvs to\nrnaterialize and index is governed by the expected workload, and the discussion\nof indexing in Chapter 20 is relevant to this question ac; well. The choice of\nviews to rnaterialize is rnore cornplex than just choosing indexes on a set of\ndatabase tables, however, because the range of alternative views to rnaterialize\nis wider. The goaJ is to rnaterialize a srnaU, carefully chosen set of views that\ncan be utilized to quickly answer rnost of the irnportant queries. COIlversely,\nonce vve have chosen a set of views to rnaterialize, we have to consider how they\ncan be used to ansvver a, given query.\nConsider the\nRJ~gi()nalSales view.\nIt involves a JOIn of Sales, Products, and\nLocations and is likely to be expensive to cornpute. On the other hand, if it\n\n[Jata\n~VaTehol1Bin,g (uLd Decis'ioTL S~Ll)poTt\n~75\nis rnaterialized and stored with a clustered B+ tree index on the search key\n(category, state, sales), we Gall ans\\ver the exarnple query by an index-only\nsearl.\nGiven the rnaterialized view and this index, we can also answer queries of the\nfollo\\ving forrn efficiently:\nSELECT\nFROM\nWHERE\nGROUP BY\nR.state, SUM (R.sales)\nRegionalSales R\nILcategory == 'Laptop'\nR.state\nTo answer such a query, we can use the index on the Inaterialized view to locate\nthe first index leaf entry with category == 'Laptop' and then scan the leaf level\nuntil we come to the first entry ¥lith category not equal to Laptop.\nThe given index is less effective on the following query, for which we are forced\nto scan the entire leaf level:\nSELECT\nFROM\nWHERE\nGROUP BY\nR,.state, SUM (R.sales)\nR,egionalSales R\nR.state == 'Wisconsin'\nR.category\nThis exanlple indicates how the choice of views to materialize and the indexes\nto create are affected by the expected workload.\n~rhis point is illustrated further\nby our next exarnple.\nConsider the following two queries:\nSELECT\nFROM\nWHERE\nGROUP BY\nSELECT\nFROM\nWHERE\nGROUP BY\nP.category, SUM (S.sales)\nProducts P, Sales S\nP.pic! == S.pic!\nP.category\nL.state, SUM (S.sales)\nI\n.\nI\nS 1\nS\n...JocatIons ,,;, . a es\n~\n.L.locid = S.locid\nL.state\n'These two queries require us to join the SaJes table (which is likely to be very\nlarge) with another table and aggregate the result. IIO'w can vve use rnaterializa-\ntion to speed up these queries? The straightforward approach is to precornpute\n\n876\n( \"'1\n.\n2·~\njHAPTER\n~)p\neach of the joins involved (Products with Sales and Locations with Sales) or to\npreconlpute each query in its entirety. An alternative approach is to define the\nfollowing view:\nCREATE\nVIEW rrotalSaJes (pid, locid, total)\nAS SELECT\nS.pid, S.locid, SUM (S.sales)\nFROM\nSales S\nGROUP BY S.pid, S.locid\nThe view TotalSales can be rnaterialized and used instead of Sales in our two\nexalnple queries:\nSELECT\nFROM\nWHERE\nGROUP BY\nSELECT\nFROM\nWHERE\nGROUP BY\nP.category, SUM (T.total)\nProducts P, TotalSales T\nP.pid = T.pid\nP.category\nL.state, SUM (T.total)\nLocations L, TotalSales T\nL.locid = rr.locid\nL.state\n25.10\nMAINTAINING MATERIALIZED VIEWS\nA materialized view is said to be refreshed when we rnake it consistent with\nchanges to its underlying tables.\nrrhe process of refreshing a view to keep it\nconsistent with changes to the underlying table is often referred to as view\nmaintenance. Two questions to consider are\n1. flow do vie refresh a view' when an underlying table is nlodified? Two issues\nof particular interest are how to Inaintain vie\\vs incTcrnentally, that is,\nwithout recornputing frolI! scratch when there is a change to an underlying\ntable; and how to rnaintain vie\\vs in a distributed environrnent such as a\ndata vvarehouse.\n2. vVhcn should \\ve refresh a view in response to a change to an underlying\ntable?\n25.10.1\nIncremental View Maintenance\nA straightforward approach to refreshing a vie\\v is to sirnply reeolnpute the\nview \\.vhen an underlying table is rnodified.\nThis rnay, in fact, be a reason-\nable strateKY in sorne ca.\"es.\nFor exarnple, if the underlying tables are in a\n\nData HIaTehc)7I8'ing and Decision Support\nr;877\nrernote databa..'3c, the view can be periodically recornputed and sent to the data\nwarehouse \\vhere \"the vie\\v is Hlaterialized.\nThis ha.'3 the advantage that the\nunderlying tables need not be replicated at the vvarehouse.\n\\Vhenever possible, however, algorithrns for refreshing a view should be incre-\nmental, in that the cost is proportional to the extent of the change rather than\nthe cost of recornputing the vie\\\\r fr(Hn scratch.\nTo understand the intuition behind incrernental view rnaintenance algorithnls,\nobserve that a given row in the rnaterialized view can appear several thnes,\ndepending on how often it was derived. (R.ecall that duplicates are not elirni-\nnated fro111 the result of an SQL query unless the DISTINCT clause is used. In\nthis section, we discuss rnultiset sernantics, even when relational algebra nota-\ntion is used.) The rHain idea behind incremental rnaintenance algorithrIls is to\nefficiently compute changes to the rows of the view, either new rows or changes\nto the count associated with a row; if the count of a row becornes 0, the row is\ndeleted frorH the view.\nWe present an incrernental 11Ulintenance algorithnl for views defined using pro-\njection, binary join, and aggregation; we cover these operations because they\nillustrate the rHain ideas. The approach can be extended to other operations\nsuch as selection, un.ion, intersection, and (rnultiset) difference, as well as ex-\npressions containing several operators.\nThe key idea is still to rnaintain the\nnurnber of derivations for each view row, but the details of how to efficiently\nconlpute the changes in view rows and associated counts differ.\nProjection Views\nConsider a view V defined in tenns of a projection on a tableR; that is,\ny\"\" = n(R).\nEvery row v in V has an associated count, corresponding to the\nnurnber of tirnes it can be derived, \\vhich is the nurnber of rows in R that yield '1)\nwhen the projection is applied. Suppose we 1nodifyR by inserting a collection\nof rows Ili and deleting a collection of existing 1'o\\vs R d .1 vVe cornpute n(l1.\"'i)\nand add it to \\l.\nIf the rnultisetrr(R.i ) contains a row T \\vith count c and r\ndoes not appear in 11 , \\ve add it to V\"with count c. If T is in V, we add c to\nits count. vVe also cornpute n(Rd) and subtract it fronl 1/. ()bserve that if r\nappe~:trs in neRd) \\\\rith count c, it 111USt also appear in y\"\" with a higher count;2\nwe subtract c frOTH r's count in V\".\n_ _'''.\n\"n__\n1'These collections can be multisets of rows. \\'/e can treat a ro\\v rnodification a.s an insert follO\\'1ed\nby\n1:1 delete, for sirnplicity.\n2As a simple exercise, consider why this rnust be so.\n\n878\n(~HAPTER 26\nAs an exanlple, consider the view Jrsales(Sales) and the instance of Sales shown\nin Figure 25.2. _Each ro\\v in the vie\\v has a single cohunn; the (n)\\vwith) value\n25 appears vvith count 1, and the value 10 appears vvith count 3. If \\lve delete\none of the rows in Sales \\vith sales 10, the count of the (l'()\\V \\vith) value 10 in\nthe vie\"v becornes 2. If \\ve insert a new row into Sales with sales 99, the vie\"'!\nno\\v has a row with value 99.\nAn hnportant point is that \\ve have to rnaintain the counts associated vvith rows\neven if the view definition uses the DISTINCT clause, rneaning that duplicates\nare elilninated frorn the view.\nConsider the saIne view with set\nselnantics~­\nthe DISTINCT clause is used in the SQL view definition··------and suppose that we\ndelete one of the rows in Sales with sales 10.\nDoes the view now contain a\nrow with value 10'1 To deterrIline that the answer is yes, we need to maintain\nthe rOw counts, even though each row (with a nonzero count) is displayed only\nonce in the Inaterialized view.\nJoin Views\nNext, consider a view V defined as a join of two tables, R [X] S. Suppose we\nmodify R by inserting a collection of rows R'i and deleting a collection of rows\nRd. We cornpute Ri [X] S and add the result to V. We also C0111pute Rd r><J S\nand subtract the result fror11 V.\nObserve that if r appears in R d\n[X] S with\ncount C, it rnust also appear in V with a higher count>J\nViews with Aggregation\nConsider a view V defined over R using GROUP BY on colUllln G and an ag-\ngregate operation on colu1nn A. Each row v in the vi(~w surnrnarizes a group\nof tuples in R and is of the fonn (g,\n8'u,'rrLrnary) , where 9 is the value of the\ngrouping colulnn G and the sununary inforInation depends on the aggregate\noperation. To lnaintain such a view incrernentally, in general, we have to keep\na lnore detailed surrllnary than just the inforrnation included in the view.\nIf\nthe aggregate operation is COUNT, we need to Inaintain only a count c for each\nrovv v in the vieVvT. If a ro\\v r is inserted intoR, and there is no I'o,v v in \"\\;7\nwith 'v.G = T.G, we add <1 new row (r.G, 1). If there is a ro,v 'I) \\vith v.C} = r.G,\nwe incrernent its count. If a row r is deleted fro111 R, \\ve decrcrnent the count\nfor the row v \"vith v.Ci = T.C}; v can be deleted if its count becornes 0, because\nthen the last row in this group ha.'3 been deleted frorn .R.\nIf the aggregate operation is SUM, we have to lllaintain a SUIll :3 and also a count\nc.\nIf a row T is inserted into If, and there is no renv\n'1) in\n~! \"lith v.C; = T.C:,\n--_..-\n...-\n.....\n:{ As another simple exercise, consider why this mllst be so.\n\nData WaTchou8\nilng and Decis'io11 8UPPOTt\ni79\nwe add a new row ('r.C, a, 1).\nIf there is a, row (T.G, 8, c), we replace it by\n(r.Ci, /3 +\" a, C -t 1). If a rO\\1ll T is deleted frolll Il, \\ve replace the row (r.G, S, c)\nwith {T.G,8 - a, C - 1);\n1) can be deleted if its count becornes O. Observe that\nwithout the count, \\-ve do not know when to delete 'u, since the Slun for a group\ncould be 0 even if the group contains SCHne rows.\nIf the aggregate operation is AVG, \\ve have to lllaintain a Slun s, a count c,\nand the average for each row in the vie\\v. The SlUll and count are rnaintained\nincrernentally as already described, and the average is corllputed as s/ c.\nThe aggregate operations MIN and MAX are potentially expensive to rnaintain.\nConsider MIN. For each group in R, we rnaintain (g, rn, c), where rn is the\nIninilnurn value for colUllln A in the group g, and c is the count of the nUlllber\nof rows l' in R with T.G == 9 and r.A == m. If a row l' is inserted into Rand\nr.G == g, if r.A is greater than the miniriulill m for group g, we can ignore r. If\nr.A is equal to the 111iniInurll m for r's group, we replace the summary row for\nthe group with (g, m, c+ 1). If r.A is less than the minirllum m for r's group, we\nreplace the SUlnrnary for the group with (g, T.A, 1). If a row r is deleted frorn\nRand T.A is equal to the minimurIl rrt for T'S group, then we HUlst decrernent\nthe count for the group. If the count is greater than 0, we sinlply replace the\nsurnmary for the group with (g, rn, c-_· 1). However, if the count becomes 0, this\nIneans the last row with the recorded rninimum A value has been deleted from\nR and we have to retrieve the sInallest A value among the relnaining rows in\nR with- group value r.G-and this might require retrieval of all rows in 11, with\ngroup value T.G.\n25.10.2\nMaintaining Warehouse Views\nThe views rnaterialized in a data warehouse can be based on source tables\nin rernote databases.\nrIhe asynchronous replication techniques discussed in\nSection 22.11.2 allow us to connnunicate changes at the source to the warehouse,\nbut refreshing vie\\vs incrernentally in a distributed setting presents sorne unique\nchallenges. To illustrate this, we consider a sirnpleview that identifies suppliers\nof Toys.\nCREATE VIEW ToySuppliers (sid)\nAS SELECT S.sid\nFROM\nSuppliers S, Products P\nWHERE\nS.pid == P.piel AND P.category == 'Tbys'\nSuppliers is a new table introduced for this exarnple; let us &ssurne that it\nhEkS just two fields, sid aIld pid, indicating that supplier s'id supplies part pill.\nrrb.c location of th(~ tables Proclucts and Suppliers and the vie\\v ToySuppliers\n\n880\ninfluences hovv we IIlaintain the\nvh~\\v. Suppose that all three are rnaintained\nat a single site. vVe can Inaintain the view increlnentally using the techniques\ndiscussed in Section 25.10.1. If a replica of the vie\\v is created at another site,\nwe can 1l1onitor changes to the Inaterialized vie\\v and apply thcIn at the second\nsite using the a..synchronous replication techniques froIn Section 22.11.2.\nBut, what if Products and Suppliers are at one site and the view is Inaterialized\n(only) at a second site? To rnotivate this scenario, \"we observe that, if the first\nsite is used for operational data and the second site supports cornplex analysis,\nthe two sites lnay well be adrninistered by different groups.\nThe option of\nlnaterializing ToySuppliers (a view of interest to the second group) at the first\nsite (run by a different group) is not attractive and may not even be possible; the\nadnlinistrators of the first site may not ,vant to deal with someone else's views,\nand the a(hninistrators of the second site n1ay not want to coordinate with\nsonleone else whenever they Inodify view definitions.\nAs another motivation\nfor rnaterializing views at a different location froIn source tables, observe that\nProducts and Suppliers may be at two different sites. Even if ·we rnaterialize\nToySuppliers at one of these sites, one of the two source tables is reillote.\nNow that we have presented Inotivation for rnaintaining rroySuppliers at a loca-\ntion (say, Warehouse) different froIn the one (say, Source) that contains Prod-\nucts and Suppliers, let us consider the difficulties posed by data distribution.\nSuppose that a new Products record (with category == 'Toys') is inserted. We\ncould try to rnaintain the view incren1entally as follows:\n1. The Warehouse site sends this update to the Source site.\n2. 1'0 refresh the view, we need to check the Suppliers table to find suppli-\ners of the itern, and so the v\\larehouse site asks the Source site for this\ninforrnation.\n3. The Source site returns the set of suppliers for the sold iteln, and the\nvVarehouse site incrernentally refreshes the view.\nThis works when there are no additional changes at the Source site in between\nsteps (1) and (3).\nIf there are changes, ho\\vever, the Inaterializecl view can\nbecorne incorrect\nreflecting a state that can never arise except for anornalies\nintroduced by the preceding, naive, increInental refresh algorithrn. To see this,\nsuppose that Pr,oducts is enlpty and Suppliers contains just the row \\81, 5)\ninitially, and consider the following sequence of events:\n1. Product pid = 5 is inserted \\vith category = 'Toys'; Source notifies\\Vare-\nhouse.\n2. Warehouse asks Source for suppliers of product pid = 5.\n(The only such\nsupplier at this instant is 81.)\n\njJata HIarehO'lt8'ing and Decisi(Y!L S7.qJ].J01'l\"\n881\n3. The row (82,5) is inserted into Suppliers; Source notifies \\Varehouse.\n4. To decide whether 82 should be added to the vie\\v, vve need to kno\\v the\ncategory of product pid = 5, and \\Varehouse asks Source. (ltVarehouse has\nnot received an anS7.lJer to its previous quest1:on.)\n5. Source now processes the first query frorn \\tVarehouse, finds two suppliers\nfor part 5, and returns this inforrnation to Warehouse.\n6. \\tVarehouse gets the answer to its first question: suppliers 81 and 82, and\nadds these to the view, each with count 1.\n7. Source processes the second query frorn \\Varehouse and responds with the\ninforll1ation that part 5 is a toy.\n8. Warehouse gets the answer to its second question and accordingly incre-\nHlents the count for supplier 82 in the view.\n9. Product pid == 5 is now deleted; Source notifies Warehouse.\n10. Since the deleted part is a toy, Warehouse decrements the counts of nlatch-\ning view tuples; 81 has count 0 and is relnoved, but s2 has count 1 and is\nretained.\nClearly, 82 should not rernain in the view after part 5 is deleted. This example\nillustrates the added subtleties of incremental view rnaintenance in a distributed\nenvironment, and this is a topic of ongoing research.\n25.10.3\nWhen Should We Synchronize Views?\nA view maintenance policy is a decision about when a view is refreshed,\nindependent of whether the refresh is incrernental or not.\nA view can be re-\nfreshed within the sallIe transaction that updates the underlying tables. This\nis called immediate view Iuaintenance. The update transaction is slowed\nby the refresh step, and the irupact of refresh increc1.'3es with the nurnber of\nmaterialized views that depend on the updated table.\nAlternatively, we can defer refreshing the vie\\v. Updates are captured in a log\nand applied subsequently to the rnaterialized vic\\vs. There are several deferred\nview maintenance policies:\n1. Lazy: The rnaterialized vie\\v\\l is refreshed at the tilne a query is evaluated\nusing V, if V is not already consistent vvith its underlying base tables. This\napproach sl()\\vs down queries rather than updates, in contra-st to iHnnediate\nvic\",-! rnaintenance.\n\n882\nCHAPTER 2&\nI~iews for De-cision Su;ort: ;-BMS ven~rs are e:hancing ;ieir m~~­\nI\nrelational products to support decision support querip.s.\nIBM DB2 sup..\nI\nports materialized views with transaction-consistent or user-invoked main-\nI\ntenance.\nl\\1icrosoft SQL Server supports partition views, \\vhich are\nI\nunions of (ruany) horizontal partitions of a table.\nThese aJ'e airned at\nI\na warehollsing envirOllrnent where each partition could be, for exalnple, a\nrnonthly update. Queries on partition vie\\vs are opthnized so that only rel-\nevant partitions are accessed. Oracle 9i supports 111aterialized views with\ntransaction-consistent, user-invoked, or tilne-scheduled nlaintenance.\nL_.~\n_\n2. Periodic: The lllaterialized view is refreshed periodically, say, once a day.\nThe discussion of the Capture and Apply steps in asynchronous replication\n(see Section 22.11.2) should be reviewed at this point, since it is very rel-\nevant to periodic view lllaintenance. In fact, many vendors are extending\ntheir asynchronous replication features to support lllaterialized views. Ma-\nterialized views that are refreshed periodically are also called snapshots.\n3. Forced:\nrrhe rnaterialized view is refreshed after a certain nurnber of\nchanges have been made to the underlying tables.\nIn periodic and forced view nlaintenance, queries rllay see an instance of the\nIIlaterialized view that is not consistent with the current state of the underlying\ntables. That is, the queries would see a different set of rows if the view definition\nwas recornputed. This is the price paid for fast updates and queries, and the\ntrade-off is sirnilar to the trade-off rnade in using asynchronous replication.\n25.11\nREVIEW QUESTIONS\nAnswers to the review questions can be found in the listed sections.\nII\nvVhat are decision support applications? :Oiscuss the relationship of co'rnple:r:\n8(2L q'lteries, OLA.P, data rnining, and data 1uarehousing. (Section 25.1)\nIIiI\nDescribe the rnultidirnensional data luodel. Explain the distinction between\nrneasurcs and dirnensions and between fact tables and\ndin~en8ion tables.\n\\\\That is a sip:r 8chenz,a? (Sections 25.2 and 25.2.1)\nII\nCornrnon OLAP operations have received special naInes:\nroll-up, drill-\ndeY/on\" pivohng7 slicing, and dicing. Describe each of these operations and\nillustrate thern using exarnples. (Section 25.3)\nII\nI)escribe the SCJL:1999 ROLLUP and CUBE features and their relationship to\nthe ()LAP operations. (Section 25.3.1)\n\nData WaTehousing and Decision Support\n883\n•\nDescribe the SQL:1999 WINDOW feature, in particular, frarning and ordering\nof windows. How does it support queries over ordered data? Give exarnples\nof queries that are hard to express without this feature. (Section 25.4)\n•\nNew query paradigrns include top N q'ue-ries and online aggTegation. Ex-\nplain the nlotivation behind these concepts and illustrate then1 through\nexaruples. (Section 25 ..5)\n•\nIndex structures that are especially suitable for OLAP systen1s include\nbitrnap indexes and join indexes.\nDescribe these structures.\nHow are\nbitrnap indexes related to B+ trees? (Section 25 ..6)\nIII\nInformation about daily operations of an organization is stored in opeTa-\ntional databases. Why is a data waTeho'i.LSe used to store data frolH oper-\national databases? What issues arise in data warehousing? Discuss data\nextTaction, cleaning, transjoTrnation, and loading. Discuss the challenges in\nefficiently TejTeshing and pUTging data. (Section 25 ..7)\nIII\nWhy are views irnportant in decision support environments? How are views\nrelated to data warehousing and OLAP? Explain the queTy\nmod~fication\ntechnique for answering queries over views and discuss why this is not\nadequate in decision support environrnents. (Section 25 ..8)\nIII\nWhat are the rnain issues to consider in maintaining materialized views?\nDiscuss how to select views to materialize and how to use rnaterialized\nviews to answer a query. (Section 25.9)\n•\nHow can views be rnaintained incTernentally?\nDiscuss all the relational\nalgebra operators and aggregation. (Section 25 ..10.1)\n•\nUse an exarnple to illustrate the added cornplications for incrernental view\nmaintenance introduced by data distribution. (Section 25.10.2)\nIII\nDiscuss the choice of an appropriate rnaintenance policy for when to refresh\na view. (Section 25.10.3)\nEXERCISES\nExercise 25.1 Briefly answer the following questions:\n1. How do warehousing, OLAP, and data rnining cornplernent each other?\n2. vVhat is the relationship between datawan~housingand data replication? Which fornl of\nreplication (synchronous or a.csynchronous) is better suited for data w(trehOllsing? \\Vhy?\n:J. \\\\That is the role of the rnetadata repository in a data warehouse'?\nHow does it differ\nfrorn a catalog in (1 relational IJB:NIS?\n4. \\Vhat considenttions are involved in designing a data warehouse'?\n\n884\nCHAPTER 251'\n5. Once a warehouse is designed and loaded 1 how is it kept current with respect to changes\nto the source databases?\nfl. One of the advantages of a waxehouse is that we can use it to track how the contents of\na relation change over titue; in contrast 1 we have only the current snapshot of a relation\nin a regular DBJ'vfS. Discuss how you would maintain the history of a relation R, taking\ninto account that 'old' infonnation lllust sOlnehow be purged to rnake space for Hew\ninfonnatioll.\n7. Describe dilnensions and rneasures in the multidirnensional data model.\n8. What is a fact table, and why is it so irnportant frOIn a performance standpoint?\n9. Vvhat is the fundarnental difference between ~fOLAP and ROLAP systems?\n10. \\Vhat is a star scheIna? Is it typicaU:y in BCNF? Why or why not?\n11. How is data rnining different from OLAP?\nExercise 25.2 Consider the instance of the Sales relation shown in Figure 25.2.\n1. Show the result of pivoting the relation on pid and tirneid.\n2. Write a collection of SQL queries to obtain the same result as in the previous part.\n3. Show the result of pivoting the relation on pid and lacid.\nExercise 25.3 Consider the cross-tabulation of the Sales relation shown in Figure 25.5.\n1. Show the result of roll-up on lacid (i.e., state).\n2. Write a collection of SQL queries to obtain the same result as in the previous part.\n3. Show the result of roll-up on lacid followed by drill-down on pid.\n4. Write a collection of SQL queries to obtain the same result as In the previous part,\nstarting with the cross-tabulation shown in Figure 25.5.\nExercise 25.4 Briefly answer the following questions:\n1. What is the difIerences between the WINDOW clause and the GROUP BY clause'?\n2. Give an example query that cannot be expressed in SQL without the WINDOW clause but\nthat can be expressed with the WINDOW clause.\n:3. What is the fTCLrne of a window in SQL: 19997\n4. Consider the fonowing simple GROUP BY query.\nSELECT\nFROM\nWHERE\nGROUP BY\nT.year, SUM (S.sales)\nSales 5, Tilnes T\nS.tilneid='T.timeid\nT.year\nCan you write this query in SQL:1999 without using a GROUP BY cIa.use? (Hint: Use the\nSQL:1999 WINDOW clause.)\nExercise 25.5 Consider the Locations, Products, and Sales relations shown in Figure 25.2.\n\\iVrite the following queries in SQL:1999 llsing the WINDOW clause whenever you need it.\n1. Find the percentage change in the total IJ10nthly sales for each location.\n2. Find the percentage chc.tnge in the total quarterly sales for each product.\n\nData Warehousing and l]eci8ion 8'UPPOTt\n885\n3.FLnd the average daily sales over the preceding ;30 days for each product.\n4. For each week, find the maximulu uloving average of sales over the preceding four \\veeks.\n5. Find the top three locations ranked by total sales.\n6.F'ind the top three locations ranked by curnulative sales, for every month over the past\nyear.\n7. Rank all locations by total sales over the past year, and for each location print the\ndifference in total sales relative to the location behind it.\nExercise 25.6 Consider the CustOIuers relation and the bitmap indexes shown in Figure\n25.9.\n1. For the same data, if the underlying set of rating values is assuIued to range froIlI 1 to\n10, show how the bitnlap indexes would change.\n2. How would you use the bitIllap indexes to answer the following queries? If the bitmap\nindexes are not useful, explain why.\n(a) How many customers with a rating less than 3 are male?\n(b) What percentage of custoIners are male?\n(c) How rnany customers are there?\n(d) How many custonlers are named Woo?\n(e) Find the rating value with the greatest number of custoIl1erS and also find the nUIll-\nbel' of custorners with that rating value; if several rating values have the maxirnurn\nnumber of custoIllers, list the requested infonuation for all of theIn. (AssuIne that\nvery few rating values have the same nUluber of customers.)\nExercise 25.7 In addition to the Customers table of Figure 25.9 with bitrnap indexes on\ngender' and 'rating, assurne that you have a table called Prospects, with fields ruting and\nprospectid. This table is used to identify potential customers.\n1. Suppose that you also have a bitrnap index on the rating field of Prospects.\nDiscuss\nwhether or not the bitnlap indexes would help in corllputing the join of Custorners and\nProspects on rating.\n2. Suppose you have no bitrnap index on the rating field of Prospects. Discuss whether or\nnot the bitrnap indexes on CustOIuers would help in conlputing the join of Custorners\nand Prospects on nLting.\n~1. Describe the use of a join index to support the join of these two relations with the join\ncondition c'Ust'id=prospectid.\nExercise 25.8 Consider the instances of the Locations, Products, and Sales relations shown\nin Figure 25.2.\n1. Consider the basic join indexes d€~scribed in Section 25.6.2. Suppose you want to optiInize\nfor the following two kinds of queries: Query 1 finds sa.les in a given city, and Query 2\nfinds sa.les in a given state. Show the indexes you would create on the excunple instances\nshown in Figure 25.2.\n2. Consider the bitIIulpped join indexes described in Section 25.6.2. Suppose you want to\noptirnize for the following two kinds of queries: Query 1 finds sales in a given city, and\nQuery 2 finds sales in a given state.\nShow the indexes that you would create on the\nexanlple instances shown in Figure 25.2.\n\n886\nCHAPTER 25\nII\n~3. Consider the basic join indexes described in Section 25.6.2. Suppose you want to optiInize\nfor these two kinds of queries: Query 1 finds sales in a given city for a given product\nI1alne~ and Query 2 finds sales in a given state for a given product category. Show the\nindexes that you would create on the exarl1ple instances shown in Figure 25.2.\n4. Consider the bitmapped join indexes described in Section 25.6.2. Suppose you want to\noptirnize for these two kincls of queries: Query 1 finds sales in a given city for a given\nproduct narne, and Query 2 finds sales in a given state for a given product category.\nShow the indexes that you would create on the example instances shown in Figure 25.2.\nExercise 25.9 Consicler the view NurnReservations defined as:\nCREATE VIEW NumReservations (sid, snarnc, nUlures)\nAS SELECT S.sid, S.snarne, COUNT (*)\nFROM\nSailors S, Reserves R\nWHERE\nS.sid = R.sid\nGROUP BY 8.sid, S.sname\n1. How is the following query, which is intended to find the highest number of reservations\nnlade by smne one sailor, rewritten using query modification?\nSELECT\nFROM\nMAX (N.numres)\nNurnReservations N\n2. Consider the alternatives of cornputing on deluand and view materialization for the\npreceding query. Discuss the pros and cons of materialization.\n3. Discuss the pros and cons of materialization for the following query:\nSELECT\nN.snarlle, MAX (N.numres)\nFROM\nNumReservations N\nGROUP BY N.sname\nExercise 25.10 Consider the Locations, Products, and Sales relations in Figure 25.2.\n1. To decide whether to rnaterialize a view, what factors do we need to consider?\n2. Assurne that we have defined the following lnaterialized view:\nSELECT\nFROM\nWHERE\nL.state~ S.sales\nLocations I-i, Sales S\n8.locid=L.locid\n(a) Describe what auxiliary infornlatioll the algorithnl for incrernental view rnainte-\nnance frorn Section 25.10.1 maintains and how this data helps in lnainta.ining the\nview incrernentally.\n(b) Discuss the pros and cons of ruaterializing this view.\n:3. Consider the rnaterialized view in the previous question.\nAssume that the relations\nLocations and Sales are stored at OIle site, but the view is rnaterialized on a second site.\nWhy would we\"ever want to luaintain the view at a second site? Give a concrete exarnple\nwhere the view could become inconsistent.\n4. ASSUITW that we have defined the following rnaterialized view:\nSELECT\nFROM\nWHERE\nGROUP BY\nT.year, I..state, SUM (S.sales)\nSales 8, 'rirnes '1', Locations L\nS.tirneid=T.tilneid AND S.locid=L.locid\nrr.year, L.state\n\n[)ata WaTeho1.l.s'ing and DeC'lsion SUppOTt\n(a) Describe what auxiliary infoflnation the algorithnl for incrernental view rnainte-\nnance frOIn Section 25.10.1 luaintains, and how this data helps in rnaintaining the\nview increluentaJly.\n(b) Discuss the pros and cons of 11laterializing this view.\nBIBLIOGRAPHIC NOTES\nA good survey of data warehousing and OLAP is presented in [161], which is the source of\nFigure 25.10. [686] provides an overview of OLAP and statistical database research, showing\nthe strong parallels between concepts and research in these two areas. The book by Kirnball\n[436], one of the pioneers in warehousing, and the collection of papers in [(2) offer a good prac-\ntical introduction to the area. The term OLAP was popularized by Codd's paper [191]. For a\nrecent discussion of the performance of algorithms utilizing bitmap and other nontraditional\nindex structures, see [575].\nStonebraker discusses how queries on views can be converted to queries on the underlying\ntables through query modification [713]. Hanson cmnpares the perfornlance of query modifi-\ncation versus immediate and deferred view maintenance [365]. Srivastava and Roterll present\nan analytical model of materialized view maintenance algorithnls [707]. A number of papers\ndiscuss how rnaterialized views can be incrementally maintained as the underlying relations\nare changed. Research into this area has become very active recently, in part because of the\ninterest in data warehouses, which can be thought of as collections of views over relations from\nvarious sources.\nAn excellent overview of the state of the art can be found in [348], which\ncontains a number of influential papers together with additional rnaterial that provides con-\ntext and background. The following partial list should provide pointers for further reading:\n[100, 192, 193, 349, 369, 570, 601, 635, 664, 705, 800].\nGray et al. introduced the CUBE operator\n[~~35], and optirnization of CUBE queries and efficient\nmaintenance of the result of a CUBE query have been addressed in several papers, including\n[12, 94, 216, 367, 380, 451, 634,\n6~38, 687, 799].\nRelated algorithrns for processing queries\nwith aggregates and grouping are presented in [160, 166]. Rao, Badia, and Van Gucht address\nthe irnplelnentation of queries involving generalized quantifiers such as a rnajor'ity of [618].\nSrivastava, Tan, and LUIIl describe an access ruethod to support processing of aggregate\nqueries [708].\nShannlugasundaranl et al.\ndiscuss how to ruaintain cornpressed cubes for\napproxirnate answering of aggregate queries in [675].\nSQL: 1999's support for OLAP, including CUBE and WINDOW constructs, is described in [52:'3].\nThe windowing extensions ::tre very sirnilar to SQL extension for querying sequence data,\ncalled SRQL, proposed in [610]. Sequence queries have received a lot of attention recently.\nExtending relational systeills, \\vhich deal with sets of records, to deal with sequences of records\nis investigated in [473, 665, 671].\nThere has been recent interest in one-pass query evaluation algorithnls and database rnanage-\nrnent for data streaIns. A recent survey of data rnanagernent for data streams and algorithrns\nfor data stream processing can be fonnd in [49J. Exarnples include quantile and order-statistics\ncOlnputation [340, 50G],\nestirnating frequency rnornents and join sizes [;34, :'35], estirnating\ncorrelated\naggregates [:310], rllultidirnensionaJ regression analysis [17J], etnd cornputing one-\ndirnensional (i.e., single-attribute) histograrns and Haar wavelet clecmnpositioI1s [:U9, :345].\nOther work includes techniques for incrementally IllElintaining equi-depth histograms [:31:3]\nand Baal' wavelets [515], rnaintaining sarnples and siluplc statistics over sliding \\vindows [201],\n\n888\nCHAP'I'ER 25$\nas well as general~ high-level architectures for stremll databa.,ge systenlS [50}. Zdonik et al. de-\nscribe the architecture of a database systern for Hl0nit;oring data streaU1S [795J. A language\ninfrastructure for developing data streaIll applications is described by Cortes 8t al. [199].\nCarey and Kossrnann discuss how to evaluate queries for which only the first few answers are\ndesired [1:3.5, 1:36]. Donjerkovic and Ralnakrishnan consider how a probabilistic approach to\nquery optiInization call be applied to this probleul [229].\n[120] compares several strategies\nfor evaluating Top N queries. Hellerstein et al. discuss how to return approxiInate answers\nto aggregate queries and to refine thern 'online.' [47, :374]. This work ha..9 been extended to\nonline cOlnputation of joins [354], online reordering [617] and to adaptive query processing\n[48].\nThere has been recent interest in approximate query answering, where a small synopsis data\nstructure is used to give fast approxiruate query answers with provable perforrnance guarantees\n[7, 8, 61, 159, 167, 314, 759].\n\n26\nDATA MINING\n..\nWhat is data mining?\n..\nWhat is lliarket basket analysis?\nWhat algorithms are efficient for\ncounting co-occurrences?\ni\"-\nWhat is the a priori property and why is it important?\n..\nWhat is a Bayesian network?\n..\nWhat is a cla..'Ssification rule? What is a regression rule?\n...\nWhat is a decision tree? How are decision trees constructed?\n...\nWhat is clustering? What is a salllple clustering algorithln?\n...\nWhat is a similarity search over sequences? How is it implmuented?\n..\nHow can data mining models be constructed increluentally?\n..\nWhat are the new mining challenges presented by data strealllS?\n..\nKey concepts: data nlining, KDD process; market basket analysis,\nco-occurrence counting, a..'Ssociation rule, generalized association rule;\ndecision tree, cla..'Ssification tree; clustering; sequence similarity search;\nincrernental model llIaintenallce, data streanls, block evolution\n1\ni he secret of success is to know sornething nobody else knows.\n·-·Aristotle Onassis\nData luining consists of finding interesting trends or patterns in large data,sets\nto guicle decisions about future activities. There is a genera] expectation that\n889\n\n890\nCHAPTER 26\ndata ruining tools should be able to identify these patterns in the data \\vith\nminirnal user input.\nThe patterns identified by such tools can give a data\nanalyst useful and unexpected insight that can be Illore carefully investigated\nsubsequently, perhaps using other decision support tools. In this chapter, we\ndiscuss several widely studied data luining tasks. COllunercial tools are avail-\nable for each of these tasks frorll major vendors, and the area is rapidly gTowing\nin ilnportance as these tools gain acceptance in the user cornrnunity.\nWe start in Section 26.1 by giving a short introduction to data mining.\nIn\nSection 26.2, we discuss the irnportant task of counting co-occurring items. In\nSection 26.3, we discuss how this ta\"k arises in data mining algorithms that\ndiscover rules froln the data. In Section 26.4, we discuss patterns that represent\nrules in the forln of a tree. In Section 26.5, we introduce a different data rnining\ntask, called clustering, and describe how to find clusters in large datasets. In\nSection 26.6, we describe how to perform siInilarity search over sequences. We\ndiscuss the challenges in rnining evolving data and data streams in Section 26.7.\nWe conclude with a short overview of other data mining tasks in Section 26.8.\n26.1\nINTRODUCTION TO DATA MINING\nData nlining is related to the subarea of statistics called exploratory data anal-\nysis, which has siruilar goals and relies on statisticalrueasures. It is also closely\nrelated to the subareas of artificial intelligence called knowledge discovery and\nrnachine learning. The important distinguishing characteristic of data rnining\nis that the volume of data is very large; although ideas froln these related areas\nof study are applicable to data nlining problems, scalability with respect to data\nsize is an important new criterion.\nAn algorithm is scalable if the running\ntirne grows (linearly) in proportion to the dataset size, holding the available\nsystenl resources (e.g., arnount of rnain rnemory and CPU processing speed)\nconstant.\nOld algorithms must be adapted or new algorithnls developed to\nensure scalability when discovering patterns fn)In data.\nFinding useful trends in datasets is a rather loose definition of data 111ining: In a\ncertain sense, all database queries can be thought of as doing just this. Indeed,\nwe have a continuurn of ana.lysis and exploration tools with SQL queries at one\nend, OLAP queries in the rniddle, and data ruining techniques at the other end.\nSQL queries are, constructed! using relational algebra (with sorne extensions),\nOLAP provides higher-level querying idiorlls ba\"sed on the rnultidirnensional\ndata rn.odel, and data rnining provides the rnost abstract analysis operations.\nvVe can think of different data rnining tasks a,,') cornplex 'queries' specified at\na high level, with a few panuneters that are user-defina.ble, and for which\nspecialized algorithrns are ilnplernented.\n\nData lvfining\n--------_\n__------\n891\nSQL/MM: Data Mining SQL/MM: The SQLfMtvi: Data IViining ex-\ntension of the SQL:1999 standard supports four kinds of data mining\nnlodels:\nfrequent itenLsets and associat'ion 'rules, clusters of records, re-\ng'ression\ntn~es, and classification trees. Several new data types are intro-\nduced. These data types play several roles. SaIne represent a particular\nclass of model (e.g.,\nDM~egressibnMod.el,D}JLClusteringModel); some\nspecify the input parameters for a mining algorithm (e.g., DM-RegTask,\nDM_ClusTask); some describe the input data (e.g., DM..LogicalDataSpec,\nDM-MiningData); and sornerepresent the result of executing a rnining algo-\nrithm (e.g., DM....RegResult, DM_ClusResult). Taken together, these classes\nand their methods provide a standard interface to data mining algorithms\nthat can be invoked frorn any SQL:1999 database systern. The data min-\ning rnodels can be exported in a standard XML format called Predictive\nModel Markup Language (PMML); models represented using PMML\ncan be hnported as well.\nIn the real world, data rnining is much more than sirnply applying one of these\nalgorithnls. Data is often noisy or inconlplete, and unless this is understood and\ncorrected for, it is likely that rnany interesting patterns will be rnissed and the\nreliability of detected patterns will be low. Further, the analyst nlust decide\nwhat kinds of rnining algoritlulls are called for, apply them to a well-chosen\nsubset of data sarnples and variables (i.e., tuples and attributes), digest the\nresults, apply other decision support and mining tools, and iterate the process.\n26.1.1\nThe Knowledge Discovery Process\nThe knowledge discovery and data mining (KDD) process can roughly\nbe separated into four steps.\n1. Data Selection: The target subset of data and the attributes of interest\nare identified by exalnining the entire raw dataset.\n2. Data Cleaning: Noise and outliers are relnoved, field values are trans-\nfonned to cornrnon units and SOUIC l1C\\V fields are created by cornbining\nexisting fields to facilitate a,nalysis. The data is typically put into a, rela-\ntional fonnat, and several tables rnight be cornbined in a denoTTnal'ization\nstep.\n3. Data Mining: \\Ve apply data rnining algorit1ll11S to extract interesting\npatterns.\n4. Evaluation: The patterns are presented to end-users ill an understandable\nfonn, for ex<unple, through visualization.\n\n892\nCHAPTER. 2&\n1\"he results of any step in the I<:DD proce:ss lllight lead us back to an earlier step\nto redo the process with the ne\\v knowledge gained. In this chapter, however,\nwe lilnit ourselves to looking at algoritlnns for SaIne specific data rnining tasks.\n\\¥e do not discuss other aspects of the I(DD process.\n26.2\nCOUNTING CO-OCCURRENCES\n\\Ve begin by considering the probleln of counting co-occurring iterns, which is\nrnotivated by problelTIs such as lllarket basket analysis. A market basket is a\ncollection of items purchased by a custOlner in a single customer transaction.\nA cnstorner transaction consists of a single visit to a store, a single order through\na mail-order catalog, or an order at a store on the Web. (In this chapter, we\noften abbreviate customer transaction to transaction when there is no confusion\nwith the usual nleaning of transaction in a DBlVlS context, which is an execution\nof a user program.) A COIIlIllon goal for retailers is to identify items that are\npurchased together.\nThis inforrnation can be used to improve the layout of\ngoods in a store or the layout of catalog pages.\nI .·transid ·1 c1tstidl\ndate\nitem . ··l.qtyl\n5/1/99\n.•\n111\n201\npen\n2\n1'-'\n5/1/99\nink\n111\n201\n1\n5/1/99\nmilk\n-'-\n111\n201\n3\n111\n201\n5/1l9'9\njuice\n6\n_\n.••.\n-\"-\n112\n105\n6/3/99\npen\n1\n-_•.\n6/3/99\n112\n105\nink\n1\n\"\n\".,\n..._-\n112\n105\n....6/3/99\nmilk\n1\n-_ ....r\nt~~\n5/10'/99\n.-\n..\n113\npen\n1\n113\n5/io/99\nInilk\n..._-\n1\n.....\n'·-\"'-6/1/99\n....-\n114\n201\npen\n2\n. ...._..\n6/1/99-.....\n114\n201\nink\n2\nf--_ .._-\n...\n114\n201\n6/1/99\njuice\n4\n,......-...__.\n6/1/99\n---\n114\n201\nwater\n1\n. \"-\n...\n....-\nFigure 26.1\n'I'he Purcha'3es Relation\n26.2.1\nFrequent Itemsets\n'vVe use the Purchases relation shovvn in Figure 26.1 to illustrate frequent item-\nsets. rrhe records are shoVi.rn sorted into groups by transaction. All tuples in\na group have the saIne tr-ansid, and together they describe a custorner trans-\naction, which involves purcha..ses of one or Inore iterns.\nA transaction occurs\n\nData lvlin'ing\n893\non a given date, and the nanle of each purcha..lSed itenl is recorded, along \\vith\nthe purella,sed quantity. ()bserve that there is redundancy in Purchases: It can\nbe decolnposed by storing\ntTansid\"c\"lJ,8l'i,d~~datetriples in a separate table and\ndropping c'lud'id and date froln Purchases; this nlay be ho\\v the data is actually\nstored. Hc)\\vever, it is convenient to consider the Purcha..ses relation, as shov.rn\nin Figure 26.1, to corupute frequent iternsets.\nCreating such \"denonnalized'\ntables for ease of data rnining is cOIIllllonly done in the data cleaning step of\nthe I(DD process.\nBy ex<:unining the set of transaction groups in\nPurcha,..~es, we can rnake obser-\nvations of the fornl: \"In 751() of the transactions a pen and ink are purchased\ntogether.\"\nrrhis stateulent describes the transactions in the database.\nEx-\ntrapolation to future transactions should be done with caution, as discussed in\nSection 26.3.6. Let us begin by introducing the terminology of rnarket basket\nanalysis. An itemset is a set of itelTIS. The support of an itelnset is the frac-\ntion of transactions in the database that contain all the iterus in the iterllset.\nIn our exaIupl.e, the itelllset {pen, ink} has 75% support in Purchases. We can\ntherefore conclude that pens and ink are frequently purchased together. If we\nconsider the itelllset {milk, juice}, its support is only 25%; milk and juice are\nnot purchased together frequently.\nUsually the nUlnber of sets of itenlS frequently purchased together is relatively\nsInall, especially as the size of the itenlsets increases.\nWe are interested in\nall iterllsets whose support is higher than a user-specified minimUIl1 support\ncalled m,insnp; we call such itemsets frequent itemsets. For exarnple, if the\nIIlinirl1Unl support is set to 70%, then the frequent iterllsets in our example\nare {pen}, {ink}, {nlilk}, {pen, ink}, and {pen, 111ilk}.\nNote that we are\nalso interested in iternsets that contain only a single iteru since they identify\nfrequently purchased iterl1s.\n\\Ve show an algorithrn for identifying frequent iterllsets in Figure 26.2.\nThis\nalgorithrn relies on a sirnple yet fundarnentaJ property of frequent iterIlsets:\nThe a Priori Property: Every subset of a frequent iterllset is also a\nfrequent itelnset.\n'fhe algorithnl proceeds iteratively, first identifying frequent iterIlsets 'with just\none itcrll.\nIn Bach subsequent iteration, frequent iterl1sets identified in the\nprevious iteration are extended with another itern to generate larger candidate\nitcrnsets. By considering only iterllsets obtained by enlarging frequent iternsets,\nwe greatly reduce the nurnber of candidate frequent itcrllsets; this optirnization\nis crucial for efficient execution.\n~rhe a priori property guarantees that this\noptilnizatic)ll is correct; that is, \\ve do not Iniss any frequent iterllsets. A single\nscan of all trans(l,(tions (the\nPllrchas(~s relation in our exarnple) suffices to\n\n894\nf oreach itelll,\nLevel 1\ncheck if it is a frequent iternset II appears in > 'rnins'l1,p transactions\nk=l\nrepeat\n/ / Iterative, level-wise identification of frequent itelllsets\nf oreach new frequent iterllset I k with k iterlls\n/ / Level k + 1\ngenerate all iterl1sets lk+l with k + 1 itelllS, lk C Ik+l\nScan all transactions once and check if\nthe generated k + 1-iterIlsets are frequent\nk=k+1\nuntil no new frequent itemsets are identified\nFigure 26.2\nAn Algorithm for Finding Frequent Itemsets\ndetermine which candidate iterllsets generated in an iteration are frequent.\nThe algorithm terminates when no new frequent itemsets are identified in an\niteration.\n'We illustrate the algorithrn on the Purchases relation in Figure 26.1, with\nminsup set to 70%.\nIn the first iteration (Levell), we scan the Purchases\nrelation and deterllline that each of these one-iterll sets is a frequent iternset:\n{pen} (appears in all four transactions), {ink} (appears in three out of four\ntransactions), and {rnilk} (appears in three out of four transactions).\nIn the second iteration (Level 2), we extend each frequent itemset with an\nadditional itenl and generate the following candidate iterIlsets: {pen, ink}, {pen,\nmilk}, {pen, juice}, {ink, rnilk}, {ink, juice}, and {rnilk, juice}. By scanning the\nPurchases relation again, we deterrnine that the following are frequent ite111sets:\n{pen, ink} (appears in three out of four transactions), and {pen, rnilk} (appears\nin three out of four transactions).\nIn the third iteration (Level 3), we extend these itelllsets with an additional\niteHl and generate the following candidate itcrl1sets:\n{pen, ink, nl/ilk}, {pen,\nink, juice}, and {pen, rnilk, fuice}.\n(Observe that fink,\nrnilk, juice} is not\ngenerated.) A third sca.n of the Pllrchc1E,es relation aJlows us to deterrnine that\nnone of these is a frequent iterTlset.\nThe sirnple algoritlnll presented here for finding frequent iternsets illustrates the\nprincipal feature of Inore sophisticated algorithrns, naruely, the iterative gener-\nation and testing of candidate itcrnsets. vVe consider one irnportant refincrnent\nof this sirnple algorithrn.\nCjenerating candidate iternsets by adding an itCHl\nto a known frequent iternset is an atterIlpt to lirnit the rnunber of candidate\nitcrIlsets using the a priori property. rrhe a priori property ~rnplies that a can-\n\nData lvJining\n89p\ndida,te iternset can be frequent only if all its subsets are frequent. Thus, we can\nreduce the nUlnber of candidate iternsets further··..··--a priari, or before scanning\nthe PurchEhses databaBe··........_·by checking whether all subsets of a newly generated\ncandidate itcIIlset are frequent. Only if all subsets of a candidate iternset are\nfrequent do we cOlnpute its support in the subsequent databa'3c scan.\nCOln-\npared to the sirnple algoritlun, this refined algoritlull generates fewer candidate\nitenlsets at each level and thus reduces the arnount of conlputation perfonned\nduring the database scan of Purchases.\nConsider the refined algorithrn on the PurchclSes table in Figure 26.1 with\nrn:inStlp= 70%. In the first iteration (Level 1), we deterrnine the frequent item-\nsets of size one: {pen}, {ink}, and {ntilk}. In the second iteration (Level 2),\nonly the following candidate itemsets rernain when scanning the Purchases ta-\nble: {pen, ink}, {pen, 'm-ilk} , and {ink, rnilk}. Since {juice} is not frequent, the\niterllsets {pen, juice}, {ink, juice}, and {rnilk, juice} cannot be frequent as well\nand we can elirninate those iterIlsets a priori, that is, without considering therll\nduring the subsequent scan of the Purchases relation.\nIn the third iteration\n(Level 3), no further candidate itemsets are generated. The iternset {pen, ink,\nm,ilk} cannot be frequent since its subset {ink, milk} is not frequent. Thus, the\nirnproved version of the algorithrll does not need a third scan of Purchases.\n26.2.2\nIceberg Queries\nWe introduce iceberg queries through an exaillple.\nConsider again the Pur-\nchases relation shown in Figure 26.1.\nAssurne that we want to find pairs of\ncustorners and iterns such that the custorner has purchased the item rllore than\nfive thnes. We can express this query in SQL as follows:\nSELECT\nFROM\nGROUP BY\nHAVING\nP.custid, P.itern, SUM\nPurch~h\"es P\nP.custid, P.itern\nSUM (P.qty) > 5\n(P.qty)\nrrhink about how this query would be evaluated by a relational DBMS. Con-\nceptually, for each (c'usLid, 'itcrn) pair, we need to check whether the surn of the\nqty field is greater than 5. One approach is to rnake a scan over the Purchases\nrelation and rnaintain running surns for each (c'Ustid, itern) pair. T'his is a fea-\nsible execution ,strategy a.s long as the nurnber of pairs is sruaU enough to fit\ninto lIlain rncIIlory. If the nurnber of pairs is larger than rnain Inernory, lnorc\nexpensive query evaluation plans,\\vhich involve either sorting or hashing, have\nto be used.\nThe query has an irnporta\"nt property not exploited by the preceding execution\nstrategy: Even though the Purcha..'3cs relation is potentially very large and the\n\n896\n(;HAP'TER26\nnurnber of (cltstid, 'itern) groups CaJl be huge, the Cyutput of the query is likely to\nbe relatively sInall because of the condition in the HAVING clause. ()nly groups\nwhere the custorner ha..9 pnrchaEied the itCHl Inure than five tiInes appear in the\noutput.\nfihr exarllple, there are nine groups in the query over the Purcha.'3es\nrelation ShOVlll in Figure 26.1, although the output contains only three records.\nThe nurnber of groups is very large, but the answer to the query---·-the tip of\nthe iceberg------is usually very sInan. Therefore, ,ve call such a query an iceberg\nquery. In general, given a relational scherna H. with attributes A.1. A 2, ... \"\nAk, and B and an aggrega,tion function aggr, an iceberg query has the follo\\ving\nstructure:\nSELECT\nFROM\nGROUP BY\nHAVING\nR.Al, Il.A2, ..., R,.Ak, aggr(R.B)\nH,elation H,\nR,.AI, ... , R.Ak\naggr(ILB) >= constant\nTraditional query plans for this query that use sorting or hashing first cornpute\nthe value of the aggregation function for all groups and then elirninate groups\nthat do not satisfy the condition in the HAVING clause.\nCornparing the query with the probleur of finding frequent itenlsets discussed in\nthe previous section, there is a striking sirnilarity. Consider again the Purchases\nrelation shown in Figure 26.1 and the iceberg query froIn the beginning of this\nsection. We are interested in (custid, itern) pairs that have SUM (P.qty) > 5.\nlJsing a variation of the a priori property, we can argue that we only have to\nconsider values of the c'Ust'id field where the custorner has purchased at least\nfive it-eurs. We can generate such iterns through the following query:\nSELECT\nFROM\nGROUP BY\nHAVING\nP.cllstid\nPurchases P\nP.cllstid\nSUM (P.qty) > 5\nSirnilarly, we can restrict the candidate values for theitern field through the\nfollowing query:\nSELECT\nFROM\nGROUP BY\nHAVING\nP.itern\nPurchases P\nP.iteul\nSUM (P.qty) > 5\nIf \\ve restrict\nth(~ corrlputation of the original\nic(~berg query to\n(C'a8t'id~ 'itern)\ngroups \\vhe1'e the field values aTe in the output of the previous t\\VO queries,\nvve elirninate a large nUlllber of (c''lJ,stid 1 'ite1n) pElirs a priori.\nSo, a possible\n\n897\nevaluation strategy is to first COlnpute candidate values for the C'tlstid and 'itcTn\nfields, and use eornbinations of only these values in the evaluation of the original\niceberg query.\n'~!e first generate candidate field values for individual fields and\nuse only those values that survive the a priori pruning step as expressed in\nthe t,vo previous queries.\n'Thus, the iceberg query is arnenable to the salIle\nbottorn-up evaluation strategy used to find frequent iternsets. In particular, \\ve\ncan use the a priori property a.'s follovls: vVe keep a counter for a group only if\neach individual cOlnponent of the group satisfies the condition expressed in the\nHAVING clause. The perfonnance irnprovernents of this alternative evaluation\nstrategy over traditional query plans can be very significant in practice.\nEven though the botto111-UP query processing strategy elinlinates lnany groups\na priori, the nlunber of (c1lstid, itern) pairs can still be very large in practice;\neven larger than Inain lllernory. Efficient strategies that use sall1pling and lllore\nsophisticated hashing techniques have been developed; the bibliographic notes\nat the end of the chapter providt~ pointers to the relevant literature.\n26.3\nMINING FOR RUI~ES\nMany algorithrIls have been proposed for discovering various fonns of rules that\nsuccinctly describe the data. We now look at some widely discussed fonns of\nrules and algorithnls for discovering thenl.\n26.3.1\nAssociation Rules\nWe use the Purcha.ses relation shown in Figure 26.1 to illustrate ck'3sociation\nrules. By eXcl1nining the set of transactions in Purchc1..ses, we can identify rules\nof the forrn:\n{pen} =? {ink}\nThis rule should be read as follows: \"If a pen is pUfcha.sed in a transaction, it is\nlikely that in}( is also be purchased in that transaction.'~ It is a staternent that\ndescribes the transactions in the databa.se; extrapolation to future tranSctctions\nshould be done \\vith caution,\nChS discussed in Section\n26.~3.6. More generally,\nan association rule has the forIn LJIS::::} RHS, where both LIIS andRJIS'\nare sets of iterns.\nThe interpretation of such a, rule is that if every itern in\nLIIS is purchased in a transaction, then it is likely that the iterIlS in IlllS are\npurcha\",sed as well.\nrThere are hvo irnportEtnt rnecl,sures for (1,n association rule:\nIIIl\nSupport: The support for a set of iterns is the percentage of transa,ctions\nthat contain all these iterIls.\nrl~he support for a rule LIIS =}\nJ~llS is the\n\n898\nC]HAPTER b6\nsupport for the set of itenlS LHS\nRf!S. For exalnple, consider the rule\n{pen} =:> {ink}. The support of this rule is the support of the itenlset {pen,\nink}, which is 75%.\n•\nConfidence: Consider transactions that contain all iterIls in LHS. The\nconfidence for a rule LlIS =? RHS is the percentage of such transactions\nthat also contain all iterIls in RHS. More precisely, let S1lp(LHS) be the\npercentage of transactions that contain LllS and let s'up(LliS U RHS) be\nthe percentage of transactions that contain both LllS and RHS. rrhen the\nconfidence of the rule LHS => RHS is sup(LHSU RIIS) / sup(LHS). The\nconfidence of a rule is an indication of the strength of the rule.\nAs an\nexalnple, consider again the rule {pen} =? {ink}.\nThe confidence of this\nrule is 75%; 75% of the transactions that contain the itenlset {pen} also\ncontain the iternset {ink}.\n26.3.2\nAn Algorithm for Finding Association Rules\nA user can ask for all association rules that have a specified minimum support\n(minsvp) and mininlum confidence (rninconf), and various algorithrns have\nbeen developed for finding such rules efficiently.\nThese algorithms proceed\nin two steps.\nIn the first step, all frequent itemsets with the user-specified\nminimum support are computed. In the second step, rules are generated using\nthe frequent itemsets as input. We discussed an algorithm for finding frequent\niternsets in Section 26.2; we concentrate here on the rule generation part.\nOnce frequent iteulsets are identified, the generation of all possible candidate\nrules with the user-specified minirnum support is straightforward. Consider a\nfrequent iternset X with support sx identified in the first step of the algorithrn.\nTo generate a rule fronl X, we divide X into two iternsets, LHS and RJIS. The\nconfidence of the rule LllS =} RHS is Sx / SLlIS, the ratio of the support of X\nand the support of LHS. Frorn the a priori property, we know that the support\nof LllS is larger than rninsup, and thus we have C0111puted the support of L1IS\nduring the first step of the algoritlnn. \\rYe can cornpute the confidence values\nfor the candidate rule by calculating the ratio support(X)/support(LlIS) and\nthen check how the ratio cornpares to 'Tnincon!\nIn general, the expensive step of the algorithnl is the cornputation of the fre-\nquent itenlsets, and lnany different algorithrns have been developed to perfonn\nthis step efficiently.\nR,ule generation\n....given that all frequent itcrl1sets have\nbeen identified·.....·..····is straightforward.\nIn the rest of this section, we discuss SOlne generalizations of the problern.\n\nData !vfin>ing\n26.3.3\nAssociation Rules and ISA Hierarchies\n89Q\nIn rnany ca.'3es, an ISA hierarchy or category hierarchy is iInposed on the\nset of iterl1s. In the presence of a hierarchy, a transaction contains, for each\nof its iteuls, irnplicitly all the iteln's ancestors in the hierarchy. For example,\nconsider the category hierarchy shown in Figure 26.3.\nGiven this hierarchy,\nthe Purcha.,es relation is conceptually enlarged by the eight records shown in\nFigure 26.4. rrhat is, the Purchases relation has all tuples shown in Figure 26.1\nin addition to the tuples shown in Figure 26.4.\nThe hierarchy allows us to detect relationships between iterns at different levels\nof the hierarchy. As an exarnple, the support of the itemset {ink, juice} is 50%,\nbut if we replace juice with the more general category beverage, the support of\nthe resulting itemset {ink, beverage} increases to 75%. In general, the support\nof an itemset can increase only if an item is replaced by one of its ancestors in\nthe ISA hierarchy.\nAssulning that we actually physically add the eight records shown in Figure\n26.4 to the Purchases relation, we can use any algorithm for computing frequent\nitemsets on the augmented database.\nAssuming that the hierarchy fits into\nrnain memory, we can also perforln the addition on-the-fly while we scan the\ndatabase, as an optimization.\nStationery\n1\\\nBeverage\n1\\\nPen\nInk\nJuice\nMilk\n, Figure 26.3\nAn ISA Category Taxonomy\nitem\n111\n201\n5/1/99\nstationery\n3\n111\n201\n5/1/99\nbeverage\n9\n- .\n6/3/99\n112\n105\nstationery\n2\n6/3/99\n..._..-.-\n112\n105\nbeverage\n1\n113\n106\n5/10/99\nstationery\n1\n----\n5/10/99\nbeverage\n11~3\n106\n1\n':::;::--\"-\n-\n..._._--\n--\n114\n201\n6/1/99\nstationery\n11\n114\n201\n6/1/99\nbeverage\n5\nFigure 26.4\nConceptual Additions to the Purchases Relation with ISA Hierarchy\n\n900\nC~HAPTER 26\n26.3.4\nGeneralized Association Rules\nAlthough association rules have been rnost \\videly studied in the context of\nrnarket basket analysis, or analysis of cllstorner transactions, the concept is\nrno1'e general. Consider the Purcha.ges relation as sh()\\vn in Figure 26.5, grouped\nby c'Ust'id. By exanlining the set of custorner groups, we can identify association\nrules such as {pen} ::::} {rnilk}. rThis rule should now be read as follows: \"If a\npen is purchased by a custorner, it is likely that Inilk is also be purchased by\nthat custcuner.\" In the Purchases relation shown in Figure 26.5, this rule ha.s\nboth support and c()nfidE~nce of 1000/(-).\nI transid. I···.c'Ustid ..1\ndate\n112\n105\n'6'73/99\npen\n1\n..•\n6/3/99\n\"-\n112\n105\nink\n1\nf------------\n112\n105\n6/3/99\nmilk\n1\n5/10/99\n.....:::..\n113\n106\npen\n1\n.........\n5/10/99\nrnilk\n113\n106\n1\n_....\n5/15/99\n114\n201\npen\n2\n-\"\n5/15/99\nink\n114\n201\n2\n114\n201\n5/15/99\njuice\n4\n_..\n114\n201\n6/1/99\nwater\n1\n-\n...._...\n5/1/99\n,.,~.....\n111\n201\npen\n2\n~-._._.\n5/1/99\n111\n201\nink\n1\n-_.\n----\nIII\n201\n5/1/99\nrnilk\n3\n-\n111\n201\n5/1/99\njuice\n6\n._--\n.......................\nFigure 26.5\nThe Purchases Helation Sorted on Customer ID\nSiInilctrly, we can group tuples by date and identify association rules that de-\nscribe purchase behavior on the SeHne day. As an exalnple consider again the\nPurchases relation. In this case, the rule {pen} =} {rnilk} is now interpreted\nas follc)\\vs: \"On a da.y when a pen is purclut.sed, it is likely that luilk is also be\npurchased.\"\nIf we use the date field ct.s grouping attribute, we call consider a rnore genenll\nprolJlern called calendric rnarket basket analysis. In calendric rnarket bas-\nket analysis, the user specifies a collection of calendars.\nA, calendar is any\ngroup of dates, such as every l..9v,rulay 'iTt the yeaT 1.999, or eucTy fiT8t of the\n'fnonth. A rule holds if it holds on every day in the calendar. Civen a calendar.\nwe can cornpute a.ssociatiol1 rules over the set of tuples \\vhose date field falls\n\\vithin the c:alendar.\n\n90,1\nBy specifying interesting calendars, 'we can identify rules that rnight not have\nenough support and confidence \\vith respect to the entire datahase but have\nenough support and confidence on the subset of tuples that fall \\vithin the\ncalendar. On the other hand, even though a rule rnight have enough support\nand confidence \\vith respect to the c0l11plete database, it Inight grtin its support\nonly £'1'0111 tuples that fall within a calendar. In this case, the support of the\nrule over the tuples within the calendar is significantly higher than its support\nwith respect to the entire database.\nAs an exarnple, consider the Purchases relation with the calendar every first of\nthe m,onth. \\Vithin this calendar, the association rule pen:::;. ju:ice has support\nand confidence of 100%, \\vhere&'3 over the entire Purcha.ses relation, this rule\nonly has 50% support. On the other hand, within the calendar, the rule pen\n=> m,ilk; has support of confidence of 50%, wherca'3 over the entire Purch&'3es\nrelation it has support and confidence of 75%.\nMore general specifications of the conditions that rIlust be true within a group\nfor a rule to hold (for that group) have also been proposed. We rnight want to\nsay that all items in the LHS have to be purchased in a quantity of less than\ntwo itelTIS, and all itenls in the RHS rnust be purchased in a quantity of more\nthan three.\nlJsing different choices for the grouping attribute and sophisticated conditions\nas in the preceding exarnples, we can identify rules Inore cornplex than the\nbasic association rules discussed earlier. These Inore cornplex rules, nonetheless,\nretain the essential structure of an association rule as a condition over a group\nof tuples, with support and confidence rneasures defined\nc1..'3 usual.\n26.3.5\nSequential Patterns\nConsider the Purchases relation sho\\vn in Figure 26.1. Each group of tuples,\nhaving the sarne c'l18tid value, can be thought of clS a sequence of trans~tctions\nordered by date. rrhis allo¥ls us to identify frequently arising buying patterns\nover tirne.\nvVe begin b,Y introducing the concept of a sequence of itel11sets. Each transac-\ntion is represeqted by a set of tuples, and by looking at the values in the itern\ncolurnn, \\ve get a set of iterns purchased in that transaction.\n1'here£o1'e, the\nsequence of transactions associated \\vith a cllstorner corresponds naturally to\na sequence of itelnsets rnlrchELsed by the custorner. For exalnplc, the sequence\nof purc!HL'3CS for cllstorner 201 is ({pen, ink, 'tn-ilk, juice}, {pen, 'iT/jIG, In'ice}).\n\n902\nCHAPTER 2t6\nA subsequence of a sequence of iternsets is obtained by deleting one or 11101'e\nitcrnsets, and is also a sequence of itenlsets. We say that a sequence (aI, ... , arn )\nis contained in another sequence S if S has a subsequence (bt , ... ,bIn) such that\na'i C bi , for 1 < 'i < rn. Thus, the sequence {{pen,}, {ink, rnilk} , {pen\" ju'ice}) is\ncontained in ({pen, link}, {shir·t} , {ju'ice, ink, m,ilk}, {juice, pen, rn'ilk}) . Note\nthat the order of itenlS within ectCh iterllset does not rnatter.\nHowever, the\norder of iterllsets does lllatter: the sequence ({pen}, {ink, rn'ilk}, {pen, flL'ice})\nis not contained in ({pen, 'ink}, {shirt}, {juice, pen, rnilk} , {juice, nLilk, 'ink}).\nThe support for a sequence S of iternsets is the percentage of custorner se-\nquences of which 8 is a subsequence.\nThe problenl of identifying sequential\npatterns is to find all sequences that have a user-specified rllinimurll support.\nA sequence (aI, a2, a3, ... ,am) with minimurn support tells us that custorners\noften purchase the itelns in set al in a transaction, then in sonle subsequent\ntransaction buy the itcrlls in set a2, then the items in set a3 in a later transac-\ntion, and so on.\nLike association rules, sequential patterns are staternents about groups of tuples\nin the current database.\nCornputationally, algorithms for finding frequently\noccurring sequential patterns resernble algorithrns for finding frequent itemsets.\nLonger and longer sequences with the required rninirnum support are identified\niteratively in a nlanner very similar to the iterative identification of frequent\niternsets.\n26.3.6\nThe Use of Association Rules for Prediction\nAssociation rules are widely used for prediction, but it is inlportant to rec-\nognize that such predictive use is not justified without additional analysis or\ndornain knowledge. Association rules describe existing data accurately but can\nbe rnisle::1ding when used naively for prediction. For exaruple, consider the rule\n{pen} => {ink}\nThe confidence a\"ssociated with this rule is the conditional probability of an ink\npurchase given a pen purcha...se over the given database; that is, it is a descriptive\nrueasure. We rnight use this rule to guide future sales prornotions. For exalllple,\n\\ve rnight offer a discount on pens to increase the sales of pens and, therefore,\naIso increase sales of ink.\nFlowever, such a prorllotion\nct.'3SU1l1CS that pen purchases are good indicators\nof ink purchases in future custC)Iuer transactions (in addition to transactions\nin the current database). This a..ssurnption is justified if there is a cav,8al hnk\nbetween pen purchases and ink purcha.,scs; that is, if buying pens causes the\nbuyer to also buy ink. Ifowever,we can infer a,,')sociation rules\\vith high support\n\nData w!'ining\n903\nand confidence in SOlnc situations \\\\There there is no causal link between L118\nand RIIS. For exarnple, suppose that pens are ahvays purchased together with\npencils, perhaps because of customers' tendency to order writing instrulllents\ntogether. vVe would then infer the rule\n{pencil} =? {ink}\nwith the saBle support and confidence as the rule\n{pen} :::} {ink}\nHowever, there is no causal link between pencils and ink. If we prornote pencils,\na custolner who purchases several pencils due to the pronlotion has no rea..son\nto buy Inore ink. Therefore, a sales prolnotion that discounted pencils in order\nto increase the sales of ink would fail.\nIn practice, one would expect that, by exallllnlng a large database of past\ntransactions (collected over a long tirne and a variety of circumstances) and\nrestricting attention to rules that occur often (i.e., that have high support),\nwe rninirnize inferring lnisleading rules. However, we should bear in rnind that\nnlisleading, noncausal rules lnight still be generated.\nTherefore, we should\ntreat the generated rules as possibly, rather than conclusively, identifying causal\nrelationships. Although association rules do not indicate causal relationships\nbetween the LHS and RHS, we elllphasize that they provide a useful starting\npoint for identifying such relationships, using eithE~r further analysis or a dornain\nexpert's judgrnent; this is the reason for their popularity.\n26.3.7\nBayesian Networks\nFinding causal relationships is a challenging task, as we saw in Section 2G.3.6.\nIn general, if certain events are highly correlated, there are rnany possible\nexplanations.\nf\"\"'cH' exalnple, suppose that pens, pencils, and ink are purchased\ntogether frequently. It rnight be that the purchase of one of these itelIlS (e.g.,\nink) depends causally on the purchase of another itern (e.g., pen). ()r it Blight\nbe that the purchase of one of these iterns\n(~.g., pen) is strongly correlated with\nthe purchase of another (e.g., pencil) because of sorne underlying phenornenon\n(e.g., users' tendency to think about \\vriting instrulnents together) that causally\ninfluences both purchcL.':'cs. IIc)\\v can we identify the true causal relationships\nthat hold between these events in the real world?\nOne approach is to consider each possible cOlnbination of causal relationships\narnong the varial)les or events of interest to us and evaluate the likelihood of\neach cornbination on the basis of the data el,vail::l,ble to us. If we think of ceLeh\ncornbination of causal relationships as a rnodel of the real world underlying the\n\n904\nCHAPTER ~6\ncollected data, we can assign a score to'each ruode! by considering ho\\v consis-\ntent it is (in tenns of probabilities, 'with senne sin1plifying assuInptions)\\vith\nthe observed data. Bayesian nebNorks are graphs that can be used to describe\na ChlSS of such Il1odels, with one node per variable or event, and arcs between\nnodes to indicate causality. For exarnpIe, a good Iuodel for our running exarn-\npIe of pens, pencils, and ink is sho\\vn in Figure 26.6. In general, the nurnber of\npossible Inodels is exponential in the nurnber of variables, and considering all\nrnodels is expensive, so SOUle subset of all possible rnodels is evaluated.\nFigure 26.6\nBayesian Network Showing Causality\n26.3.8\nClassification and Regression Rules\nConsider the following view that contains inforrnation froln a rnailing carnpaign\nperforrned by an insurance cornpany:\nInsuranceInfo( age: integer, cartype: string, highrisk: boolean)\nThe Insurancelnfo vie\\v ha...\" inforrnation about current cllston1ers. Each record\ncontains a cllstolner's age and type of ear as ,veIl as a flag indicating whether\nthe person is considered a high-risk custorner. If the flag is true, the cllstorner\nis considered high-risk. vVe would like to use this information to identify rules\nthat predict the insurance risk of new insurance applicants whose age and car\ntype are known. :For exarnple, one such rule could be:\n\"If age is bet\\veen IG\nand 25 a.n.d caTtypc is either Sports or.Truck, then the risk is high.\"\nNote that the rules we want t.o find have a specific structure.vVe are not inter-\nested in rules that predict the age or type of car of a person: \"\\ve are interested\nonly in rules that predict the insurance risk.\nT'hus, there is one designated\nattribute vvhose value we wish to predict, and\\ve call this attribute the de-\npendent attribute. rrhe other attributes aTe called predictor attributes. In\nour exarnple, the dependent attribute in the Insurancelnfo vic\\v is the highrisk\nattribute arld the predictor attributes are age and cartype. The general foru1\nof the types of rules \\ve \\Vcl,nt to discover is\n\nData i\\{ining\n90~\nThe predictor attributes Xl, ... ,\")(k are used to predict the value of the depen-\ndent attribute}\"\". Both sides of a rule can be interpreted as conditions on fields\nof a tuple. The Pi (Xi) are predicates that involve attribute ..gi. The fornl of\nthe predicate depends on the type of the predictor attribute.\n\\rVe distinguish\ntwo types of attributes: numerical and categoricaL For numerical attributes,\nwe can perfOrIn nurnerieal cornputations, such EL'3 cornputing the average of t\\VO\nvalues; whereas for categorical attributes, the only allowed operation is test-\ning \"\\vhether two values are equal. In the InsuranceInfo view, age is a nUlllerical\nattribute whereas cartype and highrisk are categorical attributes. Returning to\nthe forrn of the predicates, if Xi, is a nUlllerical attribute, its predicate Pi, is\nof the forln l'i < Xi < hi; if ..)(i is a categorical attribute, Pi is of the forIll\nX'i E {Vl, ... ,Vj}.\nIf the dependent attribute is categorical, we call such rules classification rules.\nIf the dependent attribute is nurnerical, we call such rules regression rules.\nFor exarnple, consider again our exaInple rule:\n\"If age is between 16 and 25\nand caTtype is either Sports or Truck, then highr-i8k is true.\" Since highrisk is a\ncategorical attribute, this rule is a classification rule. We can express this rule\nfonnally as follows:\n(16 < age < 25) /\\ (car-type E {Sports, Truck}) ===? highri8k = true\nWe can define support and confidence for classification and regression rules, as\nfor association rules:\nIII\nSupport: ffhe support for a condition C is the percentage of tuples that\nsatisfy C. The support for a rule G'11===? C:2 is the support for the condition\nCI/\\ C2.\nIII\nConfidence: Consider those tuples that satisfy condition (71. The confi-\ndence for a rule Cl =} G'12 is the percentage of such tuples that also satisfy\ncondition (;2.\nAs a further generalization, consider 1,118 right-hand side of a classification or\nregression rule:\ny~ =. c..Each rule predicts a v,lJue of Y- for a given tuple based\non the vaJues of predictor attributes Xl, ... ,Xk. \\Ve can consider rules of the\nfonn\nwhere f is sonlC function. VVe do not discuss such rules further.\nClassification H,1l<1 regression rules differ fr0111 clssociation rules by considering\ncontinuous and categorical fields, rather than only one field that is set-valued.\nIdentifying such rules efficiently presents a ne\\v set of challenges;\nvve do not\n\n906\nCHAPTER 26\ndiscuss the general case of discovering such rules. We discuss a special type of\nsuch rules in Section 26.4.\nCla.-ssification and regression rules have many applications. Exarnples include\nela'3sification of results of scientific experirnents, where the type of object to\nbe recognized depends on the InCa'3Urernents taken; direct lllail prospecting,\nwhere the response of a given customer to a prolnotion is a function of his 01'\nher inCOlue level and age; and car insurance risk assesslnent, where a customer\ncould be classified as risky depending on age, profession, and car type. Example\napplications of regression rules include financial forecasting, where the price of\ncoffee futures could be SOIne function of the rainfall in Colornbia a month ago,\nand Inedical prognosis, where the likelihood of a tUInor being cancerous is a\nfunction of Illeasured attributes of the tUlnor.\n26.4\nTREE·STRUCTURED RULES\nIn this section, we discuss the problem of discovering classification and regres-\nsion rules from a relation, but we consider only rules that have a very special\nstructure.\nThe type of rules we discuss can be represented by a tree, and\ntypically the tree itself is the output of the data mining activity.\nTrees that\nrepresent classification rules are called classification trees or decision trees\nand trees that represent regression rules are called regression trees\nFigure 26.7\nInsurance lUsk Example Decision Tree\nA.s an exalnple, consider the decision tree ShO\\Vll in Figure 2G.7. Each path froln\nthe root node ti) a leaf node represents one claBsification rule. For exa,rnplc, the\npath fron1 the root to t11e leftrnost leaf node represents the classification rule:\n\"If a person is 25\ny(~a,rs or .younger and drives a sedan, then he or she is likely\nto have a lo\\v insurance risk.\"\n~rhe path fforn the root to the right-Inost leaf\nnode represents tlle cla,,':)sification rule: \"If a person is older than 25 years, then\nhe or she is likely to have a low insurance risk.\"\n\nData JvIin'ing\n907\nTree-structured rules are very popular since they are easy to interpret. E<l.'3e of\nunderstanding is very hllportant because the result of any data rninillg activity\nneeds to be cOlllprehensible by nonspecialists. In addition, studies have shown\nthat, despite Ihnitations in structure, trCt'-structured rules are very accurate.\nThere exist efficient algorithrl1s to construct tree-structured rules fronl large\ndatabases. vVe discuss a sample algorithrIl for decision tree construction in the\nrernainder of this section.\n26.4.1\nDecision Trees\nA decision tree is a graphical representation of a collection of classification\nrules.\nGiven a data record, the tree directs the record frOIn the root to a\nleaf. Each internal node of the tree is labeled with a predictor attribute. This\nattribute is often called a splitting attribute, because the data is 'split' based\non conditions over this attribute. The outgoing edges of an internal node are\nlabeled with predicates that involve the splitting attribute of the node; every\ndata record entering the node must satisfy the predicate labeling exactly one\noutgoing edge.\nT'he cornbined information about the splitting attribute and\nthe predicates on the outgoing edges is called the splitting criterion of the\nnode. A node with no outgoing edges is called a leaf node. Each leaf node of\nthe tree is labeled with a value of the dependent attribute. We consider only\nbinary trees where internal nodes have two outgoing edges, although trees of\nhigher degree are possible.\nConsider the decision tree shown in Figure 26.7. The splitting attribute of the\nroot node is age, the splitting attribute of the left child of the root node is\ncar-type. The predicate on the left outgoing edge of the root node is age :s; 25,\nthe predicate on the right outgoing edge is age> 25.\n\"'e can no\\v aBsociate a classification rule with each leaf node in the tree as\nfollows. Consider the path frorH the root of the tree to the leaf node..Each edge\non that path is labeled with a predicate. 'The conjunction of all these predicates\nrnakes up the left-hand side of the rule. rrhe value of the dependent attribute\nat the leaf node rnakesup the right-ha,nd side of the rule. Thus, the deeision\ntree represents a, collection of claA~sification rules, OIle for ea..ch leaf node.\nA decision tree ,is usuaJly constructed in t\\VO pha\"ses. In phase onc, the growth\nphase, an overly large tree is constructed.\n]~his tree represents the records\nin the input database very cLccurately; for exaluple, the tree rnight contain\nleaf nodes for inclividual records frorn the input dataJ:>Hse.\nTn phase t\\VO, the\npruning phase, the final size of the tree is deterrnined.\n]~he rules represented\nby the\ntn~e constructed in\np}laS(~ one a,rc usuall:y overspecialized. By reducing\nthe size of the tree, we generate a srnaller nUlnber of lllore general rules that\n\n908\nC\n\"\"'.\n')6\nlIAPTI~J.R\n.....\nare better than a very large nUlllbcr of very specialized rules. Algorithrns for\ntree pruning are beyond our scope of discussion here.\nClassification tree algorithrlls build the tree greedily top-down in the following\n\\vay. At the root node~ the database is exarnined and the locally'best' splitting\ncriterion is cornputed. rrhe database is then partitioned, according to the root\nnode's splitting criterion, into t\"W{) parts, one paTtition for the left child and one\npa,rtition for the right child. The algoritlull then recurses on each child. rrhis\nschcrua is depicted in Figure 26.8.\nInr.ut: !loden, partition D, split selection ruethod S\n.Qutput: decision tree for D rooted at node n\nTop-Down Decision Tree Induction Schema:\nBuildTree(Node 11, data partition D, split selection rnethod S)\n(1)\nApply S to D to find the splitting criterion\n(2)\nif (a good splitting criterioll is found)\n(3)\nCreate two children nodes n 1 and n2 of n\n(4)\nPartition D into D 1 and D2\n(5)\nBuildT'ree(nl, D 1 , S)\n(6)\nBuild\nrTree(n2, D2, S)\n(7)\nendif\nFigure 26.8\nDecision Tree Induction Schema\nT'he splitting criterion at a node is found through application of a split selec-\ntion method.\nA split selection rnethod is an algorithrIl that takes as input\n(part of) a relation and outputs the locally 'best' splitting criterion.\nIn our\nexarnple, the split selection rnethod exarnines the attributes cartype and age,\nselects one of thern as splitting attribute, and then selects the splitting pred-\nicates.\nIVlany different, very sophisticated split selection rnethods have been\ndeveloped; the references provide pointers to the relevant literature.\n26.4.2\nAn Algorithm to Build Decision Trees\nIf the input database fits into rna,in Inernory,\n~Te can directly follow th.e clas-\nsification tree induction schcrna shown in Figure 26.8. flovv can we construct\ndecision trees when the input relation is larger than rnain rncrJlory? In this ca.se,\nstep (1) in Figllre 26.8 fails, since the input database does not fit in Inenl0ry.\nBut we can rnake one irnportant observation about split selection Inethods that\nhelps us to reduce the rnain rnerllory requircluents.\nConsider a node of the decision tree. The split selection rnethod ha.s to Inake\ntwo decisions after exarllining the partition at that node: It ha.'3 to select the\nsplitting attribute: and it ha,s to select\nth(~ splitting predicates for tIle outgo-\n\nData AfiTJ,ing\n23\nSedan\nfalse\n~iO\nSj)orts\nfalse\n~36\nSccran---\nfalse\n25\nTruck\ntrue\n~lO\nSedan\nfalse\n.........\"..........\n2~~\nTruck\ntrue\n30\nTruck\nfalse\n25\nSports\ntrue\n18\nSedan\nfalse\nFigure 26.9\nThe Insurancelnfo Relation\n9Q9\ning edges.\nAfter selecting the splitting criterion at a node, the algorithrn is\nrecursively applied to each of the children of the node. Does a split selection\nrnethod actually need the cornplete database partition as input? Fortunately,\nthe answer is no.\nSplit selection rnethods that cornpute splitting criteria that involve a single\npredictor attribute at each node evaluate each predictor attribute individually.\nSince each attribute is exarnined separately, we can provide the split selection\nrnethod with aggregated inforulation about the database instead of loading\nthe cornplete database into rnain rnenlory.\nChosen correctly, this aggregated\ninforrnation enables us to cornpute the senne splitting criterion as we would\nobtain by exarnining the conlplete database.\nSince the split selection rnethod exanlines all predictor attributes, we need\naggregated inforrnation about ceLeh predictor attribute. vVe call this aggregated\ninforrnation the AVe set of the predictor attribute. The AVe set of a predictor\nattribute X <tt noden is the projection of 'n's database partition onto ..:X\" and\nthe dependent attribute where counts of the individual values in the dorllain\nof the dependent attribute are aggregated. (AVe; stands for Attribute-Value,\nClass label, because the values of the dependent attribute are ofterl called class\nlabels.) For exarnple, consider the Insurancelnfo relation as shown in Figure\n26.9. rrhe AVe set of the root node of the tree for predictor attribute age is\nthe result of the following databEtse query:\nSELECT\nFLage, Il.highrisk, COUNT (*)\nFROM\nInsurancelnfo Il\nGROUP BY R.age, H,.highrisk\nThe AVe set for the left child of\nt1H'~ root node for predictor attribute car-type\nis the result of the following query:\n\n910\nSELECT\nFROM\nWHERE\nGROUP BY\nItcartype, H,.highrisk, COUNT\nInsurancelnfo R,\nR.-age <= 25\nILcartype, H..highrisk\n(*)\nCHAPTER~6\nThe t\\VO .A.VC sets of the root node of the tree are shown in Figure 26.10.\n[ Ca~~ype\n...\n,\nhighrisk\n'--\nfalse\ntrue\n\"\"'•..~\nSedan\n0\n4\n- -\nSports\n1\n1\nf---..._..\nTruck\n2\n1\n- .-\n\"'-\"'-\nAge\nhighrisk\ntrue\nfalse\n--\n_. ...\n_.-\n18\n0\n1\n23\n\"\"-\n1\n1.\n25\n2\n0\n..-\n~30\n()\n3\n......-\n36\n0\n1.\n.....\"'..\n--\"'-\nFigure 26.10\nAVe Group of the Root Node for the InsuranceInfo Relation\nWe define the AVe group of a node n to be the set of the AVe sets of all\npredictor attributes at node TL Our exarnple of the Insurancelnfo relation has\ntwo predictor attributes; therefore, the AVe group of any node consists of two\nAVe sets.\nHow large are AVe sets?\nNate that the size of the AVe set of a predictor\nattribute X at node n depends only on the nurnber of distinct attribute values\nof X and the size of the dornain of the dependent attribute.\nFor exarnple,\nconsider the AVe sets shown in Figure 26.10. The AVe set for the predictor\nattribute cartype has three entries, and the AVe set for predictor attribute age\nhas five entries, although the Insuraneelnfo relation as shown in Figure 26.9\nhas nine records. For large databases, the size of the AVe sets is independent\nof the nurnber of tuples in the databa.'3e, except if there are attributes with very\nlarge dOITlains, for exarnple, a real-valued field recorded at a very high precision\nwith rnany digits after the decirnal point.\nIf we 1nake the sirnplifying assurnption that all the AVe sets of the root node\ntogether £it into rnain rnernory, then \\ve can construct decision trees frOTH very\nlarge\nclataba,,~es as follo\\vs: \\Ve rnake a scan over the database and construct\nthe AVe group of the root node in rneIllory. Then \\ve run the split selection\nrnethod of our choicc\\vith t11e i\\Ve group as input. .AJ'ter the split selection\n1netllod cornputes the splitting attribute and the splitting predicates on the\noutgoing 1H)des 1 \\ve partition the databa,se and recurS8.\nNote that this algo-\nrithrIl is very sirnihu' to the original algorithrn shovvn in Figure 26.8; the only\nrnodification necessa.r;y is shown in Figure 2(L 11. In additioll 1 this algoritlll11 is\nstill independent of the aetuaJ split selection rnethod involved.\n\nData JvlirLing\nInput: node n, partition D, split selection 111ethod S\n2..~~_~;EI:!.~: decision tree for D rooted at node 11\nTop-Down Decision Tree Induction Schenla:\nBuHdTree(Node n, data partition D, split selection method S)\n(la) Ivlake a scan over D and construct the AVe group of n in-nlCIIlory\n(1b) Apply S to the AVe group to find the splitting criterion\nFigure 26.11\nCla...'>sification rn-ee Induction Refinement with AVe Groups\n26.5\nCLUSTERING\n911\nIn this section we discuss the clustering problem. The goal is to partition\na set of records into groups such that records within a group are shnilar to\neach other and records that belong to two different groups are dissimilar. Each\nsuch group is called a cluster and each record belongs to exactly one cluster. 1\nSirnilarity between records is Ineasured cOlnputationally by a distance func-\ntion. A distance function takes two input records and returns a value that is\na measure of their silnilarity.\nDifferent applications have different notions of\nsimilarity, and no one rneasure works for all domains.\nAs an exarnple, consider the scherna of the Custol11erlnfo view:\nCustornerInfo(age: int, salary: real)\nWe can plot the records in the view on a two-dil11ensional plane as shown in\nFiguf(~ 26.12. The two coordinates of a record are the values of the record's\nsalaTyand age fields. \\Ve can visually identify three clusters: yToung cllstorners\nwh.o have low salaries, young cllstorners with high salaries, and older cnstorners\nwith high salaries.\n{Jsnally, the output of a clustering algorithrll consists of a, summarized rep-\nresentation of each cluster. The type of sUIrllnarized representation depends\nstrongly on the type and shape of clusters the algoritlull cornputes.\nFor ex-\narnple, a.'3S111ne that we have spherical clusters as in the exalllple shuwn in\nFigure 26.12.\nvVe can surnrnarize each cluster by its centeT (often also called\nthe rnea'n) and its Tndi'l1,t), which are defined as follo\\vs.\nC;iven a collection of\nrecords '/'1, ... '17\"11' their center C: and radius .R are defined fL.'3 follows:\n.\nn\nI -·-.....n\n'f'\n(\n'1\n1 '\"\n..1 I:::>\nIL-i=l (r'i - C)\n\" == -\n.L..t. Ti, aI1C\n.1 =\n'\\\n.....-....--... .:.--.---.---\nn . .\nV\nn\n(=1\n1There are clustering algorithrns that allow overlapping clusters, \\vhere a record could belong to\nseveral clusters.\n\n912\nSalary\n60k\n30k\n---\n•\nA•-\n· _-8\n20\n40\nc•• •-\n60\nAge\n(JIIAPTgR\n·~6\nFigure 26.12\nRecords in CustomerInfo\nThere are two types of clustering algorithlns. A partitional clustering algo-\nrithnl partitions the data into k groups such that SOUle criterion that evaluates\nthe clustering quality is optirnized. The nurnber of clusters k is a parameter\nwhose value is specified by the user. A hierarchical clustering algorithnl gen-\nerates a sequence of partitions of the records. Starting with a partition in which\neach cluster consists of one single record, the algorithrn rnerges two partitions\nin each step until only one single partition rernains in the end.\n26.5.1\nA Clustering Algorithm\nClustering is a very old problern, and nurnerous algorithnls have been developed\nto cluster a collection of records. Traditionally, the nurnber of reeords in the\ninput database \\vas assurned to be relatively slnall and the cornplete database\nwa.s assurned to fit into Inain rnernory. In this section,we describe a clustering\nalgoritlnn called BIllCII that handles very la.rge\ndatab~1.ses.\nrrhe design of\nBIR,CII reflects the follovving two a\",ssurnptions:\nII\n1'he rnunber of records is potentially very large, and therefore we \\\\Tant to\nrnake only one scan over the\nda.ta,b(~se.\nII\nOnly a lirnited arnount of rnain rnenlory is available.\nj\\ user can set t\\VO pararneters to control the BIRfJII algoritllln.\nThe first\nis a thresl10lcl on the arnount of rnain luernory available. This HUlin rncrnory\nthreshold translates into a lllaxirnurn nurnber of cluster SUIJlrnaries k that can\nbe lIutintained in rncrllory. 'The second pararneter f is EUI initied threshold for\nthe radius of an,Y cluster.\n1]H~ value of E is an upper bound on the radius of\nany cluster and controls the nUlnber of clusters that the algorithrn discovers.\nIf (' is slnalI, \\eve discover TnaDy sInaII clusters; if Eis large; we discover very fe\\v\n\n91~\nclusters, each of which is relatively large. \\Ve say that a cluster is compact if\nits radius is s1na11e1' than t.\nBIF.{,CH always lTlaintains k~ or fc\\ver cluster sU1nrnaries (C/i ~ R~i) in rnain Hlcnl0ry,\n\"vhere C:i is the center of cluster 'i and lii is the radius of cluster ,i. The a1gorithrn\nahvays rnaintains cornpact clusters; that is, the radius of each cluster is less\nthan E. If this invariant cannot be rnaintained with the given arIlount of rnain\nlnerno1'y, E is increased t=ts described next.\nThe algoritlnl1 reads records frorn the database sequentially and processes the1l1\nas follows:\n1. Cornpute the distance betVileen record\nT' and each of the existing cluster\ncenters. Let i be the cluster index such that the distance between rand\nCi is the srnallest.\n2. Cornpute the value of the new radius R~ of the ith cluster under the as-\nsumption that r is inserted into it. If R~ < E, then the ith cluster rernains\ncornpact, and we assign\nT to the ith cluster by updating its center and\nsetting its radius to R~. If R:z, > E, then the ith cluster would no longer be\ncOlnpact if we insert r into it. Therefore, we start a new cluster containing\nonly the record T.\nThe second step presents a problern if we already have the rnaxinnun nurnber\nof cluster sUIInnaries, k. If we now read a record that requires us to create a\nnew cluster, we lack the rnain rne1nory required to hold its surnrnary. In this\ncase, we increase the radius threshold E-----using SOHle heuristic to detennine\nthe increase--- in order to rner:qe existing clusters:\nAn increase of c has two\nconsequences.\nFirst, existing clusters can accorl1rnodate rnore records, since\ntheir rnaxirnurn radius has increased.\nSecond, it Blight be possible to rnerge\nexisting clusters such that the resulting cluster is still cornpact.\nrrhus, an\nincrease in ( usually reduces the l1ulIlber of existing clusters.\nThe cornplete BlItCH algorithrl1 uses a balanced in-rnernory tree, which is sirn-\nilar to a B-·t- tree in structure, to quickly identify the closest cluster center for\na neV\\r record. A description of this data structure is beyond the scope of our\ndiscussion.\n26.6\nSIMILARITY SEARCH OVER S~=QUENCES\nA lot of inforrnation stored in datal)ases consists of sequences. In this section,\nw(~ introduce the problern of siInilarity search over a collection of sequences.\nOur query Inode} is very sirnple: vVe assurne that the user specifies a query\nsequence andvvants to retrieve all data sequences that are silnilar to the\n\n914\n(]H.APTER 26\nCommercial Data Mining Systems:\nThere area number of data\nruining products on the rnarket\ntod~y, such as SASEnterprise Nliner,\nSPSS Clenlcntine, CART froIn Salford SystenlS, Ariegaputer PolyAnaJyst,\nANGOSS I<nowledgeStudio. We highlight t\\VO that have strong database\nties.\nIBM's Intelligent Miner\noffers a wide ra,ngeof algorithlns, including\nassociation rules, regression, cla.'3sification, and clustering. The enlpha'3is\nof Intelligent Miner is on scalability-·~·theproduct contains versions of all\nalgorithllls for parallel cOlnputers and is tightly integrated with IBM's\nDB2 database systenl. DB2's object-relational capabilities can be used to\ndefine the data Inining classes of SQL/MM. Of course, other data 111ining\nvendors can use these capabilities to add their own data mining models\nand algorithms to DB2.\nMicrosoft's SQL Server 2000 has a component called the Analysis Server\nthat lnakes it possible to create, apply, and lnanage data mining models\nwithin the DBMS. (SQL Server's OLAP capabilities are also packaged in\nthe Analysis Server component.) The basic approach taken is to represent\na mining rrlodel as a table;\nclustering and decision tree models are\ncurrently supported. The table conceptually has one row for each possible\ncombination of input (predictor) attribute values.\nThe model is created\nusing a staternent analogous to SQL's CREATE TABLE that describes the\ninput on which the model is to be trained and the algorithrn to use in\nconstructing the model.\nAn interesting feature is that the input table\ncan be defined, using a specialized view rnechanisnl, to be a nested table.\nFor exalnple,we can define an input table with one row per custolner,\nwhere one of the fields is a nested table that describes the eustolner's\npurchases. The SQL/Ml\\;1 extensions for data ruining do not provide this\ncapability because SQL:1999 does not currently support nested tables\n(Section 23.2.1).\nSeveral properties of attributes, such\nf}\",C; whether they\nare discrete or continuous, can also be specified.\n.J.~ rnodel is trained by inserting rows into it, using the INSERT cornlnand.\nIt is applied to a llC\\V dataset to lnake predictions using a new kind of\njoin called PREDICTION JOIN; in principle, each input tuple is nlatched\n!\nwith the corxesponding tuple in the rnining lllodel to detennine the value I\nof the predicted attribute.\nThus, end users can create, train,and apply\nI,.\ndecision trees and clustering using extended SQL. 'There are aJso cornrnands\ni\nto browse rnodels.\nUnfortnnately, users cannot add new rnodels or new\ni\nalgorithrlns for models, a capability that is supported in the SQL/MlVI\nI\nproposa.\nI\nL._\n__..~\n_ _.__\n~__~\n_ __~_\n_\n_\n_\n__''.._\n.1\n\nData l\\ifining\n915\nquery sequence. Sinlilarity search is different frorH 'llornlal' queries in that \\ve\nare interested not only in sequences that rnatch the query sequence exactly but\nalso those that differ only slightly frorn the query sequence.\nWe begin by describing sequences and sirnilarity between sequences. A data\nsequence X is a series of nurnbers X\n=\n(;1~1,\"\" Xk).\nS0111etirIles X is also\ncalled a time series. \\lVe call k the length of the sequence. A subsequence\nZ = (Zl' ... ,Zj) is obtained frolll another sequence X = (Xl, ... ,Xk) by deleting\nnurnbers froln the front and back of the sequence X. ForInally, Z is a subse-\nquence of X if Z1 = Xi, Z2 == Xi+l,\n,Zj = Zi-tj-l for SaIne i E {I, ... ,k- j +I}.\nGiven two sequences .iY = (Xl,\n,Xk) and Y = (Yll\" . ,Yk), we can define the\nEuclidean Darrri as the distance between the two sequences as follows:\nk\nIIX - YII == L(Xi -\nYi)2\ni=l\nGiven a user-specified query sequence and a threshold pararneter E, our goal is\nto retrieve all data sequences that are within E-distance of the query sequence.\nSirnilarity queries over sequences can be classified into two types.\n•\nComplete Sequence Matching: The query sequence and the sequences\nin the database have the sarne length.\nGiven a user-specified threshold\nparanleter E, our goal is to retrieve all sequences in the database that are\nwithin E-distance to the query sequence.\nII\nSubsequence Matching:\nrrhe query sequence is shorter than the se-\nquences in the database. In this case, we want to find all subsequences of\nsequences in the databc1.Se such that the subsequence is within distance E\nof the query sequence. We do not discuss subsequence rnatching.\n26.6..1\nAn Algorithm to Find Similar Sequences\nGiven a collection of data sequences, a query sequence, and a distance thresh-\nold f, henv can we efficiently find all sequences within f-distance of the query\nsequence?\n()ne\np()ssibilit~y is to scan the databa.se, retrieve each data sequence, and C0111-\npute its distance to the query sequence. \\Vhile this algorithrn has the rnerit of\nbeing sirnple, it ahvays retrieves every data sequence.\nBecause\n\\V(~ consider the conlplete sequence lnatehing problenl, all data se-\nquences and the query\nsequ(~nce have tllC seune length. \\Ve can think of this\nsirnilarity search\n(1.,\"1 a. high-dirnensional indexing probleul. Each data, sequence\n\n916\nCHAPTER 2ti\nand the query sequence can be represented as a point in a k-dirnensionaJ space.\nTherefore, if we insert all data sequences into a Illuitidirnensional index, we can\nretrieve data sequences that exactly ll1atch the query sequence by qllerying the\nindex. But since 'we \\vant to retrieve not only\ndat~l sequences that Inatch the\nquery exactly but also all sequences within (-distance of the query sequence, \\ve\ndo not use a point query as defined by the query sequence. Instead, we query\nthe index 'with a hyper-rectangle that has side-length 2E and the query sequence\nas center, and \\ve retrieve all sequences that fall within this hyper-rectangle.\n\\Ve then discard sequences that are actually further than c away froln the query\nsequence.\nITsing the index allows us to greatly reduce the nurnber of sequences we con-\nsider and decreases the thne to evaluate the sirnilarity query significantly. The\nbibliographic notes at the end of the chapter provide pointers to further irn-\nprovernents.\n26.7\nINCREMENTAL MINING· AND DATA STREAMS\nReal-life data is not static, but is constantly evolving through additions or\ndeletions of records. In sorne applications, such as network Inonitoring, data\narrives in such high-speed strearns that it is infeasible to store the data for\noffline analysis.\nWe describe both evolving and strearning data in terlns of\na framework called block evolution.\nIn block evolution, the input dataset\nto the data mining process is not static but periodically updated with a new\nblock of tuples, for exarnple, every day at rnidnight or in a continuous strealn.\nA block is a set of tuples added siInultaneously to the database.\nFor large\nblocks, this Inodel captures comrnon practice in rnany of today's data warehouse\ninstallations, where updates from operational databases are batched together\nand perforrned in a block update. For srnall blocks of\ndata-~····-at the extrerne,\neach block consists of a single record····-····-this rnodel captures strealning data.\nIn the block evolution rnodel, the datab~\"kqe consists of a (conceptually infinite)\nsequence of data blocks D 1, [J2 , . .. that arrive at tilnes I, 2, ... ,\\Vhe1'8 each\nblock D i consists of a set of records. 2 \\¥e call 'i the block identifier of block 13i .\n~rherefore, at a,ny titHe t 1 the database consists of a finite sequence of blocks of\ndata (Dl 1 ••• ,I)t;) that arrived at tirnes {I, 2, ... ,t}. The databc1se at tilne t,\n\\vhic.h we denot.e by 1)[1, t], is the union of the databa\",se at tiIne t - 1 and the\nblock that arrives at tirue t, Dl .\nFor evolving data, t\\VO classes of problerns are of particular interest:\nrnodel\nInaintenance and change detection.\nrIhe goal of 1110del maintenance is to\n---\"-\"\n2In general, a block specifies records to change or delete, in addition to n~cords to insert. \\Ve only\nconsider inserts.\n\n.Data A1in'ing\n9~7\nrnaintain a data rnining ulodcl under insertion and deletions of blocks of data.\nTo incrernentally cornpute the data mining rnodel at tirne t,\\vhich \\ve denote by\nl\\:I(D[l, t)), we HUlst consider only Af(D[l, t -- 1]) and .Dt ; \\ve cannot consider\nthe data that arrived prior to tiIne t.\nFurther, a data analyst rllight specify\ntirne-dependent subsets of D [1, t], such as a window of interest (e.g., all the data\nseen thus far or last week's data).\n1\\I10re general selections are also possible,\nfor exarnple, all weekend data over the\nP<:k~t year.\nGiven such selections, we\nHlllst incrernentally CCHupute the rnodel on the appropriate subset of .D[l, t] by\nconsidering only [Jt and the model on the appropriate subset of 1)[1, t - 1].\n'Alrnost' incrernental algoritlulls that occasionally exarnine older data rnight\nbe acceptable in warehouse applications, where incrementality is lTIotivated by\nefficiency considerations and older data is available to us if necessary.\nThis\noption is not available for high-speed data strearns, where older data may not\nbe available at all.\nThe goal of change detection is to quantify the difference, in terrns of their\ndata characteristics, between two sets of data and determine whether the change\nis rneaningful (i.e., statistically significant).\nIn particular, we rnust quantify\nthe difference between the rllodels of the data as it existed at sonle tiIne tl\nand the evolved version at a subsequent ·tirne t2; that is, we Blust quantify the\ndifference between 1\\1(D[I, tl]) and 1\\1(D[1, t2]). We can also measure changes\nwith respect to selected subsets of data. Several natural variants of the problem\nexist; for exarnple, the difference between M(D[l, t - 1]) and 1\\1(Dt ) indicates\nwhether the latest block differs substantially frorn previously existing data. In\nthe rest of this chapter, we focus on rnodel rnaintenance and do not discuss\nchange detection.\nIncrernental rnodel rnaintenance has received rnuch attention. Since the quality\nof the data rllining rnodel is of utrnost irnportance, incrernental rnodel rnain-\ntena,nce a1gorithrns have concentrated on cornputing exactly the sarne Inode1\nHAS cOlnputed by running the basic rnodel construction algoritlul1 on the union\nof old and new data. ()ne \\videly used scalability technique is localization of\nchanges due to new blocks.\nFor exarnple, for density-based clustering algo-\nritluns, the insertion of a ne\"v record affects only clusters in the neighborhood\nof the record, and thus efficient algorithrlls can localize the change to a fe\\v\nclusters and avoid reccHnputing all clusters.\nAs another exarllple, in decision\ntree construction, \\ve rnight be able to shovv that the split criterion at a, node of\nthe tree changes only within acceptably srnall confidence intervals vvhen records\nare inserted, if we a..ssurne tha,t the underlying distribution of training records\nis static.\n()nc-pclSS rnodel construction over data strearllS\nha,,\"~ received particular atten-\ntion, since data arrives and rnust be processed continuously in several ernerg-\ning application dCHnains. For exarnple, network installations of la,rge TelecOll1\n\n918\n()HAPTER ~\nand Internet service providers have detailed usage inforruation (e.g., eall-detail-\nrecords, router p<:1Cket-flovv and trace data) froln different parts of the underly-\ning network that needs to be continuously analyzed to detect interesting trends.\nOther exanlples include webserver logs, streaI11S of transactional data froI11 large\nretail chains, and financial stock tickers.\n\\\\Then working with high-speed data strearlls, algoritlulls lllUSt be designed to\nconstruct data rnining rnodels while looking at the relevant data iterrlS only\nonce and in a .fixed order (deternlined by the strearn-arrival pattern), with a\nlirnited arnount of main 111eI1l0ry. Data-strearn coruputatioll has given rise to\nseveral recent (theoretical and practical) studies of online or one-pass algo-\nrithrlls with bounded HIeIIIory.\nAlgorithrns have been developed for one-pass\ncornputation of quantiles and order-statistics, estirnation of frequency I1l0Inents\nand join sizes, clustering and decision tree construction, estimating correlated\naggregates, and cOInputing one-dirnensional (i.e., single-attribute) histogranls\nand 1Iaa1' wavelet decolllpositions.\nNext, we discuss one such algorithIn, for\nincremental rnaintenance of frequent itemsets.\n26.7.1\nIncremental Maintenance of Frequent Itemsets\nConsider the Purchases Relation shown in Figure 26.1 and assurne that the\nminimum support threshold is 60%. It can be easily seen that the set of frequent\niternsets of size 1 consists of {pen }, {ink}, and {rnilk} with supports of 100%,\n75%, and 75%, respectively. T'he set of frequent itell1Sets of size 2 consists of\n{pen, ink:} and {pen, milk}, both with supports of 75%. The Purchases relation\nis our first block of data. Our goal is to develop an algorithrIl that rnaintains\nthe set of frequent itcrl1sets under insertion of nevv blocks of data.\nAs a first exarnple, let us consider the addition of the block of data shovvn\nin Figllre 26.13 to our original database (Figure 26.1).\nV'nder this addition,\nthe set of frequent itcrIlsets does not change, although their support values do:\n{pen}, {i'nk}, and {Tn'ilk} now have support values of 100%, 60%, and 60%,\nrespectively, and {pen, ink} and {pen, 'In'ilk} now have 609() support. Note that\nwe could detect this case of 'rlO change' sirnply by rnaintaining the nurnber of\nrnarket b::1.,>kets in which (~ach iternset occured. Irl this exaInple, vve update the\n(al)solute) support of itcrnset {pen} by 1.\nFigure 26.13\nThe Purchases Relation Block 2\n\nData 1\\1ining\n115\n201\n115\n201\n7/1/99\n\\vater\n7/1/99\nlllilk\n1\n1\n919\nFigure 26.14\nThe Purchases Relation Block 2a\nIn general, the set of frequent itemsets Illay change. As an exalnple, consider\nthe addition of the block shown in Figure 26.14 to the original datab&'3e shown\nin Figure 26.1.\nvVe see a transaction containing the itern water, but we do\nnot know the support of the iterllset {water}, since water was not above the\nInininUlm support in our original database. A sirnple solution in this case is to\nrnake an additional scan over the original database and cornpute the support of\nthe itenlset {water}. But can we do better? Another innnediate solution is to\nkeep counters for all possible iterllsets, but the nUlnber of all possible itemsets\nis exponential in the nurnber of iterns-·-and most of these counters would be 0\nanyway. Can we design an intelligent strategy that tells us which counters to\nruaintain?\nWe introduce the notion of the negative border of a set of iternsets to help\ndecide which counters to keep. The negative border of a set of frequent itemsets\nconsists of all iterllsets X such that X itself is not frequent, but all subsets of\nX are frequent. For example, in the case of the database shown in Figure 26.1,\nthe following iternsets rnake up the negative border: {juice}, {water}, and {ink,\nmilk}. Now we can design a more efficient algorithm for maintaining frequent\niternsets by keeping counters for all currently frequent iternsets and all iterllsets\ncurrently in the negative border.\n()nly if an iternset in the negative border\nbecomes frequent do we need to read the original data..set again, to find the\nsupport for new candidate itemsets that Blight be frequent.\nWe illustrate this point through the following t\\VO exarnples. If we add Block\n2a shown in Figure 26.14 to the original database shown in Figure 26.1, we\nincrease the support of the frequent iterllset {Tn'ilk} by one, and we increase the\nsupport of the iternset {water}, which is in the negative border, by one as well.\nBut since no iternset in the negative border beearne frequent, we do not have\nto re-scan the original database.\nIn eontrast, cbnsider the addition of Block 2b shown in Figure 26.15 to the\noriginal database shown in Figure 26.1. In this CCl\"se, the iternset {juice}, which\nwas originally in the negative border, becornes frequent with a support of 60%.\nrrhis rneans that now the following itcrnsets of size two enter the negative\nborder: {juice, pen}, {juice, ink}, and {juice, Tnilk}.\n(vVe know that {juice,\n'Water} cannot be frequent since the iteulset {water} is not freqlient.)\n\n920\nof\n. jJ\n1-\nJ:L\n:\"J..\n~\n1\n\"\n.....\n1 .....•...... !\n..,.\n[\n1\nf\nLX'{7\n115\n201\n7/1/99\njuice\n2\n115\n201\n7/1/99\nwater\n2\nFigure 26.15\nThe Purcha..\"es Rt~lation Block 2b\n26.8\nADDITIONAL DATA MINING rfASKS\nCHAPTER 2t6\nvVe focused on the problern of discovering patterns frorn a databa,sc, but there\nare several other equally inlportant data ruining tasks. vVe now discuss SOllIe\nof these briefly. The bibliographic references at the end of the chapter provide\nluauy pointers for further study.\n..\nDataset and Feature Selection:\nIt is often irnportant to select the\n'right' dataset to rnine. Dataset selection is the process of finding which\ndatasets to uline.\nFeature selection is the proeess of deciding which at-\ntributes to include in the mining process.\nII\nSampling: One way to explore a large dataset is to obtain one or luore\nsamples and analyze them.\nThe advantage of sampling is that we can\ncarry out detailed analysis on a sarnple that would be infeasible on the en-\ntire dataset, for very large datasets. The disadvantage of scunpling is that\nobtaining a representative salllple for a given task is difficult; we rnight rniss\nirnportant trends or patterns because they are not reflected in the san1ple.\nCurrent databa..'Se systerns also provide poor support for efficiently obtain-\ning sanlples. Irnproving database support for obtaining sarnples with var-\nious desirable statistical properties is relatively straightforward and likely\nto be available in future DBMSs. Applying sarnpling for data ruining is an\narea for further research.\nII\nVisualization: Visualization techniques can significantly assist in under-\nstanding cornplex datasets and detecting interesting patterns, and the iln-\nportance of visualization in data ruining is \\videly recognized.\n26.9\nREVIEW QUESTIONS\nAnswers to the revie\\v questions can be fuunel in the listed sections.\nIIlI\nvVl1at is the role of data rnining in theKI)I) process? (Secti.on 26.1)\nII\n\\Vhat is the a priori property? I)escribe an algorithnl for' firlding frequent\nitcrIlsets. (Section 26.2.1)\n\nData\ni\\~lir'l,in.9\n921\nIII\nIInw are iceberg queries related to frequent iterIlsets? (Section 26.2.2)\n•\n(jive the definition of an associat'ian rule. '\\Vhat is the difference betv'leen\nsupport and confidence of a rule? (Setion 26.3.1)\nIII\nCan you explain extensions of association rules to ISA hierarchies? \"Vhat\nother extensions of association rules are you farniliar v'lith?\n(Sections\n26.3.3 and 26.3.4)\nIII\n\"Vhat is a sequential pattern? How can we cornpute sequential patterns?\n(Section 26.3.5)\nII\nCan we use association rules for prediction? (Section 26.3.6)\nIII\nWhat is the difference bet\\'leen Bayesian Networks and association rules?\n(Section 26.3.7)\n..\nCan you give exanlples of classification and regression rules? How is sup-\nport and confidence for such rules defined? (Section 26.3.8)\n..\nvVhat are the cOlnponents of a decision tree? I{ow are decision trees con-\nstructed? (Sections 26.4.1 and 26.4.2)\nII\nWhat is a cluster? What inforrnation do we usually output for a cluster?\n(Section 26.5)\n..\nHow can we define the distance between two sequences? Describe an algo-\nrithnl to find all sequences similar to a query sequence. (Section 26.6)\n•\nDescribe the block evolution Inodel and define the problclllS of increlnental\nrnodel maintenance and change detection. \\Vhat is the added challenge in\nrnining data strearns? (Section 26.7)\n11II\nDescribe an incrernental algorithnl for conlpllting frequent iternsets. (Sec-\ntion 26.7.1)\nIII\nGive exarnples of other tasks related to data rnining. (Section 26.8)\nEXERCISES\nExercise 26.1 Briefly ans\\ver the following questions:\n1. Define 8uppor>t, and confidence for (111 association 1\"ule.\n2. Expla.in why association rules cannot be used directly for prediction, \\vithout further\nanal.'1lsis or clornain knowledge.\n:3. \\\\7ha1; axe the differences between a88oc'iat:ion 'rlde8, classification rules, (;1,11(1 regression\n'r'lLles?\n4. \\Vhat is the difference bet\\veen cla88ijic(JJio'Tl and cZ.u8tcrin,g?\n\n922\ni........\n............. .'. ·\"'.li\n..............<Jri(j~t~iiiili\n·,..······i.···f\"ll:iti ····t\nii.{':f.c,\n.~ \",.\nit\n·il.. .i\n..1i\\\nI\n111\n201\n5/1/2002\nink\n1\n111\n201\n5/1/2002\nlnilk\n2\n1---..._ .._...\n11.1\n201\n5/1/2002\n.JuIce\n1\n112\n105\n6/3/2002\npen\n1\n-\n6/3/2002\n_.-.\n112\n105\nink\n1\n.-\n112\n105\n6/3/2002\nwater\n1\n113\n106\n5/10/2002\npen\n1\n113\n106\n5/10/2002\nwater\n2\n113\n106\n5/10/2002\nmilk\n1\n_...\n6/1/2002\n114\n201\npen\n2\n114\n201\n6/1/2002\nink\n2\n114\n201\n6/1/2002\nJUIce\n4\n114\n201\n6/1/2002\nwater\n1\n114\n201\n6/1/2002\nruilk\n1\nFigure 26.16\nThe Purchases2 Relation\nC~H:APTER 26\n5. What is the role of information visualization in data mining?\n6. Give exarrlples of queries over a database of stock price quotes, stored as sequences, one\nper stock, that cannot be expressed in SQL.\nExercise 26.2 Consider the Purchases table shown in Figure 26.1.\n1. Simulate the algorithrn for finding frequent iterllsets on the table in Figure 26.1 with\nminsup=90 percent, and then find association rules with m,inconJ=90 percent.\n2. Can you modify the table so that the same frequent itemsets are obtained with 'fninsup=90\npercent as with 11Linsup=70 percent on the table shown in Figure 26.1?\n3. Sirllulate the algorithrIl for finding frequent iternsets OIl the table in Figure 26.1 with\nrn'insup=lO percent and then find association rules with rninconj=90 percent.\n4. Can you rnodi~y the table so that the sarne frequent iternsets are obtained with rnin-'i1lp=10\npercent as with minsvp=70 percent OIl the table shown in Figure 26.1?\nExercise 26.3 Assulne we are given a, data:set D of rnarket baskets and have computed the\nset of frequent iternsets X in 1) for a given support threshold rnin,B'up. Assume that we would\nlike to add. another data'Set D' to D, and rnaintain the set of frequent itmnsets with support\nthreshold 'fn'in8up in D U IJ'. Consider the following algorithrIl for incrernental Inaintenance\nof a set of frequent iternsets:\nl.vVe run the a p'T-ioT'i algoritlun\nOIl D' and find all frequent iterllsets in D' and their\nsupport. The result is a set of iterllsets .Y'. \\Ve also cornpute the support of all itcrnsets\n-,y E \"Y in J)'.\n2. \\Ve then rnake a scan over D to cornpute the support of all iternsets in .y'.\nAnswer the follo:\\ving questions about the algorithm:\n\nData lvf-in'ing\n923\n•\nThe last step of the algorithm is rnissing; that is, what should the algorithm output'?\n•\nIs this algorithm lllore efficient than the algorithm described in Secti0n 26.7.1'1\nExercise 26.4 Consider the Purchases2 table shown in Figure 26.16.\n•\nList all iterllsets in the negative border of the dataset.\n•\nList all frequent itelnsets for a support threshold of 50%.\n•\nGive an exaruple of a database in which the addition of this database does not change\nthe negative border.\n•\nGive an exarnple of a database in which the addition of this database would change the\nnegative border.\nExercise 26.5 Consider the Purchases table shown in Figure 26.1.\nFind all (generalized)\nassociation rules that indicate the likelihood of items being purchased on the same date by\nthe same customer, with minsup set to 10% and minconj set to 70%.\nExercise 26.6 Let us develop a new algorithm for the computation of all large itemsets.\nAssume that we are given a relation D siInilar to the Purchases table shown in Figure 26.1.\nWe partition the table horizontally into k parts D 1 , ... , D k .\n1. Show that, if itemset X is frequent in D, then it is frequent in at least one of the k parts.\n2. Use this observation to develop an algorithm that cornputes all frequent itemsets in two\nscans over .D.\n(Hint: In the first scan, compute the locally frequent itemsets for each\npart Di , i E {I, ... , k}.)\n3. Illustrate your algorithm using the Purchases table shown in Figure 26.1.\nThe first\npartition consists of the two transactions with transid 111 and 112, the second partition\nconsists of the two transactions with transid 113 and 114. Assulne that the minimum\nsupport is 70 percent.\nExercise 26.7 Consider the Purchases table shown in Figure 26.1. Find all sequential pat-\nterns with minsup set to 60%. (The text only sketches the algorithm for discovering sequential\npatterns, so use brute force or read one of the references for a complete algorithm.)\nExercise 26.8 Consider the SubscriberInfo Relation shown in Figure 26.17.\nIt contains\ninformation about the marketing cmnpaign of the DB Aficionado magazine.\nThe first two\ncolurnns show the age and salary of a potential customer and the subscription colurnn shows\nwhether the person subscribes to the rnagazine.\n\\Ve want to use this data to construct a\ndecision tree that helps predict whether a person will subscribe to the 11lagazine.\n1. Construct the AVC-group of the root node of the tree.\n2. Assume that the spliting predicate at the root\nnod€~ is age S; 50. Construct the AVC-\ngroups of the two children nodes of the root node.\nExercise 26.9 Assurne you are given the following set of six records:\n(7,55), (21, 202),\n(25,220), (12,7:3), (8,61), and (22,249).\n1. Assurning that all six records belong to a single cluster, cornpute its center and radius.\n2. Assurne that the first three records belong to one cluster and the second three records\nbelong to a different cluster. COlnpute the center and radius of the two clusters.\n3. \\Vhich of the two clusterings is 'better' in your opinion and why?\nExercise 26.10 Asslune you are given the three sequences (1, :3, 4), (2, :3, 2), (:3,3, 7). COln-\npute the Euclidian Bonn between all pairs of sequences.\n\n924\nCHAPTER 26\nage·······I···'s-illatry\nAmb8·C!r1pti{j~'t-··1\n-\n.....-\n37\n45k\nNo\n-_:---!---~\n. ...,,----1\n~39\n70k\nYes\n__------1\n56\n50k\nYes\nf---------...t------j---------j\n52\n43k\n\"Yes\n35\n90k\nYes\n32\n54k\nNo\n-------_.-\n40\n58k\nNo\n----+-----::---t----=-::,------\n55\n85k\nYes\n43\n68k\nYes\nFigure 26.17\nThe SubscriberInfo Relation\nBIBLIOGRAPHIC NOTES\nDiscovering useful knowledge from a large database is lllore than just applying a collection\nof data rnining algorithms, and the point of view that it is an iterative process guided by\nan analyst is stressed in [265] and [666]. \\'Fork on exploratory data analysis in statistics, for\nexample [745], and on rnachine learning and knowledge discovery in artificial intelligence was\na precursor to the current focus on data lTlining; the added ernphasis on large volunles of data\nis the inlportant new elernent. Good recent surveys of data mining algorithms include [267,\n397, 507]. [266] contains additional surveys and articles on many aspects of data mining and\nknowledge discovery, including a tutorial on Bayesian networks [:371]. The book by Piatetsky-\nShapiro and Frawley [595] contains an interesting collection of data rnining papers.\nThe\nannual SIGKDD conference, run by the AClVI special interest group in knowledge discovery\nin databases, is a good resource for readers interested in current research in data mining\n(25, 162, 268, 372, 613, 691]' as is the ]o'urnal of Knowledge D'iscovery and Data Alining.\n[363, 370, 511, 781] are good, in-depth textbooks on data nlining.\nThe problern of mining association rules was introduced by Agrawal, Itnielinski, and Swami\n[20].\nl\\!1any efficient algorithnls have been proposed for the cornputation of large iternsets,\nincluding [21,117,364,683,738,786].\nIceberg queries have been introduced by F'ang et al. [264].\nThere is also a large body of\nresearch on generalized forrns of <lssociation rules; for ex<:unple, [700, 701, 703]. The problem\nof finding rnaxirnal frequent itelnsets ha'3 also received significant attention [13, 67, 12G, ;H6,\n~~47, 479, 787].\nAlgorithrns for rnining association rules with constraints are considered in\n[68,462, 56;'{, 590, 591, 70:3].\nParallel algorithnls are described in [2:3] and [655]. Recent papers on parallel data ruining can\nbe found in [788], and work on distributed data 111ining can be found in [417].\n[291] presents an aigoritlull for discovering association rules over a continuous nUllwric at-\ntribute; association rules over numeric attributes are also discussed in [78:3J.\n1'he general\nfornl of association rules, in which attributes other than the transaction id are grouped is de-\nveloped in [529]. Association rules over iterns in a hierarchy are discllssed in [:361, 700]. F'llrther\n\nJ)ata A1in'i'TLg\n()nr.:\n, L,··i\nextensions and generalization of a,,'3sociation rules are proposed in [67, 115,\n56~3]. Integration\nof rnining for frequent itcrHsets into datalxLse systcrns has been addressed in [654, 74a]. 'rhe\nproblern of Inining sequential patterns is discussed in [24}, and further algorithrIls for rnining\nsequential patterns can be found in [510, 702].\nGeneral introductions to classification and regression rules can be found in\n[~362, 5a2]. The\nclassic reference for decision and regression tree construction is the CART book by Breilnan,\nFriccl!nau, Olsheu, and Stone [111].\nA lnachine learning perspective of decision tree con-\nstruction is given by Quinlan [603].\nRecently, several scalable algorithnls for decision tree\nconstruction have been developed [309, 311, 521, 619, 674J.\n'rhe clustering problern has been studied for decades in several disciplines. Sample textbooks\ninclude [232, 407, 418].\nScalable clustering algorithuls include CLARANS [562], DBSCAN\n[249, 250], BIRCH [798], and CURE [344]. Bradley, Fayyad, and Reina address the problem\nof scaling the K-lVIeans clustering algorithm to large databases [108, 109]. The problern of\nfinding clusters in subsets of the fields is addressed in [19]. Ganti et al. exauline the problerll\nof clustering data in arbitrary rnetric spaces [302]. Algorithrlls for clustering caterogical data\ninclude STIRR [315J and CACTUS [301]. [651] is a clustering algorithm for spatial data.\nFinding siulilar sequences from a large database of sequences is discussed in [22, 262, 446,\n606,680].\nWork on incrernental rnaintenance of association rules is considered in [174, 175, 736]. Ester\net al. describe how to nlaintain clusters incrernentally [248], and Hidber describes how to\nrnaintain large iteulsets incrernentally [378]. There has also been recent work on rnining data\nstrearns, such as the construction of decision trees over data streams [228, 309,\n39~)] and\nclustering data streanlS [343,568]. A general framework for ruining evolving data is presented\nin [299]. A framework for measuring change in data characteristics is proposed in [300J.\n\nINFORMATION RETRIEVAL\nANDXMLDATA\n...\nHow are DBNISs evolving in response to the growing aIllounts of text\ndata?\n...\nWhat is the vector space rnodel and how doe's it suppo~ text search?\n...\nHow are text collections indexed?\n...\nCornpared to IR systenls, what is new in Web search?\n...\nHow is XNIL data different from plain text and relational tables?\n...\nWhat are the main features of XQuery?\n...\nWhat are the irnplementation challenges posed by XML data?\n..\nKey concepts: information retrieval, boolean and ranked queries;\nrelevance, precision, recall; vector space model, TF/IDF terrn weight-\ning, document similarity; inverted index, signature file; Web crawler,\nhubs and authorities, Pigeon Rank of a webpage; sernistructured data\nlllodel, XML; XQuery, path expressions, FLWR queries; XML storage\nand indexing\n27\n'with Raghav Kaushik\nUniveTsity of Wi8consin---Aladison\nA Tne'rne:r is a device in vvhieh an individual stores all his books,\nrecords, and cornrnunications, and which is rnechanized so that it rnay\nbe consulted with exceeding speed and flexibility.\n--'Vannevar Bush, As We !vlay Think, 1.945\n926\n\nIR and XlvlL Data\n9~7\nThe field of inforlnation retrieval (IR) has studied the problenl of sea,rching\ncollections of text docurnents since the 19508 and developed largely indepen-\ndently of database systenls. The proliferation of text docunlents on the Web\nlllade docurnent search an everyday operation for 1110st people and led to re-\nnewed research on the topic.\nThe database field's desire to expand the kinds of data that can be managed in\na DB~IS is well-established and reflected in developments like object-relational\nextensions (Chapter 23).\nDocuments on the Web represent one of the rnost\nrapidly growing sources of data, and the challenge of rnanaging such documents\nin a DBMS has naturally become a focal point for database research.\nThe Web, therefore, brought the two fields of database rnanagement systenls\nand information retrieval closer together than ever before, and, as we will see,\nXML sits squarely in the middle ground between thenl. We introduce IR sys-\ntems as well as a data model and query language for XML data and discuss\nthe relationship with (object-)relational database systerns.\nIn this chapter, we present an overview of information retrieval, Web search,\nand the emerging XML data model and query language standards. We begin\nin Section 27.1 with a discussion of how these text-oriented trends fit within\nthe context of current object-relational database systeIns.\nWe introduce in-\nforrnation retrieval concepts in Section 27.2 and discuss specialized indexing\ntechniques for text in Section 27.3. We discuss Web search engines in Section\n27.4. In Section 27.5, we briefly outline current trends in extending database\nsystems to support text data and identify SOllle of the irnportant issues in-\nvolved. In Section 27.6, we present the XML data lllodel, building on the XML\nconcepts introduced in Chapter 7. We describe the XQuery language in Section\n27.7. In Section 27.8, we consider efficient evaluation of XQuery queries.\n27.1\nCOLLIDING WORLDS: DATABASES, IR, AND XML\n'The \\i\\Teb is the rnost widely used doculnent collection today, and search on the\nvVeb differs froIn traditional IR-style docurnent retrieval in iluportant ways.\nFirst, there is great ernphclsis on scalability to very large document collections.\nIR systerns typically dealt with tens of thousands of documents, wherea.'3 the\nvVeb contains l)illions of pages.\nSecond, the vVeb has significantly changed how docurnent collections are created\nand used. Traditionally, III systerlls were aiIned at professionals like librarians\nand legal researchers, who were trained in using sophisticated retrieval engines.\nDocurnents were carefully prepared, and docllrnents in a given collection were\ntypically on related topics. On thevVeb, docurnents are created by an infinite\n\n928\nCHAPT8R 27\nvariety of individuals for equally lllClny purposes, and reflect this diversity in\nsize and content. Searches aTe carried out by ordinary people with no training\nin using retrieval software.\nThe ernergence of Xl\\/lL has added a third interesting diInensioI1 to text search:\nEvery cloClunent can no\\v be rnarked up to reflect additional infol'lnation of\ninterest, such as authorship, source, and even details about the intrinsic content.\nThis he),s changed the nature of a '\"docurnent\" froIn free text to textual objects\n\\vith a..')sociated fields containing metadata (data about data) or descriptive\ninfonnation.\nLinks to other docurnents are a particularly irnportant kind of\nlnetadata, and they can have great value in searching docurnent collections on\nthe vVeb.\nThe Web also changed the notion of what constitutes a docunlent. Documents\non the Web may be multinledia objects such as irnages or video clips, with\ntext appearing only in descriptive tags.\nWe must be able to Inanage such\nheterogeneous data collections and support searches over thern.\nDatabase rnanagernent systenls traditionally dealt with simple tabular data. In\nrecent years, object-relational database systerns (ORDBMSs) were designed to\nsupport complex data types.\nImages, videos, and textual objects have been\nexplicitly rnentioned as exaruples of the data types ORDBMSs are intended to\nsupport. Nonetheless, current database systerns have a long way to go before\nthey can support such cOlnplex data types satisfactorily. In the context of text\nand XML data, challenges include efficient support for searches over textual\ncontent and support for searches that exploit the loose structure of XNIL data.\n27.1.1\nDBMS versus IR Systems\nI)atabc1.se and IR, systcrns have the COllllnon objective of supporting searches\nover collections of data. However, rnany irnportant differences have influenced\ntheir developrnent.\n11II\nSearches versus Queries: IR, systerns are designed to support a special-\nized class of qlH~ries that \\ve also call searches. Searches are specified in\nternlS of a. fo\",T search terms, and the underlying data is usually a collec-\ntion of unstructured text docurnents. III addition, an irnportant feature of\nTH, searches is that search resultsrnay be ranked, or ordered, in tcrrns of\nhovv '\\vell' the search results rnatch the search tenns. In contra.'3t, databa\"sc\nsysterns support a very general class of queries, and the underlying data is\nrigidly structured. Unlike III systerns~ database systerns have traditionally\nreturnedunranked sets of results. (Even the recent S(~L/()L,AP extensions\nthat support earl:y results and searches over ordered data (secl, Chapter 25)\n\nIR and )(ivlL ]Jata\n929\ndo not order results in terlTIS of how \"\"veIl they rnatch the query. Relational\nqueries are pTeC'i8e in that a ro\\v is either in the answer or it is not ; there\nis no notion of 'how vvell a row\nrnatches~ the query.)\nIn other \"\"rords, a\nrelational query only assigns two raJlks to a row, indicating 'whether the\nrow is in the ans\\ver or not.\n•\nUpdates and Tr'ansactions: IR systelns are optirnized for a read-Illostly\nworkload and do not support the notion of a transaction.\nIn traditional\nIR systerlls, ne\\v docurnents are added to the doculnent collection frorH\ntirne to titne, and index structures that speed up searches are periodically\nrebuilt or updated.\nTherefore, docllrnents that are highly relevant for a\nsearch rnight exist in the IR systeln, but not be retrievable yet because of\noutdated index structures. In contrast, databa...'3e systerns are designed to\nhandle a wide range of workloads, including update-intensive transaction\nprocessing workloads.\nThese differences in design objectives have led, not surprisingly, to very dif-\nferent research elnphases and system designs. Ilesearch in IR studied ranking\nfunctions extensively. For example, arllong other topics, research in IR investi-\ngated how to incorporate feedback frOITl a user's behavior to modify a ranking\nfunction and how to apply linguistic processing techniques to improve searches.\nDatabase research concentrated on query processing, concurrency control and\nrecovery, and other topics, as covered in this book.\nThe differences between a DB1\\tIS and an III systenl from a design and irnple-\nmentation standpoint should become clear as we introduce IR systerlls in the\nnext few sections.\n27.2\nINTRODUCTION TO INFORMATION RETRIEVAL\nThere are two COrllr110n types of searches, or queries, over text collections:\nboolean queries and ranked queries.\nIn a boolean\nquery~ the user\nspE~ci­\nfies an expression constructed using terlllS and boolean operators (And,\nOr,\nNot). For exalnple,\ndatabase And (lvlicT08ojt Or IBM)\nThis query a..sks for all docurnents that contain the terrn database and in addi-\ntion, either PvficT080jt or IBN!.\nIn a ranked query the user specifies one or rnore terrlls, a,rlcl the result of the\nquery is a list of docurllents ranked by their relevance to the query. Intuitively,\ndocurnents at the top of the result list are expected to 'rnatch' the search\n\n930\nagent Janles\nagent\nagent rnobile cornputer\nJames Madison Inovie\nJanles Bond movie\nFigure 27.1\nA Text Database with Four Records\nCHAPTER 2$7\ncondition ruore closely, or be 'rnore relevant', than doculnents lower in the result\nlist. While a document that contains Microsoft satisfies the search' Microsoft,\nIBM,' a document that also contains IBM is considered to be a better match.\nSimilarly, a docunlent that contains several occurrences of Microsoft might be\na better rnatch than a document that contains a single occurence. Ranking the\ndocurnents that satisfy the boolean search condition is an important aspect of\nan IR search engine, and we discuss how this is done in Sections 27.2.3 and\n27.4.2.\nAn important extension of ranked queries is to ask for documents that are most\nrelevant to a given natural language sentence. Since a sentence has linguistic\nstructure (e.g., subject-verb-object relationships), it provides more informa-\ntion than just the list of words that it contains. We do not discuss natural\nlanguage search.\n27.2.1\nVector Space Model\nWe now describe a widely-used franlework for representing docurnents and\nsearching over docurnent collections.\nConsider the set of all terrns that ap-\npear in a given collection of documents. We can represent each document as a\nvector with one entry per ternl. In the shnplest 1'01'111 of doclunent vectors, if\nterrn j appears k tirnes in dOCUlnent i, the document vector for docurnent i\ncontains value k in position j. The docurnent vector for i contains the value 0\nin positions corresponding to terrns that do not appear in i.\nConsider the exaInple collection of four docurnents shown in Figure 27.1. rrhe\ndocUluent vector representation is illustrated in Figurf~ 27.2; each row represents\na docurnent.\nThis representation of docurnents a,,'3 terrn vectors is called the\nvector space model.\n\nIR and XNfL Data\n1\n93L\n2\n3\n4\n1\no\no\no\no\n1\n1\no\no\no\no\no\no\n1\n1\no\n1\no\n1\no\no\no\n1\n1\nFigure 27.2\nDocument Vectors for the Example Collection\n27.2.2\nTFIIDF Weighting of Terms\nWe described the value for a terril in a document vector as simply the term\nfrequency (TF), or nurnber of occurrences of that terrn in the given document.\nThis reflects the intuition that a term which appears often is more iInportant\nin characterizing the document than a terrn that appears only once (or a term\nthat does not appear at all).\nHowever, some terms appear very frequently in the document collection, and\nothers are relatively rare. The frequency of terms is eITlpirically observed to\nfollow a Zipfian distribution, as illustrated in Figure 27.3. In this figure, each\nposition on the X-axis corresponds to a terrll and the Y-axis corresponds to\nthe nUlnber of occurrences of the term. Terms are arranged on the X-axis in\ndecreasing order by the nurnber of tirnes they occur (in the docurnent collection\nas a whole).\nAs rnight be expected, it turns out that extremely COmlTIOn terms are not very\nuseful in searches.\nExamples of such common terms include a, an, the etc.\nTerrns that occur extremely often are called stop words, and docunlents are\npre-processed to elirilinate stop words.\nEven after eliminating stop words, we have the phenorilenon that some words\nappear nluch luore often than others in the docurnent collection. Consider the\nwords Lirnl;E and kernel in the context of a collection of dOCUlnents about the\nLinux operating systern. While neither is COlnrnon enough to be a stop word,\nLinux is likely to appear much rnore often. Given a search that contains both\nthese keywords, we are likely to get better results if we give Inore irnportance\nto docurnents that contain kernel than docurnents that contain Linux.\nvVe ca.'!} capture this intuition by refining the docurnent vector representatioll as\nfollows. The value associated with ternl j in the docurnent vector for docurnent\ni, denoted a..\" 'Wij, is obtained by rnultiplying the terrIl frequency tij (the nuruber\nof tirnes tenn j appears in docurnent i) by the inverse docurnent frequency\n(IDF) of terrn j in the docurnent collection.\nIDF of a tenn j is defined\naAS\n\n932\nCHAPTER 27\nlog(lVInj); where 1'1 is the total rHnnber of dOCUlnents, and nj is the nurnber of\ncloClllnents that tenn j appears in. This effectively increases the weight given\nto rare tenns. As an exarnple, in a collection of 10,000 docurnents, a terrIl that\nappears in half the docurnents has an IDF of 0.3, and a tenll that occurs in\njust one docurnent has an IDF of 4.\nLength Normalization\nConsider a docurnent !J. Suppose that we lnodify it by adding a large nUlllber of\nnew terrns. Should a the weight of a terrn t that appears in D be the saIne in the\ndoclunent vectors for D and the rnodified dOCUlTlent? Although the TFjIDF\nweight for t is indeed the saIne in the two document vector, our intuition\nsuggests that the weight should be less in the 1110dified document.\nLonger\ndocul'llents tend to have lnore terms, and lnore occurrences of any given terrn.\nThus, if two doculnents contain the saIne nUlnber of occurrences of a given\ntenll, the importance of the ten'll in characterizing the document also depends\non the length of the docull1ent.\nSeveral approaches to length nornlalization have been proposed. Intuitively,\nall of ther'll reduce the irnportance given to how often a term occurs as the fre-\nquency grows. In traditional IR systelns, a popular way to refine the sirnilarity\nInetric is cosine length normalization:\n*\ntvij\nWij\n=\nl'£~=l 11I[\nIn this formula, t is the nurnbei' of tenns in the dOCulnent collection, 'Wij is the\nTFjIDF weight without length norrnalization, and tvij is the length adjusted\nTFjlDF weight.\nTenns that occur frequently in a doculnent are particularly problenlatic on\nthe \"Veb because webpages are often deliberately rnodified by adding rnany\ncopies of certain words···· for exarnple, sale, free, sex\nto increase the likelihood\nof their being returned in response to queries.\nl:'or this reason, \\:Veb search\nengines typically norrnalize for length by ilnposing a lnaxirnurn value (usually\n2 or 3) for terrIl frequencies.\n27.2.3\nRanking Document Similarity\n\\Ve no\\v consider ho\\\\-' the vector space representation allows us to rank dOCll-\nrnents in the result of a ranked query. A key observation is that a ranked query\ncan itself be thought of EL'S a docUlllent, since it is just a collection of terrl1s.\n1'his allows us to use document similarity as the ba\",sis for ranking query\n\nIR and Xi\\:fL Data\n933\nresults\"....-..,-the doculnent that is rnost sirnilar to the query is ranked highest, and\nthe one that is least sirnilar is ranked lowest.\nIf a total of t teru18 appear in the collection of docurnents (t is 8 in the exaulple\nsho\\~ln in Figure 27.2), \\\\\"e can visualize\ndocunH:~nt vectors in a t-diInensional\nspace in \\vhich each axis is labeled with a te1'1n. This is illustrated in Figure\n27.4, for a two-dirnensional space. The figure shows doculuent vectors for t\\VO\ndocuments, D 1 and .D2 , &'3 \\vell\n(;1\",'3 a query Q.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nTERMB\nQ :;:: (0.4, 0.8)\n01:;:: (0.8, 0.3)\nD2:;:: (0.2, 0.7)\nQ\n02\n~\n:E\nffi\nE-\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n-......\nRARE WORDS\n....-:\nSTOP WORDS\nFigure 27.3\nZipfian Distribution of Term\nFrequencies\nFigure 27.4\nDocument Similarity\nThe traditional rneasure of closeness between two vectors, their dot prodlJ,ct,\nis used as a lneasure of docurnent siInilarity. The siInilarity of query Q to a\ndoculnent IJi is Illea8Ured by their dot produet:\nt\nsim,(Q,Di )\n---..., LQj·7LJ7j\n.1=1\nIn the excunple shown in Figure 27.4, si'nt(Q, D l )\n(0.4 * 0.8) .+ (0.8 *\n0.3)\n=\n0.56, and si1n(Q, D2) =\n(0.4 * 0.2)-+- (0.8 *0.7)\n=\n0,64. Accordingly,\nD2 is ranked higher than 1)1 in the search result.\nIn the context ,of the VVeb, docurnent sirnila1'ity\nIS one of several IneaSU1'es\nthat can be used. to rank results, but should not be used exclusively.\nFirst,\nit is questionable vvheth,er users want dOCllrnents that are sirnilar to the query\n(which typically consists of OIle or tvvo 'words) or dOCUll.lenS that contajn useful\ninforrnation n~lated to the quer,Y te1'111S. IntuitivelY,we \"vant to give ilnportance\nto the Q'uality of a Web page \\vhile ranking it, in addition to reflecting the\nsirnilarity of\ntlH.~ page to a given query. Links between pages provide valuable\n\n934\nC~HAPTER '2.7\nadditional inforrnation that can be used to obtain high-quality results.\nWe\ndiscuss this issue in Section 27.4.2.\n27.2.4\nMeasuring Success: Precision and Recall\nTwo criteria are cornnlonly used to evaluate information retrieval systerlls. Pre-\ncision is the percentage of retrieved documents that are relevant to the query.\nRecall is the percentage of relevant docurnents in the database that are re-\ntrieved in response to a query.\nRetrieving all documents in response to a query trivially guarantees perfect\nrecall, but results in very poor precision.\nThe challenge is to achieve good\nrecall together with high precision.\nIn the context of search over the Web, the size of the underlying collection is\non the order of billions of docuruents.\nGiven this, it is questionable whether\nthe traditional measure of recall is very useful. Since users typically don't look\nbeyond the first screen of results, the quality of a Web search engine is largely\ndeterlnined by the results shown on the first page.\nThe following adapted\ndefinitions of precision and recall rnight be more appropriate for Web search\nengInes:\n•\nWeb Search Precision: The percentage of results on the first page that\nare relevant to the query.\n•\nWeb Search Recall: rrhe fraction N / M, expressed as a percentage, where\nM is the nUluber of results displayed on the front page, and of the M ruost\nrelevant documents, N is the number displayed on the front page.\n27.3\nINDEXING FOR TEXT SEARCH\nIn this section, we introduce two indexing techniques that support the evalu-\nation of boolean and ranked queries.\n'The 'inverted index structure discussed\nin Section 27.3.1 is widely used due to its sirnplicity and good perforlnance.\nIts rnain disadvantage is that it imposes a significant space overhead: The size\ncan be up to 300 percent the size of the original file. The signature file index\ndiscussed in Section 27.::3.2 has a sInall space overhead and offers a quick filter\nthat elirninates rnost nonqualifying docurnents. However, does not scale as well\nto larger dataha..')c sizes because the index has to be sequentially scanned.\nBefore a doeuruent is indexed, it is typically pre-processed to elirninate stop\nwords. Since the size of the indexes is very sensitive to the nurnber of tern1S\nin the docurnent collection, elirninating stop words can greatly reduce index\n\nIR and ..'KML Data\nsize.\nIR, systcrns also do certain other kinds of pre-processing. :For instance,\nthey apply stelnming to reduce related terrns to a ca,nonical forrn. This step\nalso reduces the nUluber of terrn8 to be indexed, but equally irnportantly, it\nallows us to retrieve documents that lnay not contain the exact query terrIl but\ncontain S(Hne variant. As an exarnple, the terrns T1ln, T'lLnning, and T'unncr all\nstern to 'run. The terrIl run is indexed, and every occurrence of a variant of this\nterm is treated as an occurrence of run.\nA query that specifies rtLnneT finds\ndocurnents that contain any word that stenlS to T'UTt.\n27.3.1\nInverted Indexes\nAn inverted index is a data structure that enables fast retrieval of all doc-\numents that contain a query terr11. For each ternl, the index rnaintains a list\n(called the inverted list) of entries describing occurrences of the tenn, with\none entry per docurnent that contains the ternl.\nConsider the inverted index for our running example shown in Figure 27.5. The\nterm 'Jarnes' has an inverted list with one entry each for documents 1, 3, and\n4; the term 'agent' has entries for docurnents 1 and 2.\nThe entry for document d in the inverted list for terrn t contains details about\nthe occurrences of term t in document d.\nIn Figure 27.5, this information\nconsists of a list of locations within the document that contain term t. Thus,\nthe entry for document 1 in the inverted list for terrn 'agent' lists the locations\n1 and 5, since 'agent' is the first and fifth word of docurnent 1.\nIn general,\nwe can store additional information about each occurrence (e.g., in an HTML\ndocurnent, is the occurrence in the 1'ITLE tag?) in the inverted list. We can\nalso store the length of the docurnent if this is used for length norlnalization\n(see below).\nThe collection of inverted lists is called the postings file. Inverted lists can be\nvery large for large doeurnent collections. In fact, Web search engines typically\nstore each inverted list on a separate page, and Inost lists span rnultiple pages\n(and if so, are rnaintained &'S a linked list of pages). In order to quickly find\nthe inverted list for a, query terrn, all possible query terrns are organized in a\nsecond index structure such as a B+ tree or a. hash index.\nThe second index, called the lexicon, is Inuch srnaller than the postings file\nsince it only contains one entry per terrn, and further, only contains entries for\nthe set of terl11S that aTe retained after elirninating stop words, and applying\nstenlluing rules.\nAn entry consists of the terlIl,\nSOH1C surnrnary inforrnation\nabout its inverted list, and the address (on disk) of the inverted list. In Figure\n27.5, the SllIl1lnary inforrnation consists of the lllllllber of entries in the inverted\n\n93G\nlexicon (in-memory)\nPostings file (on disk)\nCHAPTER 27\nFigure 27.5\nInverted Index for Example Collection\nlist (i.e., the nurnber of documents that the terl11 appears in). In general, it\ncould contain additional infonnation such as the IDF for the terrIl, but it is\nil11portant to keep the entry's size as slllall as possible.\nThe lexicon is rua.intained in-lllelnory, and enables fast retrieval of the inverted\nlist for a query terrn.\nrrhe lexicon in Figure 27.5 uses a hash index, and is\nsketched by showing the hash value for the terrn; entries for terms are grouped\ninto hash buckets by their hash value.\nUsing an Inverted Index\nA_ query containing a single tenn is evaluated by first searching the lexicon\nto find the address of the inverted list for the terrIl.\nThen the inverted list\nis retrieved, the docids in it are rnapped to physical doculnent addresses, and\nthe corresponding docurnents are retrieved. If the results are to be ranked, the\nrelevance of each docurnent in the inverted list to the query term is C01l1puted,\nand docurnents are then retrieved in order of their relevance rank. ()bserve that\nthe inforrna,tion needed to cornpute the relevance 1neEU3ure described in Section\n27.2 --the frequency of the query ternl in the dOCu1nent, the IDF of the terrn in\nthe docurnent collection, and the length of the docurnent if it is used for length\nnonnalizatioll-------are all available in either the lexicon or the inverted list.\n\\\"lhen inverted lists are very long, as in\\Veb search engines, it is useful to\nconsider \\vhether we should precornpute the relevance of each dOCUlnent in the\ninverted list for a terrn (with respect to that terrn) and sort the list by relevance\nrather than docurnent id. This would speed up querying because we can just\n\nIR and .i'yAtfL Data\n937\nlook at a prefix of the inverted list, since users rarely look at 11101'0 than the\nfirst fc\\v results. II(ywever 1 Ina.intaining lists in sorted order by relevance can\nbe expensive. (Sorting by dOcUll1cnt id is convenient because nevv dOCUlnents\nare assigned increasing ids, and we can therefore sirnply <1ppend entries for new\ndOCUlnents at the end of the inverted list. Further, if the sirnilarity function is\nchanged, \\ve do not have to rebuild the index.)\nA query with a conjunction of several terrns is evaluated by retrieving the\ninverted lists of the query terrns one at a tiTne and intersecting theln. In order\nto rninirnize 111en10r,}T usage, the inverted lists should be retrieved in order of\nincreasing length. A query with a disjunction of several terrns is evaluated by\nruerging all relevant inverted lists.\nConsider the exaruple inverted index shown in Figure 27.5.\nTo evaluate the\nquery 'JaUles', we probe the lexicon to find the address of the inverted list for\n'Ja1nes', fetch it from disk and then retrieve docurIlent 1. To evaluate the query\n,Jarnes' AND 'Bond', we first retrieve the inverted list for the tenn 'Bond' and\nintersect it with the inverted list for the terrn 'Janles.'\nCrhe inverted list of\nthe terrn 'Bond' has length two, whereas the inverted list of the terrIl 'Jarnes'\nhas length three.) rrhe result of the intersection of the list (1,4) with the list\n(1, ~~, 4) is the list (1,4) and doculuents 1 and 4 are therefore retrieved.\n'fa\nevaluate the query '.lalnes' OR 'Bond,' we retrieve the two inverted lists in any\norder and merge the results.\nfor ranked queries with lnultiple tenns, we 1nust fetch the inverted lists for\nall terrl1s, COlllpute the relevance of every doclunent that appears in one of\nthese lists with respect to the given collection of query terrns, and then sort\nthe docurnent ids by their relevance before fetching the docluuents in relevance\nrank order.\nAgain, if the inverted lists are sorted by the relevance rnea,sure,\nwe can support ranked queries by typically processing only sluall prefixes of\nthe the inverted lists. (()bserve that the relevance of a docuIllent with respect\nto the query is easily cornputed froIn its relevance with respect to each query\nterm.)\n27.3.2\nSignature Files\nA signature fi,le is another index structure for text data.JH-:kse systerns that\nsupports efficient evaluation of boolean queries.\nA signature file contains an\nindex record for each docurnent in the database.\nThis index record is called\nthe signature of the dOClunent.\nF~ach signature has a fixed size of b bits; b is\ncalled the signature width. rrhe bits that are set depend on the words that\nappear in the docllrnent. vVe rnap words to bits by applying a hash function\nto ea,ch vvord in the docurnent and we set the bits that appear in the result of\n\n938\nCHAPTER ~7\n1...1\n!lI\n,.\",\n'l'''''\n,.~,,± \".. '....:\",.:'.;..\n\"\"\n\"\"L£1'\nA\nI\n.....\"\"\".,,\n!\n,,---_ ..\n1\nagent Jarnes Bond good agent\n1100\n2\nagent mobile C0111puter\n1101\n3\nJames 1rfadison lnovie\n1011\n4\nJaInes Bond rnovie\n1110\n_\n...\n---\nFigure 27.6\nSignature File for Example Collection\nthe hash function. Note that unless we have a bit for each possible word in the\nvocabulary, the same bit could be set twice by different words because the hash\nfunction maps both words to the saIne bit. We say that a signature S 1 matches\nanother signature 82 if all the bits that are set in signature 82 are also set in\nsignature 8 1. If signature 8 1 Inatches signature 8 2 , then signature 8 1 has at\nleast as many bits set as signature 8 2 .\nFor a query consisting of a conjunction of terms, we first generate the query\nsignature by applying the hash function to each word in the query. We then scan\nthe signature file and retrieve all documents whose signatures match the query\nsignature, because every such document is a potential result to the query. Since\nthe signature does not uniquely identify the words that a docuInent contains,\nwe have to retrieve each potential rnatch and check whether the docunlent\nactually contains the query terms. A docurnent whose signature matches the\nquery signature but that does not contain all terms in the query is called a false\npositive. A false positive is an expensive rnistake since the docurnent has to\nbe retrieved froln disk, parsed, stemrned, and checked to determine whether it\ncontains the query terms.\nFor a\nqUf~ry consisting of a disjunction of tenns, we generate a list of query\nsignatures, one for each terrn in the query. The query is evaluated by scanning\nthe signature file to find docurnents whose signatures rnatch any signature in\nthe list of query signatures.\nAs an exarllple, consider the signature file of width 4 for our running exarnple\nshown in Figure 27.6. rrhe bits set by the hashed values of all query terrns are\nshown in the figure. To evaluat(~ the query 'Jal11es,' we first cOlnpute the hash\nvalue of the terrn; this is 1000. 'fhen we scan the signature file a11(1 find rnatch-\ning index recol~ds. As \\lve can see fronl Figure 27.6, the signatures of all records\nhave the first bit set. We retrieve all doculnents and check for false positives;\nthe only false positive for this query is docurnent with rid 2. (lJnfortunately,\nthe ha..shed value of the terrn 'agent' also happened to set the very first bit in\nthe signature.)\nC~onsider the query \\Jarnes' And 'Bond.'\n~rhe query signature\nis llOO and three docurnent signatures rnatch the query signature. Again, \\ve\nretrieve one false positive.\nAs another exarnple of a conjunctive query, con-\n\nIR andXJvfL Data\n939\nsider the query 'rnovie' And ':Nladison.' The query signature is 0011, and only\none doclunent signature lnatches the query signature.N0 false positives are\nretrieved.\nNote that for each query we have to scan the cOlllplete signature file, and there\nare a\" 11lany records in the signature file as there are documents in the database.\nTo reduce the anlount of data that has to be retrieved for each query, we can\nvertically partition a signature file into a set of bit slices, and we call such an\nindex a bit-sliced signature file. The length of each bit slice is still equal to\nthe number of doculllents in the database, but for a query with q bits set in\nthe query signature we need only to retrieve q bit slices. The reader is invited\nto construct a bit-sliced signature file and to evaluate the exarnple queries in\nthis paragraph using the bit slices.\n27.4\nWEB SEARCH ENGINES\nWeb search engines rIlust contend with extreruely large nurubers of doculllents,\nand have to be highly scalable. Docurnents are also linked to each other, and\nthis link infonnation turns out to be very valuable in finding pages relevant\nto a given search.\nThese factors have caused search engines to differ frorn\ntraditional IR systerns in irnportant ways. Nonetheless, they rely on sorne forn1\nof inverted indexes as the basic indexing mechanism. In this section, we discuss\nWeb search engines, using Google as a typical example.\n27.481\nSearch Engine Architecture\n'Veb search engines crawl the web to collect docurnents to index. 'Ihe crawling\nalgorithrn is sirrlple, but crawler software can be cornplex because of the details\nof connecting to millions of sites, minimizing network latencies, parallelizing\nthe crawling, dealing with tirneouts and other connection failures, ensuring\nthat crawled sites are not unduly stressed by\nthE~ cra\\vler, and other practical\nconcerns.\nThe search algorithrn used by a crawler is a graph traversal.\nStarting at a\ncollection of pages with rnany links (e.g. ,Yahoo directory pages), all links on\ncra\\vled pages a,re follo\\\\red to identify ne\\v pages. This step is iterated, keeping\ntrack of which pages have been visited in order to avoid re-visiting thenl.\nThe collection of pages retrieved through crawling can be enonnous, on the\norder of billions of pages. Indexing thern is a very expensive ta\"sk. Fortunately,\ntlle ta\",sk is highly parallelizable:\nEach docurnent is independently arlalyzed\nto create inverted lists for the terrns that appear in the docurnent.\nT'hese\nper-doCUlnent lists are then sorted b~y terrn and luerged to crcclte cornplete per-\n\n940\nCHAPTER '47\nterrn inverted lists that span all dOCllrnents. Ternl statistics such as IDF can\nbe cornputed during the lnerge phase.\nSupporting searc~hes over such vast indexes is another luanulloth undertaking.\nFortunately, again, the task is readily parallelized using a cluster of inexpensive\nInachines: \\Ve can deal with the anlount of data by partitioning the index across\nseveral rnachines.\nEach Inachine contains the inverted index for those terms\nthat are Inapped to that luachine (e.g., by hashing the tenn).\nQueries 111ay\nhave to be sent to luultiple Inachines if the terrl1S they contain are handled by\ndifferent rnachines, but given thatvVeb queries rarely contain rnore than two\nterrns, this is not a serious probleln in practice.\nWe rnust also deal \\vith a huge volume of queries; Google supports over 150\nlllillion searches each day, and the nUlnber is growing.\nThis is acc(nnplished\nby replicating the data across several machines. vVe already described how the\ndata is partitioned across Inachines. For each partition, we now a..ssign several\nnlachines, each of which contains an exact copy of the data for that partition.\nQueries on this partition can be handled by any rnachine in the partition.\nQueries can be distributed across rnachines on the basis of load, by hashing on\nIP addresses, etc. Replication also addresses the problern of high-availability,\nsince the failure of a Inachine only increases the load on the remaining rnachines\nin the partition, and if partitions contain several rnachines the ilnpact is sIuall.\nFailures can be rnade transparent to users by routing queries to other Inachines\nthrough the load balancer.\n27.4.2\nUsing Link Information\nwebpages are created by a variety of users for a variety of purposes, and their\ncontent does not always lend itself to effective retrieval.\nThe rnost relevant\npages for a search rnay not contain the search terrns at all and are therefore\nnot returned by a boolean keyvvord search! For exarnple, consider the query\nternl 'Web browser.' A boolea,11 text query using the tenns does not return the\nrelevant pages of Netscape Corporation or }\\trierosoft, because these pages do\nnot contain the terrn 'Web browser' at all. Sirnilarly, the horne page of 'Yahoo\ndoes not contain the terrn 'search engine.' The problenl is that relevant sites\ndo not necessarily describe their contents in a ¥,ray that is llseful for boolean\ntext queries.\nUntil no\\v, ¥,re only considered infonnation 'within a single \\vebpage to estirnate\nits relevance to a query. But webpages are connected through h:yperlinks, and\nit is quite likely that there is a \\VebpEtge containing the terrn 'search engine'\nthat has a link to Yahoo's horne page. Can we use the inforrnation hidden in\nsuch links'?\n\nIII aTul X1vIL Data\n~41\nBuilding on research in the sociology literature, an interesting analogy between\nlinks and bibliographic citations suggests a \\vay to exploit link infoI'Ination: Just\nas influential authors and pubications are cited often, good 'Vvebpages are likely\nto be often linked to. It is useful to distinguish between two types of pages,\nauthorities and hubs. An authority is a page that is very relevant to a certain\ntopic and that is recognized by other pages as authoritative on the subject.\nThese other pages, called hubs, usually have a significant nUll1ber of hyperlinks\nto authorities, although they\ntlH~nlselves are not very well known and do not\nnecessarily carry a lot of content relevant to the given query. Hub pages could\nbe cOlnpilatiol1s of resources about a topic on a site for professionals, lists of\nreco111mended sites for the hobbies of an individual user, or even a part of the\nbookIllarks of an individual user that are relevant to one of the user's interests;\ntheir Blain property is that they have IHany outgoing links to relevant pages.\nGood hub pages are often not well known and there may be few links pointing\nto a good hub. In contrast, good authorities are 'endorsed' by rnany good hubs\nand thus have many links froln good hub pages.\nThis symbiotic relationship between hubs and authorities is the basis for the\nHITS algoritlun, a link-based search algorithm that discovers high-quality pages\nthat are relevant to a user's query terrns. The HITS algorithIll rnodels \\iVeb as a\ndirected graph. Each webpage represents a node in the graph, and a hyperlink\nfroIn page A to page B is represented as an edge between the two corresponding\nnodes.\nAssulne that we are given a user query with several terIns.\nThe algorithIll\nproceeds in two steps. In the first step, the sarnpling step, we collect a set of\npages called the base set. The ba..se set 11lOSt likely includes very relevant pages\nto the user's query, but the base set can still be quite large. In the second step,\nthe itera\"tion step, we find good authorities and good hubs arnong the pages in\nthe ba..\"c set.\nThe salnpling step retrieves a set of webpages that contain the query terrns,\nusing sorne traditional technique. For exarnple, 'we can evaluate the query et.e;\na boolean key-word search and retrieve all webpages that contain the query\nterrns. vVe call the resulting set of pages the root set. 1'he root set Inight not\ncontain all relevant pages because senne <\\'uthoritative pages rnight not include\nthe user query \\vords. But \\ve expect that at lea.\"t SOlne of the pages in the root\nset contain hyperlinks to the rnost relevant authoritative pages or that SCHne\nauthoritative pages link to pages in the root set. rrhis rnotivates our notion of\na link page. \\¥e call a page a link page if it ha..\" a hyperlink to sorne page in\nthe root set or if a page in the root set has a hyperlink to it. In order not to\nIniss potentially relevant pa,ges, \\ve auglnent the root set by all link pages and\nvve call the resulting set of pages the base set.\nrrhus~ the base set includes all\n\n942\nCHAPTER i2.7\nroot pages and all link pages; \\ve refer to a webpage in the base set as a base\npage.\nOur goal in the second step of the algorithrn is to find out which base pages are\ngood hubs and good authorities and to return the best authorities and hubs\na,.\" the answers to the query. To quantify the quality of a base page as a hub\nand as an authority, we associate vvith each base page in the base set a hub\nweight and an authority weight. The hub weight of the page indicates the\nquality of the page as a hub, and the authority weight of the page indicates\nthe quality of the page as an authority. We cornpute the weights of each page\naccording to the intuition that a page is a good authority if rnany good hubs\nhave hyperlinks to it, and that a page is a good hub if it has rnany outgoing\nhyperlinks to good authorities. Since we do not have any a priori knowledge\nabout which pages are good hubs and authorities, we initialize all weights to\none. We then update the authority and hub weights of base pages iteratively\nas described below.\nConsider a base page p with hub weight hp and with authority weight ap' In\none iteration, we update ap to be the SU1U of the hub weights of all pages that\nhave a hyperlink to p. Formally:\nap =\nL\nhq\nAll base pages q that have a link to p\nAnalogously, we update hp to be the SUlll of the weights of all pages that p\npoints to:\nhp ==\nL\naq\nAll base pages q such that p has a link to q\nCornparing the algorithrn with the other approctches to querying text that\nwe discussed in this chapter, we note that the iteration step of the lIlT'S\nalgorithln-·m·the distribution of the weights·· does not take into a,ccount the\n'Vvards on the ba,,'3c pages. In the iteration step, \\ve are only concerned about\nthe relationship between the base pages as represented by hyperlinks.\nl'he lIlTS algorithrIl usually produces very good results. For exarnple, the five\nhighest ranked results frorn (ioogle (\"rhich uses a variant of the lIIT'S algorithrn)\nfar the query '.R,aghuH.arnakrishnan' are the follo'Vving webpages:\nwww.cs.wisc.edu/~raghu/raghu.html\nwww.cs.wisc.edu/~dbbook/dbbook.html\nwww.informatik.uni-trier.de/\n~ley/db/indices/a-tree/r/Ramakrishnan:Raghu.html\nwww.informatik.uni-trier.de/\n\nIR and XlvlL IJata\nComputing bub and authority weights: We can use luatrix notation\nto write the updates for allhllband cLuthotity weights in orle step. Assume\nthat we nUluber aU pages in the base set {I, 2, \"'1 n}.The·adjaeCIJ.CY matrix\nB of the ba.se set is an n x n matrix whose entries are either Oor 1. The\nrnatrix entry (i, j) is set to 1 if page 'ihas a hyperlink to page j; it is set\nto 0 otherwise. We can also write the hub weightshand authority weights\na in vector notat.ion: h == (h1, ... ,hn ) and a == (al,'\" ,an)' We can now\nrewrite our upda,te rules as follo\\vs:\nh :::;: B . a,\nand\na =::BT . h .\nUnfolding this equation once, corresponding to the first iteration, we ob-\ntain:\nh == BBTh:::;: (BBT)h,\nand\na == BTBa == (BTB)a .\nAfter the second iteration, we arrive at:\nResults from linear algebra tell us that the sequence of iterations for the\nhub (resp.\nauthority) weights converges to the principal eigenvectors of\nBET (resp.\nBTB) if we normalize the weights before each iteration so\nthat the suru of the squares of all weights is always 2 . n.\nFurthermore,\nresults from linear algebra tell us that this convergence is independent of\nthe choice of initial weights, as long as the initial weights are positive.\nThus, our rather arbitrary choice of initial weights··--we initialized all hub\nand authority weights to\nl-·u·-~does not change the outcolne of the algorithm.\n._---~--_\n_ _--------------'\nGoogle's Pigeon Rank: Google corIlputes the pigeon rank (PRJ for a\nwebpage A using the following forrIlula, which is very sirnilar to the H.ub-\nAuthority ranking functions:\n'T1 ... Tn are the pages that link (or 'point') to A, C(Ti ) is the rllllnber of\nlinks going out of page Ti , and d is a heuristically chosen constant (Google\nuses 0.85). Pigeon ranks fOflll a probability distribution over all webpages;\nthe Slun of ranks over all pages is 1. If we consider a rnodel of user behavior\nin which a user randornly chooses a page and then repeatedly clicks on links\nuntil he gets bored and randoll1ly chooses a new page, the probability that\nthe user visits a page is its Pigeon rank. 1]le pages in the result of a search\nare ranked using a cornbination of an lIt-style relevance l11etric and Pigeon\nrank.\n\n944\nCHAPTER ;27\n..........__ -_\n_--~\n_~\n_..__\n_~..~,----_\n~_.._-_._ __._ _-_ _-_ _ _._-,\nSQL/M:rvl: Full Text 'Fun text.' is described as data that can be searched,\nunlike simple\ncharac~ter strings, and a ne\\v data type called FullText is\nintroduced to support it. The Inethods associated 'with this type support\nsearching for individual \\vords, phrases, words that 'sound like' a query\nterlll, etc. Three 11lethods are of particular interest. CONTAINS checks if a\nFullText object contains a specified search terln (word or phrase).\nRANK\nreturns the relevance rank of a FullText object with respect to a specified\nsearch terln. (I-Iow the rank is defined is left to the hnplementation.) IS\nI\nABOUT detennines whether the FullText object is sufficiently related to\nI\nI\nthe specified search term.\n(The behavior of IS ABOUT is also left to the\nI\ni\niInplelllentation.)\n,\nL\nRelational DB1'ISs fnnn IBIvI, Microsoft, and Oracle all support text fields'~j\nalthough they do not currently conforrll to the SQL/JV1NI standard.\n--~..\n-\"~\n'\"\n-\n-\n-\n-\n-ley/db/indices/a-tree/s/Seshadri:Praveen.html\nwww.acm.org/awards/fellows_citations_n-z/ramakrishnan.htmI\nThe first result is Rarnakrishnan's horne page; the second is the horne page for\nthis book; the third is the page listing his publications in the popular DBLP\nbibliography; and the fourth (initially puzzling) result is the list of publications\nfor a forrner student of his.\n27.5\nMANAGING TEXT IN A DBMS\nIn preceding sections, we saw how large text collections are indexed and queried\nin JR, systerns and vVeb search engines. We now consider the additional chal-\nlenges raised by integrating text data into databa\",se systerns.\nThe basic approach being pursued by the SQI.I standards cornrnunity is to treat\ntext docllrnents a..'S a ne\\v data type, FullText, that can appear as the value of a\nfield in a table. If \\ve define a table w'ith a single cohunn of type FullText, each\nrow in the table corresponds to a docurnent in a dOClllnent collection. IVlethods\nof Fulll'ext can be llsed in the WHERE clause of SQL queries to retrieve rows\ncontaining text objects that Inatch an IR-style search criterion. The relevance\nrank of a FullText object can be explicitly retrieved using the RANK rnethod,\nfUld this can be llsed to sort results by relevance.\nSeveral points ruust be kept in rnind as '\\Te consider this approach:\nIi\nThis is an extrernely general (l,pproach, a,nel the perforlnance of a SC~L sys-\ntern that supports such an extension is likely to be inferior to a specialized\nIII SystCIIl.\n\nIR and J){lVfL Data\n945\n•\nThe rnodel of data does not ad.equately reflect docurnents with additional\nrnetadata. If \"\\ve store docurnents in a table with a FullText colurnn and\nuse additional cohl1nns to store rnetadata--for exarnple, author, title, SUIll-\nInary, rating, popularitY---~'relevancerneasures that cornbine nletadata 'with\nIR, similarity rnea..'3ures 11lUSt be expressed using lle\\V user-defined rneth-\nods, because the RANK rnethod only has access to the F\\lllText object, and\nnot the rnetadata.\nThe ernergence of XML docurnents, which have non-\nuniforrn, partial rlletadata, further cornplicates nlatters.\n•\nThe handling of updates is unclear. As we have seen, IR indexes are corll-\nplex, and expensive to 111aintain. Requiring a systern to update the indexes\nbefore the updating transaction cOl1ullits can irnpose a severe perfonnance\npenalty.\n27.5.1\nLoosely Coupled Inverted Index\nThe irrlplenlcntation approach used in current relational DBMSs that support\ntext fields is to have a separate text-search engine that is loosely coupled to the\nDBMS. The engine periodically updates the indexes, but provides no transac-\ntional guarantees. Thus, a transaction could insert (a row containing) a text\nobject and cornrnit, and a subsequent transaction that issues a. Inatching search\nmight not retrieve the (row containing the) object.\n27.6\nA DATA MODEL FOR XML\n.Aswe saw in Section 7.4.1, XML provides a way to rnark up a docurnent\nwith rneaningful tags that irnpart SaIne partial structure to the docurnent.\nSe'fnistrtLctured data rnodels, which we introduce in this section, capture rnuch\nof the structure in X~fL doculnents, while abstracting away Inany deta.ils. 1\nSernistructured data Inodels have the potential to serve as a forInal foundation\nfor XlVIL and enable us to rigorously define the sernantics of queries over XlVIL,\nwhich we disc-uBs in Section 27.7.\n27.6.1\nMotivation for Loose Structure\nConsider a set of doculnents on the Web that contain hyperlinks to other doc-\nUHlents. These docurnents, although not eornpletely\nunstructurecl~ cannot be\nrnodeled naturally in the relational data rnodel because the pattern of hyper-\nlinks is not regular across docurnents.\nIn fa,ct, every HTl\\1L docurnent ha.s\n1.An iruportant aspect of XIvII..; tha.t is not captured is the ordering of elements. A more complete\ndata model called XData ha...,,; been proposed by the \\;V:3C committee that is developing XivtL standards,\nbut we do not discuss it here.\n\n946\nCHAPTER 27\nr-\"~---\"\"'-\"\n-...---.-----.---..........---..-----....\nI XML Data Models: 'A number of data lnodels for XML are being con-\nsidered by standards COilllnittees such as ISO and W3C.vV3C's Infoset\nis a tree-structured ll10del, and each node can be retrieved through an\naccessor function. A version called Post-Validation Infoset (PSVI)\nserves as the data model for XML Schelna.\nTheXQuery language has\nyet another data model associated with it. The plethora of l110dels is due\nto parallel developrnent in SOllle cases, and due to different objectives in\nothers. Nonetheless, all these nlodels have loosely-structured trees as their\ncentral feature.\nL--\n.\n.\n,\n,\n.-1\nsome minirnal structure, such as the text in the TITLE tag versus the text in\nthe docunlent body, or text that is highlighted versus text that is not. As an-\nother example, a bibliography file also has a certain degree of structure due to\nfields such as author and title, but is otherwise unstructured text. Even data\nthat is 'unstructured', such as free text or an ilnage or a video clip, typically\nhas some associated information such as timestamp or author infornlation that\ncontributes partial structure.\nWe refer to data with such partial structure as semistructured data. There\nare rnany reasons why data might be semistructured. First, the structure of\ndata nlight be irnplicit, hidden, unknown, or the user Inight choose to ignore\nit.\nSecond, when integrating data froln several heterogeneous sources, data\nexchange and transforrnation are inlportant problerns. We need a highly flexible\ndata rnodel to integrate data froIn all types of data sources including flat files\nand legacy systenls; a structured data model such a\",s the relational rnodel is\noften too rigid. Third, we cannot query a structured database without knowing\nthe scheIna, but sOlnetimes we want to query the data without full knowledge of\nthe scherna. For exarnple, we cannot express the query \"Where in the database\ncan ,ve find the string Malgv.d'i?\"\nin a relational database systern \\vithout\nknowing the schcrna, and knowing which fields contain such text values.\n27.6.2\nA Graph Model\nAll data rnodels proposed for sernistrnctured data represent the data as scnne\nkind of labeled graph. Nodes in the graph correspond to cornpound objects or\natornic values.. Each edge indicates an object-subobject or object-value rela-\ntionship. Leaf nodes, Le; nodes with no outgoing edges have a value a.ssociatecl\n\\vith thern. rrhere is no separate scherna and no auxiliary description; the data\nin the graph is self describing. For exarnple, consider the graph shown in Figure\n27.7, which represents part of the XlVIL data f1'01n Figure 7.2. The root node\nof the graph represents the outennost elernent, BOOKLIST. The node has three\nchildren that are labeled with the elClnent narne BOOK, since the list of books\n\nIR and )(lvfL Data\nRichard\nFeynman\n('-;-~'\\ BOOKUST\n,\"~\"\"'~\\.\"-_/'~';::'':;::::::::::::_,,-~---,,\", -\"\n--..,-... ~....~\n-\"'-._--....................-\nWaiting\nlor the\nMahatma\nR.K.\nNarayan\nFigure 27.7\nThe Semistructured Data :NIodel\n\"',/'\\\n(\n13\n; PUBLISHED\n\\\n!r\n1981\n9»17\nconsists of three individual books. The numbers within the nodes indicate the\nobject identifier associated with the corresponding object.\nWe now describe one of the proposed data models for semistructured data,\ncalled the object exchange model (OEM). Each object is described by a\nquadruple consisting of a label, a type, the value of the object, and all. object\nidentifier which is a unique identifier for the object. Since each object has a\nlabel that can be thought of as a column nallle in the relational model, and each\nobject has a type that can be thought of as the column type in the relational\nrnodel, the object exchange Illodel is self-describing. Labels in OEM should be\n&'3 infol'rnative as possible, since they serve two purposes they can be used to\nidentify an object as well as to convey the meaning of an object. For example,\nwe can represent the last HaIne of an author as follows:\n(lastName, string,\n\"Feynman\")\nMore cOInplex objects are decornposed hierarchically into srnaller objects. For\nexalllple, a,n author naIne can contain a first narne and a last narne. rrhis object\nis described as follows:\n(authorName, set, {fiTstnarnel, lastnaTnel})\nfiTsi:nantel is (firstName, string, \"Richard\")\nlastnarnef is (lastName, string,\n\"Feynman\")\nAs another exarnple, an object representing a set of books is described a\"s fol-\nlows:\n(bookList, set,\n{book1 , book2 , book;3})\nbook} is (book, set,\n{(L'U,thOTl, title}, p'u,blishcd1})\n\n948\nCHAPTER 2*7\n'-~--\n.._......•.._._-_.-._._--\n---'\" _.-_.__._\n__..__\n_.__\n_..~\nSQL and XML: XQuery is a standard proposed by the vVorld-vVide Web\n1\nConsortiurn (W3C). In parallel, standards con:unittees developing the SQL I\nstandards have been working on a successor to SQL:1999 that supports\nX~fL. 1\"'he part that relates to X:NIL is tentatively called SQL/XML and\ndetails can be found at http: / / sqlx . arg.\n.\n_ ..__\n._._~._.._\n_\n__\n_ _l\nbo()k~2 is (book, set, {attthor2, t-itle2, ]Jubl'ished2 })\nbook3 is (book, set, {aui:hoT:3, t'itle3,Published3})\nauthoT3 is (author, set, {!'lrstnarnJe3, lastna'Tne3})\nt'itle3 is (title, string,\nliThe English Teacher\")\npttbl'ished3 is (published, integer,\n1980)\n27.7\nXQU'ERY: QUERYING XML DATA\nGiven that XlvII.. doculnents are encoded in a way that reflects (a consider-\nable amount of) structure, we have the opportunity to use a high-level lan-\nguage that exploits this structure to conveniently retrieve data fro111 within\nsuch documents. Such a language would also allow us to easily translate XML\ndata between different DTDs, as we lllUSt when integrating data from multiple\nsources.\nAt the tirne of writing of this book, XQuery is the W3C standard\nquery language for XML data.\nIn this section, we give a brief overview of\nXQuery.\n27.7.1\nPath Expressions\nConsider the XlvII.. dOCUlnent shown in Figure 7.2. The following exarnple query\nreturns the last nanles of all authors, assullling that our XML docurnent resides\nat the location www.ourbookstore.com/books .xml.\nFOR\n$1 IN doc(www.ourbookstore.com/books.xml)//AUTHOR/LASTNAME\nRETURN <RESULT> $1 </RESULT>\nThis exarnple illustrates sonle of the basic constructs of X(~uery.\nThe FOR\nclause in XQuer:y is roughly analogous to the FROM clause in SC:~L. The RETURN\nclause is sirnilar to the SELECT clause. \\Ve return to the general fornl of queries\nshortly, after introducing an irnportant concept called a path expression.\n'1'he expression\ndoc(www.ourbookstore.com/books.xml)//AUTHOR/LASTNAME\n\nIR and ./YA1L ]Jata\n949\nj)\nXPath and Other XML Query Languages:\nPath expressions in\nXQuery are derived frorn XPath, an earlier Xl\\:IL query facility. Path ex-\npressions in XPath can be qualified ,vith selection conditions, and can uti-\nlize several built-in functions (e.g., counting the nurnber of nodes rnatched\nby the expression). l\\tlany of XQuery's features areborro\\vecl {roIn earlier\nlanguages, including XML-QL and Quilt.\nr'~'~~~\"\"\"\"\"\"~\"\"'\"\n!\nI\nII\nI\nin the FOR clause is an exarnple of a path expression.\nIt specifies a path\ninvolving three entities:\nthe docurnent itself, the AUTHOR elernents and the\nLASTNAME elernents.\nThe path relationship is expressed through separators / and / /.\nThe sep-\narator / / specifies that the AUTHOR elernent can be nested anywhere within\nthe document whereas the separator / constrains the LASTNAME elernent to be\nnested immediately under (in terms of the graph structure of the docurnent)\nthe AUTHOR element.\nEvaluating a path expression returns a set of elernents\nthat rnatch the expression. The variable l in the example query is bound in\nturn to each LASTNAME elernent returned by evaluating the path expression.\n(To distinguish variable naInes fronl normal text, variable narnes in XQuery\nare prefixed with a dollar sign $.)\nThe RETURN clause constructs the query result-·--which is also an XML docurnent-···_·\nby bracketing each value to which the variable l is bound with the tag RESULT.\nIf the exanlple query is applied to the sarnple data shown in Figure 7.2, the\nresult would be the following X:NIL docurnent:\n<RESULT><LASTNAME>Feynman </LASTNAME></RESULT>\n<RESULT><LASTNAME>Narayan </LASTNAME></RESULT>\nWe use the docurnent in Figure 7.2 as our input in the rest of this chapter.\n27.7.2\nFLWR Expressions\nThe\nba~sic fornl of an\nX(~uery consists of a FLWR expression, where the\nletters denote the FOR, LET, WHERE and RETURN clauses.\nThe FOR etHel LET\nclauses bind variables to values through path expressions.\nThese values are\nqualified by the WHERE clause, and the result .XlVIL fragrnent is constructed by\nthe RETURN clause.\nrrhe difference between a FOR and LET clause is that while FOR binds a variable\nto each elernent specified by the path expression, LET binds a variable to the\nwhole collection of elernents. T'hus, if we change our exarnple query to:\n\n950\nCHAPTER 2;;\nLET\n$IINdoc(www.ourbookstore.com/books.xm1)//AUTHOR/LASTNAME\nRETURN <RESULT> $1 </RESULT>\nthen the result of the query beconles:\n<RESULT>\n<LASTNAME>Feynman</LASTNAME>\n<LASTNAME>Narayan</LASTNAME>\n</RESULT>\nSelection conditions are expressed using the WHERE clause. Also, the output of\na query is not lirnited to a single elernent. These points are illustrated by the\nfollowing query, which finds the first and last names of all authors who wrote\na book that was published in 1980:\nFOR $b IN doc(www.ourbookstore.com/books.xm1)/BOOKLIST/BOOK\nWHERE $b/PUBLISHED='19S0'\nRETURN\n<RESULT> $b/AUTHOR/FIRSTNAME, $b/AUTHOR/LASTNAME </RESULT>\nThe result of the above query is the following XML docurnent:\n<RESULT>\n<FIRSTNAME>Richard </FIRSTNAME><LASTNAME>Feynman </LASTNAME>\n</RESULT>\n<RESULT>\n<FIRSTNAME>R.K. </FIRSTNAME><LASTNAME>Narayan </LASTNAME>\n</RESULT>\nFor the specific DTI) in this exalnple, where a BOOK elernent hac; only one\nAUTHOR, the above query can be written by using a different path expression in\nthe FOR clause, as follows.\nFOR $a IN\ndoc(www.ourbookstore.com/books.xml)\n/BOOKLIST/BOOK[PUBLISHED='19S0']/AUTHOR\nRETURN <RESULT> $a/FIRSTNAME, $a/LASTNAME </RESULT>\nrrhe path expression in this query is an instance of a branching path ex-\npression. The variable l is now bound to every AUTHOR elernent that rnatches\nthe path doc/BOOKLIST/BOOK/AUTHOR where the intennediate BOOK elClnent is\nconstrained to have a PUBLISHED elernent nested inunediatelv within it with\n./\nthe value 1980.\n\nIR and ){NfL IJata\n27.7.3\nOrdering of Elements\n951\nXML data consists of ordered doculnents and so the query language IllUSt return\ndata in SOUle order. The selnantics of X(~uery is that a path expression returns\nresults sorted in document order. Thus, variables in the FOR clause are bound\nin doculnent order. If however, we desire a different order, we can explicitly\norder the output as shown in the follo\\ving query, which returns TITLE elernents\nsorted lexicographically.\nFOR\n$b IN doc(www.ourbookstore.com/books.xml)/BOOKLIST/BOOK\nRETURN <BOOKTITLES> $b/TITLE </BOOKTITLES>\nSORT BY TITLE\n27.7.4\nGrouping and Generation of Collection Values\nOur next example illustrates grouping in XQuery, which allows us to generate\na new collection value for each group.\n(Contrast this with grouping in SQL,\nwhich only allows us to generate an aggregate value (e.g., SUM) per group.)\nSuppose that for each year we want to find the last narnes of authors who\nwrote a book published in that year.\nWe group by year of publication and\ngenerate a list of la..\"t names for each year:\nFOR $p IN DISTINCT\ndoc(www.ourbookstore.com/books.xml)/BOOKLIST/BOOK/PUBLISHED\nRETURN\n<RESULT>\n$p,\nFOR $a IN DISTINCT /BOOKLIST/BOOK[PUBLISHED=$pJ/AUTHOR\nRETURN $a\n</RESULT>\nThe keyword ,DI8TINC}T elirninates duplicates fronl the collection returned by\na, path expression. Using the XML docurnent in Figure 7.2 as input, the above\nquery produces the following result:\n<RESULT>: <PUBLISHED>1980</PUBLISHED>\n<LASTNAME>Feynman</LASTNAME>\n<LASTNAME>Narayan</LASTNAME>\n</RESULT>\n<RESULT> <PUBLISHED>1981</PUBLISHED>\n<LASTNAME>Narayan</LASTNAME>\n</RESULT>\n\n952\nCHAPTER 2¥\"f\n27.8\nEFFICIENT EVALUATION OF XML QUERIES\nX.Query operates on XKJIL data and produces XTvIL data as output. In order to\nbe able to evaluate queries efficiently, we need to address the follo\\ving issues.\n•\nStorage: \\Ve can use an existing storage systerIl like a relational or object\noriented systerll or design a new storage forInat for X1tIL doclunents. There\nare several ways to use a relational systenl to store XML. One of thern is\nto store the X1VIL data as Character Large Objects (CLOBs).\n(CLOBS\nwere discussed in Chapter 23.)\nIn this case, hc)\\vever, we cannot exploit\nthe query processing infrastructure provided by the relational systerrl and\nwould instead have to process XQuery outside the database systenl.\nIn\norder to circumvent this problenl, we need to identify a scherna according\nto which the XML data can be stored.\nrr'hese points are discussed in\nSection 27.8.1.\n•\nIndexing: Path expressions add a lot of richness to XQuery and yield\nlllany new access patterns over the data. If we use a relational system for\nstoring XML data, then we are constrained to use only relational indexes\nlike the B-n·ee. However, if we use a native storage engine, then we have\nthe option of building novel index structures for path expressions, some of\nwhich are discussed in Section 27.8.2.\n•\nQuery Optimization:\nOptirnization of queries in XQuery is an open\nproblern. The work so far in this area can be divided into three parts. 'rhe\nfirst is developing an algebra for XQuery, analogous to relational algebra.\nThe second research direction is providing statistics for path expression\nqueries. Finally, SOlne work has addressed sirnplification of queries by ex-\nploiting constraints on the data. Since query optirnization for X.Query is\nstill at a prelirninary stage, we do not cover it in this chapter.\nAnother issue to be considered while designing a ne\\v storage systeul for X1.1L\ndata is the verbosity of repeated tags.\nAs we see in Section 27.8.1) using a\nrelational storage systelu addresses this problern since tag narnes are not stored\nrepeatedly. If on the other hand, we \\vant to build a native storage systcrn, then\nthe rnanner in which the X.~lL data is cornpressed becornes significant. Several\ncornpression. algorithrHs aTe known that achieve cOlnpression ratios close to\nrelational storage, 1n1t \\ve do not discuss ther11 here.\n27.8.1\nStoring XML in RDBMS\n()n(~ nEttural candidate for storing X1JIL data is a relational dataJKlse systern.\nThe ruain issues involved in storing XJ\\;IL data in a relational systelTI are:\n\nIR and XlvIL Data\n953\n},)\nCommercial database systems and XML: wiany relational and objeet-\nrelational database systerll vendors are currently looking into support for\nXML in their database engines. Several vendors of object-oriented database\n!\niI!\nInanagenlent systems already offer database engines that can store XML\ni\ndata 'whose contents can be accessed through graphical 11ser interfaces orJI\nserver-side Java extensions.\n+++-,~~-~,.~.\"-_.~.. ~\n~ _ _\n_._\n_._~._._-_.\ngenre\nformat\nTITLE\nPUBLISHED\nAUTHOR\n~~\nFIRSTNAME\nLASTNAME\nFigure 27.8\nBookstore X:NIL DTD Element Relationships\nII\nC;hoice of relational scherr~a: In order to use an RDBMS, we need a scherna.\nvVhat relational schema should we use even assuming that the XML data\nCOUles with an &'Ssociated scherna?\nII\nQueries: Queries on XML data are in XQuery wherea\" a relational systern\ncan only handle\nS(~L. Queries in XQuery therefore need to be translated\ninto SQL.\nIII\nIlecon8tructioT~:\nrrhe output of XQuery is X~1L. T\n1hus, the result of a S(~L\nquery needs to be converted back into XNIL.\nMapping XML Data to Relations\n\\lVe illustrate the rnapping process through our bookstore exarnple. rrhe nesting\nrela,tionships alIlong the different elernents in the DTD is sho\\vn in Figure 27.8.\nrrhe edges indicate the nature of the nesting.\n()ne way to derive a l'clation.al schelna is as follovvs. \\Ve begin at the BOOKLIST\nelernent anel create\n(1 relation to store it. rrraversing down froIn BOOKLIST, we\nget BOOK foIlc)\\ving (l. * edge. This edge indic.ates that \\~le store the BOOK elernents\nin a separate reJation. l'r;:l,versing further down, we see that all elcrnents and\n\n954\nCHAPTER 27\nattributes nested \\vithin BOOK occur at 1I10St once. Hence, we can store thenl\nin the saIne relation a..\" BOOK. The resulting relational schelIla Relschernal is\nshown below.\nBOOKLIST(id: integer)\nBOOK (booklistid: integer, author_firstna'me: string,\nauthor_lastnarne: string, title: string,\npublished: string, genre: string, format: string)\nBOOK. booklistid connects BOOK to BOoKLIST. Since a DTD has only one base\ntype, string, the only base type used in the above schelna is string. The\nconstraints expressed through the DTD are expressed in the relational schema.\nFor instance, since every BOOK must have a TITLE child, we Illust constrain the\ntitle column to be non-null.\nAlternatively, if the DrrD is changed to allow BOOK to have more than one\nAUTHOR child, then the AUTHOR elements cannot be stored in the sallie relation\nas BOOK. This change yields the following relational schema Relschema2.\nBOOKLIST(id: integer)\nBOOK (id: integer, booklistid: integer,\ntitle: string, published: string, genre: string, for1nat: string)\nAUTHoR( bookid: integer, firstname: string, lastname: string)\nThe column AUTHOR. bookid connects AUTHOR to BOOK.\nQuery Processing\nConsider the following example query again:\nFOR\n$b IN doc(www.ourbookstore.com/books.xml)/BOOKLIST/BooK\nWHERE $b/PUBLISHED='1980'\nRETURN\n<RESULT> $b/AUTHOR/FIRSTNAME,\n$b/AUTHOR/LASTNAME </RESULT>\nIf the nlapping between the XML data and relational tables is known, then\nwe can construct a SQL query that returns all colunlIls that are needed to\nreconstruct the result XlvIL docuIIlent for this query. (JonditioIlS enforced by\nthe path expressions and the WHERE clause are translated into equivalent con-\nditions in the S(~L query. VVe obtain the following equivalent SQL query if we\nuse llel,C3chernal El...'3 our relational scherna.\nSELECT BOOK. author.J: irstname,\nBOOK. author~astname\n\nSELECT\nFROM\nWHERE\nIR and XML Data\nFROM\nBOOK, BOOKLIST\nWHERE\nBOOKLIST.id = BOOK.booklistid\nAND BOOK.published='1980'\nThe results thus returned by the relational query processor are then tagged,\noutside the relational systern, as specified by the RETURN clause. This is the\nresult of the reconstruct'ion phase.\nIn order to understand this better, consider what happens if we allow a BOOK\nto have 111ultiple AUTHOR children.\nAssume that we use Rel8chema2 as our\nrelational schema.\nProcessing the FOR and WHERE clauses tells us that it is\nnecessary to join relations BOOKLIST and BOOK with a selection on the BOOK\nrelation corresponding to the year condition in the above query.\nSince the\nRETURN clause needs information about AUTHOR elements, we need to further\njoin the BOOK relation with the AUTHOR relation and project the jir8tname\nand lastname columns in the latter. Finally, since each binding of the variable\n$b in the above query produces one RESULT element, and since each BOOK is\nnow allowed to have more than one AUTHOR, we need to project the id column\nof the BOOK relation. Based on these observations, we obtain the following\nequivalent SQL query:\nBOOK.id, AUTHOR. firstname , AUTHOR.lastname\nBOOK, BOOKLIST, AUTHOR\nBOOKLIST.id = BOOK.booklistid AND\nBOOK.id = AUTHOR.bookid AND BOOK.published='1980'\nGROUP BY BOOK.id\nT'he result is grouped by BOOK.id. The tagger outside the database system\nnow receives results clustered by the BOOK element and can tag the resulting\ntuples on the fly.\nPublishing Relational Data as XML\nSince XlV1L has elnerged as the standard data exchange forrnat for business\napplications, it is necessary to publish existing business data as XML. NJost\noperational business data is stored in relational systerns. Consequently, 111ech-\nanisrns have\nbe~~n proposed to publish such data as XlV1L docul11ents. These\ninvolve a language for specifying henv to tag and structure relational data and\nan irnplernentation to carry out the conversion. This 111apping is in SCHne sense\nthe reverse of the Xl\\1:L-to-relationaJ rnapping used to store XlVIL data. 'The\nconversion process Inirnics the reconstruction pha..'3c when \\ve execute XQuery\nusing a relational systern. The published Xl\\1L data can be thought of <:1...') an\n.X.lVIT..; vic¥l of relational data.\nThis view can be queried using X(~uery. One\n\n956\nCHAPTER 27\nlnethod of executing XCluery on such vie'ws is to translate thCIIl into SQL and\nthCIl construct the XrvlL result.\n27.8.2\nIndexing XML Repositories\nPath expressions are at the heart of all proposed XIVIL query languages, in\nparticular XQuery. A natural question that arises is how to index X:NIL data\nto support path expression evaluation.\nThe ainl of this section is to give a\nflavor of the indexing techniques proposed for this probleul. vVe consider the\nOENI rnodel of senlistructured data, 'where the data is self-describing and there\nis no separate scherna.\nUsing a B+ Tree to Index Values\nConsider the following XQuery exaluple, which we discussed earlier on the\nbookstore XML data in Figure 7.2. The OEM representation of this data is\nshown in Figure 27.7.\nFOR\n$b IN doc(www.ourbookstore.com/books.xml)/BOOKLIST/BOOK\nWHERE\n$b/PUBLISHED='1980'\nRETURN\n<RESULT> $b/AUTHOR/FIRSTNAME,\n$b/AUTHOR/LASTNAME </RESULT>\nThis query specifies joins alIlong the objects with labels BOOKLIST,\nBOOK,\nAUTHOR,\nFIRSTNAME,\nLASTNAME and PUBLISHED \\vith a selection condition on\nPUBLISHED objects.\nLet us suppose that \\ve are evaluating this query in the absence of any indexes\nfor path expressions. I-Io\\vever, we do have a value index such as a B-T'ree that\nenables us to find the ids of all objects with label PUBLISHED and value 1980.\nThere are several \\vays of executing this query under these a'3surnptions.\nFor instance, \\ve could begin at the docurncnt root and traverse down the data\ngraph through the BOOKLIST object to the BOOK objects. By further traversing\nthe data graph downwards, for each BOOK object we can check whether it sat-\nisfies the valuc'predicate (PUBLISHED=~1980'). Finally, for those BOOK objects\nthat satisfy the predicate, we can find the relevant FIRSTNAME and LASTNAME\nobjects. This approach corresponds to a top-down evaluation of the query.\nAlternatively, \\ve could begin by using the value index to find all PUBLISHED\nol)jects that satisfy PUBLISHED='1980'. If the data graph can be traversed in\nthe reverse directiono·that is, given an object, \\ve can find its parent-~~then we\n\nIR and -\",[AIL Data\n957;\nFigure 27.9\nPath Expressions in a B-Tree\ncan find all parents of the PUBLISHED objects retaining only those that have\nlabel BOOK. We can continue in this manner until we find the FIRSTNAME and\nLASTNAME objects of interest. Observe that we need to perforrll all joins in the\nquery on the fly.\nIndexing on Structure vs. Value\nNow let us ask ourselves whether traditional indexing solutions like the B-Tree\ncan be used to index path expressions. We can use the B-Tree to rllap a path\nexpression to the ids of all objects returned by it.\nThe idea is to treat all\npath expressions as strings and order therIl lexicographically. Every leaf entry\nin the B-Tree contains a string representing a, path expression and a list of\nids corresponding to its result.\nFigure 27.9 shows how such a B-Tree \\vould\nlook.\nLet us contrast this with the traditional problern of indexing a well-\nordered dornain like integers for point queries. In the latter case, the nurnber\nof distinct point queries that can be posed is just the rnllnber of data values\nand so is linear in the data size.\nThe scenario \\vith path indexing is fundarnentally difI(J,rent----the variety of\nways in which \\ive can cornbine tags to forrn (sirnple) path expressions\nC011-\npled with the power of placing / / separators leads to a rnuch larger nurnber\nof possible path expressions.\nFor instance, an AUTHOR clcrnent in the exarn-\npIe in Figure 21.7 is returned &'3 part of the qllcries BOOKLIST/BOOK/AUTHOR,\n/ / AUTHOR, / /BOOK// AUTHOR, BOOKLIST/ / AUTHOR and so OIl.\nThe nurnber of\ndistinct queries can in fact be exponential in the data size (lneasured in tenns\nof the rnunber of XIVIL elelnents) in the \\V01'st case. This is \\vhat rnotivates the\nseaxch for alternative strategies to index path expressions.\n\n958\nFigure 27.10\nExample Path Index\nCHAPTER 27\nThe approach taken is to represent the mapping between a path expression and\nits result by means of a structural sunlIllary which takes the fornl of another\nlabeled, directed graph. rrhe idea is to preserve all the paths in the data graph\nin the surllrnary graph, while having far fewer nodes and edges.\nAn extent\nis associated with each node in the SUllllnary.\nThe extent of an index node\nis a subset of the data nodes.\nThe surnmary graph along with the extents\nconstitutes a path index.\nA path expression is evaluated using the index by\nevaluating it against the sumrnary graph and then taking the union of the\nextents of all rnatching nodes. This yields the index result of the path expression\nquery. The index covers a path expression if the index result is the eorrect\nresult; obviously, we can use an index to evaluate a path expression only if the\nindex covers it.\nConsider the structural SUlnrnary shown in Figure 27.10. rrhis is a path index\nfor the data in Figure 27.7. Tlhe nurnbers shown beside the nodes correspond\nto the respective extents. Let us now exarnine how this index can change the\ntop-down evaluation of the exaruple query used earlier to illustrate B+ tree\nvalue indexes.\nrrhe top-down evaluation a..s outlined above begins at the docurnent root and\ntraverses down to the BOOK objects. rrhis can be achieved rnore efficiently by\nthe path index. Instead of traversing the data graph, \\ve can traverse the path\nindex down to the BOOK object in the index and look up its extent, which gives\nus the ids of all BOOK objects that rnatch the path expression in the FOR clause.\nThe rest of the evaluation then proceeds as before. 1'hus, the path index saves\nus frorn perfonning joins by essentially precorIlputing thern. VVe note here that\nthe path index shown in Figure 27.10 is isornorphic to the DTD schcIIla graph\nShO\\Vll in Figure 27.8. This drives horne the point that the path index \\vithout\nthe extents is a structural SUHllnary of the data.\n\n.lR and XlvfL Data\n9511\nrrhe ahove path index is the Strong Dataguide. If \\ve treat path expressions\nas strings, then the dataguide is the trie representing thern.\nThe trie is a\nwell-known data structure used to search regular expressions over text. This\nshows the deeper unity between the research on indexing text and the X.NIL\npath indexing work. Several other path indexes have been also proposed for\nsenli-structured data, and this is an active area of research.\n27.9\nREVIEW QUESTIONS\nAnsvvers to the review questions can be found in the listed sections.\n•\nWhat is information retrieval? (Section 27.1)\n•\nWhat are some of the differences between DBMS and IR systems? Describe\nthe differences between a ranked query and a boolean query.\n(Section\n27.2)\n•\nWhat is the vector space model, and what are its advantages? (Section\n27.2.1)\n•\nWhat is TF/IDF terrn weighting, and why do we weigh by both? We do we\neliminate stop words? What is length norrnalization, and why is it done?\n(Section 27.2.2)\n•\nHow can we measure document similarity? (Sections 27.2.3)\n•\nWhat are precision and recall, and how do they relate to each other? (Sec-\ntion 27.2.4)\n•\nDescribe the following two index structures for text: Inverted index and\nsignature file. What is a bit-sliced signature file? (Section 27.3)\n•\nllow are web search engines architected?\nIlow does the \"hubs and au-\nthorities\" a.lgorithrn work? Can you illustrate it on a srnall set of pages?\n(Section 27.4)\nl1li\n\\iVhat support is there for rnanaging text in a f)BI\\1S? (Section 27.5)\nII\nDescibe the OE~'1 data rnodel for sernistructured data. (Section 27.6)\nII\n\\iVhat are the elernents of XQuery? \\;Vhat is a path expression? What is\nan FLWR expression? flow can we order the output of query? flow do ,ve\ngroup query outputs? (Section 27.7)\n•\nDescribe how XTvIL data can be stored in a relational I)B~1S.How do we\nrna,p XrvlL data to relations? Can ,ve use the query processing infrastruc-\nture of the relational DBIvIS? ITovv do 'we publish relational data as XI\\1L?\n(Section 27.8.1)\n\n960\n•\nI-Iow do we index collections of XNIL doeunlents? \\rVhat is the difference\nbetvveen indexing on structure versus indexing on value? vVhat is a path\nindex'? (Section 27.8.2)\nEXERCISES\nExercise 27.1 Carry out the following tasks.\n1. Given an ASCII file, cOInpute the frequency of each word and create a plot siInilar to\nFigure 27.3.\n(Feel free to use public dornain plotting software.)\nRun the progralll on\nthe collection of files currently in your directory and see whether the distribution of\nfrequencies is Zipfian. How can you use such plots to create lists of stop words?\n2. The Porter stenliller is widely used, and code irnplernenting it is freely available. Down-\nload a copy, and run it on your collection of docuInents.\n3. One criticisIn of the vector space nlodel and its use in sirnilarity checking is that it treats\ntenns as occurring independently of each other. In practice, Inany words tend to occur\ntogether (e.g., ambulance and emergency). Write a program that scans an ASCII file and\nlists all pairs of words that occur within 5 words of each other. For each pair of words,\nyou now have a frequency, and should be able to create a plot like Figure 27.3 with pairs\nof words on the X-axis. Run this program on some sample docuIll€nt collections. What\ndo the results suggest about co-occurrences of words?\nExercise 27.2 Assunle you are given a docurnent database that contains SIX documents.\nAfter stemming, the docurnents contain the following ternlS:\nl-I5~i'ii~~tJ\nTerrns\n_.\n.__.._.__..._~\n1\ncar rnanufacturer rionda auto\n.__.._...__... ~..._...-\n2\nauto cornputer navigation\n3\nHonda navigation\n4\n11lanufactllrer cOlnputer IB1\\,1\n5\nIBNI personal cOInputer\n-6--·--,···,,\"\"m \".\"'\"._.._,,--\ncar Beetle V\\V\n._---------''---------_.__._.._..- __..-\nAnswer the following questions.\n1. 8ho\\v the result of creating an inverted file on the docurncnts.\n2. Show the result of creating a signature file wilh a width of 5 bits. Construct your O\\Vll\nhashing function that rnaps terms to bit positions.\n3. Evaluate the following boolea.n queries using the inverted file and the signature file that\nyou created: 'car', 'IBM' AND 'COIuputer', 'IB\"NP AND 'car', 'Illl\\'.p OR '<.tuto', and\n'IB~'I'\nAND 'cornputer' AND 'rnanufacturer'.\n4. Assurne that the query loacl against the docurnent databa.se consists of exactly the queries\nthat were stated in the previous question.\nAlso c1SS11rne that each of these queries is\nevaluated exactly OIlCC.\n(a) Design\n<1 signature file with a width of :3 bits and design a hashing function that\nminimizes the overall nurnber of false positives retrieved \\vhon evaluating the\n\nIR and )(l\\;fL Data\n961\n(b) Design a signature file with a width of 6 bits and a hashing function that IlliniInizes\nthe overall nUlnber of false positives.\n(c) Assume you want to construct a signature file.\nvVhat is the sInallest signature\nwidth that allows you to evaluate all queries without retrieving any false positives?\n5. Consider the following ranked queries: 'car, 'UH..I COIIlputer' l\nIIB~/1 car', IIBl\\l auto', and\n'IBl\\1 COIllputer rnanufacturer'.\n(a) Calculate the IDF for every tenn in the database.\n(b) For each doculnent, show its doctunent vector.\n(c) For each query, calculate the relevance of each doclunent in the database, with and\nwithout the length norrnalization step.\n(d) Describe how you would use the inverted index to identify the top two documents\nthat Illatch each query.\n(e) How would having the inverted lists sorted by relevance instead of document id\naffect your answer to the previous question?\n(f) Replace each docurnent with a variation that contains 10 copies of the same docu-\nment. For each query, recompute the relevance of each document, with and without\nthe length normalization step.\nExercise 27.3 Assume you are given the following steIIllned docurnent database:\nTerms\n....-\n-\n1\ncar car IIlanufacturer car car Honda auto\n2\nauto computer navigation\n.......\"\"',\n._---\n3\nHonda navigation auto\nmanufacturer computer IBl\\II graphics ------\n4\nf---.----\nIBM personal IBM computer IBl\\II IBl\\I! IBM IBM\n5\n6\ncar Beetle VW Honda\n_._..\nUsing this databa..'5e, repeat the previous exercise.\nExercise 27.4 You are in charge of the Genghis ('We execute fast') search engine. You are\ndesigning your server cluster to handle 500 Inillion hits a day and 10 billion pages of indexed\ndata. Each rnachine costs $1000, and can store 10 million pages and respond to 200 queries\nper second (against these pages).\n1. If you were given a budget of $500,000 dollars for purchasing Inachines, and were required\nto index all 10 billion pages, could you do it?\n2. What is the IIlinirIlurIl budget to index all pages? If you assurne that each query can\nbe answered by looking at data in just one (10 rnillion page) partition, and that queries\nare unifornlly distributed across partitions, what peak load (in nuruber of queries per\nsecond) can such a cluster handle?\n3. How would your answer to the previous question change if each query, on average, ac-\ncessed two partitions?\n4. What is the ruinirlllnl1 budget required to handle the desired load of 500 rnillion hits per\nday if all queries are on a single partition? Assurne that queries are uniforrnly distributed\nwith respect to tirTle of day.\n\n962\nCHAPTgR ~7\n5. How would your answer to the previous question change if the rllllnher of queries per day\nwent up to 5 billion hits per day? How would it change if the number of pages went up\nto 100 billion'?\n6. Assurne that each query accesses just one partition, that queries are ullifonnly distributed\nacross partitions, but that at any given tiulC, the peak load on a partition is upto 10\ntimes the average load. What is the rniniIlHlnl budget for purchasing Inachines in this\nscenario?\n7. Take the cost for rnachines [raIn the previous question and rnultiply it by 10 to reflect\nthe costs of Illaintenance, adrninistration, network bandwidth, etc. This anlount is your\nannual cost of operation. Assume that you charge advertisers 2 cents per page. What\nfraction of your inventory (i.e., the total nUlllber of pages that you serve over the course\nof a year) do you have to sell in order to make a profit?\nExercise 27.5 Assume that the base set of the HITS algorithrn consists of the set of Web\npages displayed in the following table. An entry should be interpreted as follows: Web page\n1 has hyperlinks to pages 5 and 6.\nI Webpage! Pages that this page has links to I\n1\n5 6, 7\n,\n_._..._.\n2\n5, 7\n\"\"'...........\n3\n6, 8\n_.\n4\n.-\n5\nI, 2\n.. -\n6\n1, 3\n7\n1, 2\n8\n4\n,--_._.__.\n--\n1. Run five iterations of the HITS algorithlll and find the highest ranked authority and the\nhighest ranked hub.\n2. Cornpute Google's Pigeon Rank for each page.\nExercise 27.6 Consider the following description of itelllS shown in the Eggface cornputer\nrnail-order catalog.\n\"Eggface sells hardware and software. We sell the new PalIn Pilot V for $400; its part nUlnber\nis 345. We also sell the IBM ThinkPad 570 for only $1999; its part nUIllber is :3784. Vve sell\nboth business and entertainrnent software.\nI:vlicrosoft Office 2000 has just arrived and you\ncan purchase the Standard Edition for only $140, part number 974; the Professional Edition\nis $200, part 975. '1'he new desktop publishing software from Adobe called InDesign is here\nfor only $200, part 664:. \\iVe carry the newest gaInes from Blizzard sofhvare. You can start\nplaying Diablo II for only $:30, petrt nurnber 12, and yon can purchase Starcraft for only $10,\npart nlllIlber 812.\nOur goal is cornplete cllstorner satisfaction·····-·if we don't have what you\nwant in stock, we'll give you SIO off your next purchase!\"\n1. Design an 11'r1.1L docllrnent that depicts the itelIlS offered by Eggface.\n2. Create a well-formed XrvIL doculnent that describes the contents of the Eggfi:1Ce catalog.\n:'3. Create a TYr.D for your XI:vlL docurnent and rnake sure that the docuJnent you created\nin the last question is valid with respect to this 1Y1'1),\n\nIR and .IY!v.fL Data\n963\n4. Write an XQuery query that lists all software items in the catalog, sorted by price.\n5. Write an XQuery query that, for each vendor, lists all software iterl1s froIn that vendor\n(i.e., one row in the result per vendor).\n6. Write an XQuery query that lists the prices of all hardware itmlls in the catalog.\n7. Depict the catalog data in the semistructured data model as shown in Figure 27.7.\n8. Build a dataguide for this data. Discuss how it can be used (or not) for each of the above\nqueries.\n9. Design a relational schellla to publish this data.\nExercise 27.7 A university database contains infonnation about professors and the courses\nthey teach. The university has decided to publish this information on the Web and you are\nin charge of the execution. You are given the following information about the contents of the\ndatabase:\nIn the fall sernester 1999, the course 'Introduction to Database Management Systems' was\ntaught by Professor Ioannidis. The course took place Mondays and Wednesdays from\n9~10\na.m. in room 101. The discussion section was held on Fridays fTOIn 9-10 a.m. Also in the fall\nsemester 1999, the course 'Advanced Database Management Systems' was taught by Professor\nCarey.\nThirty five students took that course which was held in room 110 Tuesdays and\nThursdays from 1-·-2 p.m. In the spring semester 1999, the course 'Introduction to Database\nManagement Systems' was taught by U.N. Owen on Tuesdays and Thursdays frOIn 3·_·-4 p.m.\nin room 110.\nSixty three students were enrolled; the discussion section was on Thursdays\nfrom\n4~5 p.m.\nThe other course taught in the spring semester was 'Advanced Database\nManagement Systems' by Professor Ioannidis, Monday, Wednesday, and Friday frorn 8-9 a.m.\n1. Create a well-formed XIvIL document that contains the university database.\n2. Create a DTD for your XML docurnent.\nMake sure that the XIvIL docurnent is valid\nwith respect to this DTD.\n3. Write an XQuery query that lists the names of all professors in the order they are listed\non the Web.\n4. Write an XQuery query that lists all courses taught in 1999.\nThe result should be\ngrouped by professor, with one row per professor, sorted by last narne.\nFor a given\nprofessor, courses should be ordered by BaIlIe and should not contain duplicates (Le.,\neven if a professor teaches the sarne course twice in 1999, it should appear only once in\nthe result).\n5. Build a dataguide for this data. Discuss how it can be used (or not) for each of the above\nqueries.\n6. Design a relational schcrna to publish this data.\n7. Describe the infonnation in a different XML docurnent--a docurnent that ha,,5 a different\nstructure. Create\n;:'1. corresponding DTD and make sure that the docurnent is valid. Rc-\nfonnulate the queries you wrote for preceding parts of this exercise to work with the new\nDTD.\nExercise 27.8\nC~onsider the databa..5e of the Fa..rnilyWear clothes manufacturer. F'anlily,\"Vear\nproduces three types of clothes: wornen's clothes, Incn's clothes, and children's clothes.l\\Ih.m\ncan choose between polo shirts and. 1'-shirts.Each polo shirt lul.s a list of available colors,\nsizes, and a unifonn price. Each T-shirt ha..o;; a price, a list of available colors, and a list of\n\n964\nC~HAPTER27\navailable sizes. vVornen have the sarne choice of polo shirts and T-shirts as Iuen. In addition\nwornen Ci:Ul choose between three types of jeans: sHIn fit, ea\"sy fit 1 and relaxed fit jeans. Each\npair of jeans h~LS a list of possible waist sizes and possible lengths. The price of a pair of jeans\nonly depends on its type.\nChildren can choose between T-shirts and baseball caps.\nEach\nT-shirt has a price, a list of available colors, and a list of available patterns.\nT-shirts for\nchildren aU have the sarne size. Baseball caps COlne in three different sizes: sInall, Iucdiurll,\nand large. Each itern has an optional sales price that is offered on special\nocca.~ions. Write\nall queries in XQuery.\n1. Design an Xrv1L D1'D for FamilyWear so that FamilyWear call publish its catalog on the\nWeb.\n2. \"Vrite a query to find the most expensive iteIIl sold by F'aulilyWear.\n3. Write a query to find the average price for each clothes type.\n4. Write a query to list all iterns that cost Inore than the average for their type; the result\nInust contain one row per type in the order that types are listed on the Web. For each\ntype, the items must be listed in increasing order by price.\n5. Write a query to find all itelns whose sale price is rnore than twice the normal price of\nsorne other itern.\n6. Write a query to find all items whose sale price is rnore than twice the nonnal price of\nsome other item within the same clothes type.\n7. Build a dataguide for this data. Discuss how it can be used (or not) for each of the above\nqueries.\n8. Design a relational schema to publish this data.\nExercise 27.9 With every element e in an Xl\\1L document, suppose we associate a triplet\nof nurnbers <begin, end, level>, where begin denotes the start position of e in the docurnent\nin terms of the byte offset in the file, end denotes the end position of the element, and level\nindicates the nesting level of e, with the root element starting at nesting level O.\n1. Express the condition that element e 1 is (i) an ancestor, (ii) the parent of element e2 in\nterms of these triplets.\n2. Suppose every element has an internal system-generated id and, for every tag naUle I, we\nstore a list of ids of all elernents in the document having tag I, that is, an inverted list\nof ids per tag. Along with the element id, we also store the triplet associated with it,\nand sort the list by the begin positions of elernents. Now, suppose we wish to evaluate\na path expression allb. The output of the join rnust be <'ida, ich> pairs such that ida\nand idb are ids of elements Cu with tag name a and eb with tag IlaIlle b respectively, and\nCa is an ancestor of eb. It Illust be sorted by the COlllposite key < begi:n position of ea ,\nbegin position of eb >.\nDesign an algoritllln that rnerges the lists for a and band perforrns this join. The nurnber\nof position cornparisoIls rnust be linear in the input and output sizes. Hint: The approach\nis sirnilar to a sort-lnerge of two sorted lists of integers.\n~). Suppose that we have k sorted lists of integers where k is a constant. Assurne there are\nno duplicates; that is, each value occurs in exactly one list and exactly once. Design an\nalgoritlnn to rnerge these lists where the nurnber of cornparisons is linear in the input\nsize.\n4. Next, suppose we wish to perfonn the join all/a2/1...//ak (again, k; is a constant). The\noutput of the join IllllSt be a list of k-tuples <id1,id2 , . •. ,'idk > such that 'idi is the id\n\nIII and ){lvILData\n965\nof an elernent ei with tag narne (Li a.nd Ci is an ancestor of Ci+l for all 1 ::s\n>i ::; k- 1.\nThe list lnust be sorted by the conlposite key < begin position of (;1 ~ ... be-gcin position\nof Ck >. Extend the algorithnls you designed in parts (2) and (3) to cOlupllte this join.\nThe nuruber of position cornparisons Illust be linear in the cOlllbined inpllt and output\nsize.\nExercise 27.10 This exercise exalnines why path indexing for XrvlL data is different frorll\nconventional indexing probleills such as indexing a linearly ordered dOlnain for point and\nrange queries. The following illodel has been proposed for the problenl of indexing in general:\nThe input to the problern consists of (i) a dOlnain of elerr18nts \"D, (ii) a data instance I which\nis a finite subset of 'D, and (iii) a finite set of queries Q; each query is a non··,ernpty subset of\nI. This triplet < D, I, Q > represents the indexed workload. An indexing scherne S for this\nworkload essentially groups the data elernents into fixed size blocks of size B. Fonnally, S is\na collection of blocks {51, 52, ... ,5kJ, where each block is a subset of I containing exactly B\nelements. These blocks must together exhaust I; that is, I = 51 U Eh ... U Sk;.\n1. Suppose D is the set of positive integers and I consists of integers fronl 1 to n. Q consists\nof all point queries; that is, of singletons {I}, {2}, ... , {n}.\nSuppose we want to index\nthis workload using a B+ tree in which each leaf level block can hold exactly [ integers.\nWhat is the block size of this indexing schelne? What is the number of blocks used?\n2. The storage redundancy of an indexing scherne S is the maxilllurn nUlllber of blocks that\ncontain an elenlCnt of I. What is the storage redundancy of the B+ tree used in part (1)\nabove'?\n~3. Define the access cost of a query Q in Q under scherne S to be the rninirnum number of\nblocks of S that cover it. The access overhead of Q is its access cost divided by its ideal\naccess cost, which is IIQI/B\"l. What is the access cost of any query under the B+ tree\nscheme of part (I)? What about the access overhead?\n4. The access overhead of the indexing scherne itself is the ITlaxinllun access overhead mnong\nall queries in Q. Show that this value can never be higher than B. What is the access\noverhead of the B+ tree scherne?\n5. We now define a workload for path indexing. The domain D = {i : i is a positive integer}.\nThis is intuitively the set of all object identifiers. An instance can be any finite subset of\n'D. In order to define Q, we ilnpose a tree structure on the set of object identifiers in [.\nThus, if there are n identifiers in I, we define a tree T with n nodes and associate every\nnode with exactly one identifier frorn I. The tree is rooted and node-labeled where the\nnode labels corne fronl an infinite set of labels Z:. The root of T ha.s a distinguished label\ncalled root.\nNow, Q contains a subset 5 of the object identifiers in 1 if S is the result\nof sorne path expression on T. rrhe\ncl~hSS of path expressions we consider involves only\nsirnplc path expressions; that is, expressions of the fonn PE = rooV, 1h 82[2 ... in where\neach 8 1 is a separa.tor which can either be / or / / and each lz is a label froIn }::. This\nexpression returns the set of all object identifiers corresponding to nodes in T tha.t have\na path rnatching P B conling in to them.\nShow that for any T) there is a. path indexing workload such that any indexing scheme\nwith redundancy (It Iuost T will have access overhead B····.., 1.\nExercise 27.11 rrhis exercise introduces the notion of graph sim:lLlation in the context of\nquery Inininlization.\nConsider the following kind of constraints on the data:\n(1) llequired\nparent constraints) where we can specify that the parent ()f an element of tag b always has\ntag a, and (2) Required rmcestor constraints, where we can specify that that HJl elelnent of\nUtg b always has an ancestor of tag a\"\n\n966\nCHAPTER 27\n1. We represent a path expres..\"ion query PB = rootsllts212 .. . In, where each Si is a sepa-\nrator and each Ii is a label, as a directed graph with one node for root and one for each\nIi. Edges go froIll root to 11 and from Ii to li+l. An edge is a parent edge or an ancestor\nedge according to whether the respective separator is j or j j.\nWe represent a parent\nedge frOIn 11 to 'U in the text as 1L -+ v and an ancestor edge as 1L :::::> v.\nRepresent the path expression root//ajbjc\na.~ a graph, as a simple exercise.\n2. The constraints are also represented &'3 a directed graph in the following lnanner. Create\na node for each tag name. A parent (ancestor) edge is present frorn tag nanle a to tag\nHallle b if there is a constraint asserting that every b elmnent rnust have an a parent\n(ancestor). Argue that this constraint graph must be acyclic for the constraints to be\nmeaningful; that is, for there to be data instances that satisfy them.\n3. A simulation is a binary relation :S on the nodes of two rooted directed acyclic graphs\nG 1 and G2 that satisfies the following condition: If u :S v, where u is a node in G 1 and\nv is a node in G2 , then for each node 'u' ---+ u, there must be v'\n--)0 v such that u' :S v'\nand for each u\"\n:::::> u, there must be v\" that is an ancestor of v (i.e., has smne path to\nv) such that utI :S v\". Show that there is a unique largest simulation relation :sm. If\nu ::;m V then u is said to be sirnulated by v.\n4. Show that the path expression rootlIbl Ie can be rewritten as j Ie if and only if the e\nnode in the query graph can be simulated by the e node in the constraint graph.\n5. The path expression Illjsj+llj+l .. . In (j > 1) is a suffix of rootsdlS2l2 .. . In. It is an\nequivalent suffix if their results are the same for all database instances that satisfy the\nconstraints. Show that this happens if Ij in the query graph can be simulated by lj in\nthe constraint graph.\nBIBLIOGRAPHIC NOTES\nIntroductory reading material on infonnation retrieval includes the standard textbooks by\nSalton and McGill [646] and by van Rijsbergen [753].\nCollections of articles for the nlore\nadvanced reader have been edited by Jones and Willett [411] and by Frakes and Baeza-Yates\n[279].\nQuerying text repositories has been studied extensively in information retrieval; see\n[626] for a recent survey.\nFaloutsos overviews indexing rnethods for text databases [257].\nInverted files are discussed in [540] and signature files are discussed in [259]. Zobel, I:vloffat,\nand RarnanlOhanarao give a cornparison of inverted files and signature files [802]. A survey of\nincrernental updates to inverted indexes is presented in [179]. Other aspects of inforrnation\nretrieval and indexing in the context of databases are addressed in [604], [290], [656], and\n[803]\" arnollg others.\n[~~~~O] studies the problem of discovering text resources on the Web.\nThe book by Witten,\n~loffat, and Bell ha'3 a lot of material on cornpression techniques for\ndocument databases [780].\nThe nUlnber of citation counts as a llleasure of scientific impact has first been studied by\nGarfield U307]; see also [763].\nUsage of hypertextual infonna1,ion to irnprove the quality of\nsearch engines lU1s been proposed by Spertus [699] and by Weiss e1, al.\n[771].\nThe HITS\nalgorithln was developed by Jon Kleinberg [438]. Concurrently, Brin and Page developed the\nPagerank (now called PigeonRank) algoritlnn, which also takes hyperlinks between page..c; into\naccount [116]. A thorough analysis and cornparison of several recently proposed algorithms\nfor deterrnining authoritative pages is presented in [106]. The discovery of structure in the\nWorld Wide Web is currently a very active area of research; see for exaruple the work by\nGibson et a1.\n[~n6].\n\nIR and -\"YNIL Data\n9Q7\nThere is a lot of research on sCluistructured data in the databa.'5e cOIluI1unity. The T'siunnis\ndata integration systeIn uses a s€ruistructured data Inodel to cope with possible heterogeneity\nof data sources [584, 583]..vVork on describing the structure of semistructured databa.,es can\nbe found in [561].\n\\\\Tang and Liu consider scherna discovery for seInistructured documents\n[766].\nfvlapping between relational and XML representations is discussed in [271, 676, 103]\nand\n[1~~4].\nSeveral new query languages for semistructured data have been developed: LOREL (602),\nQuilt [152], UnQL [124], StruQL [270], WebSQL (528), and XML-QL [217]. The current W3C\nstandard, XQuery, is described in [153]. The latest version of several standards rnentioned\nin this chapter, including XML, XSchenla, XPath, and XQuery, can be found at the website\nof the World Wide Web Consortiuln (www.w3.org).\nKweelt [645] is an open source system\nthat supports Quilt, and is a convenient platform for systerlls experimentation that can be\nobtained online at http://k'weelt.sourceforge .net.\nLORE is a database management system designed for semistructured data [518]. Query op-\ntinlization for semistructured data is addressed in [5] and [321], which proposed the Strong\nDataguide. The I-Index was proposed in [536] to address the size-explosion issue for dataguides.\nAnother XML indexing schenle is proposed in [196]. Recent work [419] aims to extend the\nframework of structure indexes to cover specific subsets of path expressions. Selectivity esti-\nrnation for XML path expressions is discussed in [6]. The theory of indexability proposed by\nHellerstein et al. in [375] enables a formal analysis of the path indexing problenl, which turns\nout to be harder than traditional indexing.\nThere has been a lot of work on using seluistructured data models for Web data and several\nWeb query systems have been developed: WebSQL [528], W3QS [445], WebLog [461], We-\nbOQL [39], STRUDEL [269], ARANEUS [46]' and FLORID [379]. [275] is a good overview\nof database research in the context of the Web.\n\n28\nL~\n~.~~\nSPATIAL DATA\nMANAGEMENT\n...\nWhat is spatial data, and how can we classify it?\n..\nWhat applications drive the need for spatial data nlanagenlent?\n..\nWhat are spatial indexes and how are they different in structure from\nnon-spatial data?\n..\nHow can we use space-filling curves for indexing spatial data?\n..\nWhat are directory-based approaches to indexing spatial data?\n..\nWhat are R trees and how to they work?\n..\nWhat special issues do we have to be aware of when indexing high-\ndimensional data?\n..\nKey concepts:\nSpatial data, spatial extent, location, boundary,\npoint data, region data, ra...o;;;ter data, feature vector, vector data, spa-\ntial query, nearest neighbor query, spatial join, content-based image\nretrieval, spatial index, space-filling curve, Z-orclering, grid file, R tree,\nR+ tree, R* tree, generalized search tree, contrast.\nNothing puzzles rne more than tiTne and space; a.nd yet nothing puzzles Ine less,\nas I never think about theIn.\n.... Charles Larnb\nIVlany applications involve large collections of spatial objects; and querying, in-\ndexing, and rnaintaining such collections requires S()lne specialized techniques.\nIn this chapter, we rnotivate spatial data lnanagenlent and provide an intro-\nduction to the required techniques.\n968\n\n81J(Ltial Data lvfanagctnent\n969\nt\n.....---,\nSQL/MM: Spatial The SQL/Mlvl standard supports points, lines, and\n2-dirnensional (planar or surface) data.f\\lture extensions are expected to\nsupport 3-dhnensional (voIUlnetric) and Ll-din1ensional (spatia-temporal)\ndata as \\veIl.\nThese new data types are supported through a type hi-\nerarchy that refines the type ST_Geometry.\nSubtypes include ST_Curve\nand ST_Surface, and these are further refined through ST-LineString,\nST_Polygon, etc.\nThe rnethods defined for the type\nST_Geonl(~try sup-\nport (point set) intersection of objects, union, difference, equality, contain-\nment, cornputation of the convex hull, and other siInilar spatial operations.\nrrhe SQL/MM: Spatial standard has been designed with an eye to conl-\npatibility with related standards such as those proposed by the Open GIS\n(Geographic Inforrnation Systenls) Consortiunl.\nWe introduce the different kinds of spatial data and queries in Section 28.1 and\ndiscuss several important applications in Section 28.2. We explain why indexing\nstructures such a') B+ trees are not adequate for handling spatial data in Section\n28.3.\nWe discuss three approaches to indexing spatial data in Sections 28.4\nthrough 28.6: In Section 28.4, we discuss indexing techniques ba.sed on space-\nfilling curves; in Section 28.5, we discuss the Grid file, an indexing technique\nthat partitions the data space into nonoverlapping regions; and in Section 28.6,\nwe discuss the R tree, an indexing technique based on hierarchical partitioning\nof the data space into possibly overlapping regions.\nFinally, in Section 28.7\nwe discuss S0111e issues that arise in indexing datasets with a large nurnber of\ndiInensions.\n28.1\nTYPES OF SPATIAL DATA AND QUERIES\nWe use the ternl spatial data in a broad sense, covering rnultidirnensional\npoints, lines, rectangles, polygons, cubes, and other geoilletric objects. A spa-\ntial data object occupies a certain region of space, called its spatial extent,\nwhich is characterized by its location and boundary.\nFraIn the point of view of a DBMS, we can classify spatial data &'3 being either\np()'int data or Tegion data.\nPoint Data: A point has a spatial extent characterized cOIllpletely by its\nlocation; intuitively, it occupies no spa..ce and has no clssociated area or voh.llne.\nPoint data consists of a collection of points in a InultidirrH:~nsional space. Point\ndata stored in a databa.se can be ba,,'3ed on direct rnCi::1Enlrernents or generated\nby transfonning data obtained through rnea,surcrnents for ea.,se of storage and\nquerying.\nRaster data is an exarnple of directly rneasured point data and\n\n970\nCHAPTER 2&\nincludes bitrnaps or pixel Inaps such as satellite imagery.\nEach pixel stores\na ruea..'3ured value (e.g., ternperature or color) for a corresponding location in\nspace. Another exarnple of such rneasured point data is rnedical iInagery such\n<:4'1 three-dhnensional llulgnetic resonance irnaging (l\\tIRI) brain scans. feature\nvector's extracted frorn irnages, text, or signals, such a...') tirne series are examples\nof point data obtained by transforrning a data object. As we will see, it is often\neasier to use such a representation of the data, instead of the actual irnage or\nsignal, to answer queries.\nRegion Data: A region has a spatial extent with a location and a boundary.\nThe location can be thought of a..\" the position of a fixed 'anchor point' for the\nregion, such as its centroid. In two dirnensions, the boundary can be visualized\nas a line (for finite regions, a closed loop), and in three diInensions, it is a\nsurface. Region data consists of a collection of regions. Region data stored in\na database is typically a simple geornetric approxirnation to an actual data ob-\nject. Vector data is the ternl used to describe such geometric approximations,\nconstructed using points, line segrnents, polygons, spheres, cubes, and the like.\nMany examples of region data arise in geographic applications. For instance,\nroads and rivers can be represented as a collection of line segrnents, and coun-\ntries, states, and lakes can be represented as polygons. Other exarnples arise\nin computer-aided design applications.\nFor instance, an airplane wing nlight\nbe rnodeled as a wire jra'm,e using a collection of polygons (that intuitively tile\nthe wire frame surface approximating the wing), and a tubular object rI1ay be\nrnodeled as the difference between two concentric cylinders.\nQueries that arise over spatial data are of three ruain types:\nspatial range\nqucr'les, nearest neighbor' queries, and spatial join queries.\nSpatial Itange Queries: In addition to rnultidimensional queries, such\n~.kS,\n\"Find all ernployees with salaries between $50,000 and $60,000 and ages be-\ntween 40 and 50,\" we can ask queries such as \"Find all cities within 50 rniles of\n:NIadison\" or \"Find all rivers in \\Visconsin.\" A spatial range query ha~'3 an a..'3SO-\neiated region (vvith a location and boundary). In the presence of region data,\nspatial fflnge queries can return all regions that overlap the specified range or\nall regions contained within the specified range. Both variants of spatial range\nqueries are useful, and algorithrns for evaluating one variant are ea.sily adapted\nto solve the other. H,ange queries occur in a \\vide variety of applications, in-\ncluding relational queries, cas queries, and CAD/CA1Vl queries.\nNearest Neighbor Queries: A typical query is \"Find the 10 cities nearest\nto wladison.\"\n\\Ve usuallv want the answers ordered by· distance to Madison,\n,1\n,\nthat is, by proxil11ity. Such queries are especially irnportant in the context of\nrnultirnedia databases, where an object (e.g., irnages) is represented by a point,\n\nSpatial Data !v!a'nagernwnt\n971\nand 'siInilar' objects are found by retrieving objects whose representative points\nare closest to the point representing the query object.\nSpatial Jain Queries: Typical exarnples include \"Find pairs of cities within\n200 rniles of each other\" and \"Find all cities near a lake.\" These queries can\nbe quite expensive to evaluate. If we consider a relation in which each tuple is\na point representing a city or a lake, the preceding queries can be answered by\na join of this relation with itself, Vorhere the join condition specifies the distance\nbetween two rnatching tuples.\nOf course, if cities and lakes are represented\nin Inore detail and have a spatial extent, both the Ineaning of such queries\n(are we looking for cities whose centroids are \\vithin 200 Iniles of each other or\ncities whose boundaries conle within 200 rniles of each other?), and the query\nevaluation strategies become more cornplex. Still, the essential character of a\nspatial join query is retained.\nThese kinds of queries are very common and arise in lllost applications of spatial\ndata. Some applications also require specialized operations such as interpola-\ntion of llleasurelnents at a set of locations to obtain values for the rneasured\nattribute over an entire region.\n28.2\nAPPLICATIONS INVOLVING SPATIAL DATA\nMany applications involve spatial data.\nEven a traditional relation with k\nfields can be thought of as a collection of k-diInensional points, and as we\nsee in Section 28.3, certain relational queries can be executed faster by using\nindexing techniques designed for spatial data.\nIn this section, however, we\nconcentrate on a,pplications in which spatial data plays a central role and in\nwhich efficient handling of spatial data is essential for good perforrnance.\nGeogT'aphic InfoTTnat'ion SystcTns ((jIS) deal extensively with spatial data, in-\ncluding points, lines, and t\\\\TO- or three-diInensional regions.\nFor exalnple, a\nrnap contains locations of srnall objects (points), rivers and highways (lines),\nand cities and lakes (regions).\nA (as systern rnust efficiently rnanage two-\ndirnensional and three-dirnensional data...'3cts. All the classes of spatial queries\nwe described axise naturally, and both point data and region data rnust\nb(~\nhandled. Cornrnercial GIS systerns such as ArcInfo are in \\vide use today, and\n~\nv\nobject database systerns rtirll to support: (jIS applications as well.\n()oTnptdCT-aided design and rnanufactv,T\"ing (CA D/ CiA M) SystCIllS and rnedical\nirnaging systcrIls store spatial objects, such as surfaces of design objects (e.g.:\nthe fuselage of an aircraft). A.s \\vith (}IS systelI1S, both point and region data\nrnust be stored. Ilange queries and spatial join queries are probably the rnost\ncornrnon queries, and spatial integrity constraints, sueh c1S \"There Illust be\n\n972\n(;HAPTER 28\na rnininuUll clearance of one foot bet\\veen the wheel and the fuselage,\" can be\nvery useful. (CAD/CAIVI wa\" a rnajor reason behind the developlnent of object\ndatabases.)\nA1'uli'i'm,edia databases, \\vhich contain rnultiIncdia objects such\nct.-I;;) images, text,\nand various kinds of tirne-series data (e.g., audio), also require spatial data 1na11-\nagernent. In particular, finding objects shnilar to a, given object is a comnlon\nquery in a rllultirncdia systern, and a popular approach to answering siInilar-\nity queries involves first rnapping lIlultilnedia data to a, collection of points,\ncalled feature vectors. A sirnilarity query is then converted to the problenl\nof finding the nearest neighbors of the point that represents the query object.\nIn rnedical image databases, we store digitized t'wo-dirnensional and three-\ndirnensional ilnages such as X-rays or J\\1RI irnages. Fingerprints (together with\ninforrnation identifying the fingerprinted individual) can be stored in an image\ndatabase, and we can search for fingerprints that nlatch a given fingerprint.\nPhotographs frorn driver's licenses can be stored in a database, and we can\nsearch for faces that rnatch a given face. Such image database applications rely\non content-based image retrieval (e.g., find images shnilar to a given irn-\nage). Going beyond irnages, we can store a database of video clips and search\nfor clips in which a scene changes, or in which there is a particular kind of\nobject. We can store a database of signals or tim,e-series and look for sirnilar\ntiule-series. We can store a collection of text documents and search for shnilar\ndocurnents (i.e., dealing with similar topics).\nFeature vectors representing rnultirnedia objects are typically points in a high-\ndimensional space.\nFor exarnple, we can obtain feature vectors froln a text\nobject by using a list of keywords (or concepts) and noting which keywords are\npresent; we thus get a vector of Is (the corresponding keyword is present) and\nOs (the corresponding keyword is Inissing in the text object) whose length is\nequal to the nurnber of keywords in our list.\nLists of several hundred words\nare cornrnonly used. vVe can obtain feature vectors froIn an inlage by looking\nat its color distribution (the levels of red, green, and blue for each pixel) or by\nusing the first several coefficients of a mathernatical function (e.g., the Hough\ntransfonn) that closely approxirnates the shapes in the irnage. In general, given\nan arbitrary signal, 'we can represent it using a rnathernatical function having\na standard series of ternlS and approxirnate it by storing the coefficients of the\nlnost significant tenns.\nvVhen rnapping rnultirnedia data to a collection of points, it is irnportant to\nensure that a there is a rneasure of distance betweent\\vo points that captures\nthe notion of sirnilarity bct\\veen the corresponding rnultilnedia objects. Thus,\ntwo irnages that rnap to t\\VO nearby points Inust be Inore sirnilar than two\nirnages that rnap to t\"vo points far frolH each other. ()nce objects are rnapped\n\nSpatial Data A1anagen~ent\nr-~\n~.- , ,\n,\n\\ ,\n1\ni\n,t\nr ....\n.\",.............\n-\n-\n'\"'I\n80\n,,:\nI\n~\n80\n~\n•\n1 ,\n!\nI\n~\n!\nI-\n,\n70\nI\nj\nI\n70\n•\nI\n~ ,\nI\nI\nI\nI\n~ -\n____\n.••\n_••\n_\nJ\n~\nI\n60\nA\nI\n,~\nI\nI\n60\n\"I' ,\ni\n~\\\nI\n\"2\n50\nI\n~\nI\nI\n-\n50\nI:C\nrn\nI\n,\nI\nI\nrn\n,\n~\nI\nI\n40\nI\nI\nI\nI\n40\nI\nI\nI\nI\nI\n30\nI\nI\nI\n30\nI\n1 .:\nI\nI\n20\nI\nI\n20\nI •\nI\nL\nI\n\\\nI\nI\nI\n10\nI .'\n10\nI\nI •\n!\nI\nI\nL_...._.\n!\n!\n_..-:>-\n..;>-\n11\n12\n13\nage\n11\n12\n13\nage\nFigure 28.1\nClustering of Data Entries in B+ 'free vs. Spatial Indexes\ninto a suitable coordinate space, finding siInilar images, siInilar documents, or\nsirnilar tilne-series can be Illodeled as finding points that are close to each other:\nWe map the query object to a point and look for its nearest neighbors. The rIlost\nCOl1UllOn kind of spatial data in lllultinledia applications is point data, and the\nlllost COllllIlon query is nearest neighbor. In contrast to GIS and CAD/CAM,\nthe data is of high dirnensionality (usually 10 or rnore dirnensions).\n28.3\nINTRODUCTION TO SPATIAL INDEXES\nA multidimensional or spatial index, in contrast to a B-t- tree, utilizes seHne\nkind of spatial relationship to organize data, entries, with each key value seen\nas a point (or region, for region data) in a k-dimensional space, where k is the\nnumber of fields in the search key for the index.\nIn a B+ tree index, the t\\vo-diJnensional space of (age, 8a0 values is linearized--·--\nthat is, points in the two-dirnensional doruain are totally ordered····..···by sorting\non age first and then on sal. In Figure 28.1, the dotted line indicates the linear\norder in which points are stored in a B-+ tree. In contrast, a spatial index. stores\ndata entries baA'3ed on their proxirnity in the underlying t\\vo-dirnensional space.\nIn Figure 28.1, the boxes indicate huw points are stored in a spatial index.\nI.Jct us corrlpare a 13-+· tree index on key (age, 8a0 with a spatial index on the\nspace of age and sal values, using several exalnple queries:\n1. age < 12: The B·+ tree index perforrns very well. 1\\8 we will sec, a spatial\nindex handles such a query qllitewell, although it cannot rnateh a B+- tree\nindex in this casc.\n\n974\nCHAPTER d8\n2. sal < 20: The B-+- tree index is of no use, since it does not Inatch this\nselection. In contr&')t, the spatial index handles this query just as \\vell\nBo,,\"\"\nthe previous selection OIl age.\n~3. age < 12 1\\ sal < 20: The B+ tree index effectively utilizes only the selection\non age. If 1110st tuples satisfy the age selection, it perforrns poorly. The\nspatial index fully utilizes both selections and returns only tuples that\nsatisfy both the age and sal conditions.\nTo achieve this \\vith B+ tree\nindexes, we have to create two separate indexes on age and sal, retrieve\nrids of tuples satisfying the age selection by using the index on age and\nretrieve rids of tuples satisfying the sal condition by using the index on sal,\nintersect these rids, then retrieve the tuples \\vith these rids.\nSpatial indexes are ideal for queries such as \"Find the 10 nearest neighbors of\na given point\" and, \"Find all points within a certain distance of a given point.\"\nThe drawback with respect to a B+ tree index is that if (alrnost) all data entries\nare to be retrieved in age order, a spatial index is likely to be slower than a B+\ntree index in which age is the first field in the search key.\n28.3.1\nOverview of Proposed Index Structures\nMany spatial index structures have been proposed. Some are designed primarily\nto index collections of points although they can be adapted to handle regions,\nand SaIne handle region data naturally. ExaInples of index structures for point\ndata include Grid files, hE trees, KDtrees, Point Quad trees, and\nSI~ trees.\nExamples of index structures that handle regions &'3 well as point data include\nIlegion Quad trees, R trees, and SKD trees. These lists are far from c()lnplete;\nthere are rnany variants of these index structures and ITlany entirely distinct\nindex structures.\n1\"here is as yet no consensus on the 'best' spatial index structure. I-Iowever,\nIl trees have been widely irnplcInented and found their way into cOHllnercial\nDBMSs. This is due to their relative sirnplicity, their ability to handle both\npoint and region data, and their perforrnance,\\vhich is at least cornparable to\n1nore cornplex. structures.\n'VVe discuss three approaches that are distinct and, taken together, illustrate of\nInany of the pr6posed indexing aJternatives. First,vve discuss index structures\nthat rely on space-filling c'urvcs to organize points. We begin by discussing Z-\nordering for point data, and then for region elata, which is essentiall~y the iclea\nbehind llegion Quad trees. Ilegion (~uad trees illustrate an indexing approach\nbclEied on recursive subdivision of the rnultidiInensional space, independent of\nthe actual dataset. rfhere are several variants of Region (~uad trees.\n\nSpatial Data 1'\\;!anagc1nent\n979\nSecond, we discuss Grid files, which illustrate how an Extendible Ha.-,hing style\ndirectory can be used to index spatial data.\nIvlany index structures such as\nBang files, B1.Lddy trees, and lv!'ult'ilevel Gr'id files have been proposed refining\nthe basic idea. Finally, \\ve discuss R trees, which also recursively subdivide the\nmuitidilllensional space. In contra.'3t to Region Quad trees, the decolllposition\nof space utilized in an R tree depends on the indexed data.,'3et. \\lVe can think\nof R. trees as an adaptation of the B+ tree idea to spatial data. Many variants\nof R trees have been proposed, including Cell trees, HilbeTt R trees, Packed II\ntr'ees, R* trees, R+ trees, TV tTees, and ..,:r trees.\n28.4\nINDEXING BASED ON SPACE-FILLING CURVES\nSpace-filling curves are based on the assulnption that any attribute value can be\nrepresented with SaIne fixed nUlnher of bits, say k bits. The luaximulu nUluber\nof values along each dirnension is therefore 2k . v\\le consider a two-dimensional\ndataset for sirnplicity, although the approach can handle any nUluber of diluen-\nsions.\nZ-ordering with two bits\nZ-ordering with three bits\nHilbert curve with three bits\no\n,......\no\n,......\n,......\n000\no\n,......--\noo\n0\no\n,......\n,......\n,......\no\n0\n111\n110\n101\n100\n011\n010\n001\n000 .\n-'-----_...L--.-L....-_-L-~_\nog g\n111\n110\n101\n100\nall\n010\n001\n000 -_--+-->O~~~~~~\nooo\n11\n15\n10\n14\n01\n11\n10\n00\n---?>--\n00\n01\n10\n11\nFigure 28.2\nSpa.ce Filling Curves\nA space-filling curve irnposes a linear ordering on the dornain, as illustrated\nin Figure 28.2. The first curve shows the Z-ordering curve for dornains with\n2-bit representations of attribute values. A given datc'tset contains a subset of\nthe points in the dornain, and these are ShC)\\Vll. as filled circles in the figure.\nDornain points Jlot in the given dataset are shown as unfilled circles. Consider\nthe point with X = 01 and y\" = 11 in the first curve. The point ha\",s Z-value\n0111, obtained by interleaving the bits of the X and Y'\" values; vve take the first\n..\\'\" bit (0), then the first yr bit (1), then the second X bit (1), and finally the\nsecondY bit (1). In decirnal representation, the Z-value 0111 is equal to 7, and\nthe point X\n:= 01 and y\" = 11 has the Z-value 7 shown next to it in Figure\n\n97f3\nCHAPTER 28\n28.2. l'his is the eighth dOlllain point 'visited' by the space-fining curve, which\nstarts at point X = 00 and Y- := 00 (Z-value 0).\nThe points in a datac;et are stored in Z-value order and indexed by a traditional\nindexing structure such as a B+ tree. That is, the Z-vaJue of a point is stored\ntogether \\vith the point and is the search key for the B+ tree.\n(Actually, \\ve\nneed not need store the X and\nY'~ values for a point if we store the Z-value, since\nwe can COlllpute thern froln the Z-value by extracting the interleaved bits.) To\ninsert a point, \\ve COlnpnte its Z-value and insert it into the B+ tree. Deletion\nand search are sinlilarly based on COlllputing the Z-value and using the standard\nB+ tree aJgorithrns.\nThe advantage of this approach over using a B+ tree index on S0111e cornbination\nof the X and Y fields is that points are clustered together by spatial proxirnity\nin the ...X\"--y\" space. Spatial queries over the .X_,.}T space now translate into linear\nrange queries over the ordering of Z-values and are efficiently answered using\nthe B+ tree on Z-values.\nThe spatial clustering of points achieved by the Z-ordering curve is seen rnore\nclearly in the second curve in Figure 28.2, which shows the Z-ordering curve\nfor dornains with 3-bit representations of attribute values. If we visualize the\nspace of all points as four quadrants, the curve visits all points in a. quadra,nt\nbefore nloving on to another quadrant. This Ineans that all points in a quadrant\nare stored together. This property holds recursively within each quadrant as\nwell~each of the four subquadrants is cornpletely traversed before the curve\nlnoves to another subquadrant. Thus, all points in a subquadrant are stored\ntogether.\nThe Z-ordering curve achieves good spatial clustering of points, but it can be\ninrproved orl. Intuitively, the curve occasionally Inakes long diagonal 'juInps,'\nand the points connected by the jurnps, \\vhile far apart in the\nx,·,y~ space of\npoints, are nonetheless close in Z-ordering.\nrrhe THIbert curve, shown as the\nthird curve in Figure 28.2, addresses this problern.\n28.4.1\nRegion Quad Trees and Z..Ordering: Region Data\nZ-ordering givE~s us a \\vay to group points according to spatial proxiInity. \\Vhat\nif we have region data? rrhe key is to understa,nd ho\\v Z-ordering recursively\ndecornposes the data space into quadrants and subquadrants,\n(1\",'; illustrated in\nFigure\n28.~~.\nThe R,egion (~uad tree structure corresponds directly to the recursive decornpo-\nsition of the data space. Each node in the tree corresponds to a square-shaped\n\nSpatial Data J1vfanagenu:nt\n11\n10\n01\n00\nFigure 28.3\nZ-Ordering and Region Quad Trees\nregion of the data space. As special cases, the root corresponds to the entire\ndata space, and S0111e leaf nodes correspond to exactly one point.\nEach in-\nternal node has four children, corresponding to the four quadrants into which\nthe space corresponding to the node is partitioned: 00 identifies the bottom\nleft quadrant, 01 identifies the top left quadrant, 10 identifies the bottorn right\nquadrant, and 11 identifies the top right quadrant.\nIn Figure 28.3, consider the children of the root.\nAll points in the quadrant\ncorresponding to the 00 child have Z-values that begin with 00, all points in\nthe quadrant corresponding to the 01 child have Z-values that begin with 01,\nand so on. In fact, the Z-value of a point can be obtained by traversing the\npath froIn the root to the leaf node for the point and concatenating all the edge\nlabels.\nConsider the region represented by the rounded rectangle in Figure 28.3. Sup-\npose that the rectangle object is stored in the DBMS and given the unique\nidentifier (aid) R. R includes all points in the 01 quadrant of the root as well\nas the points with Z-values 1 and 3,which are in the 00 quadrant of the root.\nIn the figure, the nodes for points 1 and 3 and the 01 quadrant of the root are\nshown 'with dark boundaries. Together, the dark nodes represent the rectangle\nR. ffhe three records (0001, R), (OOll, R), and (01, R) can be used to store this\ninfonnation. The first field of each record is a Z-valuc; the records a,re clus-\ntered and indexed on this colurun using a B+ tree. Thus, a B+ tree is used to\nirnplcInent a\nH,(~gion Quad tree, just &'3 it was used to irnplernent Z-ordering.\nNote that a region object can usually be stored using fewer records if it is\nsufficient to represent it at a coarser level of detail. For exarl1ple, rectangle R\ncan be represented using t\\VO records (00, R) and (01, R). This approxirnates R\nby using the bottorn-Ieft and top-left qua.drants of the root.\n\n978\nCHAPTER 28\n1~he Region Quad tree idea can be generalized beyond two dilncnsions. In k\ndirnensions, at each node we partition the space into 2k subregions; for k == 2,\n\\ve partition the space into four equal parts (quadrants). vVe will not discuss\nthe details.\n28.4.2\nSpatial Queries Using Z-Ordering\nRange queries can be handled by translating the query into a collection of\nregions, each represented by a Z-value. (vVe saw how to do this in our discussion\nof region data and R,egion Quad trees.)\nWe then search the B+ tree to find\nrnatching data iterns.\nNearest neighbor queries can also be handled, although they are a little trickier\nbecause distance in the Z-value space does not always correspond well to dis-\ntance in the original X - Y coordinate space (recall the diagonal jumps in the\nZ-order curve). The basic idea is to first compute the Z-value of the query and\nfind the data point with the closest Z-value by using the B+ tree. Then, to\nrnake sure we are not overlooking any points that are closer in the X-Y space,\nwe cornpute the actual distance r between the query point and the retrieved\ndata point and issue a range query centered at the query point and with radius\nr. We check all retrieved points and return the one closest to the query point.\nSpatial joins can be handled by extending the approach to range queries.\n28.5\nGRID FILES\nIn contrast to the Z-ordering approach, which partitions the data space inde-\npendent of anyone dataset, the Grid file partitions the data space in a way\nthat reflects the data distribution in a given dataset. rrhe Inethocl is designed\nto guarantee that any point q'U,CTy (a query that retrieves the illfonnation asso-\nciated with the quer:y point) can be ansvvered in, at rnost, two disk a,ccesses.\nGrid files rely upon a grid directory to identify the data, page containing a\ndesired point. rrhe grid directory is sirnilar to the directory used in Extendible\nIIashing (see Chapter 11).vVhen seaTching for a point,we first find the C01'1'e-\nsponcling entry in the grid directory. The grid directory entry, like the directory\nentry in Extendible flashing, identifies the page on which the desired point is\nstored, if the point is in the database. To understand the Cjrid file structure,\n\\ve need to understand ho\\v to find the grid directory entry for a giverl point.\n\\Ve describe the (jrid file structure for two-dirnensional data.\nIThe rnethod\ncan be generalized to any nurnber of dilnensions, but \\ve restrict ourselves to\nthe t\\vo-diInensional C(1.'3e for sirnplicity.\nThe C;ricl file partitions sl>(1ce into\n\nSpatial Data A1anagernent\n979\nrectangular regions using lines parallel to the axes. Therefore, we can describe\na Grid file partitioning by specifying the points at which each &,xis is 'cut.' If\nthe ,X axis is cut into 'i segrnents and the y\" axis is cut into j segments, we have\na total of i x j partitions. The grid directory is an 'i by j array with one entry\nper partition. This description is Inaintained in an array called a linear scale;\nthere is one linear scale per CLxis.\nLI,~~:~~ ~~~~~ ~~~,~-.~?'!S__:\n...\n:\nI\nI\nI\n0\n1000\n1500\n1700\n~ 2500\n3500\n:\n: . ..\n.\n1\n_\nQuery:\n(1800,~ut)\nI\nI\nI\nI\n1\nF\"\n-\n•••••~\n~\n-\nI\nI\nI\nI\nI\na\nI\nI\nI\nI\nI\nI\n•\nI\nI\nI\nI\nI\nI f\nI\nI\nI\n•\nI\nI\nI\nI\nI\nI k\n:\n-\n~-\n~.~~--+,\nI P\nI\nI\n•\n: z\nI\nL\n, I\n---_\n,\n~._~~_.--_.-\n,--+-~- ...,.,.. -+---1\nGRID DIRECTORY\nStored on Disk\nLINEAR SCALE FOR Y-AXIS\nFigure 28.4\nSearching for a Point in a Grid File\nFigure 28.4 illustrates how we search for a point using a Grid file index. First,\nwe use the linear scales to find the ..,X- segulent to which the .LY value of the given\npoint belongs and the Y segrnent to which the y\" value belongs. This identifies\nthe entry of the grid directory for the given point. We assurne that all linear\nscales are stored in rnain rnernory, and therefore this step does not require any\nl/C). Next, we fetch the grid directory entry. Since the grid directory rnay be\ntoo large to .fit in rnain rnenlory, it is stored on disk. Flowever, we can identify\nthe disk page containing a given entry and fetch it in one I/O because the grid\ndirectory entries are arra,nged sequentially in either row\\vise or cohuunwise\norder. The grid directory entry gives us the ID of the data page containing the\ndesired point, and this page can now be retrieved in one l/C). 'rhus, we can\nretrieve a point in t\\VO l/Os . one l/C) for the directory entry and one for the\ndata page.\nR.ange queries and nearest neighbor queries are e&l;)ily answered using the Cjrid\nfile.B-br rttnge queries, we use the linear scaJes to identify the set of grid\ndirectory entries to fetch.\nFor nearest neighbor queries, we first retrieve the\ngrid directory entry for the given point and search the data page to which it\nPOit1tS. If this data page is crnpty,\\ve use the linear scales to retrieve the data\nentries for grid partitions that are adjacent to the partition that contains the\n\n980\nC~HAPTER 28\nquery point. We retrieve all the data points within these partitions and check\nthern for nearness to the given point.\nThe Grid file relies upon the property that a grid directory entry points to a\npage that contains the desired data point (if the point is in the databa,se). T'his\nrneans that \\ve are forced to split the grid directory·····and therefore a linear\nscale along the splitting dirnension··-·-··-if a data page is full and a new point is\ninserted to that page. To obtain good space utilization, we allow several grid\ndirectory entries to point to the saIne page. That is, several partitions of the\nspace Inay be rnapped to the saIne physical page, a.s long as the set of points\nacross all these partitions fits on a single page.\n3\n4\nA\nB\nc\n2\nFigure 28.5\nInserting Points into a Grid File\nInsertion of points into a Grid file is illustrated in Figure 28.5, which has four\nparts, each illustrating a snapshot of a Grid file. Each snapshot shows just the\ngrid directory and the data pages; the linear scales are ornitted for sirnplicity.\nInitially (the top-left part of the figure), there are only three points, all of\nwhich fit into a single page (A). 'rhe grid directory contains a single entry,\nwhich covers the entire data space and points to page A.\nIn this exaInple, we aSSUlne that the capacity of a data page is three points.\nTherefore, 'when a 11e\\V point is inserted, we need an additional data page. We\nare also forced to split the grid directory to accornrnodate an entry for the new\npage. \\¥e do this by splitting along the X axis to obtain two equal regions;\none of these regions points to page A and the other points to the new data\npage B. The data points are redistributed across pages A and B to reflect the\npartitioning of the grid directory. 1'he result is shown in the top-right part of\nFigure 28.5.\nThe next part (bottorll left) of Figure 28.5 illustrates the Grid file after two\nrnore insertions. rrhe insertion of point 5 forces us to split the grid directory\nagain, because point 5 is in the region that points to page A, and page A is\n\nSpatial IJata Nfanagc'rnent\n981\nalready full. Since we split along the ...X\" axis in the previous split, \\ve now split\nalong the 1/\" axis, and redistribute the points in page A acrex..,s page A and a\nIle\\V data page, C. (Choosing the a.xis to split in a round-robin fashion is one of\nseveral possible splitting policies.) ()bserve that splitting the region that points\nto page A also caiuses a split of the region that points to page B, leading to t\\VO\nregions pointing to page B. Inserting point 6 next is straightforward because it\nis in a region that points to page 13, and page B h<:1...1;) space for the new point.\nNext, consider the bottonl right part of the figure. It shows the exarnple file\nafter the insertion of two additional points, 7 and 8. The insertion of point 7\nfills page C, and the subsequent insertion of point 8 causes another split. This\ntiIne, we split along the\n~X axis and redistribute the points in page C across\nC and the new data page, D. Observe how the grid directory is partitioned\nthe most in those parts of the data space that contain the rnost points-··-the\npartitioning is sensitive to data distribution, like the partitioning in Extendible\nHashing, and handles skewed distributions well.\nFinally, consider the potential insertion of points 9 and 10, which are shown\nas light circles to indicate that the result of these insertions is not reflected in\nthe data pages. Inserting point 9 fills page B, and subsequently inserting point\n10 requires a new data page. However, the grid directory does not have to be\nsplit further~\npoints 6 and 9 can be in page B, points 3 and 10 can go to a new\npage E, and the second grid directory entry that points to page B can be reset\nto point to page E.\nDeletion of points from a Grid file is cOITlplicated. When a data page falls below\nSaIne occupancy threshold, such as, less than half-full, it luust be rnerged with\nscnue other data page to rnaintain good space utilization. We do not go into\nthe details beyond noting that, to siInplify deletion, a conve:£'ity requirernent is\nplaced on the set of grid directory entries that point to a single data page: The\nregion defined by this set of grid directory erd'ries rnust be conve:r.\n28.5.1\nAdapting Grid Files to Handle Regions\nThere are two basic approaches to handling region data in a Grid file, nei-\nther of which is satisfactory. First, vve can represent a region by a point in a\nhigher-dirnens~onal space. E'or exarnple, a box in tvvo diInensions can be repre-\nsented as a four-dirnensional point by storing t\\VO diagonal corner points of the\nbox. This approach does not support nearest neighbor and spatial join queries,\nsince distances in the original space are not reflected in the distances between\npoints in the higher-dirnensional space.\nf'urther, this approach increases the\ndirnensionality of the stored data, which leads to various problcrns (see Section\n28.7).\n\n982\nCHAPTER 28\nThe second approach is to store a record representing the region object in each\ngrid partition that overlaps the region object. This is unsatisfactory because it\nleads to a lot of additional records and 111akes insertion and deletion expensive.\nIn SUIJllnary, the Grid file is not a good structure for storing region data.\n28.6\nR TREES: POINT AND REGION DATA\nThe R tree is an adaptation of the B+ tree to handle spatial data, and it is a\nheight-balanced data structure, like the B+ tree. The search key for an Il tree\nis a collection of intervals, with one interval per diInension. We can think of\na search key value as a box bounded by the intervals; each side of the box is\nparallel to an axis. We refer to search key values in an R tree a'S bounding\nboxes.\nA data entry consists of a pair (n-dim,ensional box, riel), where rid identifies an\nobject and the box is the smallest box that contains the object. As a special\ncase, the box is a point if the data object is a point instead of a region. Data\nentries are stored in leaf nodes.\nNon-leaf nodes contain index entries of the\nforIll (n-dimensional box, pointer to a child node). The box at non-leaf node\nN is the srnallest box that contains all boxes associated with the child nodes;\nintuitively, it bounds the region containing all data objects stored in the subtree\nrooted at node N.\nFigure 28.6 shows two views of an example R tree. In the first view, we see the\ntree structure. In the second view, we see how the data objects and bounding\nboxes are distributed in space.\nRoot\n1~1~- ~~':~~l\n~~~~j\nI§4\nIII\nR2\n---1\nItt:::-:::·:::~::::::::::::::::::::::::::·:::·::::-:::::=:~;-I\n---------- --,--\n--~---,.-......,;J.. :::::::::=:::==j'j\nR6 L. _\nI\nR15 1,·-----'\nFigure 28.6\nTwo Views of an Example R Tree\nThere are 19 regions in the exarnple tree. R,egiolls RB through R19 represent\ndata objects and are shown in the tree\n(1..'3 data entries at the leaf level. The\nentry R.8*, for exarnple, consists of the bounding box for region Its and the\nrid of the underlying data ol>ject. R,egions III through 117 represent boundirlg\n\n81)(ltial Data Nfanagenl,cnt\nboxes for internal nodes in the tree. Region Rl, for exanlple, is the bounding\nbox for the space containing the left subtree, which includes data objects RB,\nR9, RIO, Rll, R12, R13, and R14.\nThe bounding boxes for two children of a given node can overlap; for ex,unplc,\nthe boxes for the children of the root node, Rl and R2, overlap. rrhis 111eans\nthat rnore than one leaf node could accornrnodate a given data object while\nsatisfying all bounding box constraints. However, every data object is stored\nin exactly one leaf node, even if its bounding box falls within the regions cor-\nresponding to two or Illore higher-level nodes. For exarnple, consider the data\nobject represented by R9. It is contained within both R3 and R4 and could be\nplaced in either the first or the second leaf node (going from left to right in the\ntree). We have chosen to insert it into the left-rnost leaf node; it is not inserted\nanywhere else in the tree. (We discuss the criteria used to Blake such choices\nin Section 28.6.2.)\n28.6.1\nQueries\nTo search for a point, we cornpute its bounding box B, which is just the point,\nand start at the root of the tree. We test the bounding box for each child of\nthe root to see if it overlaps the query box B, and if so, we search the subtree\nrooted at the child.\nIf more than one child of the root has a bounding box\nthat overlaps B, we lTIUSt search all the corresponding subtrees.\nThis is an\nirnportant difference with respect to B+ trees:\nThe seaTch faT even a single\npoint can lead us down several paths in the tree. When we get to the leaf level,\nwe check to see if the node contains the desired point. It is possible that ·we\ndo not visit any leaf node------this happens when the query point is in a region\nnot covered by any of the boxes associated with leaf nodes. If the search does\nnot visit any leaf pages, we know that the query point is not in the indexed\ndataset.\nSearches for region objects and range queries are handled sirnilarly by COluput-\ning a bounding box for the desired region and proceeding as in the search for\nan object. For a range query, when we get to the leaf level we ITlllst retrieve\nall region objects that belong there and test \"vhether they overlap (or are con-\ntained in, depending on the query) the given range. The reason for this test\nis that, even if the bounding box for an object overlaps the query region, the\nobject itself rnay not!\nAs an exalnple, suppose we want to find all objects that overlap our query\nregion, and the query region happens to be the box representing object R8.\nWe start at the root and find that the query box overlaps RJ but not R2.\nl\"herefore, we search the left subtree but not the right subtree. We then find\n\n984\n(;HAPTER 28\nthat the query box overlaps R,3 but not RA or ItS. So we seal\"eh the le:ft-rnost\nleaf and find object RB. As another exarnple, suppose that the query region\ncoincides \\vith Il9 rather than RB. A.gain, the query box overlaps RJ but not\nR,2 and so we search (only) the left subtree. Now we find that the query box\noverlaps both R3 and R,4 but not H,5. 'Vo therefore search the children pointed\nto by the entries for R3 and HA.\nAs a refinernent to the basic search strategy, we can approxirnate the query\nregion by a convex region defined by a collection of linear constraints, rather\nthan a bounding box, and test this convex region for overlap with the bounding\nboxes of internal nodes a'S we search down the tree. The benefit is that a convex\nregion is a tighter approxirnation than a box, and therefore we can sometirnes\ndetect that there is no overlap although the intersection of hounding boxes is\nnonernpty. 'rhe cost is that the overlap test is Inore expensive, but this is a\npure CPU cost and negligible in cOillparison to the potential I/O savings.\nNate that using convex regions to approximate the regions associated with\nnodes in the Il tree would also reduce the likelihood of false overlaps-----the\nbounding regions overlap, but the data object does not overlap the query\nregion-··-but the cost of storing convex region descriptions is rlluch higher than\nthe cost of storing bounding box descriptions.\n1'0 search for the nearest neighbors of a given point, we proceed as in a search\nfor the point itself.\nWe retrieve all points in the leaves that we exarnine a.s\npart of this search and return the point closest to the query point. If we do\nnot visit any leaves, then we replace the query point by a srnall box centered\nat the query point and repeat the search. If we still do not visit any leaves, we\nincrease the size of the box and search again, continuing in this fashion until\nwe visit a leaf node. 'Ve then consider all points retrieved froIll leaf nodes in\nthis iteration of the search and return the point closest to the query point.\n28.6.2\nInsert and Delete Operations\nTo insert a data object with rid T, we cornpute the bounding box B for the\nobject and insert the pair (B, r) into the tree. We start at the root node and\ntraverse a single path frorH the root to a leaf (in contrast to searching, where\n\"vo could traverse several such paths). At each level, 'we choose the child node\n\\V~hose bounding box needs the least enla.rgcruent (in tenns of the increase in its\narea) to cover the box [3. If several chilclren have bounding boxes that cover 13\n(or that require the sarriC enlargcrnent in order to cover 13), frorn these children,\n·we choose the one with the slnallest bounding box.\n\nSpatial I)ata !v{anageTnent\nAt the leaf level, we insert the object, and if necessary we enlarge the bounding\nbox of the leaf to cover box B.\nIf we have to enlarge the bounding box for\nthe leaf, this IllUSt be propagated to ancestors of the leaf-after the insertion is\ncOlnpleted, the bounding box for every node IIlUst cover the bounding box for\nall descendants. If the leaf node lacks space for the new object, we IllUSt split\nthe node and redistribute entries between the old leaf and the new node. \\lVe\nlllust then adjust the bounding box for the old leaf and insert the bounding\nbox for the new leaf into the parent of the leaf.\nAgain, these changes could\npropagate up the tree.\nr-----\n-\n-\n- --\nI\nI\nI ~~~~~~=tt::::::11\nI\nI\nI\nI\n,r-~\nI\n/\nI\nI\nBAD SPLIT\nI\nR3\nI ~DOOD SPLIT\n~--++--t-r----;---+--+~\nI\nFigure 28.7\nAlternative Redistributions in a Node Split\nIt is important to minimize the overlap between bounding boxes in the R tree\nbecause overlap causes us to search down multiple paths. The amount of overlap\nis greatly influenced by how entries are distributed when a node is split. Figure\n28.7 illustrates two alternative redistributions during a node split. There are\nfour regions, Rl, R2, R3, and R4, to be distributed across two pages. The first\nsplit (shown in broken lines) puts Rl and R,2 on one page and R3 and R4 on\nthe other. The second split (shown in solid lines) puts Rl and R4 on one page\nand R2 and R,3 on the other. Clearly, the total area of the bounding boxes for\nthe new pages is lnuch less with the second split.\nMinirnizing overlap using a good insertion algorithrll is very irnportant for good\nsearch perforrnance. A variant of the R, tree, called the R* tree, introduces the\nconcept of forced reinserts to reduce overlap: vVhen a node overflows, rather\nthan split it irnrnedia,tely, we rernove senne rnunber of entries (about ~30 percent\nof the node's contents works well) and reinsert thern into the tree. This rnay\nresult in all entries fitting inside sorne existing page and elirninate the need for\na split. The ~,* tree insertion algoritlllIlS also try to Ininirnize box peTirneteT8\nrather tha.n bo:r areas.\nTo delete a data object frOID an H, tree, vve have to proceed as in the search\nalgoritlun and potentially ex(unine several leaves. If the object is in the tree,\n\"ve rcrnove it.\nIn principle,\\ve can try to shrink the bounding box for the\n\n986\nCHAPTER 28\nleaf containing the object and the bounding boxes for all ancestor nodes. In\npractice, deletion is often irnplernented by sirnply rernoving the object.\nAnother variant, called the R+ tree, avoids overlap by inserting an object into\nlllultiple leaves if necessary. Consider the insertion of an object with bounding\nbox B at a node lV. If box B overlaps the boxes associated with more than\none child of N, the object is inserted into the subtree associated with each\nsuch child. For the purposes of insertion into child C with bounding box Be,\nthe object's bounding box is considered to be the overlap of Band Be. 1 The\nadvantage of the more cornplex insertion strategy is that searches can now\nproceed along a single path froln the root to a leaf.\n28.6.3\nConcurrency Control\nl'he cost of implernenting concurrency control algorithms is often overlooked in\ndiscussions of spatial index structures. l'his is justifiable in environments where\nthe data is rarely updated and queries are predominant. In general, however,\nthis cost can greatly influence the choice of index structure.\nWe presented a simple concurrency control algorithm for B+ trees in Section\n17.5.2: Searches proceed from root to a leaf obtaining shared locks on nodes;\na node is unlocked as soon as a child is locked. Inserts proceed from root to a\nleaf obtaining exclusive locks; a node is unlocked after a child is locked if the\nchild is not full. This algorithrn can be adapted to R trees by lllodifying the\ninsert algorithm to release a lock on a node only if the locked child has space\nand its region contains the region for the inserted entry (thus ensuring that the\nregion modifications do not propagate to the node being unlocked).\nWe presented an index locking technique for B+· trees in Section 17.5.1, which\nlocks a range of values and prevents new entries in this range frorn being inserted\ninto the tree. This technique is used to avoid the phantorn problern. Now let\nus consider how to adapt the index locking approach to R trees. The ba...\"ic idea\nis to lock the index page that contains or would contain entries with key values\nin the locked range. In R, trees, overlap between regions associated with the\nchildren of a node could force us to lock several (non-leaf) nodes on different\npaths frorn the root to SOH1C leaf. Additional cornplica..tions a.rise fronl having to\ndeal with changes \"in pi:lIticular, enlargernents due to insertions....·in the regions\nof locked nodes. vVithout going into further detail, it should be clear that index\nlocking to avoid phant0l11 insertions in H. trees is both harder and less efficient\nthan in 13+ trees.\nFurther, idea\",') such ae:; forced reinsertion in It* trees and\n----_\n_--\n1Insertion into an R+ tree involves additional details. For example, if box B is not contained in the\ncollection of boxes associat(~d with the children of N whose boxes 13 overlaps, one of the childnm must\nluwe its box enlarged so that 13 is contajned in the collection of boxes associ<lf;ed with\nthf~ children.\n\nSpatial Data !vIanage7Tu~nt\n98}\nrIlultiple insertions of an object in R+ trees nlake index locking prohibitively\nexpenSIve.\n28.6.4\nGeneralized Search Trees\nThe B+ tree and R tree index structures are sirnilar in 111any respects: Both\nare height-balanced, in which searches start at the root of the tree and proceed\ntoward the leaves; each node covers a portion of the underlying data space, and\nthe children of a node cover a subregion of the region associated with the node.\nThere are irnportant differences of course-for exa111ple, the space is linearized\nin the B+ tree representation but not in the R\ntree·~··~~but the cornrnon features\nlead to striking siruilarities in the algorithms for insertion, deletion, search, and\neven concurrency control.\nThe generalized search tree (GiST) abstracts the essential features of tree\nindex structures and provides 'template' algorithms for insertion, deletion, and\nsearching. The idea is that an ORDBMS can support these template algorithnls\nand thereby make it easy for an advanced database user to implement specific\nindex structures, such as R trees or variants, without nlaking changes to any\nsystem code. The effort involved in writing the extension 1nethods is l11uch less\nthan that involved in ilIlplementing a new indexing 111ethod frolIl scratch, and\nthe performance of the GiST te111plate algorithms is cornparable to specialized\ncode.\n(For concurrency control, 1110re efficient approaches are applicable if\nwe exploit the properties that distinguish B+ trees from R trees.\nHowever,\nB+ trees are irnplernented directly in most cOllllIlercial DBMSs, and the GiST\napproach is intended to support 1nore conlplex tree indexes.)\nrrhe ternplate algorithlIls call on a set of extension methods specific to a par-\nticular index structure, and these 111USt be supplied by the irnplernentor. For\nexarnple, the search te1nplate searches all children of a node whose region is\nconsistent with the query. In a B-t- tree the region a.ssociated with a node is\na range of key\nvalues~ and in an R tree, the region is spatial.\nThe check to\nsee whether a region is consistent with the query region is specific to the index\nstructure and is an exarnple of an extension rnethod. As another exa.rnple of an\nextension rnethod, consider ho\\;y to choose the child of an Il tree node to insert\na new entry into. This choice can be rnade based on \\vhich candidate child's\nregion needs expanded the least; an extension rnethod is required to calculate\nthe required expansions for candidate children and choose the child into Vlhich\nto insert the entry.\n\n988\n(;HAPTER 28\n28.7\nISSUES IN HIGH-DIMENSIONAL INDEXING\nThe spatial indexing techniques just discussed ''lark quite \\vell for t\\VO- and\nthree-dirnensional dat&\"ets, which are encountered in Illany applications of spa-\ntial data. In SCHne applications, such as content-based ilnage retrieval or text\nindexing, however, the nurIlber of dirnensions can be large (tens of dirnensions\nare not unCOtnlnon). Indexing such high-dirnensional data presents unique chal-\nlenges, and ne\\'l techniques are required. For exanlple, sequential scan becomes\nsuperior to R, trees even when searching for a single point for datasets with\n1nore than about a dozen dirnensions.\nIIigh-dirnensional datasets are typically collections of points, not regions, and\nnearest neighbor queries are the rnost cotnrIlon kind of queries. Searching for\nthe nearest neighbor of a query point is rneaningful when the distance frotn the\nquery point to its nearest neighbor is less than the distance to other points.\nAt the very least, we want the nearest neighbor to be appreciably closer than\nthe data point farthest from the query point. High-dimensional data poses a\npotential problem: For a wide range of data distributions, as dimensionality d\nincreases, the distance (frolll any given query point) to the nearest neighbor\ngrows closer and closer to the distance to the farthest data point! Searching\nfor nearest neighbors is not lneaningful in such situations.\nIn many applications, high-dirnensional data may not suffer frorn these prob-\nlenls and may be amenable to indexing. However, it is advisable to check high-\ndimensional datasets to rnake sure that nearest neighbor queries are meaningful.\nLet us call the ratio of the distance (frorn a query point) to the nearest neigh-\nbor to the distance to the farthest point the contrast in the dataset. We can\nmeasure the contra.'3t of a dataset by generating a number of sarnple queries,\nmeasuring distances to the nearest and farthest points for each of these sarIlple\nqueries and cornputing the ratios of these distances, and taking the average\nof the 111easured ratios. In applications that call for the nearest neighbor, we\nshould first ensure that datasets have good contrast by ernpirical tests of the\ndata.\n28.8\nREVIEW QUESTIONS\nAnswers to th~ review questions can be found in the listed sections.\n•\nvVhat are the characteristics of spatial data?\nvVhat is a spatial extent?\nWhat are the differences between spatial range queries, nearest neighbor\nqueries, and spatial join queries? (Section 28.1)\n\nSpatial Data 1\\1an,age'lnent\n989\n$\nIJII\nNalne several applications that deal ,,\"ith spatial data and specify their\nrequircrnents on a database systeln. \\Vhat is a feature vector and ho\\v is it\nused? (Section 28.2)\nIII\nvVhat is a IIlulti-dirnensional index'? \\\\That is a spatial index? \\tVhat are\nthe differences bet\\vccn a spatial index and a B+ tree? (Section 28.3)\nIlIl\n\\iVhat is a space-filling curve, and hovv can it be used to design a spatial\nindex?\nDescribe a spatial index structure ba\"oscd on space-filling curves.\n(Section 28.4)\nII\nWhat data structures are lnaintained for the Grid file index?\nHow do\ninsertion and deletion in a Grid file work? For what types of queries and\ndata are Grid files especially suitable and why? (Section 28.5)\nII\nWhat is an R tree?\n\"Vhat is the structure of data entries in R trees?\nHow can we lninimize the overlap between bounding boxes when splitting\nnodes? lIow does concurrency control in a R tree work? Describe a generic\nteulplate for tree-structured indexes. (Section 28.6)\n•\nWhy is indexing high-dilnensional data very difficult? What is the ilnpact\nof the dirrlensionality on nearest neighbor queries? What is the contrast of\na dataset? (Section 28.7)\nEXERCISES\nExercise 28.1 Answer the following questions briefly:\n1. How is point spatial data different frolll nonspatial data?\n2. How is point data different fronl region data?\n~). Describe three cornrnon kinds of spatial queries.\n4. Why are nearest neighbor queries irnportant in rnultin1edia applications?\n5. How is a 13+ tree index different froIll a spatial index? vVhen would you use a 13+ tree\nindex over a spatial index for point data? vVhen would you use a spatial index over a\n13+ tree index for point data?\n6. vVhat is the relationship between Z-ordering and Region Quad trees?\n7. Compare Z-ordering and H.ilbert curves as techniques to cluster spatial data.\nExercise 28.2 Consider Figure 28.3, \\vhich illustrates Z-ordering and Region Quad trees.\nAnswer the following questions.\n1. Consider the region cOInposed of the points with these Z-values: 4, 5, 6, and 7. lVlark the\nnodes that represent this region in the Region Quad tree shown in Figure 28.:3. (Expand\nthe tree if necessary.)\n2. llepeat the preceding exercise for the region cornposed of the points with Z-vaJues 1 and\n~~.\n\n990\nCHAPTER 28\n3. Repeat it for the region composed of the points with Z-values 1 and 2.\n4. Repeat it for the region cOll1posed of the points with Z-values 0 and 1.\n5. Repeat it for the region coruposed of the points with Z-values 3 and 12.\n6. Repeat it for the region cmnposed of the points with Z-values 12 and 15.\n7. Repeat it for the region COITlposed of the points with Z-values 1, 3, 9, and 11.\n8. Repeat it for the region COITlposed of the points with Z-values 3, 6, 9, and 12.\n9. Repeat it for the region COITlposed of the points with Z-values 9, 11, 12, and 14.\n10. Repeat it for the region cornposed of the points with Z-values 8, 9, 10, and 11.\nExercise 28.3 This exercise also refers to Figure 28.3.\n1. Consider the region represented by the 01 child of the root in the Region Quad tree\nshown in Figure 28.3. What are the Z-values of points in this region?\n2. Repeat the preceding exercise for the region represented by the 10 child of the root and\nthe 01 child of the 00 child of the root.\n3. List the Z-values of four adjacent data points distributed across the four children of the\nroot in the Region Quad tree.\n4. Consider the alternative approaches of indexing a two-dimensional point dataset using a\nB+ tree index: (i) on the composite search key (X, Y), (ii) on the Z-ordering computed\nover the X and Y values. Assuming that X and Y values can be represented using two\nbits each, show an example dataset and query illustrating each of these cases:\n(a) The alternative of indexing on the COITlposite query is faster.\n(b) The alternative of indexing on the Z-value is faster.\nExercise 28.4 Consider the Grid file instance with three points 1, 2, and 3 shown in the\nfirst part of Figure 28.5.\n1. Show the Grid file after inserting each of these points, in the order they are listed: (), 9,\n10, 7, 8, 4, and 5.\n2. Assume that deletions are handled by sirnply rernoving the deleted points, with no at-\nterl1pt to merge empty or underfull pages. Can you suggest a siruple concurrency control\nscheme for Grid files?\n3. Discuss the use of Grid files to handle region data.\nExercise 28.5 Answer each of the following questions independently with respect to the R\ntree shown in Figure 28.6.\n(That is, don't consider the insertions corresponding to other\nquestions when answering a given question.)\n1. Show the bounding box of a new object that can be inserted into R4 but not into n:3.\n2. Show the bounding box of a new object that is contained in both Rl and R6 but is\ninserted into R6.\n3. Show the bounding box of a new object that is contained in both Rl and R6 and is\ninserted into Rl. In which leaf node is this object placed?\n4. Show the bounding box of a new object that could be inserted into either R4 or R5 but\nis placed in R5 based on the principle of least expansion of the bounding box area.\n\n8pat'ial Data l\\lanagernent\n991\n5. Given an exarIlple of an object such that searching for the object takes us to both the\nRl and R2 subtrees.\n6. Give an eXCllnple query that takes us to nodes Ra and R5. (Explain if there is no such\nquery.)\n7. Give an exanlple query that takes us to nodes R3 and R4 but not to R5.\n(Explain if\nthere is no such query.)\n8. Give an eXaInple query that takes us to nodes Ra and R5 but not to R4.\n(Explain if\nthere is no such query.)\nBIBLIOGRAPHIC NOTES\nSeveral multidimensional indexing techniques have been proposed. These include Bang files\n[286], Grid files [565], hB trees [491]' KDB trees [630], Pyrarnid trees [80] Quad trees[649],\nR trees [350], R* trees [72], R+ trees, the TV tree, and the VA file [767].\n[322] discusses\nhow to search R trees for regions defined by linear constraints. Several variations of these,\nand several other distinct techniques, have also been proposed; Samet's text [650] deals with\nmany of them. A good recent survey is [294].\nThe use of Hilbert curves for linearizing multidimensional data is proposed in [263]. [118] is an\nearly paper discussing spatial joins. Hellerstein, Naughton, and Pfeffer propose a generalized\ntree index that can be specialized to obtain many of the specific tree indexes mentioned\nearlier [376]. Concurrency control and recovery issues for this generalized index are discussed\nin [447].\nHellerstein, Koutsoupias, and Papadinlitriou discuss the complexity of indexing\nschemes [377], in particular range queries, and Beyer et a1. discuss the problerlls arising with\nhigh dimensionality [93].\nFaloutsos provides a good overview of how to search multirnedia\ndatabases by content [258]. A recent trend is towards spatiotemporal applications, such as\ntracking rnoving objects [782].\n\n29\nFURTHER REi\\DING\n..\nWhat is next?\n..\nKey concepts: TP monitors, real-tirne transactions; data integra-\ntion; mobile data; main meInory\ndataba.~es; multimedia databases;\nGIS; tenlporal databases; Bioinformatics; infonnation visualization\nThis is not the end. It is not even the beginning of the end. But it is, perhaps,\nthe end of the beginning.\n······-·Winston Churchill\nIn this book, we concentrated on relational databa.'3e systerus and discussed\nseveral fundaruental issues in detail.\nHowever, our coverage of the database\narea, and indeed even the relational database H,rea, is far from exhaustive. In\nthis chapter, we look briefly at several topics vve did not cover, with the goal of\ngiving the reader SOUle perspective and indicating directions for further study.\nvVe begin with a discussion of advanced transaction processing concepts in\nSection 29.1. vVe discuss integrated access to data frOUl rnultiple databases in\nSection 29.2 and touch on Inobile applications that connect to databases in Sec-\ntion 29.3. \\Ve consider the irnpact of increasingly larger rnain Inenlory sizes in\nSection 29.4. \\Ve discuss rnultirnedia databctses in Section 29.5, geographic in-\nforrnation systerns in Section 29.G, tcrnporaJ data in Section 29.7, and sequence\ndata in Section 29.8. \\Ve conclude with a look at inforrnation visualization in\nS ·,t·\n. ')0 C)\nk. ec ,Ion ....J ••'.\n992\n\nFUTthcr- Read'ing\n99a\nT'he applications covered in this chapter push the lirnits of currently available\ndatabase technology and drive the developrnent of new techniques. As even our\nbrief coverage indicates, Innch \\vork lies ahead for the database field!\n29.1\nADVANCED TRANSACTION PROCESSING\nThe concept of a transaction has wide applicability for a variety of distributed\ncOlnputing tasks, such as airline reservations, inventory rnanagernent, and elec-\ntronic COlnnlerce.\n29.1.1\nTransaction Processing Monitors\nCornplex applications are often built on top of several resource managers,\nsuch as database managernent systenls, operating systerns, user interfaces, and\nmessaging software. A transaction processing (TP) monitor glues together\nthe services of several resource managers and provides application programmers\na uniform interface for developing transactions with the ACID properties. In\naddition to providing a uniform interface to the services of difl'erent resource\nillanagers, a TP rnonitor also routes transactions to the appropriate resource\nrnanagers.\nFinally, a TP monitor ensures that an application behaves as a\ntransaction by implernenting concurrency control, logging, and recovery func-\ntions and by exploiting the transaction processing capabilities of the underlying\nresource rnanagers.\nTP rnonitors are used in environments where applications require advanced\nfeatures, such as access to rnultiple resource lllanagers, sophisticated request\nrouting (also called workflow management); assigning priorities to trans-\nactions and doing priority-based load-balancing across servers, and so on. A\nDBlVIS provides lllany of the functions supported by a TP monitor in addition\nto processing queries and database updates efficiently. A DBMS is appropri-\nate for environrnents where the wealth of transaction rnanagernent capabilities\nprovided by a TP rnonitor is not necessary and, in particular, \\vhere very high\nscalability (with respect to transaction processing activity) and interoperability\nare not essential.\nThe transaction processing capabilities of database systerlls are irnproving con-\ntinually. For eKarnple, rnany vendors offer distributed DBMS products today in\nwhich a transaction can execute across several resource rnanagers, each of which\nis a DBMS. Currently, all the DBlVISs Inust be frorn the saIne vendor; however,\nas transaction-oriented services frorn different vendors becom.e rnore standard-\nized, distributed, heterogeneous DBlV'ISs should becorne available. Eventually,\nperhaps, the functions of current rrp rnonitors will also be available in rnany\n\n994\nDBIV1Ss; for now, 'I'P rnonitors provide essential infrastructure for high-end\ntransaction processing ellviroIllnents.\n29.1.2\nNew Transaction Models\nConsider an application such as cornputer-aided design, in which users retrieve\nlarge design objects froIn a database and interactively analyze and 1110dify thenl.\nEach transaction takes a long tiIne---Ininutes or even hours, whereas the TPC\nbench1nark transactions take under a millisecond-----and holding locks this long\naffects perfonnance. F\\uther, if a crash occurs, undoing an active transaction\ncOlllpletely is unsatisfactory, since considerable user effort may be lost. Ideally,\nwe want to restore 1nost of the actions of an active transaction and reSlune\nexecution. Finally, if several users are concurrently developing a design, they\nnlay want to see changes being rnade by others without waiting until the end\nof the transaction that changes the data.\nTo address the needs of long-duration activities, several refinements of the\ntransaction concept have been proposed. The basic idea is to treat each trans-\naction as a collection of related subtransactions. Subtransactions can acquire\nlocks, and the changes made by a subtransaction become visible to other trans-\nactions after the subtransaction ends (and before the nlain transaction of which\nit is a part commits). In multilevel transactions, locks held by a subtrans-\naction are released when the subtransaction ends. In nested transactions,\nlocks held by a subtransaction are assigned to the parent (sub)transaction when\nthe subtransaction ends. These refinements to the transaction concept have a\nsignificant effect on concurrency control and recovery algorithnls.\n29.1.3\nReal-Time DBMSs\nSOllIe transactions 1nust be executed within a user-specified deadline. A hard\ndeadline Ineans the value of the transaction is zero after the deadline.\nFor\nexa1nple, in a DBMS designed to record bets on horse races, a transaction\nplacing a bet is worthless once the race begins.\nSuch a transaction should\nnot be executed; the bet should not be placed.\nA soft deadline rIlcal1S the\nvalue of the transaction decrccl..'3cs after the deadline, eventually going to zero.\n:For exarnple, in a DB1rlS designed to rnonitor S(Hne activity (e.g., a c01nplex\nreactor), a transaction that looks up the current reading of a sensor rnust be\nexecuted within a sl10rt tiIne, sa.y, one second. The longer it takes to execute\nthe tra.nsaction, the less useful the reading becorn.es. In a real-tirne DBl\\1S, the\ngoal is to 1naxirnize the value of executed transactions, and the DBlVIS 111Ust\nprioritize transactions, taking their deadlines into account.\n\n[further Ileading\n29.2\nDATA INTEGRATION\n995\n$\nAs datab&ses proliferate, users want to access data fronl rnore than one source.\nFor exaulple, if several travel agents rnarket their travel packages through the\nWeb, custorners would like to cornpare packages from different agents. A rnore\ntraditional exaruple is that large organizations typically have several databases,\ncreated (and rnaintained) by different divisions, such as Sales, Production, and\nPurchasing. \\i\\Thile these databases contain much common inforrnation, deter-\nmining the exact relationship between tables in different databases can be a\ncomplicated problem.\nFor example, prices in one database might be in dol-\nlars per dozen items, while prices in another database might be in dollars per\nitelll. The developruent of XML DTDs (see Section 7.4.3) offers the pronlise\nthat such sernantic rnisrnatches can be avoided if all parties conforrll to a single\nstandard DTD. However, there are many legacy databases and rllost dornains\nstill do not have agreed-upon DTDs; the problem of selllantic rnismatches will\nbe encountered frequently for the foreseeable future.\nSemantic mismatches can be resolved and hidden fronl users by defining rela-\ntional views over the tables from the two databases. Defining a collection of\nviews to give a group of users a uniform presentation of relevant data frorn\nrnultiple databases is called semantic integration. Creating views that mask\nsernantic mismatches in a natural manner is a difficult task and has been widely\nstudied. In practice, the task is rllade harder because the scheruas of existing\ndatabases are often poorly documented; hence, it is difficult to even understand\nthe meaning of rows in existing tables, let alone define unifying views across\nseveral tables frorll different databases.\nIf the underlying databases are rnanaged using difl\"erent DBlVISs, as is often\nthe case, SaIne kind of 'middleware' rnust be used to evaluate queries over the\nintegrating views, retrieving data at query execution tirne by using protocols\nsuch as Open Database Connectivity (ODBC) to give each underlying databa..'3c\na uniforrn interface, as discussed in Chapter 6. Alternatively, the integrating\nviews can be nlaterialized and stored in a data warehouse, as discussed in\nChapter 25. Queries can then be executed over the warehoused data without\naccessing the source DBlVISs at run-tirne.\n29.3\nMOBILE DATABASES\nThe availability of portable coruputers and wireless eorrnnunications has created\na nevv breed of nornadic database users. At one level, these users are sirnply\naccessing a databa.>se through a network, which is silnilar to distributed DBMSs.\nAt another level, the network a:s well as data and user characteristics now have\nseveral novel properties, Wllich affect b<l..sic assurnptioI1S in rnany cornponents\n\n996\n(;HAPTER 49\nof a\nDB~fS~ including the query\nengine~ transaction rnanager, and recovery\nInanager:\n•\n1Isers are connected through a \\vireless link whose band,vidth is 10 times\nless than Ethernet and 100 tiIIles less than ATM networks. COIIul1unication\ncosts are therefore significantly higher in proportion to I/O and CPU costs.\n11\nUsers' locations constantly change, and Inobile corllputers have a liInited\nbattery life.\n'Therefore, the true cOllullunication costs reflect connection\ntiIne and battery usage in addition to bytes transferred and change con-\nstantly depending on location. Data is frequently replicated to lninirnize\nthe cost of accessing it from different locations.\n•\nAs a user moves around, data could be accessed froIn multiple database\nservers within a single transaction.\nThe likelihood of losing connections\nis also lnnch greater than in a traditional network.\nCentralized transac-\ntion rnanagenlent may therefore be irnpractical, especially if sorne data is\nresident at the mobile computers.\nWe may in fact have to give up on\nACID transactions and develop alternative notions of consistency for user\nprograms.\n29.4\nMAIN MEMORY DATABASES\nThe price of rnain llleInory is now low enough that we can buy enough main\nrnernory to hold the entire database for many applications; with 64-bit ad-\ndressing, rnodern CPUs also have very large address spaces. Sorne commercial\nsysterns now have several gigabytes of rrlain IneInory. This shift proInpts a reex-\narnination of scnne basic DBMS design decisions, since disk accesses no longer\ndorninate processing tilue for a Inemory-resident database:\n•\nlVlain nlerllory does not survive systelll crashes, and so we still have to\niluplernent logging and recovery to ensure transaction atolnicity and dura-\nbility. Log records rnust be written to stable storage at conlluit tirne, and\nthis process could becorne a bottleneck. To rninirnize this problern, rather\nthan comnlit each transaction ,1S it conlpletes, we can collect cOlupleted\ntransactions and cornlnit theIu in batches; this is called group commit.\nRecovery algorithrns can also be optirnized, since pages rarely have to be\nvvrritten out to rrlake roorn for other pages.\n•\nThe irnplerrlentation of in-lnelnory operations has to be optinlized carefully,\nsince disk accesses are no longer the lirniting factor for perforrnance.\n•\nA ne,v criterion llUlst be considered while optirnizing queries, the alllount\nof space required to execute a plan. It is iInportant to rninirnize the space\n\nFUTther llearling\n991\noverhead because exceeding available physical Incrnory would lead to swap-\nping pages to disk (through the operating systcrIl'8 virtual rncillory ruech-\nanisIIls), greatly slo\\ving do\\vn execution.\n•\nPage-oriented data structures becolue less iInportant (since pages are 110\nlonger the unit of data retrieval), and clustering is not ilnportant (since\nthe cost of accessing any region of IIlain rneIIlory is uniforrn).\n29.5\nMULTIMEDIA DATABASES\nIn an object-relational DB~1S, users can define ADTs \\vith appropriate rneth-\nods, which is an irnprovement over an RDBI\\1S. Nonetheless, supporting just\nADTs falls short of what is required to deal with very large collections of\nmultimedia objects, including audio, irnages, free text, text nlarked up in\nHTML or variants, sequence data, and videos. Illustrative applications include\nNASA's EGS project, which aims to create a repository of satellite irnagery;\nthe JIUlnan Genorne project, which is creating databases of genetic inforrnation\nsuch as GenBank; and NSF/DARPA's Digital Libraries project, which aiIns to\nput entire libraries into database systems and make thelll accessible through\ncOlnputer networks. Industrial applications, such as collaborative developnlent\nof engineering designs, also require multimedia database rnanagernent and are\nbeing addressed by several vendors.\nWe outline some applications and challenges in this area:\n•\nContent-Based Retrieval: Users 111Ust be able to specify selection concli-\ntions based on the contents of rllultilnedia objects. For exanlplc, users lllay\nsearch for inlages using queries such as \"Find all irnages that are sirnilar to\nthis image\" and \"Find all inlages that contain at lea\"t three airplanes.\" As\ninulges are inserted into the database, the DBMS lllUSt analyze thern and\nautomatically eJ:tract features that help answer such content-based queries.\n1~his inforrnation can then be used to search for inlages that satisfy a given\nquery, ae;,; discussed in Chapter 28. As another exarnple, users would like to\nsearch for docurnents of interest using infonnation retrieval techniques and\nkeyword searches.\nVendors are rnoving toward incorporating such tech-\nniques into DBMS products. It is still not clear how these dOlnain-specific\nretrieval and search techniques can be corubined effectively \\vith traditional\nDBIvIS quei-ies.\nR,csearch into abstract data types and ()R,DBMS query\nprocessing has provided a starting point, but lnore work is needed.\nII\nManaging Repositories of Large Objects: Traditionally, DBlVISs have\nconcentrated on tables that contain a large nurnber of tuples, each of \\vhich\nis relatively srnaii. ()nce Illultilnedia objects such as irnages, sound clips,\nand videos are stored in a database, individual objects of very large size\n\n998\nCHAPTER 39\nhave to be handled efficiently. For example, compression techniques lllUSt\nbe carefully integrated into the DBIvlS environrnent. As another exarnple,\ndistributed DBMSs HUlst develop techniques to efficiently retrieve such\nobjects. Retrieval of r11ultir11edia objects in a distributed systern has been\naddressed in liInited contexts, such as client-server systerns, but in general\nreulains a difficult probleul.\n•\nVideo-On-Denland: lVlany cornpanies want to provide video-on-denland\nservices that enable users to dial into a server and request a particular\nvideo. The video Inust then be delivered to the user's COI11puter in real time,\nreliably and inexpensively. Ideally, users nlust be able to perform farniliar\nVCR functions such as fast-forward and reverse. Fronl a database perspec-\ntive, the server has to contend with specialized real-time constraints; video\ndelivery rates must be synchronized at the server and at the client, taking\ninto account the characteristics of the communication network.\n29.6\nGEOGRAPHIC INFORMATION SYSTEMS\nGeographic Information Systems (GIS) contain spatial information about\ncities, states, countries, streets, highways, lakes, rivers, and other geographical\nfeatures and support applications to combine such spatial information with\nnon-spatial data. As discussed in Chapter 28, spatial data is stored in either\nraster or vector formats. In addition, there is often a terTIporal dirnension, a'S\nwhen we measure rainfall at several locations over time. An important issue\nwith spatial datasets is how to integrate data froIn rnultiple sources, since each\nsource rnay record data using a different coordinate system to identify locations.\nNow let us consider how spatial data in a GIS is analyzed. Spatial informa-\ntion is lllost naturally thought of as being overlaid on maps. rTypical queries\ninclude \"What cities lie on 1-94 between Madison and Chicago?\" and \"What\nis the shortest route from Madison to St. Louis?\" These kinds of queries can\nbe addressed using the techniques discussed in Chapter 28. An emerging ap-\nplication is in-vehicle navigation aids. With Global Positioning Systcrn (CPS)\ntechnology, a car's location can be pinpointed, and by accessing a databa.se of\nlocal rnaps, a driver can receive directions froIn his or her current location to a\ndesired destination; this application also involves rnobile databa..'3e access!\nIn addition, many applications involve interpolating rneasurernents at certain\nlocations across an entire region to obtain a rnodel and cornbining overlapping\nrnodels. For exarnple, if \\-ve have rneasured rainfall at certain locations, we can\nuse the Triangulated Irregular Network (TIN) approach to triangulate\nthe region, with the loeations at which we have measurcrnents being the ver-\ntices of the triangles.\nThen, we use sorne forrn of interpolation to estirnate\n\n}tuTther Reading\n999\nthe rainfall at points within triangles. Interpolation, triangulation, Il1ap over-\nlays, visualization of spatial data, and rnany other dornain-specific operations\nare supported in GIS products such a,,'3ESRI Systerlls' ARC-Info. l'he1'efore,\n\\vhile spatial query processing techniques as discussed in Chapter 28 are an\nirnportant part of a GIS product, considerable additional functionality rnust be\nincorporated as well. How best to extend 0 ltDB1.1S systenls with this addi-\ntional functionality is an irnportant problenl yet to be resolved. Agreeing on\nstandards for data representation forrnats and coordinate systeuls is another\nlTIajor challenge facing the field.\n29.7\nTEMPORAL DATABASES\nConsider the following query:\n\"Find the longest interval in which the same\nperson managed two different departlTIents.\" Many issues are associated with\nrepresenting telnporal data and supporting such queries. We need to be able to\ndistinguish the times during which sOlnething is true in the real world (valid\ntime) from the times it is true in the database (transaction time).\nThe\nperiod during which a given person rnanaged a departrnent can be indicated by\ntwo fields from and to, and queries must reason about time intervals. further,\ntemporal queries require the DBMS to be aware of the anolTIalies associated\nwith calendars (such as leap years).\n29.8\nBIOLOGICAL DATABASES\nBiolnfornlatics is an emerging field at the intersection of Biology and COHlputer\nScience. FraIn a database standpoint, the rapidly growing data in this area h~LS\n(at lea..'3t) two interesting characteristics. First, a lot of loosely str'UchLTcd data\nis widely exchanged, leading to interest in integration of such data. This ha\",,,\nrnotivated SOlne of the research in the area of XML repositories.\nThe second interesting feature is sequence data.\nDNA sequences are being\ngenerated at a rapid pace by the biological cOllnnunity. The field of biological\ninforrnation rnanagernent and analysis ha\"s becorne very popular in recent years,\ncalled bioinformatics. Biological data, such as DNA sequence data, charac-\nterized by cornplex structure and nurnerous relationships arnong data elernents,\nrllany overlapping and incoruplete or erroneous data fragrnents (because experi-\nInentally collected data froIll several groups, often working on related problelIls,\nis stored in the databa\"ses), a need to frequently change the databa..se 8chcrna\nitself as ne¥l kinds of relationships in the data are discovered, and the need to\nrnaintain several versions of data for archival and reference.\n\nIOOO\nCHAPTER ~g\n29.9\nINFORMATION VISUALIZATION\nAs coruputors becoule faster and rnain rnornory cheaper, it beconies increa...~­\ningly feasible to create visual presentations of data, rather than just text-b&'3ed\nreports. Data visualization rnakes it easier for users to understand the infor-\nInation in large cornplex data..'3ets.\nThe challenge here is to lTlake it easy for\nusers to develop visual presentations of their data and interactively query such\npresentations.\nAlthough a nurnber of data visualization tools are available,\nefficient visualization of large datasets presents Inany challenges.\nThe need for visualization is especially irnportant in the context of decision\nsupport; when confronted with large quantities of high-dhnensional data and\nvarious kinds of data sUffirnaries produced by using analysis tools such as SQL,\nOLAP, and data ll1ining algorithrns, the inforrnation can be overwhehning.\nVisualizing the data, together with the generated sumrnaries, can be a powerful\nway to sift through this infonnation and spot interesting trends or patterns.\nThe hurnan eye, after all, is very good at finding patterns. A good framework\nfor data mining lTIUst combine analytic tools to process data and bring out\nlatent anolllalies or trends with a visualization environment in which a user\ncan notice these patterns and interactively drill down to the original data for\nfurther analysis.\n29.10\nSUMMARY\n'rhe database area continues to grow vigorously, in terrns of both technology\nand applications. The fundarnental rea...,on for this growth is that the amount\nof inforrnation\nstorE~d and processed using computers is growing rapidly. Re-\ngardless of the nature of the data and the intended applications, users need\ndatabase rnanagernent systems and their services (concurrent access, crash re-\ncovery, ea...,y and efficient querying, etc.)\na'3 the vohllue of data increases. As\nthe range of applications is broadened, however, SOIlIC shortcornings of current\nDBMSs becoIne serious lilTlitations. These problerus are being actively studied\nin the database research cornrnunity.\n'The coverage in this book provides an introduction, but is not intended to cover\nall aspects of datab<:\"k'3e systerns. Anlple rnaterial is available for further study,\nas this chapter Hlustrates, and we hope that the reader is rnotivated to pursue\nthe leads in the bibliography. Bon voyage!\n\nFU7theT Recui'ing\nBffiLIOGRAPHIC NOTES\n1001\n[338] contains a conlprehensive treatnlent of all &<spects of transaction processing. See [241J\nfor several papers that describe new transaction models for nontraditional applications such\nas CAD/CATv1. [1,577,696,711,761] are SaIne of the Inany papers on real-tirne databases.\nDetenllining which entities are the same across different databases is a difficult probleIn;\nit is an example of a semantic lllisrnatch.\nResolving such nlismatches h3..\" been addressed\nin rIlany papers, including [424, 476, 641, 663].\n[389] is an overview of theoretical work in\nthis area. Also see the bibliographic notes for Chapter 22 for references to related work on\nrnultidatabases, and see the notes for Chapter 2 for references to work on view integration.\n[304] is an early paper on main mernory databa..ses. [102, 406] describe the Dali rnain rllerllory\nstorage manager.\n[421] surveys visualization idioms designed for large databases, and [342]\ndiscusses visualization for data mining.\nVisualization systerns for databases include DataSpace [592], DEVise [489], IVEE [27], the\nMineset suite from SGI, Tioga [31], and VisDB [420]. In addition, a number of general tools\nare available for data visualization.\nQuerying text repositories has been studied extensively in information retrieval; see [626] for\na recent survey. This topic has generated considerable interest in the database cOITnnunity\nrecently because of the widespread use of the Web, which contains many text sources.\nIn\nparticular, HTML dOCUITlents have sonle structure if we interpret links as edges in a graph.\nSuch documents are examples of selllistructured data; see [2] for a good overview.\nRecent\npapers on queries over the Web include [2, 445, 527, 564].\nSee [576] for a survey of multimedia issues in database management. There has been much\nrecent interest in database issues in a mobile computing environment; for example, [387,398].\nSee [395] for a collection of articles on this subject. [728] contains several articles that cover\nall aspects of telnporal databases.\nThe use of constraints in databases has been actively\ninvestigated in recent years; [416] is a good overview. Geographic Infonnation SysteIT1S have\nalso been studied extensively; [586] describes the Paradise systern, which is notable for its\nscalability.\n'The book [794] contains detailed discussions of ternporal databases (including the TSQL2\nlanguage, which is influencing the SQL standard), spatial and nnIltimedia databases, and\nuncertainty in databa.ses.\n\n30\nTHE MINIBASE SOFTWARE\nPractice is the best of all instructors.\n-Publius Syrus, 42 B.C.\nMinibase is a small relational DBMS, together with a suite of visualization\ntools, that has been developed for use with this book. While the book rnakes\nno direct reference to the software and can be used independently, Minibase\noffers instructors an opportunity to design a variety of hands-on &'3signments,\nwith or without programming.\nTo see an online description of the software,\nvisit this URL:\nhttp://www.cs.wisc.edu/-dbbook/minibase.html\nThe software is available freely through ftp. By registering themselves as users\nat the UR,L for the book, instructors can receive prompt notification of any\nInajor bug reports and fixes. Sarnple project assignments, which elaborate on\nSOIne of the briefly sketched ideas in the project-based exercises at the end of\nchapters, can be seen at\nhttp://www.cs.wisc.edu/-dbbook/minihwk.html\nInstructors should consider making sillall lllodifications to each ::t..ssignlnent\nto discourage undesin1ble 'code reuse' by students; assignrnent handouts for-\nrnatted using Latex are available by ftp.\nInstructors can also obtain solu-\ntions to these assiglunents by contacting the authors (raghu@cs. wise. edu,\nj ohannes@cs. cornell. edu).\n30.1\nWHAT IS AVAILABLE\nNIiniba.se is intcllded to snpplCIllent the use of a cornrnercial DBlVfS such as\n()racle or Sybase in course projects, not to replace theIn. \\Vhile a cornlnerciaJ\nDBl\\r1.S is ideal for\nSC:~L assignrnents, it does not help students understand hovv\nthe DBNIS w'orks. IVlinibase is intended to address the latter issue; the subset\nof S(~L that it supports is intentionally kept 8rnall, and students should also\nbe asked to use a connnercialDBIVlS for writing SQL queries and prograrns.\n1002\n\nThe N!inibase Soft'wo/re\n1003\nt\n:NIinibase is provided on an as-is basis with no \\varrantics or restrictions for\neducational or personal use. It includes the fol1o\\ving:\n•\nCode for a sl11al1 single-user relational DB1VIS, including a parser and query\noptirnizer for a subset of SQL, and cOlnponents designed to be (re)written\nby students as project assignrnents: heap files, buffer 1nanager, B+ trees,\nBorting, and jo'ins.\n30.2\nOVERVIEW OF MINIBASE ASSIGNMENTS\nSeveral assignrnents involving the use of l\\!linibase are described here. Each of\nthese has been tested in a course already, but the details of how l\\!linibase is\nset up might vary at your school, so you 111ay have to rnodify the a..'3Sigluncnts\naccordingly. If you plan to use these assignrnents, you are advised to download\nand try thern at your site well in advance of handing thern to students. We\nhave done our best to test and docurnent these assignrnents and the Minibase\nsoftware, but bugs undoubtedly persist. Please report bugs at this URL:\nhttp://www.cs.wisc.edu/-dbbook/minibase.comments.html\nWe hope users will contribute bug fixes, additional project assignments, and\nextensions to Minibase.\nThese will be rnade publicly available through the\nMinibase site, together with pointers to the authors.\nIn several assignrnents, students are asked to rewrite a cornponent of Minibase.\nThe book provides the necessary background for all these assignrnents, and\nthe cl-'3signrnent handout provides additional systern-Ievel details.\nThe online\nI-rrML docurnentation provides an overvic\\v of the softv.rare, in particular the\ncorllponent interfaces, and can be downloaded and installed at each school that\nuses Minibcl-'3e.\n~rhe projects that follow should be assigned after covering the\nrelevctnt rnaterial fro1ll the indicated chapter:\nII\nBuffer Manager (Chapter 9): Students c\\,re given code for the layer\nthat\nrnanag(~s space on disk and supports the concept of pages \\vith page\nids.\n~rhey are a\"sked to i1nplelnent a buffer lnanager that brings requested\npages into Inelnory if they an.'. not already there.\n()ne variation of this\nassignrnent could use differerlt repla,ceruent policies. Students are asked to\naSSlllne a single-user enVir0l11nent, vvith no concurrency control or recovery\n1nanagclnent.\n11III\nHF Page (Chapter 9): Students rllust\\vrite code that rnanages records\non a page using\n(1, slot-directory pa,ge forrnat to keep track of the records.\nPossible variants include fixed-length versus variable-length records and\nother vvays to keep track of records on a pagf'..\n\n1004\nCHAPTER 30\n•\nHeap Files (Chapter 9): {,Ising the HF page and buffer manager code,\nstludents are asked to inlplernent a layer that supports the abstraction of\nfiles of unordered pages, that is, heap files.\n•\nB+ Trees (Chapter 10): This is one of the lnore cornplex assignrnents.\nStudents have to implernent a page class that Inaintains records in sorted\norder vvithin a page and iUlplernent the B+ tree index structure to iInpose a\nsort order across several leaf-level pages. Indexes store (key, record-pointer)\npairs in leaf pages, and data records are stored separately (in heap files).\nShnilar assignments can easily be created for Linear Hashing or Extendible\nIIa.'1hing index structures.\n•\nExternal sorting (Chapter 13): Building on the buffer manager and\nheap file layers, students are asked to irnplelnent external 111erge-sort. The\nenlphasis is on rninimizing I/O rather than on the in-melnory sort used to\ncreate sorted runs.\n•\nSort-Merge Join (Chapter 14): Building upon the code for external\nsorting, students are asked to implelnent the sort-merge join algorithm.\nThis assignment can be easily lnodified to create assignments that involve\nother join algorithms.\n•\nIndex Nested-Loop Join (Chapter 14): rrhis assignrnent is silnilar to\nthe sort-merge join assignruent, but relies on B+ tree (or other indexing)\ncode, instead of sorting code.\n30.3\nACKNOWLEDGMENTS\nThe Minibase software was inpired by Minirel, a sInall relational DBMS de-\nveloped by David DeWitt for instructional use. l\\rlinibase wa'S developed by a\nlarge nUlllber of dedicated students over a long tilne, and the design was guided\nby Mike Carey and R. Ralnakrishnan. See the online docurnentation for more\non rvlinibase's history.\n\nREFERENCES\n[1] R. Abbott and ,H. Garcia-Nlolina.\nScheduling real-titne transactions:\nA perfonnance\nevaluation. ACNf Transactions on Database SY8te7ns: 17(3), 1992.\n[2] S. Abiteboul. Querying serni-structured data. In Intl. Conf. on Database Theory, 1997.\n[~3] S. Abiteboul, R. Hull, and V. Vianu. Fo'U'ndations of Databases. Addison-vVesley, 1995.\n[4] S. Abiteboul and P. Kanellakis. Object identity as a query language prirnitive. In Proc.\nACM SIGAI0D Conf. on the NIanagerrtent of Data, 1989.\n[5] S. Abiteboul and V. Vianu. Regular path queries with constraints. In Proc. ACM Symp.\non Principles of Database Systems, 1997.\n[6] A. Aboulnaga, A. R. Almneldeen, and J. F. Naughton.\nEstimating the selectivity of\nXML path expressions for Internet scale applications. In Proceedings of VLDB, 2001.\n[7] S. Acharya, P. B. Gibbons, V. Poosala, and S. Ramaswamy. The Aqua approxinu\\te\nquery answering system. In Proc. AClvI SIGMOD Conf. on the Managem,ent of Data,\npages 574-576. ACl\\!I Press, 1999.\n[8] S. Acharya, P. B. Gibbons, V. Poosala, and S. Ramaswamy. Join synopses for approx-\nirnate query answering.\nIn Proc. ACM SIGNJOD Conf. on the ]l,1anagernent of Data,\npages 275~286. ACM Press, 1999.\n[9] K. Achyutuni, E. Omiecinski, and S. Navathe. Two techniques for on-line index rnod-\nification in shared nothing parallel databases.\nIn Proc. ACM SIGMOD Conf. on the\nJvIanagement of Data, 1996.\n[10] S. Adali, K. Candan, Y. Papakonstantinou, and V. Subrahrnanian. Query caching and\noptirnization in distributed rnediator systems. In Pr·oc. ACM SIGlvIOD Conf. on the\nNIanagernent of Data, 1996.\n[11]\n.Nt E. Adiba.\nDerived relations:\nA unified rnechanisHl for views, snapshots and dis-\ntributed data. In Proc. Intl. Conf. on Very La·rge Databases, 1981.\n[12] S. Agarwal: R. Agrawal, P. Deshpande, A. Gupta, J. Naughton, R. Rmnakrishnan, and\nS. Sarawagi. On the cornputation of InultidinlCnsionaJ aggregates. In Proc. Intl. Conf.\non Very Large Databases, 1996.\n[la] R. C. Agarwal, C. C. Aggarwal, and V. V. V. Prasad.\nA tree projection algorithrn\nfor generation of frequent itern sets.\n]()'unl,al of Parallel and Distributed Cornputing,\nG1 U~) :~~50··-\"371: 2001.\n[14]D. Agntwal and A. El Abbadi.\nThe generalized tree quorurn protocol:\nAn efficient\napproach for rnanaging replicated data. ACA1 Trn'nsactions on Database Systems, 17(4),\n1992.\n[15] D. Agrawal, A. El Abbadi, and R. Jeffers. Using delayed cornlnitrnent in locking pro-\ntocols for real-tirne databases. In Proc. AC/1;1 8IGA.fOD Conf. on the Nlanagerncnt of\nData, 19H2.\n1005\n\n1006\nDATABASE NIANAGElYfENT SYSTEIVIS\n[16] R. Agrawal, LvI. Carey, and 1\\/t Livny.\nConcurrency control pcrfornlance-ulOdeling:\nAlternatives and ilnplications. In Prof. AClvf SIG/dOD Conf. (rn the Ilfanagement of\nData, 1985.\n[17] R. Agrawal and D. De\\iVitt.\nIntegrated concurrency control and recovery Hlccha-\nniSII1s: Design and perforrnance evaluation. AC.A1 T'ransaciions on Database SystenlS,\n10(4) :529---564, 1985.\n[18] R.. Agra\\val and N. Gehani. ODE (Object Database and Envirolllnent): The language\nand the data rnode!. In Proc. ACA1 SIGA10D ConI on the A:lanage'l7tent of Data, 1989.\n[19] R. Agrawal, J. E. Gehrke, D. Gunopulos, and P. Raghavan. Autoillatic subspace clus-\ntering of high dirnensional data for data rnining.\nIn Proc. ACl\\!I SIGJvIOD Conlon\nIVIanagement of Data, 1998.\n[20] R. Agrawal, T. Imielinski, and A. Swanli. Database rnining: A performance perspective.\nIEEE Transactions on Knowledge and Data Engi'neering, 5(6):914--925, December 1993.\n[21] R. Agrawal, H. NIannila, R. Srikant, H. Toivonen, and A. I. Verkamo. Fast discovery of\nassociation rules. In U. M. Fayyad, G. Piatetsky-Shapiro, P. Srnyth, and R. UthurusanlY,\neditors, Advances in Knowledge Discovery and Data Mining, chapter 12, pages 307-328.\nAAAI/NIIT Press, 1996.\n[22] R. Agrawal, G. Psaila, E. Wimmers, and M. Zaot.\nQuerying shapes of histories.\nIn\nPTOC. Inti. Conf. on Very Large Databases, 1995.\n[23] R. Agrawal and J. Shafer. Parallel mining of association rules. IEEE Transactions on\nKnowledge and Data Engineering, 8(6):962-969, 1996.\n[24] R. Agrawal and R. Srikant. Mining sequential patterns. In Proc. IEEE Inti. Conj. on\nData Engineering, 1995.\n[25] R. Agrawal, P. Stolorz, and G. Piatetsky-Shapiro, editors. Proc. Intl. Conf. on Knowl--\nedge Discovery and Data Mining. AAAI Press, 1998.\n[26] R. Ahad, K. BapaRao, and D. McLeod. On estimating the cardinality of the projection\nof a database relation. ACiVf TTansactions on Database Systerns,\n14(1):28~-40, 1989.\n[27] C. Ahlberg and E. Wistr·and. IVEE: An information visualization exploration environ-\nHlCnt. In Intl. Sy'mp. on InjoTrnation V'isualization, 1995.\n[28] A. Aho, C. Beeri, and J. Ulhnan. The theory of joins in relational databases.\nACAf\nTransactions on Database System,s,\n4(~3):297--314, 1979.\n[29] A. Aho, J. Hopcroft, and J. Ulhnan. The Design and Analysis of ComputeT Algorithrns.\nAddison-Wesley, 1983.\nP30] A. Aha, Y. Sagiv, and J. Ulhnan.\nEquivalences aIllong relational expressions.\nSIAA1\nJ01LTnal of Cornput'l.ng, 8(2):218--246, 1979.\n[31] A. Aiken, .1. Chen, IvI. Stonebraker, and A. VVoodruff. rr'ioga-2: A direct rnanipulation\ndatabase visualization envirOIunent. In PT()c. IEEE Intl. ConI on Data Eng'ineeT'ing,\n1996.\n[:32] A. Aiken, .J. Widorn, and .J. Hellerstein. Static analysis techniques for predicting the\nbehavior of active database rules. ACl\\11Jransactions on Database Systems, 20(1):3-41,\n1995.\n[:3:3] A. AilanHlki,\nIO). De\\Vitt,lVI. Hill, and NI. Skounakis.\n\\\\leaving relations for cache\nperfonnance. In PTOC. Intl. Conj. on\n-VeT~1J Lwge Data Bases, 2001.\n[:34] N. Alon, P. 13. Gibbons,Y. rVlatias, and IV!. Szegedy. ]'racking join a,nd self-join sizes in\nlirnited storage. In Proc. A CAf 8yrnp08iurn on Pri.nC'iples of Database Syste'ln8,Philadc-\nplphia, Pennsylvania, 1999.\n\nRE~FERENC!E8\n1007\n~\n[35] N. AloIl, Y. iYfatias, and Nt Szegedy.\nThe space cmllplexity of approxilnating the\nfrequency mornents. In Proc. of the ACAf 81rrnp. on Theory of Computing, pages 20.-29,\n1996.\n[36J E. An\\vl.-lr, L.J\\1augis, and U. Chakravarthy.\nA new perspective OIl rule support for\nobject-oriented databa..-;es. In Proc. A C/I;f SIGAfOD Conf. on the A1arwgernent of Data,\n1993.\n[37] K. Apt, H. Blair, and A. \\iValker.\nTm,vards a theory of declarative knowledge.\nIn\nJ. 1IIinker, editor, PO'undations of Deductive Databases and Logic Prog'rarnm.ing. J\\1organ\nKaufInann, 1988.\n[38] W. Arrnstrong.\nDependency structures of database relationships.\nIn Proc. IFIP\nCongress, 1974.\n[39] G. Arocena and A. O. Iv1endelzon. WebOQL: restructuring doculnents, databases and\nwebs. In Proc. Inti. Conf. on Data Engineering, 1988.\n[40]\nIv!. Astrahan, rv1. Blasgen, D. Chaluberlin, K. Eswaran, J. Gray, P. Griffiths, W. King,\nR. Lorie, P. McJones, J. 1I1ehl, G. Putzolu, 1. Traiger, B. Wade, and V. Watson. Systenl\nR: a relational approach to database Inanageluent.\nA CM Transactions on Database\nSysterns, 1(2) :97~~137, 1976.\n[41 J :tvI. Atkinson, P. Bailey, K. Chishohn, P. Cockshott, and R. Morrison. An approach to\npersistent programming.\nIn Readings in Object-Oriented Databases. eds. S.B. Zdonik\nand D. 1\\lIaier, Morgan Kaufmann, 1990.\n[42]\n:tvI. Atkinson and P. Buneman. Types and persistence in database programming lan-\nguages. ACJvI Cornputing Sur'Veys, 19(2):105-\"-190, 1987.\n[43] R. Attar, P. Bernstein, and N. Goodman. Site initialization, recovery, and back-up in a\ndistributed database systern. IEEE Transactions on Software Engineering, 10(6):645---\n650, 1983.\n[44] P. Atzeni, L. Cabibbo, and G. Mecca.\nIsalog:\nA declarative language for complex\nobjects with hierarchies. In P'roc. IEEE Intl. ConI on Data Engineering, 1993.\n[45] P. Atzeni and V. De Antonellis.\nRelational Database Theory.\nBenjarnill-Culnmings,\n1993.\n[46] P. Atzeni, G. 1Vlecca, and P. .lVlerialdo. To weave the web. In P'roc. Intl. Conf. Very\nLaTye Data Bases, 1997.\n[47] H.. Avnur, .1. Hellerstein, B. Lo, C. Olston, B. Rarnan, V. Ranlan, T. Roth, and K. Wylie.\nControl: Continuous output and navigation technology with refinernent online In Proc.\nACA1 SIGNIOD Conf. on the fvlanagement of Data, 1998.\n[48] R. Avnur and .J.\n~t Hellcrstcin. Eddies: Continuously adaptive query processing. In\np.roc. A CAl 8IGA10D ConI on the fvfanagernent of Data, pages 261·..272. AC1Vl, 2000.\n[49J B. Babcock, S. Babu, I'vl. Datal', R. l\\Jlotwani, and J. Widom. 1Vlodels and issues in data\nstreanl systerns. In Proc. A CM Syrnp. on on Principles of Database Systems, 2002.\n[50J S. Bahu and J. \\Vidoln. Continous queries over data strealIlS. AC1\\;[ SIGA10D Record,\n:30(3): 109-·120, 2001.\n[51] D. Badal and G. Popek. Cost and perfonnance analysis of senHlntic integrit,Y validation\nruethods. In Proc. ACA1 SIGJl.10D Conf.\nOTt the !I/[anagernent of Data, 1979.\n(52] A. Badia, D. Van Gucht, and Iv!. Gyssens. Querying with generalized quantifiers. In\nApplicabons of LOg'lC Databases. cd. R. Ranutkrishnan, Killwer Acadelnic, 1995.\n[5:3] 1. Balbin, G. Port, K. RanUll110hanarao, and K. .Nleenakshi. Efficient bottorn-up COInpu-\ntation of queries on stratified dcl.tabc\\...;;;es. .fo'UTT/,al of Logic PTograrnrn'in!ll 11 (:3) :295·<~44,\n1991.\n\n1008\nDATABASE lVIANAGEIVIENT SYSTElVl;;\n[54] 1. Balbin and K. Rarnarllohanarao.\nA generalization of the differential approach to\nrecursive query e'laluation. Journal of Loqic Progrmnm,ing, 4(3):259\"-262, 1987.\n[55}\n_F. Bancilhon, C. Delobel, and P. Kanellakis.\nBuilding an Ob.iect-Oriented Database\nSystem. Morgan Kaufnlann, 1991.\n[56] F. Bancilhon clnd S. Khoshafian. A calculus for corIlplex objects. Jml'rnal of C01npnter\nand System Sciences,\n:38(2):~~26--\"\"\"340, HJ89.\n[57] .F. BancilhoIl, D. l\\1aier, Y. Sagiv, and J. Ullnlan. IVIagic sets and other strange ways\nto inlplement logic progranlS. In A ()Al Sy·mp. on Principles of Database Systerns, 1986.\n[58] F. Bancilhon and H. Rarnakrishnan.\nAn anlateur's introduction to recursive query\nprocessing strategies.\nIn Proc. A CAl SIG1\\,I0D Conf. on the\n.ft.,1anage'f1~ent of Data,\n1986.\n[59] F. Bancilhon and N. Spyratos. Update senlantics of relational views. A CM Transactions\non Database Systems, 6(4):557--575, 1981.\n[60] E. Baralis, S. Ceri, and S. Paraboschi. lVlodularization techniques for active rules design.\nACAI Transactions on Database Syste'ms, 21(1):1-29, 1996.\n[61] D. Barbara, W. DUNlouchel, C. Faloutsos, P. J. Haas, J. 1\\1. Hellerstein, Y. E. Ioannidis,\nH. V. Jagadish, T. Johnson, R. T. Ng, V. Poosala, K. A. Ross, and K. C. Sevcik. The\nNew Jersey data reduction report. Data Engineering Bulletin, 20(4):3-45, 1997.\n[62] R. Barquin and H. Edelstein. Planning and Designing the Data Warehouse. Prentice-\nHall, 1997.\n[63] C. Batini, S. Ceri, and S. Navathe. Database Design: An Entity Relationship Approach.\nBenjarnin/Cummings Publishers, 1992.\n[64] C. Batini, IvI. Lenzerini, and S. Navathe. A comparative analysis of ruethodologies for\ndatabase schema integration. A ONI Computing Surveys, 18(4) :323-364, 1986.\n[65] D. Batory, J. Barnett, J. Garza, K. Smith, K. Tsukuda, B. Twichell, and T. Wise.\nGENESIS: An extensible database 11lanageruent system.\nIn S. Zdonik and D. Maier,\neditors, Readings in Object-Oriented Databases. Ivlorgan Kaufrnann, 1990.\n[66] B. Baugsto and J. Greipslancl.\nParallel sorting rnethods for large data volunH~s on a\nhypercube database cornputer. In Proc. Intl. vVorkshop on Database JvIachines, 1989.\n[67] R. J. Bayardo. Efficiently ruining long patterns frorn databases. In Proc. A CAl SICA10D\nInt!. Con]. on JvIanagernent of Data, pages 85-93. AClVl Press, 1998.\n[68] R. J. Bayardo, R. Agrawal, and D. Gunopulos. Constraint-based rule ruining in large,\ndense databases. Data AIining and Knowledge Discovery, 4(2/3):217···240, 2000.\n[69] R. Bayer and E.\n~/IcCreight. Organization and rnaintenance of large ordered indexes.\nActa Info T'TrWtica, 1(3):173-189, 1972.\n[70J R. Bayer and IVI. Schkolnick. Concurrency of operations on B-trees. Acta lnforrnal;ica,\n9(.1): 1--21, 1977.\n[71]\nIV1. Beck, D. Bitton, and \\\\T. \\Nilkinson. Sorting large files on a backend rTIultiprocessor.\nIEEE 7'ransdct'ions on C'omp'uters, 37(7) :769--778, 1988.\n[72] N. Becknlann, H.-P. Kriegel, R. Schneider, and B. Seeger. The R* tree: An efficient\nand robust access ruethod for points and rectangles. In Proc. ACAI SIGAI0D Conf. on\nthe Afanagernent of Data, 1990.\n[7~3] C. Beeri, R. Fagin, and .J. Howard. A complete axiOluatization of functional and rnul-\ntivalued dependencies in database relations.\nIn Proc. A01\\4 SIG/vIOD Con]. on the\n1\\1anagernent of Data, 1977.\n\nREFEREIVCES\nlOQ9\n[74] C. Beeri and P..Honeylnall.\nPreserving functional dependencies.\nSIA1H Journal of\nComputing, lO(:3):G4T·-656, 1982.\n[75] C. Beeri and T. ivIilo. A model for active object-oriented database. In Proc. Intl. Conf.\non llery Large Databases, 1991.\n[76} C. Beeri, S. Naqvi, R. Rmnakrishnan, O. Shmueli, and S. Tsur. Sets and negation in\na logic database language (LDLI ). In ACNf Symp. on Prirtciples of Database Systems,\n1987.\n[77] C. Beeri and R. RaIllakrishnan. On the power of rnagic. In AC1\\1 Syrnp. on Principles\nof Database Sy8terns, 1987.\n[78] D. Bell and J. GriIDson. Dist1'ib1lted Database Systems. AddisoIl-\\\\lesley, 1992.\n[79] J. Bentley and J.\n~'riedman. Data structures for range searching.\nA CAl Cornputing\n8'urveys, 13(3):397-·409, 1979.\n[80] S. Berchtold, C. Bohm, and H.-P. Kriegel.\nThe pyramid-tree: breaking the curse of\ndimensionality. In ACM SIG1\\10D Conf. on the l\\lanagement of Data, 1998.\n[81] P. Bernstein. Synthesizing third normal form relations from functional dependencies.\nA CM Transactions on Database Systerns, 1(4):277-298, 1976.\n[82] P. Bernstein, B. Blaustein, and E. Clarke.\nFast maintenance of sernantic integrity\nassertions using redundant aggregate data. In Proc. Intl. Conf. on Very Large Databases,\n1980.\n[83] P. Bernstein and D. Chiu. Using senli-joins to solve relational queries. Journal of the\nACM, 28(1):25-40, 1981.\n[84] P. Bernstein and N. Goodman. Tirnestamp-based algorithms for concurrency control in\ndistributed database systems. In Proc. Intl. Conf. on Very Large Databases, 1980.\n[85] P. Bernstein and N. Goodman. Concurrency control in distributed database systems.\nAClVf Computing S1Lrveys, 13(2):185-222, 1981.\n[86] P. Bernstein and N. Goodlnan. Power of natural semijoins. SIAM Journal of Corrtput'ing,\n10(4) :751-~771, 1981.\n[87] P. Bernstein and N. Goodulan.\nMultiversion concurrency control-Theory and algo-\nrithms. A ClVI Transactions on Database Systerns, 8(4):465-483, 1983.\n[88] P. Bernstein, N. Goodnlan, E. Wong, C. Reeve, and J. Rothnie. Query processing in a\nsystern for distributed databases (SDD-1 ).\nA C1\\1 11ransactions on Database Systerns,\n6( 4):602··~~625, 1981.\n[89] P. Bernstein, V. Hadzilacos, and N. Goodlnan.\nConc'u,rrency Cont'rol and Recovery in\nDatabase System,s. Addison-vVesley, 1987.\n[90] P. Bernstein and E. Newcomer.\nPr\"inciples of Transaction Processing.\nrvlorgan Kauf-\nmann, 1997.\n[91] P. Bernstein, D. Shiprnan, and J. Rothnie.\nConcurrency control in a SystCIll for dis-\ntributed databa..'5es (SDD-l).\nACAITransactions on Database Systerns, 5(1):18-51,\n1980.\n[92] P. Bernstein, D. Shiprnan, and \\V. Wong. Forrnal aspects of serializability in database\nconcurrency control. IEEE 7r'CLnsactions on Software Engineering, 5(;3):203-··-21G, 1979.\n[9~3] K. Beyer, J. Goldstein, R. RaInakrishnan, and U. Shaft.\n\\\\Then is nearest neighbor\nrneaningful? In IEEE International Conference on Database Theory, 1999.\n[94] K. Beyer and R. Rarnakrishnan. BottOlIl-UP cornputatioll of sparse and iceberg cubes\nIn Proc. ACA1 SIGlVfOD ()onf. on the Alanagernent of Data, 1999.\n\n1010\nDATABASE lVIANAGEMENT SYSTEJ\\iS\n[95] B. Bhargava, editor. Concurrency Control and Reliability in\nDiBt1~ibuted Systc'ms. Van\nNostrand Reinhold, 1987.\n[96] A. Biliris.\nThe perfornutnce of three database storage structures for r.nanaging large\nobjects. In Proc. A CAf SIGAfOf) Conf. on the AianagC'Tnent of Data, 1992.\n[97] J. Biskup and B. Convent. A fonnal view integration method. In Proc. ACA1 SIClvIOD\nConf. on the l\\1anagem,ent of Data, 1986.\n[98] J. Biskup, U. Dayal, and P. Bernstein. Synthesizing independent database schenlas. In\nProc. ACi\\1 SICA,iOD Conf. on the A'ianage7nent of Data, 1979.\n[99J D. Bitton and D. DevVitt.\nDuplicate record elimination in large data files.\nACA1\nTransactions on Database System.s, 8(2):255-265, 1983.\n[100] J. Blakeley, P.-A. Larson, andF'. TOInpa. Efficiently updating lllaterialized views. In\nProc. AC1V[ SIGN/OD Conf. un the llfanagem,ent of Data, 1986.\n[101] Iv1. Blasgen and K. Eswaran. On the evaluation of queries in a database systenl. Tech-\nnical report, IBM FJ (R.J1745), San Jose, 1975.\n[102] P. Bohannon, D. Leinbaugh, R. Rastogi, S. Seshadri, A. Silberschatz, and S. Sudarshan.\nLogical and physical versioning in main memory databases. In Proc. Intl. Conf. on Very\nLarge Databases, 1997.\n(103] P. Bohannon, J. Freire, P. Roy, and J. Siuleon.\nFrom XML schema to relations:\nA\ncost-based approach to XML storage. In Proceedings of ICDE, 2002.\n[104] P. Bonnet and D. E. Shasha.\nDatabase Tuning:\nPr'lnciples, Experirnents, and Trou-\nbleshooting Techniq1Les. J\\lIorgan Kaufrnann Publishers, 2002.\n[105] G. Booch, 1. Jacobson, and J. Rurnbaugh. The Unified Model'lng Language User Guide.\nAddison-Wesley, 1998.\n[106] A. Borodin, G. Roberts, J. Rosenthal, and P. Tsaparas. Finding authorities and hubs\nfrorn link structures on Roberts G.O. the world wide web. In World Wide Web Con-\nference, pages 415~429, 2001.\n[107] R. Boyce and D. Chamberlin. SEQUEL: A structured English query language. In PTOC.\nACM SIGAIOD Conf. on the lvfanagement of Data, 1974.\n[108] P. S. Bradley and U. 1\\1. Fayyad. Refining initial points for K-1\\1eans clustering. In Pr'oc.\nIntl. Conlon IHachine Learning, pages 91--'-99. :NIorgan Kaufnlann, San :Francisco, CA,\n1998.\n[109] P. S. Bradley, U.N!. Fayyad, and C. Reina.\nScaling clustering algorithnls to large\ndataba.ses. In Pr·oc. Intl. Conlon Knowledge D'lscoveTy and Datafvhning, 1998.\n[lID] K. Bratbergscngen. I-lashing rnethods and relational algebra operations. In Pr·oc. Intl.\nConI on Very Lca:qe Databases, 1984.\n[111] L. Brcilnan, J. II. Friechnan, It A. Olshen, and C~. J. Stone. Classificat'ion CLnd Reg'rcssion\nTrees. Wadsworth, Belrnont. CA, 1984.\n[112] Y. Breitbart, H. Garcia-l\\/Iolina, and A. Silberschatz. Overvic\\v of llluitidataba,c.;e trans-\naction rnanagernent. In p.roc. Ina. C'onf. on 1/ery Large Databases, 1992.\n[11~3] Y. Breitbeut, A. Silberschatz, and G. Thornpson. Reliable transaction rllanagcrllent in\na llmltidatabase systerll. In Proc. A()A:[ SIGA/[OD Conf. on the l\\![anagernen,t of Data,\n1990.\n[114J Y. Breitba.rt, A. Silberschatz, and\nC~. Thompson. An a,pproach to recovery Inanagcrnent\nin\n1:1 lIlultidataba.sc system. In PTOC. IntI. Conf. on\n\\/er~y Large Databases, 1992.\n\n1011\n\\1\n[115] S. Brin, R. wlotwani, and C. Silverstein. Beyond Inarket baskets: Generalizing a\"c;,..;;ocia-\ntion rules to correlations. In Proc. ACJVl SIGlvfOD Conf. on the lvfanagement of Data,\n1997.\n[116] S. Brin and L. Page. The anatorny of a large-scale hypertextual web search engine. In\nProceediT/,gs of 7th ItFodd Wide Web Conference, 1998.\n(117) S. Brin, R. rvlotwani, J. D. lHlrnan, and S. Tsur. Dynaruic itertlset counting and inlplica-\ntion rules for rnarket ba\"c;ket data. In Proc. ACiV[ SlGlvl0D lntl. Conf. on A'[anagement\nof Data, pages 255···264. ACIVI Press, 1997.\n[118] T. Brinkhofl', H.-P. Kriegel, and R. Schneider. Cornparison of approximations of cOlllplex\nobjects used for approximation-ba..'3ed query processing in spatial database systerIls. In\nProc. IEEE IntZ. Conf. on Data Engineering, 1993.\n[119] K. Brown, M. Carey, and\n~L Livny.\nGoal-oriented bufl'er rnanagement revisited.\nIn\nProc. ACj\\;f SIC.NI0D Conf. on the NJanagernent of Data, 1996.\n[120] N. Bruno, S. Chaudhuri, and L. Gravano.\nTop-k selection queries over relational\ndatabases:\n~fapping strategies and performance evaluation.\nA C1v! Transactions on\nDatabase System,s, To appear, 2002.\n[121] F. Bry. Towards an efficient evaluation of general queries: Quantifier and disjunction\nprocessing revisited. In Proc. AC}.;1 SIC1vl0D Conf. on the Management of Data, 1989.\n[122] F. Bry and R. Manthey. Checking consistency of database constraints: A logical basis.\nIn Proc. Intl. Conf. on Very Large Databases, 1986.\n[12~~] P. Bunernan and E. Clemons. Efficiently rnonitoring relational databases. A CNf Trans-\nactions on Database Systerns, 4(3), 1979.\n[124] P. Bunernan, S. Davidson, G. Hillebrand, and D. Suciu. A query language and optimiza-\ntion techniques for unstructured data. In Proc. ACM SIG.1I.10D Conf. on Management\nof Data, 1996.\n[125] P. Buneman, S. Naqvi, V. Tannen, and L. Wong.\nPrinciples of prograrnrning with\ncomplex objects and collection types. Theoretical Compl1ter Science, 149(1) ::3-48, 1995.\n[126] D. Burdick, l\\!I. Calirnlim, and J. E. Gehrke. lVlafia: A rnaxirnal frequent itemset alga-\nritlull for transactional databases. In Proc. Intl. Conf. on Data Engineerving (JCDE).\nIEEE Cornputer Society, 2001.\n[127] l\\!I. Carey. Granularity hierarchies in concurrency control. In A CA1 .9yrnp. on Principles\nof Database Systern8,\n198~3.\n[128]\n.LvI. Carey, D. Charnberlin, S. Narayanan, B. Vance, D. Doole, S. Rielau, R. Swagerrnan,\nand N. lVlattos. 0-0, what's happening to DB2? In Prnc. ACAf SICAI0D Conf. on the\nA/anagernent of Data, 1999.\n[129]\n~/I. Carey, D. DeWitt, Nt Franklin, N. HaIl,N1.\n~lcAuliffe, .1. Naughton, D. Schuh,\nJVI. SOlOlIlOIl, C. 'rau, O. Tsatalos, S. White, and tvI. Zwilling.\nShoring up persistent\napplications. In Plue. AC)Af SIGAI0D Can]. on the l'vlanagernent of Data, 1994.\n[1~30)rvL Carey, D. De\\Vitt, G. Graefe, D. Haight, .1. Richardson, D. Schuh, E. Shekita, and\nS. Vandenberg. The EXODUS Extensible DBtv1S project: An overvimv. In S. Zdonik\nand D. l\\!:Iaier, editors, Readings 'in Ob,jed-Oriented Databases. l\\Iorgan K.aufrnann, 1990.\n[1~n)\n1\\'1. Carey, IJ. De\\Vitt, and .1. Naughton. The 007 benchrnark. InP.roc. ACiV! SICA/OD\nConj. on, the fl.Ianagernent of Data,\n199;~.\n[132)~J. Carey, D. DevVitt, J. Na,ughton, ~L Asgarian, J. Gehrke, andD. Shah. The BUGK\\{\nobject-rehltional benchlnark.\nIn Proc. A CA.1 S'IGAIOD Conj. on the A[anagernent of\nData, 1997.\n\n1012\nDATABASE NIANAGElVIENT SVSTEN\\S\n[1:33] Ivi. Carey, D. DeWitt, J. Richardson, and E. Shekita. Object and file rnanageInent in\nthe Exodus extensible database system. In Pr'Oc. IntI. Conf. on Very Lar:qe Databases,\n1986.\n[1;34] M. Carey, D. Florescu, Z. Ives, Y. Lu, J. Shanmugasundaraul, E. Shekita, and S. Sub-\n.f<:llnanian. XPF;RANTO: publishing object-relational data as XNIL. In Pr'oceedings of\nthe Third InteT7wt'ional lIVOTkshop on the Web and Databases, Nlay 2000.\n[1:35] rvi. Carey and D. Kosslllan.\nOn saying \"Enough Already!\"\nin SQL\nIn Proc. ACA1\nSIGlvlOD Conf. on the lvlanagernent of Data, 1997.\n[136] M. Carey and D. Kossrnan. Reducing the braking distance of an SQL query engine In\nPTOC. Intl. Conf. on \"{ler1j Large Databases, 1998.\n[137] 11. Carey and M. LivllY. Conflict detection tradeoffs for replicated data. A CA1 Trans-\nactions on Database Systerns, 16(4), 1991.\n[138] M. Casanova, L. Tuchennan, and A. F\\utado.\nEnforcing inclusion dependencies and\nreferential integrity. In Proc. Intl. Conf. on \"{lery Large Databases, 1988.\n[139] M. Casanova and M. Vidal. Towards a sound view integration lnethodology. In AC1\\1.\nSymp. on Principles of Database Systems, 1983.\n[140] S. Castano, M. Fugini, G. Martella, and P. Samarati.\nDatabase Security.\nAddison-\nWesley, 1995.\n[141] R. Cattell. The Object Database Standard: ODMG-93 (Release 1.1). Morgan Kaufmann,\n1994.\n[142] S. Ceri, P. Fraternali, S. Paraboschi, and L. Tanca. Active rule lnanagement in Chimera.\nIn J. Widom and S. Ceri, editors, Active Database Systems. Morgan Kaufmann, 1996.\n[143] S. Ceri, G. Gottlob, and L. Tanca. Logic Programming and Databases. Springer Verlag,\n1990.\n[144] S. Ceri and G. Pelagatti.\nDistributed Database Design:\nPrinciples and Systems.\nMcGraw-Hill, 1984.\n[145] S. Ceri and J. Widom. Deriving production rules for constraint maintenance. In Proc.\nIntl. Conf. on \"{leTy Large Databases, 1990.\n[146] F. Cesarini, M. Missikoff, and G. Soda. An expert systeln approach for database appli-\ncation tuning. Data and Knowledge Engineering, 8:35\"'55, 1992.\n[147] U. Chakravarthy.\nArchitectures and rnonitoring techniques for active databases: An\nevaluation. Data and Knowledge Engineering, 16(1):1'-26, 1995.\n[1.48] U. Chakravarthy, .1. Grant, and J. Minker.\nLogic-based approach to semantic query\noptimization. ACAI Tran8actions on Database Systern8, 15(2):162····207, 1990.\n[149] I). Charnberlill. Using the New DB2. Morgan Kaufrnann, 1996.\n[150] D. Chaluberlin, M. Astrahan, M. Blasgen, J. Gray, W. King, B. Lindsay, R. Lode,\n.1. Ivlehl, T. Price, P. Selinger, 1V1. Schkolnick, D. Slutz, I. Traiger, B. Wade, and R. Yost.\nA history and evaluation of System R\nComm7J:nication.s of the ACM, 24(10):m32\"\"'646,\n1981.\n[151] D. Charnberlin,tv1. Astrahan, K. Eswaran, P. Griffiths, R. Lorie, .J.f\\.1ehl, P. Reisner,\nand B. Wade. Sequel 2: a unified approach to data definition, manipulation, and control.\nIB1v1 Journal of ReseaT(;h and Developrnent, 20(6):560\"\"\"575, 1976.\n[152] D. Charnberlin, D. Florescu, a,nd .1. Robie. Quilt: an XIvIL query language for hetero-\ngeneous data sources. In P1YJceeclings of WebDB, Dalla.,s, 'TX, May 2000.\n\n1013\nt\n[153] D. ChaInberlin! D. Florescu, J. Robie, .1. Sinwoll, and ivI. Stefanescu. XQuery: A query\nlanguage for XivIL. World \\VideWeb ConsortiUJIl, http://www .w3. org!TR!xquery, Feb\n2000.\n[154] A. Chandra and D.Harel. Structure and complexity of relational queries. J. CornputeT\nand SY.5tern Sciences, 25:99--128, 1982.\n[155] A. Chandra and P. Ivlerlin. Optinlal ilnplernentation of conjunctive queries in relational\ndatabases. In Proc. ACiVl SIGACT Syrnp. on Theo'ry of Co'mp'uting, 1977.\n[156]\n]\\;1. Chandy, L. Haas, and J. 1/Iisra. Distributed deadlock detection. A CA,l Trunsact'ions\non Co'mputer\nSY.5tel1~S, 1(3): 144--156,\n198~~.\n[157] C. Chang and D. Leu. Ivlulti-key sorting as a file organization schenle when queries are\nnot equally likely. In Proc. Intl. Syrnp. on Database Systems for Advanced Applications,\n1989.\n[158] D. Chang and D. Harkey. Client/ server data access with Java and ...YML. John Wiley\nand Sons, 1998.\n[159] M. Charikar, S. Chaudhuri, R. Motwani, and V. R. Narasayya.\nTowards estimation\nerror guarantees for distinct values. In Proc. A CAf Symposium on Principles of Database\nSystems, pages 268~279. ACM, 2000.\n[160] D. Chatziantoniou and K. Ross. Groupwise processing of relational queries. In Proc.\nInti. ConI on Very Large Databases, 1997.\n[161] S. Chaudhuri and U. Dayal. An overview of data warehousing and OLAP technology.\nSIGMOD RecoTd, 26(1):65-74, 1997.\n[162] S. Chaudhuri and D. Madigan, editors. Proc. ACM SIGKDD Inti. ConfeTence on Knowl-\nedge Discovery and Data lvIining. ACIvI Press, 1999.\n[163] S. Chaudhuri and V. Narasayya. An efficient cost-driven index selection tool for Mi-\ncrosoft SQL Server. In Proc. Inti. Conf. on Very Large Databases, 1997.\n[164] S. Chaudhuri and V. R. Narasayya. Autoadrnin 'what-if' index analysis utility. In Pr'Oc.\nACM SIGMOD Inti. Conf. on lvIanagernent of Data, 1998.\n[165] S. Chaudhuri and K. Shirrl. Optimization of queries with user-defined predicates. In\nPr'Oc. Intl. Conf. on Very Large Databases, 1996.\n[166] S. Chaudhuri and K. Shirn. Optimization queries with aggregate views. In Intl. Conf.\non Extending Database Technology, 1996.\n[167] S. Chaudhuri, G. Das, and V. R. Narasayya. A robust, optirrlization-hased approach\nfor approximate answering of aggregate queries. In Proc. AC]t;J SIG1v/OD Conf. on the\nManagement of Data, 2001.\n[168] J. Cheiney, P. Faudenlay, R. J'vfichel, and J. Thevenin. A reliable parallel backend using\nrnultiattribute clustering and select-join operator. In Proc. Intl. Conf. on Very Lm:qe\nDatabases, 1986.\n[169] C. Chen and N.Roussopoulos.\nAdaptive\ndataba~e buffer rnanagernent using query\nfeedback. In Proc. Inti. Conj. on Very Lar:qe Databases, 199;3.\n[170J C. Chen and N. Roussopoulos. Adaptive selectivity estimation using query feedback.\nIn Proc. ACM SIGMOD Conf. on the Manage1nent of Data, 1994.\n[171] P. M. Chen, E. K. Lee, G. A. Gibson, R. H. Katz, andD. A. Patterson. RAID: High-\nperforrnance, reliable secondary storage. AClvI Computing SuT7H':-YS, 26(2):14,5·....185, June\n1994.\n[172} P. P. Chen. The entity-relationship rnodel-········toward a unified view of data. ACM Trans-\nactions on Database System,s, 1(1):9--36, 1976.\n\n1014\nI)ATABASE NIANAGENIENT SVSTEiVlS\n[173] Y. Chen, G. Dong, J. Han, B. vv. Wah, and J.vVang.\n1Vlulti-dinlcnsional regression\nanalysis of tinIe-series data strearllS.\nIn Pro£:. Intl. ConI on Very Larye Datn Bases,\n2002.\n[174] D. \\V. Cheung, .1. Han, V. T'. Ng, and C. Y. Wong. ~i[aintenanceof discovered association\nrules in large databases: An incrernental updating technique. In P1Y)c. InL Conf. Data\nEngineer\"ing, 1996.\n[175] D. VV. Cheung, V. T. Ng, and B. W. Tarn ?viaintenance of discovered knowledge: A\ncase in rnlllti-level association rules. In Proc. Inti. Conf. on Knowledge Discover:lI and\nData A1in:ing. AAAI Press, 1996.\n[176] D. Childs. Feasibility of a set theoretical data structure-~\"Ageneral structure based on\na reconstructed definition of relation. Proc. Tri-annual IFIP Conference, 1968.\n[177] D. Chimenti, R. Garnboa, R. Krishnarnurthy, S. Naqvi, S. Tsur, and C. Zaniolo. The lell\nsystem prototype. IEEE Tra'nsactions on Knowledge and Data Engineering, 2(1):76\"-90,\n1990.\n[178] F. Chin and G. Ozsoyoglu. Statistical database design. AClv/ TTansactions on Database\nSysterns, 6(1):113--139, 1981.\n[179] T.-C. Chiueh and L. Huang. Efficient real-time index updates in text retrieval systems.\n[180] J. ChOillicki. Real-time integrity constraints. In ACM Symp. on Principles of Database\nSyste'ms, 1992.\n[181] H.-T. Chou and D. DeWitt. An evaluation of buffer rnanagelnent strategies for relational\ndatabase systerns. In Proc. Intl. Conf. on VeTy Large Databases, 1985.\n[182] P. Chrysanthis and K. Ramarnritharn. Acta: A framework for specifying and reason-\ning about transaction structure and behavior. In Proc. ACM SIGN/OD Conf. on the\nlvlanagement of Data, 1990.\n[183] F. Chu, .J. Halpern, and P. Seshadri.\nLeast expected cost query optinlization:\nAn\nexercise in utility A CM Symp. on Principles of Database System,s, 1999.\n[184] F. Civelek, A. Dogac, and S. Spaccapietra. An expert systern approach to view definition\nand integration. In Pr'oc. Entity-Relationship ConfeTence, 1988.\n[185] R.. Cochrane, H. Pirahesh, and N. Mattos.\nIntegrating triggers and declarative con-\nstraints in SQL database systems. In Pr'Oc. Intl. Conf. on Very Large Databases, 1996.\n[186] CODASYL. Report of the CODASYL Data Base Task Group. ACM, 1971.\n[187]E. Codd. A relational lllodel of data for large shared data banks. Cornmunications of\nthe A C-\"'f,\n1~~(6) :377--<387, 1970.\n[188] E. Codd. Further norrnalization of the data base relational rnodeL In R. Rustin, editor,\nData Base Systetn8. Prentice Hall, 1972.\n[189J E. Codd.\nRelational cOlnpleteness of data base sub-languages. Inll. Rustin, editor,\nData Base System,s. Prentice Hall, 1972.\n[190] E. Codd.\nExtending the database relational lnodel to capture rnore IllCC1ning.\nAClv[\nTrnnsactions on Database 8ystcrns,\n4(4):~{97\"\"'4:34, 19n).\n[191] E. Codd. Twelve rules for on-line analytic processing. CornputC'rwoTld, April\n1~3 1995.\n[192] L. Colby, T. Griffin, L. Libkin, 1. Nhunick, anei H. 'Trickey. Algorith.IllS for deferred view\nrnaintenance. In Prnc. AC?I.! SIGAfOD ConI on the A1anagernent of Data, 1996.\n[19:3] L. Colby, A. Kawaguchi, D.Lieuwen, 1. l\\'l111nick, i:tnd K. 11.oss.\nSupporting nnIltiple\nview rnaintenance policies: Concepts, algorithnls, and performance analysis. In Fmc.\nA CAl 8IGA10D ()onj. on the A1anagernent of Data, 1997.\n\nREFEREIVC}ES\n1015\n[194} D. COIner. The ubiquitous B-tree. ACA1 C. SU7\"l.Jeys, 11(2):121··1:37, 1979.\n[195] D. Connolly, editor.\nXlvfL Priru:iples, Tools and Techniques.\nO'Reilly & Associates,\nSebastopol, USA, 1997.\n[196J B. Cooper, N. Sample, :tVl. J. Franklin, G. R. Hjalta':ion, and :Nt Shadmon. A fast index\nfor senlistructured data. In Proceedings of VLDB, 2001.\n[197J D. Copeland and D. I\\1aier. Making SMALLTALK a database systerll. In Proc. ACJi1\nSIGA10D Conf. on the lvfanagerment of Data, 1984.\n[198] G. Cornell and K. Abdali. CGI Prograrnm'ing With Java. PrenticeHall, 1998.\n[199} C. Cortes, K. Fisher, D. Pregibon, and A. Rogers. Hancock: a language for extracting\nsignatures fronl data strearllS. In Proc. ACM SIGKDD Inti. Conference on Knowledge\nDiscovery and Data A1ining, pages\n9~-17. AAAI Press, 2000.\n[200] J. Daenlen and V. RijrrleIl.\nThe Design of Rijndael: AES -The Advanced Encryption\nStandard (Information Security and CryptogTaphy). Springer Verlag, 2002.\n[201] M. Datal', A. Gionis, P. Indyk, and R.. Motwani.\nMaintaining stream statistics over\nsliding windows.\nIn PTOC. of the Annual ACM-SIAJvf Symp. on Discrete Algorithms,\n2002.\n[202] C. Date. A critique of the SQL database language. ACid SIGM·OD Record, 14(3):8-54,\n1984.\n[203] C. Date. Relational Database: Selected Writings. Addison-Wesley, 1986.\n[204] C. Date. An Introduction to Database Systems. Addison-Wesley, 7 edition, 1999.\n[205] C. Date and R. Fagin.\nSiIIlpie conditiolls for guaranteeing higher norrnal forms In\nrelational databases. ACAI Transactions on Database Systerns, 17(3), 1992.\n[206] C. Date and D. :NIcGoveran. A Guide to Sybase and SQL Server. Addison-Wesley, 1993.\n[207] U. Dayal and P. Bernstein. On the updatability of relational views. In Proc. Intl. Conf.\non Very Large Databases, 1978.\n[208] U. Dayal and P. Bernstein. On the correct translation of update operations on relational\nviews. A CAf Transactions on Database Systems, 7(3), 1982.\n[209] P. DeBra and J. Paredaens. Horizontal decompositions for handling exceptions to FDs.\nIn H. Gallaire, .1. Minkel', and J.-M. Nicola..'5, editors, Advances in Database Theory,.\nPlenurIl Press, 1981.\n[210] J. Deep and P. Holfelder. Developing CGI applications with PerL Wiley, 1996.\n[211] C. Delobel. Norrrialization and hierarchial dependencies in the relational data model.\nACAI TTansactions on Database Systerns,\n~~(~3):201-222, 1978.\n[212] D. Denning. Secure statistical databa'5es with randOlIl sC1nlple queries. ACA1 TTansac-\ntions on Database Systems,\n5(~3):291-'--315, 1980.\n[21~3] D. E. Denning. Cryptogrnphy and Data 8ec'UT'ity. AddisOI1-Wesley, 1982.\n[214] M. Derr, S. Nlorishita, and G. Phipps. The glue-nail deductive database systern: Design,\nimplernentation, and evaluation. VLDB Journal, 3(2):123-..-160, 1994.\n[215] A. Deshpailde. An iruplelnentation for nested relational databases. l'echnical report,\nPhD thesis, Indiana University, 1989.\n[216] P. I)eshpande, K. RaInasaIny, A. Shukla, and J. F. Naughton. Caching rIlultidirnensional\nqueries using chunks. In Proc. ACiV! SIGA10D Ina. Conf. on A1anagernent of Data, 1998.\n[217] A. Deutsch, :tvt Fernandez, D. Florescu, A. Levy, and D. Sueiu. XI\\1L-QL: A query lan-\nguage for XJ\\tlL. WorldWide Web Consortium, http://WTilTN .w3 .org/TR/NOTE-xml-ql,\nAug 1998.\n\n1016\nI)ATABASE lVIANAGEMENT SYSTEfvlS\n(218] O. e. a. Deux. The story of 02. IEEE 7'rft1lSact<ions on Knowlen,ge and Data BnfliTtecring,\n2(1), 1990.\n[219] D. DeVvitt, II.-T. Chou, Fe Katz, and A. Klug.\nDesign and inlplenlcntation of the\nWisconsin Storage Systern. Software Practice a'nd Experience, 15(10):943--962, 1985.\n[220] D. De\\Vitt, H. Gerber, G. Graefe,.lVL Heytens, K. Kumar, and NI. i\\1uralikrishna.\nGanuua-····-··A high perforrnance dataflow databa..o;;;e Inachine. In Pr'Oc. Intl. Conf. on Very\nLa'l~qe Datllbases, 1.986.\n[221] D. DeWitt and J. Gray.\nParallel database systenls: The future of high-perfornlance\ndatabase systerIls. Cornm:unications of the ACA1, 35(6):85-98, 1992.\n[222] D. DeWitt, R. Katz, F. Olken, L. Shapiro, M. Stonebraker, and D. vVood. Inlplelnen-\ntation techniques for rnain menlory databases. In Proc. AC1l1 8IG1'vfOD Conf. on the\nAIanagement of Dat.a, 1984.\n(223] D. DeWitt, J. Naughton, and D. Schneider. Parallel sorting on a shared-nothing archi-\ntecture using probabilistic splitting. In Proc. Conf. on Parnllel and Distributed Infor-\nrnation Systerns, 1991.\n[224] D. DeWitt, J. Naughton, D. Schneider, and S. Seshadri.\nPractical skew handling in\nparallel joins. In Proc. Inti. Conf. on Very Large Databases, 1992.\n[225] O. Diaz, N. Paton, and P. Gray.\nRule rnanagenlent in object-oriented databases:\nA\nuniform approach. In Proc. Ina. Conf. on Very Larye Databases, 1991.\n[226] S. Dietrich.\nExtension tables:\nMerno relations in logic programming.\nIn Proc. Intl.\n8yrrtp. on Logic Programming, 1987.\n[227] W. Diffie and M. E. Hellman. New directions in cryptography. IEEE lrnnsactions on\nInformation Theory,\n22(6):644~654, 1976.\n[228] P. Domingos and G. Hulten. Mining high-speed data strearns. In Pr'Oc. ACM 8IGI(DD\nInti. ConfeTence on }(nowledge Discovery and Data lVlining. AAAI Press, 2000.\n[229] D. Donjerkovic and R. Ramakrishnan. Probabilistic optiInization of top N queries In\nPTOC. Inti. Conf. on Very Large Databases, 1999.\n[230] W. Du and A. Eltnagarrnid.\nQuasi-serializability:\nA correctness criterion for global\nconcurrency control in interbase. In Proc. Intl. Conf. on Very La'rye Databases, 1989.\n(231] \\V. Du, R. Krishnarnurthy, and M.-C. Shan.\nQuery optiruization in a heterogeneous\nDBlVIS. In PTOC. Int!. ConI on VeTy LaTge Database8, 1992.\n[232] R. C. Dubes and A. .Jain.\nClustering f\\1ethodologies in ExplonLtory Data Analysi.s,\nAdvances 'in C01npnters. Acadelnic Press, New York, 1980.\n[233] N. Duppe!. Parallel SQL on TANDE1\\1 's NonStop SQL. IEEE COA1PCON, 1989.\n[2:34] H..Edelstein. The challenge of replication, Parts 1 and 2. DBAfS: Database and Client-\nServer Solutions, 1995.\n[235J \\V.Effelsberg and T. Haerder. Principles of database buffer rnanagement. AC1\\l[ Trans-\nactions on, Database Systems,\n~)(4) :560--595, 1984.\n[236]\n1\\/[' H. Eich. A classification and cOlTlparison of rnain lTlCrnory database recovery tech-\nniques. In PrOf:. IEEE IntZ. Conf. on Data Engineering, 1987.\n(2~37J A. Eisenberg and J. TYlelton.\nSQL:1999 , forrnerly kno\\vn as SQL:3\nACNf SIG.A10D\nRecord,\n28(1):1:31·-1~38, 1999.\n[238] A. El Abbadi.\nAdaptive protocols for rnanaging replicated distributed databases.\nIn\nIEBB 8yrnp.\n011 Parnllel and Distributed Proccs\"in,g, 1991.\n\nREFERE1VCE\",\n1017\n,$\n[239] A. EI Abbadi, D. Skeen, and F. Cristiano An efficient, fault-tolerant protocol for repli-\ncated data managenlent. In ACk[ Syn\"tp. on Principles of DatabaBc Systems, 1985.\n[240J C. Ellis.\nConcurrency in Linear Hashing.\nA Clv[ Trnnsuct'lons all, Database Sllsterns,\n12(2):195·····217, 1987.\n[241] A. Ehnagarrnid.\nDatabase Trunsaction J.vfodels faT Ad'uanced Applications.\nl\\ilorgan\nKaufIllann, 1992.\n[242] A. Elrnagannid, J. ,Hng, \\V. Kinl, O. Bukhres, and A. Zhang.\nGlobal cOllunitability\nin liluitidatabase systerns.\nIEEE Transactions on j(rwwledge and Data Bngirteering,\n8(5):816824, 1996.\n[243] A. Elrnagarrnid, A. Sheth, and N1. Liu. Deadlock detection algorithms in distributed\ndatabase systenls. In Proc. .IEEE Intl. Conf. on Data Engineering, 1986.\n[244] R. Elmasri and S. Navathe. Object integration in database design. In Proc. IEEE Intl.\nConf. on Data Engineer'ing, 1984.\n[245] R. Ehnasri and S. Navathe. Fundamentals of Database Systems. Benjamin-Curnrnings,\n3 edition, 2000.\n[246] R. Epstein.\nTechniques for processing of aggregates in relational database systeIlls.\nTechnical report, DC-Berkeley, Electronics Research Laboratory, M798, 1979.\n[247] R. Epstein, M. Stonebraker, and E. Wong. Distributed query processing in a relational\ndata base system. In Proc. AC!vI SICAI0D Conf. on the !vlanagement of Data, 1978.\n[248] M. Ester, H.-P. Kriegel, J. Sander, M. Wimmer, and X. Xu. Incremental clustering for\nmining in a data warehousing environment. In Proc. Intl. Conf. On Very Large Data\nBases, 1998.\n[249] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for discov-\nering clusters in large spatial databases with noise. In Proc. Intl. ConI. on Knowledge\nDiscovery in Databases and Data Jl;/ining, 1995.\n[250] J\\1. Ester, H.-P. Kriegel, and X. Xu. A database interface for clustering in large spatial\ndatabases. In Proc. Intl. ConI. on Knowledge Discovery in Databases and Data Mining,\n1995.\n[251] K. Eswaran and D. Chamberlin. Functional specification of a subsysteIll for data base\nintegrity. In Proc. Intl. ConI. on Very Lar:qe Databases, 1975.\n[252] K. Eswaran, .J. Gray, R. Lorie, and 1. Thaiger. The notions of consistency and predicate\nlocks in a data base systern. CornrnunicatJons of the AC!v[,\n19(11):624~-6~3~3, 1976.\n[253]\n[254]\n[256]\n[257]\n[258]\n[259]\nR.Fagin.\n~1ultivalued dependencies a.nd a new nornml fonn for relational databases.\nAC]v[ Trnnsact'ions on Database Systerns,\n2(~3):262·278, 1977.\nR. Fagin.\nNormal fonns and relational database operators.\nIn Proc. A C.l\\.:f SlCAI0D\nConf. on the Jvlanagernent of Data, 1979.\nR. Fagin. A nonnal form for relational databclses that is ba...'3ed on dornains and keys.\nA C.J\\,f Trnnsactions on Database Systerns, 6(;3) :~)87 -~415, 198!.\nR. Fagin, .1. Nievergelt, N. Pippenger, and H. Strong. Extendible Hashing--·a fast access\nrnethod for dynarnic files. A Cit! Transactions on Database Systems, 4(3), 1979.\nC. Faloutsos. Access rnethods for text. ACl\\l Cmn]Juting Survey\",\n17(1):49~\"74, 1985.\nC. Faloutsos.\n8eaTC~hing l\\Iultirnedia Databases by Content Kluwer Acadernic, 1996.\nC. I.i'aloutsos and S. Christodoulakis. Signature files: An access rnethod for docurnents\nand its analytical perforrnance evaluation.\nA CAl T1'anBact'ions on Oifice Inforrnal'ion\nSystems, 2(4):267288, 1984.\n\n1018\nDA.'I'ABASE NIANAGEivIENT SYSTEI\\1S\n[260] C. Faloutsos and H. Jagadish. On B-Tree indices for skewed distributions. In Proc. Inti.\nConf. on Very Large Databa..ges, 1992.\n[261J C. Faloutsos, R. Ng,and T. Sellis. Predictive load control for flexible buffer allocation.\nIn Proc. Intl. Conf. on \\fery Larye Databases, 1991.\n[262J C. Faloutsos, M. Ranganathan, and Y. ivfanolopoulos.\nFa..\"t subsequence lnatching in\ntitne-series databases.\nIn Proc. ACA1 SIGAfOD Conj. on the A1anagement of Data,\n1994.\n[263] C. Faloutsos and S. Rasenlan. Fractals for secondary key retrieval. In A C1\\I1 Symp. on\nPr~inciples of Database 8yste'ms, 1989.\n[264] M. Fang, N. ShivakuIIlar, H. Garcia-Molina, R. Motwani, and J. D. Ullrnan. Cornputing\niceberg queries efficiently. In Proc. Intl. ConlOn Very Large Data Bases, 1998.\n[265] U. Fayyad, G. Piatetsky-Shapiro, and P. Srnyth. The KDD process for extracting useful\nknowledge from volumes of data. Co'rnmunications of the ACJvI, 39(11):27--34, 1996.\n[266] U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors. Advances in\nKnowledge Discovery and Data Mining. MIT Press, 1996.\n[267] U. Fayyad and E. Simoudis. Data mining and knowledge discovery: T'utorial notes. In\nInti. Joint Conf. on Artificial Intelligence, 1997.\n[268] U. M. Fayyad and R. Uthurusamy, editors. Proc. Intl. ConI 'on Knowledge Discovery\nand Data Mining. AAAI Press, 1995.\n[269] M. Fernandez, D. Florescu, J. Kang, A. Y. Levy, and D. Suciu. STRUDEL: A Web site\nmanagement system. In Proc. ACM SIGMOD Conf. on Management of Data, 1997.\n[270] M. Fernandez, D. Florescu, A. Y. Levy, and D. Suciu. A query language for a Web -site\nmanagement system. SIGMOD Record (ACM Special Interest Group on Management\nof Data), 26(3):4-11, 1997.\n[271]\n]\\II. Fernandez, D. Suciu, and W. Tan. SilkRoute: trading between relations and XML.\nIn Proceedings of the WWW9, 2000.\n[272] S. Finkelstein, M. Schkolnick, and P. Tiberio. Physical database design for relational\ndatabases. IBM Research Review RJ5034, 1986.\n[273] D. Fishrnan, D. Beech, H. Cate, E. Chow, T. Connors, J. Davis, N. Derrett, C. Hoch,\nW. Kent, P. Lyngbaek, B. Nlahbod, M..-A. Neirnat, T. Ryan, and M.-C. Shan. Iris: an\nobject-oriented database rnanagernent system ACM Transactions on Office Infor\"mation\nSysterns, 5(1):48--69, 1987.\n[274J C. Flerning and B. von Halle. IIandbook of Relational Database Desig'n. Addison-Wesley,\n1989.\n[275] D. Florescu, A. Y. Levy, and A. O. IVlendelzon.\nDatabase techniques for the World-\nWide Web: A survey. SIGlvl0D Record (ACM Special Interest Group on lYfanagement.\nof Data), 27(a):59··74, 1998.\n[276]\nvV. Ford and :LvI. S. Balun. Sec7.lre ElectnJrt'ic Cornrnerrx: Building the Infrastructure for\nDigital 8-ignaturcs and Bnc'ryption (2nd Edition). Prentice Hall, 2000.\n[277] F. F'otouhi ahd S. Prarnanik. OptirnaJ secondary storage access sequence for perfonning\nrelational join. IEEE 7'ransactions on j{nowledge and Data Engineering, 1(:3):J18··';328,\nl!)89.\n[278J\n:LvI. I''owler and K. Scott.\n[fAIL D'lstilled: Applying the S't(J/ndaTd Object. A1.odeli'11,g Lan-\nguage. Addison-\\Nesley, 1999.\n[279] \\N. B. Frakes and R. Baeza-Yates, editors. Inforrnation Retrieval: J)al;a 8tructuTes and\nAlgorithms. PrenticeHall, 1992.\n\nREFERENCES\n1019\n[280J P. Franaszek, J. Robinson, and A. Thornasian. Concurrency control for high contention\nenvironrnents. ACArf Transactions on Database Systems, 17(2), 1992.\n[281] P. Franazsek, J. Robinson, and A. Thornasian. Access invariance and its use in high\ncontention enviroIlrnents. In Proc. IEEE Intel'nat'ional Conference on Data Eng'ineering,\n1990.\n[282]\nNI. Franklin.\nConcurrency control and recovery.\nIn Handbook of COmp'l.lter Science,\nA.B. Tucker (ed.)) eRe Press, 1996.\n[283]\nIV1. :FrankliIl, N1. Carey, and IV1. Livny.\nLocal disk caching for client-server database\nsysterlls. In Prnc. Intl. Conj. on Very Large Databases, 1993.\n[284]\nIV1. Franklin, B. Jonsson, and D. Kosslnan. Perfonnance tradeoffs for client-server query\nprocessing. In Proc. AC]l.l SIGMOD Conj. on the Managernent of Data, 1996.\n[285] P. Fraternali and L. Tanca. A structured approach for the definition of the semantics\nof active databases. AeM TrYlnsactions on Database Systems, 20(4) :414---471, 1995.\n[286] M. W. Freeston. The BANG file: A new kind of Grid File. In Proc. ACM SIGMOD\nConj. on the JvIanagement of Data, 1987.\n[287]\n.1. Freytag. A rule-based view of query optimization. In Proc. ACJvI SIGJvIOD Conj. on\nthe Managernent of Data, 1987.\n[288]\nO. Friesen, A. Lefebvre, and L. Vieille. VALIDITY: Applications of a DOOD system.\nIn IntZ. Can/. on Extending Database Technology, 1996.\n[289] J. Fry and E. Sibley. Evolution of data-base management systems. ACM Computing\nSurveys, 8(1):7-42, 1976.\n[290] N. Fuhr.\nA decision-theoretic approach to database selection in networked ir.\nAClvI\nTransactions on Database Systems, 17(3), 1999.\n[291] T. Fukuda, Y. Morimoto, S. Morishita, and T. Tokuyama. Mining optinlized association\nrules for numeric attributes. In ACJvI Syrnp. on .Principles of Database Systems, 1996.\n[292] A. F\\utado and M. Casanova.\nUpdating relational views.\nIn Query Processing in\nDatabase Systems. eds. W. Kiln, D.S. Reiner and D.S. Batory, Springer-Verlag, 1985.\n[293] S. Fushin1i, M. Kitsuregawa, and H. Tanaka.\nAn overview of the systems software\nof a parallel relational database machine: Grace.\nIn Proc. Intl. Conf. on Very Large\nDatabases, 1986.\n[294] V. Gaede and O. Guenther.\nMultidinlensional access rnethods.\nC01nputing Surveys,\n30(2):170---231, 1998.\n[295] H. Gallaire, J. IV1inker, and .1.-11. Nicolas (eds.). Advances in Database TheoT1.J) Vols. 1\nand 2. Plenum Press, 1984.\n[296] H. Gallaire and J. l\\!Iinker (cds.). Logic and Data Bases. Plcnurn Press, 1978.\n[297] S. Ganguly, \\tv. Hasan, and R. Krishnarnurthy. Query optirnizatioll for parallel execu-\ntion. In PTOC. A CAl SIGAfOD Conj. on the A1anagement of Data, 1992.\n[298J R. Ganski and H. \\i\\long. Optirnization of nested SQL qUf~ries revisited. In PTOC. AOAI\nSIGAI0D C7onj. on the Alanagement of Data, 1987.\n[299) V. Ganti, .1. Gehrke, and R. Rmnakrishnan. DenlOu: rnining and rnonitoring evolving\ndata. IEEE Trnnsaclions on KnO'llJledgc and Data Enginee1\"ing,\n1;·:~(1), 2001.\n[:300] V. Ganti, J. Gehrke, R. H.arn::lkrishnan, and \\tV.-Y. Loh. Focus: a franH~\\Vork for rneasur-\ning changes in data characteristics. In Pr'Oc. A Of'll SyrnposiurT1 on,Principlcs of Databa.se\nBysterns, 1999.\n\n1020\nDATABASE J\\'IANAGElVIENT SYSTEN!S\n(301] V. Ganti, J. E. Gehrke, and R. RaIuakrishnan. Cactus·-clustering categorical data using\nsununaries. In PruG. A CAl IntI. Conf. on Knowledge Discovery in Databases, 1999.\n[302] V. Ganti, R. Rarnakrishnan, .T. E. Gehrke, A. Powell, and J. French. Clustering large\ndatasets in arbitrary lnetric spaces. In Froe. IEEE IntI. Conf. Data Engineering, 1999.\n[~30a] H. Garcia-IvIolina and D. Barbara. How to assign votes in a distributed systenl.\nJou17~al\nof the ACAJ, 32(4), 1985.\n[~304J H. Garcia-J\\Iolina, R. Lipton, and J. Valdes. A 11lassive Inemory systern machine. IEEE\nTran8action.s on Compute'rs, C33(4)::391'-'-399, 1984.\n[305] H. Garcia-rvIolina, J. Ullman, and J. Widom.\nDatabase SY8tem,s: The CO'mplete Book\nPrentice Hall, 2001.\n[306] H. Garcia-:Nlolina and G. Wiederhold. Read-only transactions in a distributed database.\nACM Transactions on Database Syste'ms,\n7(2):209~234, 1982.\n[307J E. Garfield. Citation analysis as a tool in journal evaluation. Science, 178(4060):471-\n479, 1972.\n[308] A. Garg and C. Gotlieb. Order preserving key transformations. A CM Transaction8 on\nDatabase Systems, 11(2):213--234, 1986.\n[309] J. E. Gehrke, V. Ganti, R. Ramakrishnan, and W.-Y. Loh. Boat: Optimistic decision\ntree construction. In Proc. ACNI SIGNIOD Conj. on Managnwnt of Data, 1999.\n[310] J. E. Gehrke, F. Korn, and D. Srivastava.\nOn computing correlated aggregates over\ncontinual data streams. In Proc. ACNI SIGil/lOD Conj. on the l\\1anagement of Data,\n2001.\n[311] J. E. Gehrke, R. RaInakrishnan, and V. Ganti. Rainforest: A framework for fast decision\ntree construction of large datasets. In Proc. Intl. Conf. on Very Large Databases, 1998.\n[312] S. P. Ghosh. Data Base Organization for Data Manage'rnent (2nd ed.). Academic Press,\n1986.\n[313] P. B. Gibbons, Y. l\\1atias, and V. Poosala. Fast increlnental rnaintenance of approximate\nhistogran1s. In Proe. of the Conf. on Very Large Databases, 1997.\n[314] P. B. Gibbons and Y. Matias. New sarnpling-based summary statistics for irnproving\napproximate query answers.\nIn Proc. ACM SIGNIOD Conf. on the Nlanagement of\nData, pages 331-342. ACM Press, 1998.\n[~n5] D. Gibson, J. N1. Kleinberg, and P. Raghavan. Clustering categorical data: An approach\nbased on dynamical systems. In Proc. Int!. Conj. Very LaTge Data Bases, 1998.\n[316] D. Gibson, J. :N1. Kleinberg, and P. Raghavan.\nInferring web comrnunities fronl link\ntopology. In Proc. A C/vI Conj. on Hypertext, 1998.\n[317J G. A. Gibson. Redundant Disk Arrays: Reliable: Parallel Secondary Storage. An ACl\\1\nDistinguished Dissertation 1991. :NUT Press, 1992.\n[318] D. Gifford. vVeighted voting for replicated data. In ACAtf Syrnp. on Operating Systems\nPrinciples, 1979.\n[319] A. C. Gilbert, Y.\nKotidis~ S. Muthukrishnan, and N1. J. Strauss. Surfing wavelets 011\nstreams: One-pass sumrnaries for approximate aggregate queries. In Proc. of the Conf.\non Very Large Databases, 2001..\n[320] C. F. Goldfarb and P. Prescod. The )(ML Handbook. PrenticeHall, 1998.\n[321] R. Goldnlan and .1. vVidorIl. DataGuides: enabling query forrnulation and optiInization\nin semistructured databases.\nIn .Proc. Intl. Conf. on Very Large Data Bases, pa,ges\n436--445,\nH~97.\n\n1021\n[322] J. Goldstein l R. Ramakrishnan, U. Shaft, and J.-B. Yu. Processing queries by linear\nconstraints. In Proc. A CAl Syrnposiurn on Pl'ineiples of Database Sy8telns, 1997.\n(32:3] G. Graefe.\nEncapsulation of parallelisln in the Volcano query processing systeul. In\nProc. ACAtf 8IGA-fOD Con]. on the Jvfanagernent of Data, 1990.\n(324] G. Graefe. Query evaluation techniques for large databases. AGA£ Cornp'uti>ng Surveysl\n25(2), 1993.\n[325] G. Graefe l R. Bunker, and S. Cooper.\nHa.\"h joins and hash tealns ill lnicrosoft SQL\nServer: In Proc. Inti. Conlon Very Lar:qe Databases, 1998.\n[:326] G. Graefe and D. De\\iVitt. The Exodus optirnizer generator. In Proc. ACi\\,1 SIGAlOD\nConf. on the 1vfanagernent of Data, 1987.\n[327] G. Graefe and K. ¥lard. Dynanlic query optimization plans. In Proc. A CAl SIG1vfOD\nConf. on the Alanagement of Data, 1989.\n[328] M. Graham, A. :Nlendelzon, and M. Vardi. Notions of dependency satisfaction. Journal\nof the ACiVf, 33(1):105-129, 1986.\n[329] G. Grahne. The PToblem of lncornplete Inform,ation in Relational Databases. Springer-\nVerlag, 1991.\n[~~~~O] L. Gravano, H. Garcia-lVlolina, and A. Tornasic. Gloss: text-source discovery over the\ninternet. A CAf Transactions on Database Systerns, 24(2), 1999.\n[331] J. Gray. Notes on data base operating systems. In Operating Systems: An Advanced\nCourse. eds. Bayer, Grahanl, and Seegmuller, Springer-Verlag, 1978.\n[332] J. Gray. The transaction concept: Virtues and lilnitations. In PT'oc. Intl. Conf. on Very\nLaTge Databases, 1981.\n[333] J. Gray. Transparency in its place~thecase against transparent access to geographically\ndistributed data. Tandem ComputeT's: TR-8.9-1, 1989.\n[334] .J. Gray. The Bench'markHandbook: foT' Database and Transaction Processing System,s.\n1\\1organ Kaufmann, 1991.\n[3:3.5] .J. Gray, A. Bosworth, A. Layrnan, and H. Pirahesh. Data cube: A relational aggregation\noperator generalizing group-by, cross-tab and sub-totals. In Proc. IEEE Intl. COrtf. on\nData Engineering, 1996.\n[:3:36] J. Gray, R. Lorie, G. Putzolu, and 1. Traiger.\nGranularity of locks and degrees of\nconsistency in a shared data base.\nIn Proc. of IFf? Working Conf. on 1I,lodelling of\nData Base lvfanagement 8ysterns, 1977.\n[:3:37]\n..J. Gray, P. McJones,\n]\\:1. Blasgen, B. Lindsay, R. Lorie, G. Putzolu, T. Price, and\n1. Traiger. The recovery manager of the Systern R database Inanager. A CA1 C;ornp'llf'ing\nS'vrvcys, 13(2):223\"'242, 1981.\n[3:38] J. Gray and A. Reuter.\nTTansaction Processing:\nConcepts and Techniq11es.\nrv10rgan\nKaufmann, 1992.\n[339] P. Gray. Logic. Algebral and Databases. John \\\\liley, 1984.\n[:3,10]\n1\\-1. Greenwald and S. Khanna. Space-efficient online cornputation of quantile SUlIlluaries.\nIn Proc. AC1\\1 SlGJt10D Conf. on A1anagement of Data, 2001.\n[:341] P. Griffiths and B. \"Vade. An authorization rnechanisrn for a relational databC:kse system.\n/leAf rlhlnsactions on Database 8yslerns, 1(:3):242---255, 1976.\n[:342J G. Grinsteill. VisualizaJion and data rnining. In Inti. Con]. on Knowledge Disco'uery in\nDatabases, 1996.\n\n1022\nDATABASE rvlANAGEfvlENT SYSTE¥S\n[343] S. Guha, N. lVlishra, R. lVIotwani, and L. O'CaHaghan.\nClustering data streanlS.\nIn\nProc. of the Annlwl SYlnp. on Foundations of Cornputer Seience, 2000.\n[~~44] S. Guha, R. Rastogi, and K. Shilll.\nCure: an efficient clustering algorithrn for large\ndataba.,,;;es. In Proc. AClvl SIGlvfOD Conf. on A1anagernent of Data, 1998.\n[345] S. Guha, N. Kondas, and K. Shirn. Data streams and histogralns. In Proc. of the ACA:f\nSymp. on Theory of Computing, 2001.\n[346] D. Gunopulos, H. l\\1annila, R. Khardon, and H. Toivonen. Data rnining, hypergraph\ntransversals, and rnachine learning. In Proc. A C}v[ Syrnposium on Principles of Database\nSystem,s, pages 209-216, 1997.\n[347] D. Gunopulos, H. Nlannila, and S. Saluja.\nDiscovering all most specific sentences by\nrandomized algorithms. In Proc. of the Inti. Conf. on Database Theory, voluille 1186 of\nLecture Notes in Co'mputer Science, pages 215-229. Springer, 1997.\n[348] A. Gupta and 1. Munlick. Materialized Views: Techniques] Implementations] and Ap-\nplications MIT Press, 1999.\n[349] A. Gupta, I. Munlick, and V. Subrahmanian. Maintaining views incrementally. In Proc.\nACNI SIG1vfOD Conf. on the lvIanagement of Data, 1993.\n[350] A. Guttman. R-trees: a dynarnic index structure for spatial searching. In Proc. ACM\nSIGMOD Conf. on the lvfanagernent of Data, 1984.\n[351] L. Haas, W. Chang, G. Lohman, J. McPherson, P. Wilrns, G. Lapis, B. Lindsay, H. Pi-\nrahesh, M. Carey, and E. Shekita.\nStarburst mid-flight:\nAs the dust clears.\nIEEE\nTransactions on Knowledge and Data Engineering, 2(1), 1990.\n[352] P. Haas, J. Naughton, S. Seshadri, and L. Stokes.\nSanlpling-based estimation of the\nnumber of distinct values of an attribute. In Proc. Intl. Conf. on Very Large Databases,\n1995.\n[353] P. Haas and A. Swarni. Sampling-based selectivity estinlation for joins using augmented\nfrequent value statistics. In Prne. IEEE Intl. Conf. on Data Engineering, 1995.\n[354] P. J. Haas and .J. M. Hellerstein. Ripple joins for online aggregation. In Proc. A ClvI\nSIGMOD Conf. on the A:fanagement of Data, pages 287-298. ACM Press, 1999.\n[355] T. Haerder and A. Reuter.\nPrinciples of transaction oriented database recovery-a\ntaxonorny. ACM Cornputing Surveys, 15(4), 1982.\n[356) U. Halici and A. Dogac.\nConcurrency control in distributed databases through tirne\nintervals and short-term locks. IEEE Transaction8 on SoftwaTe Engineering, 15(8):994--\n100~3, 1989.\n[;357]\nN1. HalL\nCOTe Web Prograrnrning: IfTML ] Java, CGI\nJ €.1 Javasc'lipt. Prentice-Hall,\n1997.\n[358) P. Hall.\nOptirnization of a sinlple expression in a relational data base systern.\nIBM\nJournal of Research and Develop'ment, 20(3):244--257, 1976.\n[;359] G. Harnilton, R. G. Cattell, and 1:1. Fisher.\nJDBC Database Accc.ss With Java:\nA\nTutorial and Annotated Refe-rence. Java Series. Addison-Wesley, 1997.\n[360]\nIVI. Harnrner and D. lIIcLeod. Semantic integrity in a relational data ba,se system. In\nProc. Intl. Conf. on Very Large Databases, 1975.\n[~~61]\nJ. Han and Y. Fu. IJiscovery of rnultiple-Ievel association rules frorn large databases.\nIn Pr'Oc. Intl. Conf. on Ver'Y Lar:qe Databases, 1995.\n[:362] D. Hanel.\nConstr\"uction and Asscssrnent of Classification Rules. John \\,\\liley &\nSOilS,\nChichester, England, 1997.\nt\n\nlO~3\n[:36:3] J. Han and ivi. Kamber. Data lvlining: Cancellt\" and Techn'iques. iv1.organ Kauflnann\nPublishers, 2000.\n[364] J, Han, J. Pei, and Y. Yin. wIining frequent patterns without candidate generation. In\nProc. ACAri 8IGlvfOD Inti, Conf. on lvfanagernent of Data, pages\n1~12, 2000.\n[;365] E. Hanson. A perfonnanee analysis of vie-\\v lllaterialization strategies. In Proc. A CAf\n8IOA10D Conf. on the A1anagement of Data, 1987.\n[366] E. Hanson. Rule condition testing and action execution in Ariel. In Proc. ACAf SIGA10D\nConf. on the Afanagernent of Data, 1992.\n[367] V. Harinarayan, i\\. Rajaralnan, and J. Ulhnan. Implelnenting data cubes efficiently. In\nProc. ACAf SIGltfOD Conf. art the Management of Data, 1996.\n[368] J. Haritsa, 1V1. Carey, and 11. Livny. On being optilnistic about real-tilne constraints.\nIn A CA'1 Syrnp. onp,,'inciples of Database Systems, 1990.\n[369] J. Harrison and S. Dietrich. Nlaintenance of rnaterialized views in deductive databases:\nAn update propagation approach. In Proc. Workshop on Deductive Databases, 1992.\n[370] T. Ha..stie, R.. Tibshirani, and J. H. Friednlan.\nThe Elements of Statistical Learning:\nData Mining, Inference, and Prediction. Springer Verlag, 2001.\n[371] D. Heckennan. Bayesian networks for knowledge discovery. In Advances in Knowledge\nDiscovery and Data Mining. eds. U.M. Fayyad, G. Piatetsky-Shapiro, P. SInyth, and R..\nUthurusaIny, MIT Press, 1996.\n[372] D. Heckerman, H. Mannila, D. Pregibon, and R, Uthurusamy, editors. Proc. .Intl. Conf.\non Knowledge Disc07Jery and Data A1ining. AAAI Press, 1997.\n[373]\nJ. Hellerstein. Optimization and execution techniques for queries with expensive meth-\nods. Ph,D. thesis, University of Wisconsin-Aladison, 1995.\n[374] J. Hellerstein, P. Haas, and H. Wang.\nOnline aggregation\nIn Proc. ACM SIGNIOD\nConf. on the Nlanagernent of Data, 1997.\n[~375] J. Hellerstein, E. Koutsoupias, and C. Papadirnitriou.\nOn the analysis of indexing\nschemes.\nIn Proceed'ings of the A CN! Syrnposi7.lm, on Principles of Database Systems,\npages\n249~256. AClVI Press, 1997.\n[376]\n.1. Hellerstein, .1. Naughton, and A. Pfeffer. Generalized search trees for database sys-\ntems. In Prnc. Inti. Conf. on Very Lar:qe Databases, 1995.\n[377] J.N1. Hellerstein, E. Koutsoupias, and C. H. Papadirrlitriotl. On the analysis of indexing\nschclnes. In PTOC. A CAf Symposium, on Principles of Database SysterrLs, pages 249····256,\n1997.\n[:378] C. Hidber\nOnline association rule rnining.\nIn Proc. A CAf SIGNfOD ()onf.\non, the\nAfanagernent of Data, pages 145····-156, 1999.\n[:379] R. H.imnlcrocder, G. Lausen, B. Ludaescher, and C. Schlepphorst.\nOn a declarative\nsernantics for Web queries. Lect-ure Notes 'in Computer Science, 1341:386··...;398, 1997.\nU~80]\nC.-'1'. Ho, R. Agrawal, N. rvIegiddo, and R. Srikant. Range queries in OLAP data cubes.\nIn Proc. A CAl SIGN/OJ) Conf. on the A1anagernent of Data, 1997.\n[:381J S. Holzner. Xl'l/[L Cornplete. ?v1c Craw-Hill, 1998.\n[:382] I). Hong, T. Johnson, and U. Chakravarthy. Real-tirne transaction scheduling: A cost\nconscious approach. In Pr'Oc. ACAf SIGAfOD Conf. on the Alanagcment of Data, 1993.\n[383J \\V. Hong and Iv1. Stonebraker. Optirnization of parallel query execution plans in XPRS.\nIn Proc. Intl. Conf. on Parallel and Distributed Injo-rmation 8ystem,8, 1991.\n\n1024\nDATABASE IviANAGEMENT SYSTEMS\n[:384} \\¥.-C. HOll and G. Ozsoyoglu.\nStatistical estimators for aggregate relational algebra\nqueries. AG1Vf Transactions on Database Systems, 16(4), 1991.\n[385] H. Hsiao and D. DeWitt. A performance study of three high availability data replication\nstrategies. In Pr-oe. Inti. Conf. o'n Parallel and D'istributed Info'rmation Systerrts, 1991.\n[386J J. Huang, J. Stankovic, K. RamaIllrithaIll, and D. Towsley. Experimental evaluation of\nreal-tirne optiInistic concurrency control SChell1es. In Proc. Intl. Conf. on Very La'rge\nDatabases, 1991.\n[387} Y. Huang, A. Sistla, and O. vVolfson. Data replication for rrlObile cOlnputers. In Proc.\nAC.L\\>f SIGA10D Conj. on the .A1anagement of Data, 1994.\n[388] Y. Huang and O. Wolfson. A cOlnpetitive dynarnic data replication algorithm. In Proc.\nIEEE CS IEEE Inti. Conf. on Data Engineering, 1993.\n[389] R. Hull. Managing semantic heterogeneity in databases: A theoretical perspective. In\nACM Symp. on Principles of Database Syste'ms, 1997.\n[390] R. Hull and R. King. Semantic database modeling: Survey, applications, and research\nissues. A CM Cornputing Surveys, 19(19):201-260, 1987.\n[391J R. Hull and J. Suo Algebraic and calculus query languages for recursively typed complex\nobjects. Journal of Computer and System Sciences, 47(1):121-156, 1993.\n[392] R. Hull and M. Yoshikawa.\nILOG: Declarative creation and rnanipulation of object-\nidentifiers. In Proc. Inti. Conf. on Very Large Databases, 1990.\n[393] G. Hulten, L. Spencer, and P. Domingos. Mining tillIe-changing data strearns. In Proc.\nAClvI SIGKDD Intl. Conference on Knowledge Discovery and Data l\\!Jining, pages 97-\n106. AAAI Press, 2001.\n[394] J. Hunter. Java Servlet Programming. O'Reilly Associates, Inc., 1998.\n[395] T. Imielinski and H. Korth (eds.). Mobile Computing. Kluwer Acadeluic, 1996.\n[396] T. Imielinski and W. Lipski. Incomplete information in relational databases.\nJournal\nof the AClvI, 31(4):761-791, 1984.\n[~{97] T. Imielinski and H. Mannila. A database perspective on knowledge discovery.\nCom-\nmunications of the ACM, 38(11):58-64, 1996.\n[398J T. Imielinski, S. Viswanathan, and B. Badrinath. Energy efficient indexing on air. In\nProc. ACM SIGJv!OD Conf. on the Management of Data, 1994.\n[399] Y. Ioannidis. Query optimization. In Handbook of Comp'uteT Science. ed. A.B. Tucker,\nCRC Press, 1996.\n[400] Y. Ioannidis and S. Christodoulakis. Optirnal histograms for liIlliting worst-case error\npropagation in the size of join results. ACM Transactions on Databw,e Systems, 1993.\n[401] Y. Ioannidis and Y. Kang. Randornized algorithms for optimizing large join queries. In\nProc. ACA1 SIGlvl0D Conf. on the Jvfanagement of Data, 1990.\n[402] Y. Ioannidis and Y. Kang.\nI.left-deep vs. bushy trees: An analysis of strategy spaces\nand its irnplications for query optirnization.\nIn\nPTOC. A CAf SIG1VfOD Conf. on the\njVfanagemerd of Data, 1991.\n[403] Y. Ioannidis, R. Ng, K. Shirn, and T. Sellis. Parall1etric query processing. In Pn)c. Ina.\nConf. on Very Large Databases, 1992.\n[404] Y. Ioannidis and R. Rarnakrishnan. Containment of conjunctive queries: Beyond rela-\ntions\n(l\"S sets. ACA1 TransactioTl,s on Database Sy8terns, 20(3):288--324, 1995.\n[405J Y. E. Ioannidis. Universality of serial histograrns. In Proc. Intl. Conf. on Ve'ry Large\nDatabase8, 199:3.\n\nREF\"1ERENGES\n1025\n~\n[406] H. .Jagadish, D. Lieuwen, R. Rastogi, A. Silberschatz, and S. Sudarshan.\nDali:\nA\nhigh perfonnance Inain-rnClnory storage rnanager. In Proc. Inti. Conf. on Very La'rge\nDatabases, 1994.\n[407] A. K. Jain and R. C. Dubes. Algor'ithrns for Clllster'ing Data. PrenticeHall, 1988.\n[408] S. Jajodia and D. Mutchler. Dynamic voting algorithllls for rnaintaining the consistency\nof a replicated database. ACA,1 Transact'ions on Database Systerns, 15(2):23{}-280, 1990.\n(409) S. Jajodia and R. Sandhu. Polyinstantiation integrity in multilevel relations. In Proc.\nIEEE Syrnp. on Security and PT'ivacy, 1990.\n[410] NI. Jarke and J. Koch.\nQuery optimization in database systerns.\nACA1 Cornputing\nSUT'veys, 16(2):111-152, 1984.\n[411] K. S. Jones and P. Willett, editors.\nReadings in Information RetT'ieval.\nIvIultimedia\nInfonnation and Systems. Morgan Kaufmann Publishers, 1997.\n[412] J . .lou and P. Fischer. The complexity of recognizing 3NF schemes. Inforrnation Pro-\ncessing Letters, 14(4):187---190, 1983.\n[413] N. Kabra and D. J. DeWitt. Efficient mid-query re-optirnization of sub-optimal query\nexecution plans. In Proc. ACM SIGMOD Intl. Conf. on lvIanagement of Data, 1998.\n[414] Y. Kambayashi, M. Yoshikawa, and S. Yajirna.\nQuery processing for distributed\ndatabases using generalized semi-joins.\nIn Proc. ACM SIGMOD Conf. on the lvlan-\nagement of Data, 1982.\n[415] P. Kanellakis.\nElements of relational database theory.\nIn Handbook of Theoretical\nComputer Science. ed. J. Van Leeuwen, Elsevier, 1991.\n[416] P. Kanellakis. Constraint programming and database languages: A tutorial. In A CM\nSymp. on Principles of Database Systems, 1995.\n[417] H. Kargupta and P. Chan, editors.\nAdvances in Distributed and Parallel Knowledge\nDiscovery. MIT Press, 2000.\n[418] L. Kaufman and P. Rousseeuw.\nFinding Groups in Data: An Introduction to Cluster-\nAnalysis. John Wiley and Sons, 1990.\n[419] R. Kaushik, P. Bohannon, J. F. Naughton, and H. F. Korth.\nCovering indexes for\nbranching path expression queries. In Proceedings of SIG!v[OD, 2002.\n[420] D. Keirn and H.-P. Kriegel. VisDB: a system for visualizing large databases. In PTOC.\nACM SIGMOD Conf. on the Management of Data, 1995.\n[421] D. Keirn and H.-P. Kriegel.\nVisualization techniques for mining large databases:\nA\ncomparison.\nIEEE Transactions on Knowledge and Data Engineering, 8(6):923-----938,\n1996.\n[422) A. Keller. Algorithrns for translating view updates to database updates for views involv-\ning selections, projections, and joins. A CAf Syrnp. on Principles of Database Syste'ms,\n1985.\n[423] \\V. Kent. Data and Reality, Basic Ass'LLTnptions in Data Processing Reconsidered. North-\nHolland, 1978.\n[424] W. Kent, R. Ahrned, J. Albert, IVI. Ketabchi, and fv1.-C. Shan. Object identification in\nrnulti-database systerns. In IFIP Intl. Conf. on Data S'ernantics, 1992.\n[425] L. Kerschberg, A. Klug, and D. Tsichritzis. A taxonorny of data rnodels. In Systerns\nfaT Lm:qe Data Bases. eds. P.C. Lockernann and E.J. Neuhold, North-Holland, 1977.\n[426]\nVV'. Kiessling.\nOn senlantic reefs and efficient processing of correlation queries with\naggregates. In Pr-oc. Intl. Conf. on Very Lar:ge Databases, 1985.\n\n1026\nI)ATABASE JVIANAGE?v1ENT\nSYSTEtvi~\n[427J Ivi. Kifer, \\V. Kiln, and Y. Sagiv. Querying object-oriented databases. In Proc. ACAJ\nSIOlviODCon,j. on the lvfanagerncnt of Data, 1992.\n[428J1V1:. Kifer, G. Lausen, and J. \\tVu. Logical foundations of object-oriented and fraIne-based\nlanguages. .lou'rnal of the AOA4, 42(4):741-····843, 1995.\n[429J\n:tv1. Kifer and E. Lozinskii. Sygraf: lrnpleuwIlting logic prograrns in a database style.\nIEEE Transactions on Software Bngineering, 14(7):922-935, 1988.\n[4:30] W. Kirn. On optimizing an SQL -like nested query.\nAONf Transactions on Database\nSystents, 7(3), 1982.\n[431] \\V. Kirn. Object-oriented database systerIls: Prornise, reality, and future. In Proc. Intl.\nConI on Ve1'y Large Database8,\nH)9~3.\n[432] V\\l. Kim, J. Garza, N. Ballou, and D. Woelk. Architecture of the ORION next-gcneration\ndatabasc systeln.\nIEEE Transactions on Knowledge and Data Engineering, 2(1):109·'\"\n124, 1990.\n[433] W. Kiln and F. Lochovsky (eds.).\nObject-Oriented Concepts, Databa8es, and Applica-\nt'ions. Addison-Wesley, 1989.\n[434] W. Kim, D. Reiner, and D. Batory (eds.).\nQuer1.J Processing in Database Systems.\nSpringer Verlag, 1984.\n[435] W. Kirn (ed.). A10dern Database Systerns. ACM Press and Addison-Wesley, 1995.\n[436] R. Kimball. The Data WarehO'lL8e Toolkit. John Wiley and Sons, 1996.\n[437] J. King. Quist: A system for semantic query optilnization in relational databases. In\nPTOC. Inti. Conf. on Very Large Databases, 1981.\n[438] J. M. Kleinberg. Authoritative sources in a hyperlinked environrnent. In Proc. AClV!\n-SIAlV! Syrnp. on Discrete AlgoTithrns, 1998.\n[439] A. Klug.\nEquivalence of relational algebra and relational calculus query languages\nhaving aggregate fUIlctions. Jov,1twl of the AClV[, 29(3):699-717, 1982.\n[440] A. Klug. On conjunctive queries containing inequalities. JouTnal of the ACA1, 35(1):146---\n160, 1988.\n[441] E. Knapp.\nDeadlock detection in distributed databa.ses.\nACA1' C01npu,t'ing Sur''Vcys,\n19(4) :30:l<{28, 1987.\n[442] D. Knuth. The Art of Cornpv,{cr Pr-ograrnrning, Vol.3--..-\"-Sorting and Sear-ching. Addison-\nWesley, 1973.\n[443] G. Koch and K. Loney.\nOr-acle:\nThe Cornplete Reference.\nOracle Press, Osborne-\nlVlcGraw-Hill, 1995.\n[444]\n\\tV. Kohler.\nA survey of techniques for synchronization and recovery in decentralized\ncornputer systerns. AC1\\lf ()ornputing\nS1Lr~lJeys, 1:3(2):149..--184, 1981.\n[445] D. Konopnicki and O. Shmueli. W:3QS: A systern for WW\\V querying. In Proc. IEEE\nIntl. Conf. on Data Bng'ine(:T'ing, 1997.\n[446] F. Korn, H. Jagadish, and C. Faloutsos.\nI~fficiently supporting ad hoc queries in large\ndatasets of tirne sequences. In PTOC. AeA1 SIGllifOD Conf. on A1anagernent of Data,\n1997.\n[4471 lYt Kornacker, C. fvlohan, etnd J. Hellerstein. Concurrency and recovery in generalized\nsearch trees. In Proc. ACM SIGJI.;10D Conf. on the !'v1anagenwnt of Data, 1997.\n[448] II. Korth, N. Soparkar, and A. Silberschatz. Triggered real-tilne databa..\"es with consis-\ntency constraints. In PTOC. 1ntL Conf. on Very Large IJatabase8, 1990.\n\nREJ?ERE1VCES\nlCl;27\n[449] H. F. Korth.\nDeadlock freedoIIl using edge locks.\nACAJ Trunsact'ions on Database\n8:t/stem,s, 7(4) :6;32--..652, 1982.\n[450J I). Kosslnann. The state of the art in distributed query processing.\nACAtf Cornputing\nBuT'veys, :32(4) :422-~·4G9, 2000.\n[451] Y. Kotidis and N. Roussopoulos. An alternative storage organization for ROLAP ag-\ngregate views based on cubetrees. In Proc. A (;.Nf SIGAI0D Inti. Conf. on Alanagernent\nof Data, 1998.\n[452J N. Krishnakulllar and A. Bernstein. High throughput escrow algorithrns for replicated\ndatabases. In Proc. Intl. Conf. on Very Large Databases, 1992.\n[45~1] R. Krishnarllurthy, H. Boral, and C. Zaniolo. Optilnization of nonrecursive queries. In\nProc. Inti. Conf. on Very Large Databases, 1986.\n[454] J. Kuhns. Logical aspects of question answering by cOlnputer. Technical report, Rand\nCorporation, R?\\1-5428-Pr., 1967.\n[455] V. Kumar. Perjorrnance of COnC'/1,Trency ContTol lv1echanisrns in Centralized Database\nSysterns. PrenticeHall, 1996.\n[456] H. Kung and P. Lehillan. Concurrent lnanipulation of binary search trees. AClv1 Trans-\nactions on Database Systems, 5(3):354-382, 1980.\n[457] H. Kung and J. Robinson. On optimistic rnethods for concurrency control. PTOC. Inti.\nConf. on Very Large Databases, 1979.\n[458] D. Kuo. Model and verification of a data manager based on ARIES. In Inti. Conf. on\nDatabase Theory, 1992.\n[459] M. LaCroix and A. Pirotte. Domain oriented relational languages. In PTOC. Inil. Conf.\non Very Large Databases, 1977.\n[460] M.-Y. Lai and W. Wilkinson. Distributed transaction management in Jasmin. In Proc.\nIntl. Conf. on VeTy Large Databases, 1984.\n[461] L. Lakshmanan, F. Sadri, and 1. N. Subramanian.\nA declarative query language for\nquerying and restructuring the web. In Pr\"Oc. Inti. Conf. on Re8eaTch Issues ,in Data\nEngineeTing, 1996.\n[462] L. V. S. Lakshrnanan,\nRayrnond '1'. Ng, J. Han, and A. Pang.\nOptirnization of con--\nstrained frequent set queries with 2-variable constraints. In Prvc. AC!vl SIGA10D Inti.\nConf. on Afanagernent of Data, pages 157-168. ACM Press, 1999.\n[46~i] C. Larn, G. Landis, J. Orenstein, and D. Weinreb. The Objectstore database systern.\nCorn'l7'LuTt'ications of the AC1vf, 34(10), 1991.\n[464] L.Laruport. TiIHe, clocks and the ordering of events in a distributed system. Cornrnu-\n'nications of the A CAl, 21(7):558---565, 1978.\n[465] B. l..larnpson and D. Lornet. A new presurned comnlit optirnization for two phase cOHauit.\nIn Proc. Intl. Conf. on -Very Large Databases, 199:3.\n[466] B. Larnpson and H. Sturgis.\nCrash recovery in a distributed data storage systen1.\nTechnical teport, Xerox PARC, 197fi.\n[467] C. Landwehr. Fonnal rnodels of cornputer security. A CAl Cornputing 5'uT'ueys, 13(:3):247-\n278, 1981.\n[468] R. Langerak. View updates in relational databases with an independent scheIne. A (7/1;1\n7hrnsactions on Dat;abase Systems, 15(1):40-66, 1990.\n[469] P.-A. Larson. Linear hashing with overflow-handling by linear probing. !lelv! 7''rnnsac-\ntions on Database Systems, 10(1) :75---89, 1985.\n\n1028\nDATABASE JVIANAGEMENT SYSTEIVPS\n[470] P.-A. Larson.\nLinear hashing with\nseparators~~-A dyuarnic hashing scheIlle achieving\none-access retrieval. AC!t:f Transact'ions on Database Systerns,\n13(3):~366·-388, 1988.\n[471) P.-A. Larson and G. Graefe. lVlernory 1vlanageluent During Run Generation in External\nSorting. In Pror:. ACAI SIGAIOD Conf. on N!anagernent of Data, 1998.\n[472J P. LehIuan and S. Yao. Efficient locking for concurrent operations on b trees.\nA CN!\nTransactions on Database Systerns, 6(4):65(}--·670, 1981.\n[473} T. Leung and R. lVIuntz. Tenlporal query processing and optilllization in rllultiprocessor\ndatabase machines. In Proc. Intl. Conf. on 'Very Large Databases, 1992.\n[474] Ivt Leventhal, D. Lewis, and\n~1. J:.'uchs.\nDesigning XlvIL Internet applications.\nThe\nCharles F. Goldfarb series on open infornlation managelnent. PrenticeHall, 1998.\n[475] P. Lewis, A. Bernstein, and 1\\l1. Kifer. Databases and 1ransaction Processing. Addison\n\\Vesley, 200l.\n[476] E.-P. Lim and J. Srivastava. Query optirnization and processing in federated database\nsystenls. In Proc. Intl. Conf. on Intelligent Knowledge N[anagement, 1993.\n[477] B. Lindsay, J. McPherson, and H. Pirahesh. A data InanageIllent extension architecture.\nIn PTOC. AC]v[ SIGMOD Conf. on the Management of Data, 1987.\n[478] B. Lindsay, P. Selinger, C. Galtieri, J. Gray, R. Lorie, G. Putzolu, I. Traiger, and\nB. Wade.\nNotes on distributed databases.\nTechnical report, RJ2571, San Jose, CA,\n1979.\n[479] D.-I. Lin and Z. M. Kedem. Pincer search: A new algorithnl for discovering the maxi-\nmUIn frequent set. Lecture Notes in Computer Science, 1377:105-77, 1998.\n[480] V. Linnemann, K. Kuspert, P. DadaIn, P. Pistol', R. Erbe, A. Kenlper, N. Sudkamp,\nG. Walch, and J\\r1. Wallrath.\nDesign and implementation of an extensible database\nmanagement systern supporting user defined data types and functions.\nIn Proc. Intl.\nConf. on Very Large Databases, 1988.\n[481] R. Lipton, J. Naughton, and D. Schneider.\nPractical selectivity estirnation through\nadaptive sanlpling. In Proc. ACA1 SIGN[OD Conf. on the l\\1anagement of Data, 1990.\n[482] B. Liskov, A. Adya, J\\r1. Castro, ]\\1. Day, S. Ghemawat, R. Gruber, U. Maheshwari,\nA. Myers, and L. Shrira. Safe and efficient sharing of persistent objects in Thor. In\nProc. ACM SIGN/OD Conf. on the A1anagem,ent of Data, 1996.\n[48;)] \"V. Litwin.\nLinear Hashing: A new tool for file and table addressing.\nIn Proc. Intl.\nConf. on Very Large Databases, 1980.\n[484] W. Litwin. Trie Hashing. In Proc. ACNI SIGAI0D Conf. on the A1anagelnent of Data,\n1981.\n[485] W. Litwin and A. AbdellatiL\nJ\\rIultidataln-Lc;e interoperability.\nIEEE ComputeT,\n12(.1.9):l(}-··18, 1986.\n[486] vV. Litwin, L. Nlark, and N. Roussopoulos.\nInteroperability of rnultiple autonornous\ndatabases. A ChI Cornputing Surveys,\n22(~3), .1.990.\n[487] \\V. Litwin, NI.-A. Neirnat, and D. Schneider. LH *..u_/'r. scalable, distributed data struc-\nture. AC/I;I Transactions on Database Systerns, 21 (4):48(}·\"·525, 1996.\n[488] !vI. Liu, A. Sheth, and A. Singhal. An adaptive concurrency control strategy for dis-\ntributed datab<:h'Sc systern. InPn)(;. IE'EE Intl. Can!. on Data Bng'ineering, 1984.\n[489J\n:NI. Livny, R. Rarnakrishnan,K. Beyer, G. Chen, D. Donjerkovic, S. Lawande, .1. IvIyl-\nlyrnaki, and K. Wenger. DEVise: Integrated querying and visual exploration of large\ndatasets. In Pr\"Oc. AC!vf SIG/I;10D Con]. on the Afanagernent of Data, 1997.\n\n1029\n[490] G. Lohrnau. Granuuar-like functional rules for representing query optiInization alter-\nnatives. In ProG. AC1\\f SIGA·fOD ConI on the lvlanagement of Data, 1988.\n[491J D. Lomet and B. Salzberg. The hB-T ree: A rnultiattribute indexing lllethod with good'\nguaranteed perforrnance. ACJ\\;! Transactions on Databa.se Sy.stems, 15(4), 1990.\n[492) D. Lomet and B. Salzberg. Access method concurrency with recovery. In ProG. AC]tf\nSIGA10D Conf'. on the l\\Ilanagement of Data, 1992.\n[493] R. Lorie.\nPhysical integrity in a large segnlented database.\nA CM Transactions on\nDatabase SystenL.s, 2(1):91-104, 1977.\n[494] R. Lorie and H. Young. A low COlllll1Unication sort algorithm for a parallel database\nrnachine. In ProG. Intl. Conf. on Very Large Database.s, 1989.\n[495] Y. Lou and Z. Ozsoyoglu. LLO: An object-oriented deductive language with methods\nand method inheritance. In ProG. ACNf SIGlvlOD Conf. on the Management of Data,\n1991.\n[496] H. Lu, B.-C. Ooi, and K.-L. Tan (eds.). Query Processing in Par-allel Relat'ional Database\nSy.stems. IEEE Computer Society Press, 1994.\n[497] C. Lucchesi and S. Osborn.\nCandidate keys for relations.\nJ. Com,puter and System\nSciences, 17(2):270-279, 1978.\n[498] V. Lum. Multi-attribute retrieval with combined indexes. Communications of the ACM,\n1(11) :660-665, 1970.\n[499] T. Lunt, D. Denning, R. Schell, M. Heckman, and W. Shockley. The seaview security\nIllodel. IEEE Transactions on Software Engineering, 16(6):593---607, 1990.\n[500] L. Mackert and G. Lohrnan. R* optimizer validation and performance evaluation for\nlocal queries. Technical report, IBM RJ-4989, San Jose, CA, 1986.\n[501] D. Maier. The Theor-y of Relational Databases. Computer Science Press, 1983.\n[502] D. .l\\1aier, A. Mendelzon, and Y. Sagiv. Testing irnplication of data dependencies. ACM\nTr-ansactions on Database Systerns, 4(4), 1979.\n[503} D. Maier and D. Warren.\nCornputing wdh Logic:\nLogic Progrurnming with Prolog.\nBenjaminjCurnnlings Publishers, 1988.\n[504] A. Makinouchi. A consideration on normal fonn of not-necessarily-nonnalized relation\nin the relational data rnodel. In Proc. Intl. Conf. on Very Lar-ge Databases, 1977.\n[505] U. .l\\1anber and R. Ladner. Concurrency control in a dynalnic search structure. A CiV!\nTr-ansact'ions\nOTt Database Systerns, 9(3) :439·--455, 1984.\n[506] G. l\\1anku, S. Rajagopalan, and B. Lindsay.\nHandolIl salnpling techniques for space\nefficient online COlTlputation of order statistics of large datasets. In P7\"OC. ACM SIGJvfOD\nConf. on Nfanagement of Data, 1999.\n[507] H. Ivlannila. lVfethods and problerns in data nlining. In Intl. Conf. on Database Theory,\n1997.\n[508] H. f\\1annila and K.·-J. Raiha. Design by Exarnple: An application of ArlTlstrong relations.\nJournal of Cornputcr- and System Sciences, 3:3(2):126--141, 1986.\n[509] H. rvIannila and K.-J. Raiha.\nThe De8'ign of Relational Databases.\nAddison-Wesley,\n1992.\n[510] H. f\\'lannila, H. Toivonen, and A. 1. VerkarTlo. Discovering frequent episodes in sequences.\nIn PTOC. Intl. Conf. on Kno'wledge\nDi8cover~1J in Databases and DataA1'in:ing, 1995.\n[511] H. .l\\ifannila, P. SIllyth, and D. J. Hanel. Principlc.s of Data Al'ining. I\\lIT Press, 20tH.\n\n1030\nDATABASE }\\/IANAGElvIENT SYSTEmS\n[512] IvI. Ivlannino, P. Chu, and T. Sager. Statistical profile estirnation in database SystCIUS.\n11C1\\[ Comp'ltting Surveys, 20(:3):191-221, 1988.\n[513] V. l\\ilarkowitz.\nRepresenting processes in the extended entity-relationship Illodel.\nIn\nProc. IEEE Intl. Conf. on Data Engineering, 1990.\n[514] V. 1rfarkowitz. Safe referential integrity structures in relational databases. In Pr'Oc. Inti.\nConf. on\n~!eTY Large Databases, 1991.\n[515J Y.\n~latias, J. S. Vitter, and NI. \\Vang.\nDyumuic Iuaintenance of wavelet-based his-\ntograrns. In Proc. of the Conf. on l./ery Large Databases, 2000.\n[516J D. :NIcCarthy and U. Dayal.\nThe architecture of an active data\nba..~e manageIuent\nsystem. In Proc. A CAl SIGA10D Conf. on the ]V!anagernent of Data, 1989.\n[517] vV. wIcCune and L. Henschen. Nlaintaining state constraints in relational databases: A\nproof theoretic basis. Jo'urnal of the ACNf, 36(1):46~68, 1989.\n[518]\n.1. IVIcHugh, S. Abiteboul, R. Goldman, D. Quass, and J. Widom. Lore: A database\nrnanagement systmu for smnistructured data. ACJ11 SIGN/aD Record, 26(3):54----66, 1997.\n(519] S. Mehrotra, R. Rastogi, Y. Breitbart, H. Korth, and A. Silberschatz. Ensuring trans-\naction atonlicity in rnultidatabase systerns. In A CN! Symp. on Principles of Database\nSysterns, 1992.\n[520] S. Mehrotra, R. Rastogi, H. Korth, and A. Silberschatz.\nThe concurrency control\nproblem in multidatabases:\nCharacteristics and solutions.\nIn PTOC. ACA1 SIGlvIOD\nCon]. on the lvIanagement of Data, 1992.\n[521] M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: A fast scalable classifier for data nlining.\nIn Proc. IntZ. Conf. on Extending Databa8e Technology, 1996.\n[522]\nNI. lvlehta, V. Soloviev, and D. DeWitt. Batch scheduling in parallel database systerns.\nIn Proc. IEEE Int!. Can]. on Data Engineer\"ing, 1993.\n[523] J. rvlelton.\nAdvanced SQL:1999, Under8tanding Under-8tanding Object-Relational and\nOther- Advanced Features. :Nlorgan Kaufmann, 2002.\n[524]\n.1. l\\1elton and A. Sirnon.\nUnderstanding the New SQL: A Cornplete Guide.\nlVIorgan\nKaufmann, 1993.\n[525]\n.1. NIelton and A. Sirnon. SQL:1.999, Under-standing Relational Language Components.\nNIorgan Kauflnann, 2002.\n[526] D. rvIenasce and R. rvIuntz. Locking and deadlock detection in distributed data bases.\nIEEE Transact'ions on Software Bngineering, 5(3):195~222, 1979.\n[527] A. IVlendelzon and T. IvIilo. Forrnal rnodels of web queries. In A CIlI Syrnp. on Pr'inciples\nof Database Systems, 1997.\n[528] A. O. lVlendelzon, G. A. Nlihaila, and T. l\\lilo. Querying the World \\Vide YVeb. .l(mrnal\non Digital Dibrar'ies,\n1:54~-67, 1997.\n[529]R. 1\\1eo, G. Psaila, and S. Ceri. A new SQL -like openltor for rnining association rules.\nIn Proc. Int!. Conf. on Very) Large Databases, 1996.\n[5:30] T. l\\'Ierrett. The extended relational algebra, a basis for query languages. In Databases.\ned. Shneidennan, Acadmnic Press, 1978.\n[5:31] T. IVlerrett. Relational Inforrnation System\",. Reston Pul)lishing Cornpa,ny, 198:3.\n[532] D. 1\\'1ichie, D. Spiegelhalter, and C. Taylor, editors.\nJ\\.1ach:ine LearniTtg,. Ne'uTal and\nStatistical Classification. Ellis Horwood, London, 1994.\n[5:3:3] J\\:Iicrosoft. lviicr080ft ODBC 8.0 80ftuwre DeveloprnentKit and Progrnrrt1fWT'S Reference.\n.Nlicrosoft Press, 1997.\n\nREFERENCES\n1031\n(534) K. wIikkilineni and S. Suo An evaluation of relationaJ join algorithIllS in a pipelined query\nprocessing envirOlunent. .IEEE Transactions on Software Eng'ineering, 14(6):S:38····848,\n1988.\n(5:35J R. Nliller, Y. Ioannidis, and R. Ram,akrishnan.\nThe nse of infonnation capacity in\nscheuHi integration and translation. InProc. .Inti. Conf. on Very Lm:qe Databases,\n199~~.\n[538]\n[543]\n[539]\n[544]\nT. whlo and D. Suein. Index structures for path expressions. In .IeDT: 7th International\nC\"1onference on Database\nTheO'l~IJ, 1999.\nJ. l\\!Iinker (cd.). Foundat'ions of Deductive Databases and Logic Pr'Ogrnn~rning. l\\!Iorgan\nKauflllann, 1988.\nT. l\\!1inoura and G. Wiederhold. Resilient extended true-copy token schelne for a dis-\ntributed datab&se. IEEE TInnsactions in Software Engineer\"ing,\n8(3):173~189, 1982.\nG. Mitchell, U. Dayal, and S. Zdonik.\nControl of an extensible query optimizer:\nA\nplanning-based approach. In Proc. Inti. Con]' on\nVe7~1J Large Databases, 1993.\nA. Moffat and .J. Zobel. Self-indexing inverted files for fast text retrieval. ACA1 Trans-\nactions on InforiTwtion Systerns, 14(4):349'-\"'379, 1996.\nC. Mohan. ARIES/NT: A recovery Inethod based on write-ahead logging for nested. In\nPTOC. Inti. Conf. on Very Lar'!1e Databases, 1989.\nC. 1I1ohan. Commit LSN: A novel and simple 11lethod for reducing locking and latching\nin transaction processing systems. In PTOC. Inti. Conf. on Very Large Databases, 1990.\nC. }\\tfohan.\nARIES/LHS: A concurrency control and recovery rnethod using write-\nahead logging for linear hashing with separators. In Proc. IEEE Intl. Conf. on Data\nEngineer'ing, 1993.\nC. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz. ARIES: a transaction\nrecovery rnethod supporting fine-granularity locking and partial rollbacks using write-\nahead logging. ACiVf Transactions on Database Systerns, 17(1):94·..-162, 1992.\nC. Mohan and F. Levine.\nARIES/IN! An efficient and high concurrency index rnan-\nagerrlent rrlCthod using write-ahead logging.\nIn Proc. A CAf SIGlV/OD ConI on the\nlvfanage'ment of Data, 1992.\n[546] C. lVIohan and B. Lindsay. Efficient cOlnrnit protocols for the tree of processes rnodel of\ndistributed transactions. In ACA1 SIGACT-SIGOPS Syrnp. on Principles of Distributed\nCornputing, 1983.\n[545]\n[541]\n[542]\n[540]\n[547] C. l\\10han, B. Lindsay, and R. Obernlarck.\nTransaction rnanagernent in the R* dis··\ntributed database rnanagClnent systern.\nA CAf TraruJaction8 on Database S'ystcrns,\n11(4) :378·-<396, 1986.\n[548J C. Nlohan and 1. Narang. Algoritlulls for creating indexes for very large tables without\nquiescing updates. In Proc. ACAI 8IGAI0D Conj. on theAlanagement of Data, 1992.\n[549] K. l\\v10rris, J. Naughton, 't'. Saraiya, J. Ullrnau, and A. Van Gelder.\nYAWN! (Yet\nAnother \\Vindow on NAIL! ). Databa.se Bngirwering, 6:211··226, 1987.\n[5.50] A. NIotro.Superviews: Virtual integration of rnultiple databases. IEEE Ihl'nsactions\non Software En.gineering, 13(7):785···798, 1987.\n[551] A. 1V1otro and P. Bunenla,n. Constructing superviews. Tn p.roc. AClVl S.lGJtvfOD Conf.\non the Jvlanagement of Data, 1981.\n[552] R. rVlukkanmla..\nl\\!lea.suring the effect of data distribution and replication Inodels on\nperforrnance evaluation of distributed database systerns. In Proc. IEEE Inti. ConI on\nData Engineering, 1989.\n\n1032\nDATABASE l\\1ANAGEIvIENT SYSTEMS\n[553J 1.lvlunlick, S. Finkelstein, H. Pirahesh, and R. Rarnakrishnan.\nNlagic is relevant. In\nProc. ACNJ SICA/OD Conf. on the !vfanagcrnent of Data, 1990.\n(554] 1. N1ulllick, S. Finkelstein, H. Pirahesh, and R. Rmnakrishnan. l\\rlagic conditions. A C1\\1\nTransactions on Database Systems, 21 ('1):107-155, 1996.\n[555J 1. ~1umick, H. Pirahesh, and R. Ranlakrishnan. Duplicates and aggregates in deductive\ndatabases. III Proc. Intl. Conf. on Very Lar:qe Databases, 1990.\n[556] 1. rvhllnick and K. Ross.\nNoodle:\nA language for declarative querying in an object-\noriented database. In Intl. Conf. on Deductive and Object-Oriented Databases, 1993.\n[557]\n1\\1.\n~1uralikrishna.\nImproved unnesting algorithrns for join aggregate SQL queries. In\nProc. Intl. Conf. on Very Lar:qe Databases, 1992.\n[558] Iv!. Muralikrishna and D. DeWitt. Equi-depth histograms for estirnating selectivity fac-\ntors for multi-dimensional queries. In Proc. AC1'vf SICMOD Conj. on the Management\nof Data, 1988.\n[559] S. Naqvi. Negation as failure for first-order queries. In AClVI Symp. on Principles of\nDatabase Systems, 1986.\n[560] M. Negri, G. Pelagatti, and L. Sbattella.\n:Formal semantics of SQL queries.\nA CM\nTransactions on Database Systems, 16(3), 1991.\n[561] S. Nestorov, J. Ullman, J. Weiner, and S. Chawathe.\nRepresentative objects:\nCon-\ncise representations of sernistructured, hierarchical data. In Proc. Intl. Conj. on Data\nEngineering. IEEE Computer Society, 1997.\n[562] R. T. Ng and J. Han. Efficient and effective clustering methods for spatial data mining.\nIn Proc. Intl. Conj. on Very Large Databases, Santiago, Chile, September 1994.\n[563] R. T. Ng, L. V. S. Lakshmanan, J. Han, and A. Pang. Exploratory lllining and pruning\noptirnizations of constrained association rules. In Pr'Oc. A eM SIGMOD Intl. Conf. on\n1'vlanagernent of Data, pages 13-24. ACl\\JI Press, 1998.\n[564] T. Nguyen and V. Srinivasan.\nAccessing relational databases from the World Wide\nWeb. In Proc. ACM SIClvfOD Conj. on 'the Managemen't of Data, 1996.\n[565]\n.1. Nievergelt, H. Hinterberger, and K. Sevcik. The Grid File: An adaptable symnletric\nrnultikey file structure. A()M Transactions on Database Systerns, 9(1):38-,71, 1984.\n[566] C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. LonIet.\nAlphasort:\na cache-\nsensitive parallel external sort. VLDB Jounwl, 4(4) :603·\nH·627, 1995.\n[567] R. Obermarck. Global deadlock detection algorithrn. A CAf Trnnsactions on Da'tabase\nSystems, 7(2):187---208, 1981.\n[568] L. O'Callaghall, N. 1Hshra, A.\n~'leyers()n, S. Guha, and R. IVIotwani. Strearning-data\nalgorithlns for high-quality clustering. In Proc. of the Intl. ConfeTencc on Data Engi-·\nneer'ing. IEEE, 2002.\n[569] F. Olken and D. Rotem. SiInple random sal1lpling fronl relational databa'3€s. In Proc.\nIntl. Conj. on Very Large Databases, la80.\n[570J F. Olken and D. Rotern.\n1\\1aintenance of rnaterialized views of sarnpling queries.\nIn\nProc. IEEE Intl. Conj. on Data Eng'iru;cx'ing, 1992.\n[571] C. Olston, B. T. Loa, and .J. Wiclorn. Adaptive precision setting for cached approxiInatc\nvalues. In Proc. ACAI SIGlliOD Conj. on the A1anagernent of Data, 2001.\n[572] C. Olston and J. vVidOII1.\nOffering a precision-perfonnance tradeoff for aggregation\nqueries over replicated data.\nIn\nPTOC. of the Conj. on Vcr?! Lar:qc Databases, pages\n144155, 2000.\n\nREFEREN()ES\n1033\n[57:3] C. Olston and .T. Widonl. Best-effort cache synchronization with source cooperation. In\nProc. ACM SIGlv[OD Conf. on the ,lvfanagernent of Data, 2002.\n[574] P. O'Neil and E. O'Neil. Database PrinC'iples.. Prograrnrning, and Perfonnance. Addison\n\\Vesley, 2 edition, 2000.\n[575] P. O'Neil and D. Quass. IInproved query perfonnance with variant indexes. In Proc.\nACAI SIG1l:fOD Conf. on the Alanagemertt of Data, 1997.\n[576] B. Ozden, R. Rastogi, and A. Silberschatz. 1rfultilnedia support for databases. In A CAf\nSym,p. on Principles of Database Systems, 1997.\n[577] G. Ozsoyoglu, K. Du, S. GuruswanIy, and \\V.-C. Hou.\nProcessing real-tirne, non-\naggregate queries with time-constraints in ca.'le-db. In Proc. IEEE Inti. Conf. o'n Data\nEngineering, 1992.\n[578] G. Ozsoyoglu, Z. Ozsoyoglu, and V. Matos. Extending relational algebra and relational\ncalculus with set-valued attributes and aggregate functions.\nACAl Transactions on\nDatabase Systems, 12(4):566-··592, 1987.\n[579] Z. Ozsoyoglu and L.-Y. Yuan. A new normal form for nested relations. ACM Transac-\ntions on Database Systerns, 12(1):111--136, 1987.\n[580] M. Ozsu and P. Valduriez. Principles of Distributed Database Systems. PrenticeHall,\n1991.\n[581] C. Papadimitriou. The serializability of concurrent database updates.\nJournal of the\nACM, 26(4):631--653, 1979.\n[582] C. Papadimitriou.\nThe Theory of Database Concurrency Control.\nComputer Science\nPress, 1986.\n[583] Y. Papakonstantinou, S. Abiteboul, and H. Garcia-Molina. Object fusion in mediator\nsystems. In Proc. Intl. Conf. on Very Large Da'ta Bases, 1996.\n[584] Y. Papakonstantinou, H. Garcia-Molina, and .1. Widom. Object exchange across het-\nerogeneous information sources. In Proc. Inti. Conf. on Data Engineering, 1995.\n[585]\n.1. Park and A. Segev. Using comrnon subexpressions to optimize lllultiple queries. In\nProc. IEEE Inti. Conf. on Data Engineering, 1988.\n[586] J. Patel, .1.-B. Yu, K. Tufte, B. Nag, J. Burger, N. Hall, K. Rarnasarny, R. Lueder,\nC. Elllllan, J. Kupsch, S. Guo, D. DeWitt, and .T. Naughton. Building a scaleable geo-\nspatial DBl\\IIS: Technology, implernentation, and evaluation. In Proc. ACM SIGA10D\nConf. on the lvlanagernent of Data, 1997.\n[587] D. Patterson, G. Gibson, and R. Katz. RAID: redundant arrays of inexpensive disks.\nIn Proc. ACJi.1 SIGA10D Conj. on the lv!anagernent of Data, 1988.\n[588] H.-B. Paul, H.-J. Schek, 1VI. Scholl, G. vVeikurn, and U. Deppisch.\nArchitecture and\nirnplernentation of the Dar.mstadt database kernel system.\nIn Proc. ACJi.1 SIGlv!OD\nConj. on the Nfanagement of Data, 1987.\n[.589] J. Peckhalli and F. IvIaryanski.\nSernantic data lliodels.\nA CA1 Cornpnting S'urveys,\n20(3):153--1'89, 1988.\n[590]\n.T. Pei and J. Han.\nCan we push Inore constraints into frequent pattern ruining?\nIn\nA CAl SICKDD Conference, pages ;350....·354, 2000.\n[591]\n.T. Pei, J. Han, and L. V. S. Lakshrnanan. 1Vfining frequent itenl sets with convertible\nconstraints.\nIn PT()c. Intl. Conf. on Data Engin,eer\"ing (ICDE), pages 433,442. IEEE\nCOluputer Society, 2001.\n\n1034\nDATABASEJVIA,NAGElvIENT SYSTEMS\n[592] E. Petajan, Y. Jean, D. Lieuwen, and V. Anuparn.\nData-Space: An autoillated visu-\nalization systenl for large\ndatab~:k,;es. In Proc. of SPIE, ViS1WI Data EX1)lo7'ation and\nAnalysis, 1997.\n[593] S. Petrov. Finite axiOInatization of languages for representation of systeIll properties.\nI f '\nQ'\n4- 3''3'9 ''172\n'198()\n11,) OT'matt.on I..Jcunces,\n(:\n-d\n\"\n.' .\n[594] G. Piatetsky-Shapiro and C. Cornell.\nAccurate estinmtion of the nurl1ber of tuples\nsatisfying a condition.\nIn Proc. ACiV! SIGlv!OD Conf. on the !v!anagernent of Data,\n1984.\n[595] G. Piatetsky-Shapiro and W . .1. Frawley, editors.\nKnowledge Di,scovery iTt Databases.\nAAAI/~nT Press, J\\'Ienlo Park, CA, 1991.\n[596] H. Pirahesh and J. Hellerstein.\nExtensible/rule-based query rewrite optimization in\nstarburst. In Proc. A CAl SIGA10D Conf. on the Managernent of Data, 1992.\n[597] N. Pitts-J\\laultis and C. Kirk. XA1L black book: Indispensable problem solver. Corialis\nGroup, 1998.\n[598] V. Poosala, Y. Ioannidis, P. Haas, and E. Shekita. Improved histogranls for selectivity\nestirnation of range predicates. In Proc. ACM SIGAt/OD Conf. on the Management: of\nData, 1996.\n[599] C. Pu. Superdatabases for composition of heterogeneous databases. In Proc. IEEE Intl.\nConf. on Data EngineeTing, 1988.\n[600] C. Pu and A. Leff. Replica control in distributed systems: An asynchronous approach.\nIn Proc. ACl\\l SIGlv10D Conf. on the Managernent of Data, 1991.\n[601] X.-L. Qian and G. Wiederhold. Incrmnental recOInputation of active relational expres-\nsions. IEEE Transactions on Knowledge and Data Engineering, 3(3):337-341, 1990.\n[602] D. Quass, A. Rajaraman, Y. Sagiv, and J. Ullman. Querying selnistructured heteroge-\nneous inforrnation. In Proc. Intl. Conf. on Ded'l.tctive and Object- Oriented Databases,\n1995.\n[603] J. R. Quinlan. C4.5: ProgTarns faT Nfachine Learning. Morgan Kaufrnan, 1993.\n[604] H. G. rv1. R. Alonso, D. Barbara. Data caching issues in an inforrnation retrieval systerll.\nA CN! Transactions on Database Systerns, 15(3), 1990.\n[605] The RAIDBook:\nA source book for RAID technology.\nThe RAID Advisory Board,\nhttp://www.raid-advisory.com.NorthGrafton.IvlA. Dec. 1998. Sixth Edition.\n[606] D. Rafiei cHId A. J\\Jendelzon.\nSirnilarity--based queries for tirn.e series data.\nIn PT'(Jc.\nAClkf SIGl\\10D Conj. on the lvlanagernent of Data, 1997.\n[607] M. Rarnakrishna. An exact probability rnodel for finite hash tables. In PTOC. IEEE Intl.\nConf. on Data Engineer\"ing, 1988.\n[608] wI. Ramakrishna and P.-A. Larson. li'ile organization using cOlnposite perfect hashing.\nACAI Transadio'ns on Database Systems, 14(2):2;:31,,26;3, 1989.\n[609] 1. Ramakrishna.n, P. Rao, K. Sagonas, T. Swift, and D. Warren.\nEfficient tabling\nnrechanisrns for logic progranlS. In Inti. Conf. on Log'ic Prograrnrning, 1995.\n[610] R. Ramakrishnan, 1). I)onjerkovic, A,. RaJlganathan, K. Beyer, and .NI. Krishnaprasad.\nSRQL: Sorted relationa.l query language In PTOC. IEEE InU. Conj. on Scientific and\nStatistical DBA:18, 1998.\n[GIl] R. Rarnakrishnan, D. Srivastava, and S. Slldarshan.Efficient bottom-up evaluation of\nlogic programs. In The State of the Art in Computer SY8lerns and SoftWtLTC EnginJ'J~rving.\ned. J. Vandewalle, Klllwer Acadernic, 1992.\n\nlO~35\n[612] R.lliunakrishnan, D. Srivastava, S. Sudarshan~ and P. Seshadri. The CORAL: deductive\ns:ystenl. VLDB\nJournlll~ ~3(2):161·-210~ 1994.\n[613J R. Ralnakrishnan~ S. Stolfo, R. J. Bayardo., and 1. Parsa, editors. Proc. ACi\\;f 8IGI{DD\nInil. Conference on J{nowledgc Discovery and Data IV[ining. AAAI Press, 2000.\n[614] R. Rarnakrishnan and J. Ullrnan.A survey of deductive database systcrIls. .IO'1lT'rwl of\nLogic Prvgrarrnning, 23(2):125149, 1995.\n[615) K. RanHtITIohanarao. Design overview of the Aditi deductive database system. In Proc.\nIEEE IntI. Conf. on Data Engineering, 1991.\n[616] K. Rauuunohanarao, J. Shepherd, and R. Sacks-Davis. Partial-match retrieval for dy-\nnamic files using linear hashing with partial expansions. In Intl. Conf. on Foundat'ions\nof Data Organization and Algorithrns, 1989.\n[617] V. Raman, B. Raman, and J.\n:~v1. Hellerstein. Online dynamic reordering for interactive\ndata processing. In Proc. of the Conf. on Very Large Databases, pages 709--720. I:v10rgan\nKaufrnann, 1999.\n(618] S. Rao, A. Badia, and D. Van Gucht. Providing better support for a class of decision\nsupport queries. In Proc. ACAI SIG1\\10D Con]. on the 1\\1anagement of Data, 1996.\n[619) R. Rastogi and K. Shim. Public: A decision tree classifier that integrates building and\npruning. In Proc. Intl. Con]' on VeTy Large Database8, 1998.\n[620] D. Reed. Implen1enting atomic actions on decentralized data.\nACM TranBaciions on\nDatabase Systems, 1(1) :3~23, 1983.\n[621] G. Reese. Database Programming With .IDBC and Java. O'Reilly & Associates, 1997.\n[622] R. Reiter. A sound and sometiInes con1plete query evaluation algorithrll for relational\ndatabases with null values. Jo'uTnal of the ACM, 33(2):349<~70, 1986.\n[623] E. Rescorla. SSL and TLS: Designing and Building Secure Systern8. Addison Wesley\nProfessional, 2000.\n[624] A. Reuter. A fast transaction-oriented logging scherne for undo recovery. IEEE Trans-\nactions on Software Engineering, 6(4) :348--356, 1980.\n[625] A. Reuter. Performance analysis of recovery techniques. AClv[ Trnnsact'ions on Database\nSy8terns, 9(4) :526·····559, 1984.\n[626] E. Riloff and L. Hollaar.\nrI'ext databa--c;;es and infonnation retrieval.\nIn Handbook of\nCornputer Science. ed. A.B. T'ucker, eRe 1)ress, 1996.\n[627] J. Rissanen.\nIndependent cOlnponents of relations.\nA CM Tran8action8 on Database\nSyste'm8, 2(4)::,317325, 1977.\n[628} R. Rivest. Partial Illatch retrieval algoritlulls. SIAlvl Journal on Cornputing, 5(1):19---50,\n1976.\n[629] R. L. Rivest, A. Sharnir, and L. M. Adlernan. A rnethod for obtaining digital signatures\nand public-key cryptosysterns. Cornrnunications of the ACA1, 21(2):12(}··126, 1978.\n[6:~OJ J. T. H..obinson. The KDB tree: A search structure for large rIlultidinlensional dynamic\nindexes. In Proc. ACiVI SIGAfOD Int. Conf. on M'anagement of Data, 198.1.\n[G3l] .J. H,ohrner, F. Lescocllr, a,nd .J. Kerisit.\nThe Alexander rnethod, a technique for the\nprocessing of recursive queries. New Generation Corrq)'lding,\n4(~3):27~3~-285, 1986.\n[G:32] D. Rosenkrantz, R. Stearns, and P. Lewis.\nSystern level concurrency control for dis-\ntributed datatlClse systerns. AC\"\"1.iVf 1\"1nnsactio1Ls on Database SY8tern,s1 ;:3(2), 1978.\n[G:>3~:3] A. Rosenthal a.nd U. Chakravarthy. Anatorny of a rnodular rnultiple query optiInizer.\nIn PTOC. Inil. Conf. o'n \"Very Lar:qe DalabCL8cs, 1988.\n\n1036\nDATABASE IVlANAGEwiENT SYSTE~IS\n[634J K. Ross and D. Srivastava. Fast coruputation of sparse datacubes. In Proc. Intl. Conf.\non Very Large Databases, HJ97.\n[635J K. Ross, D. Srivastava, and S. Sudarshan. Nfaterialized view nlaintenance and integrity\nconstraint checking: Trading space for tinIe.\nIn PnJC. ACA1 SIG!vl0D Conf. on the\nfl,Janagernent of Data, 1996.\n[636] J. Rothl1ie, P. Bernstein, S. Fox, N. Goodnlan, M. HamITIer, T. Landers, C. Reeve,\nD. Shipman, and E. Wong. Introduction to a systeln for distributed databa.'3es (SDD\n-1). AC!vl Transactions on Database Systems, 5(1), 1980.\n[637] J. Rothnie and N. Goodman.\nAn overview of the prelirninary design of SDD -1: A\nsystern for distributed data ba')es.\nIn Proc. Berkeley Workshop on Dist\",\"ibuted Data\nAlanagement and Computer NetwoTks, 1977.\n[638] N. Roussopoulos, Y. Kotidis, and M. Roussopoulos.\nCubetree: Organization of and\nbulk updates on the data cube. In PTOC. ACM SIGNIOD Conf. on the Management of\nData, 1997.\n[639] S. Rozen and D. Shasha. Using feature set compromise to automate physical database\ndesign. In Proc. Intl. Conf. on VeTY LaTge Databases, 1991.\n[640] J. Rumbaugh, I. Jacobson, and G. Booch.\nThe Unified Modeling Language Reference\nManual (Addison- Wesley Object Technology Series). Addison-Wesley, 1998.\n[641] M. Rusinkiewicz, A. Sheth, and G. Karabatis. Specifying interdatabase dependencies\nin a multidatabase environment. IEEE ComputeT, 24(12), 1991.\n[642] D. Sacca and C. Zaniolo. Magic counting methods. In PTOC. ACM SIGMOD Conf. on\nthe Management of Data, 1987.\n[643] Y. Sagiv and M. Yannakakis. Equivalence among expressions with the union and dif-\nference operators. Journal of the A eNI, 27(4) :633~655, 1980.\n[644] K. Sagonas, T. Swift, and D. Warrell. XSB as an efficient deductive database engine.\nIn PTOC. ACM SIGMOD Conf. on the Management of Data, 1994.\n[645] A. Sahuguet, L. Dupont, and T. Nguyen. Kweelt: Querying XIvIL in the new millenium.\nhttp://kweelt.sourceforge .net, Sept 2000.\n[646] G. Salton and M. J. McGill. IntToduetion to l\\!Iodern Information Retrieval. McGraw-\nHill, 1983.\n[647] B. Salzberg, A. Tsukerman, J. Gray, M. Stewart, S. Uren, and B. Vaughan. Fastsort:\nA distributed single-input single-output external sort. In PTOC. ACM SIGMOD Conf.\non the Management of Data, 1990.\n[648] B. J. Salzberg. Pile StructuTes. PrenticeHall, 1988.\n[649] H. Salnet. The Quad Tree and related hierarchical data structures. A CAf Computing\nSUT1Jeys, 16(2), 1984.\n[650] H. Sarnet. The Design and Analysis of Spatial Data Structures. Addison-Wesley, 1990.\n[651] J. Sander, 1\\1. Ester, H.-P. Kriegel, and X. Xu.\nDensity-based clustering in spatial\ndataba')es. 1. of Data llJining and Knowledge DiscoveTY, 2(2), 1998.\n[652] R. E. Sanders. ODBC 3.5 Developer's Guide. McGraw-Hill Series on Data Warehousing\nand Data 1tIanagelnent. McGraw-Hill, 1998.\n[653] S. Sarawagi and lVL Stonebraker. Efficient organization of large multidirnensional arrays.\nIn Proc. IBEE Inil. Conf. on Data Engineering, 1994.\n[654] S. Sarawagi, S. T'holnas, and R. Agrawal. Integrating mining with relational database\nsystems: Alternatives and inlplications. In Pr-oc. ACM SIGMOD Intl. Conf. on NIan-\nagernent of Data, 1998.\n\nREFERE1VCES\n1037\n•\nfu\n[655] A. Sava..,ere, E. Omiecinski, and S. Navathe. An efficient algorithm for ruining associa-\ntion rules in large databases. In Proc. Intl. Conf. on Very Large Databases, 1995.\n[656] P. Schauble. Spider: A multiuser illfornlatioll retrieval systetll for semistructured and\ndynanlic data.\nIn P1'OC. AClVf 8IGIR Conference on Research and Developrnent in\nInfor7nat.zon Retrieval, pages 318- 327, 1993.\n[657] H.-J. Schek, H.-B. Paul, M. Scholl, and G. Weikulll. The DASDBS project: Objects, ex-\nperiences, and future projects. IEEE Transactions on Knowledge and Data Engineering,\n2(1), 1990.\n[658]\n1\\11. Schkolnick. Physical database design techniques. In NYU Symp. on Database Design,\n1978.\n[659] M. Schkolnick and P. Sorenson. The effects of denormalization on database performance.\nTechnical report, IB1.-1 RJ3082, San Jose, CA, 1981.\n[660] G. Schlageter.\nOptimistic methods for concurrency control in distributed database\nsystems. In Proc. Intl. Conf. on Very Large Databases, 1981.\n[661] B. Schneier. Applied Cryptography: Protocols, Algorithms, and Source Code in C. John\nWiley & Sons, 1995.\n[662] E. Sciore. A complete axiomatization of full join dependencies.\nJournal of the ACM,\n29(2):373--393, 1982.\n[663] E. Sciore, M. Siegel, and A. Rosenthal.\nUsing semantic values to facilitate interop-\nerability among heterogeneous information systems.\nA CM Transactions on Database\nSystems, 19(2):254-290, 1994.\n[664] A. Segev and J. Park. Nlaintaining materialized views in distributed databases. In Pr'oc.\nIEEE Intl. Conf. on Data Engineering, 1989.\n[665] A. Segev and A. Shoshani. Logical rnodeling of ternporal data. Proc. ACM SIGMOD\nConf. on the Managerr~ent of Data, 1987.\n[666] P. Selfridge, D. Srivastava, and L. Wilson.\nIDEA: Interactive data exploration and\nanalysis. In Proc. ACM SIGMOD Conf. on the Management of Data, 1996.\n[667] P. Selinger and M. Adiba. Access path selections in distributed data base management\nsystems. In Proc. Intl. Conf. on Databa8es., Brit'ish Computer Society, 1980.\n[668] P. Selinger, M. Astrahan, D. Charnberlin, R. Lorie, and T. Price. Access path selection\nin a relational database management system.\nIn P1'OC. ACM SIGJt10D Conf. on the\nlvfanagement of Data, 1979.\n[669] T. K. Sellis.\nMultiple query optimization.\nAC!VI Transactions on Database Systerns,\n13(1.):23\"..-52, 1988.\n[670] P. Seshadri, J. Hellerstein, H. Pirahesh, T. Leung, R. Rarnakrishnan, D. Srivastava,\nP. Stuckey, and S. Sudarshan. Cost-based optirnization for\n~1agic: Algebra and itnple-\nmentation. In Proc. ACiVi SIGJt10D Conf. on the\nManage'TT~ent of Data, 1996.\n[671] P. Seshadri, 1\\1. Livny, and R. Ratnakrishnan.\nThe design and ilnpletuentation of a\nsequence database systern. In Proc. Intl. Conf. on Very Large Databases, 1996.\n[672J P. Seshadri, IV!. Livny, and R. Ramakrishnan.\nThe ca.se for enhanced abstract data\ntypes. In Proc. Intl. Conf. on VeT1j Lar:qe Databases, 1997.\n[67:3] P. Seshadri, H. Pirahesh, and T. Leung. Cornplex query decorrelation. In Proc. IBBE\nInti. Conf. on Data Engineer~ing, 1996.\n[674] .J. Shafer and R. Agrawal. S,PRINT: a scalable parallel classifier for data tllining. In\nProc. Intl. Conf. on Ve11/ Large Databases, 1996.\n\n1038\nDATABASE~!1ANAGEMENT SYSTEtv'IS\n[675J J. Shanumgac;undaraln, U. Fayyad, and P. Bradley. COlIlpressed data cubes for olap ag-\ngTegate query approxiInation OIl continuous dirnensions. In Pr'oc. Inti. Conf. on }(nowl-\nedge Di,sC01Jcry and Data Jtyhn'ing (I{DD), 1999.\n[676] J. Shaulllugasundaram, J. Kiernan, E. ,T. Shekita, C.Fan, and J.Funderburk. Querying\nXlVII\" views of relational data. In Pmc. Intl. Conf. onVety Large Data Bases, 200l.\n[677]\n°L. Shapiro.\n.Join processing in databa..se systenls with large rnain memories.\nACN!\nTransaction.s on Databa8e 8;ljsterns,\n11(~3):239~264, 1986.\n[678] D. Shasha and N. Goodrnan. Concurrent search structure algorithnls. ACA1 Tr'ansac-\ntions on Database Systerns, 13:53.0..90, 1988.\n[679] D. Shasha, E. Siruoll, and P. Valduriez.\nSinlple rational guidance for chopping up\ntransactions. In Proc. ACNJ SIGlvfOD Conf. on the lvlanagement; of Data, 1992.\n(680] H. Shatkay and S. Zdonik.\nApproxinlate queries and representations for large data\nsequences. In Proc. IEEE Intl. Conf. on Data Engineering, 1996.\n[681] T. Sheard and D. Sternple. Autonlatic verification of database transaction safety. AC\"J\\,1\nTrawwctions on Database Systerns, 1989.\n[682] S. Shenoy and Z. Ozsoyoglu. Design and irnplelnentation of a seruantic query optiInizer.\nIEEE Transactions on Knowledge and Data Engineering, 1(3):344'00-361, 1989.\n[683) P. Shenoy, J. Haritsa, S. Sudarshan, G. Bhalotia, M. Bawa, and D. Shah.\nTurbo-\ncharging vertical mining of large databases.\nIn Proc. AClvI SIGMOD Int!. Conf. on\nlvIanagernent of Data, pages\n22~···33, May 2000.\n[684] A. Sheth and J. Larson. Federated database systerns for nlanaging distributed, hetero-\ngeneous, and autonomous databases. Computing Surveys, 22(3):183-'-236, 1990.\n(685] A. Sheth, J. Larson, A. Cornelio, and S. Navathe.\nA tool for integrating conceptual\nschemas and user views. In Proc. IEEE Intl. Conf. on Data Engineering, 1988.\n[686] A. Shoshani.\nOLAP and statistical databases: Sirnilarities and differences.\nIn ACM\nSyrnp. on Principles of Database Systems, 1997.\n[687) A. Shukla, P. Deshpande, J. Naughton, and K. Rarnasalny.\nStorage estirnation for\nmultidirnensional aggregates in the presence of hierarchies. In Prnc. Intl. Conj. on VeTY\nLarge Databases, 1996.\n[688]\n1\\11. Siegel, E. Sciore, and S. Salveter. A method for autornatic rule derivation to support\nsemantic query optirnization. A CNI Transact;ions on Database 8ystems, 17(4), 1992.\n[689] A. Silberschatz, H. Korth, and S. Sudarshan.\nDatabase System Concepts (4th ed.).\nrVIcGraw-Hill, 4 edition, 2001.\n[690] E. Simoll, J. Kiernan, and C. de 1\\1aindreville. hnplernenting high-level active rules on\ntop of relational databases. In Proc. Intl. Conj. on Very Large Databases, 1992.\n[691] E. Silnoudis, .J. VVei, and U. 1\\;1. Fayya.d, editors.Proc. Intl. Conf. on I{nowledge Dis-\ncovery and Data Alining. AAAI Press, 199fL\n(692] D. Skeen. Nonblocking COlnnlit protocols. In Proc. ACAI SIGA;fO[) Conf. on the AIano-\nagcTnent of Data, 1981.\n[GD~~] J. Srnith and D. Srnith.Database abstractions: Aggregation and generalization. ACAI\nTransact'ions\n0'/1, Database SystcTns,\n1(1):1051~3~~, 1977.\n[694] K. Slnith and ]\\11. VVinslett. Entity modeling in the l\\lLS relational model. In Proc. Inti.\nConf. on VC'7/ Large Databases, 1992.\n[G95] P. Srnith and IvT. Barnes. Piles and Databases: An IntToduction. Addison-\\\\fesley, 1987.\n\nFlEFEREIVCES\n1039\n[696J N. Soparkar, H. Korth, and A. Silberschatz. Databases with deadline and contingency\nconstraints.\nIEEE Tr-allsact'ions on K'nowledge and Data Engineering, 7(4):552\"-565,\n1995.\n[697] S. Spaccapietra, C. Parent, and Y. Dupont. ~1odel independent assertions for integration\nof heterogeneous schemas. In PTOC. Intl. ConI on, Verll Large Databases, 1992.\n[698} S. Spaccapietra (ed.). Bnt'ity-Relationsh'ip Appr'oach: Ten YeaTs of Exper'ience in, I11,for-\nrnation Modeling, Prnc. Entity-Relationship Conf. North-Holland, 1987.\n[699] E. Spertus. ParaSite: Inining structural infonnation on the web. In Intl. World Wide\nWeb Conference, 1997.\n[700] R. Srikant and R. Agrawal. Mining generalized association rules. In Proc. Intl. Conf.\non 1len) Large Databases, 1995.\n[701] R. Srikant and R. Agrawal. Mining Quantitative Association Rules in Large Relational\nTables. In PTOC. ACM SIGAtl0D ConI. on AtJanagement of Data, 1996.\n[702] R. Srikant and R. Agrawal. Mining Sequential Patterns: Generalizations and Perfor-\nmance Improvements. In Pr'Oc. Intl. ConI. on Extending Database Technology, 1996.\n[703] R. Srikant, Q. VU, and R. Agrawal. Mining Association Rules with Item Constraints.\nIn Proc. Intl. Conf. on Knowledge Discovery in Databases and Data Mining, 1997.\n[704] V. Srinivasan and M. Carey. Performance of B-Tree concurrency control algorithms. In\nPTOC. ACM SIGlvIOD Conf. on the Management of Data, 1991.\n[705] D. Srivastava, S. Dar, H. Jagadish, and A. Levy. Answering queries with aggregation\nusing views. In PTOC. Intl. Conf. on Very Large Databases, 1996.\n[706] D. Srivastava, R. Ralnakrishnan, P. Seshadri, and S. Sudarshan.\nCoral++: Adding\nobject-orientation to a logic database language.\nIn PTOC. Intl. Conf. on Very Large\nDatabases, 1993.\n[707] J. Srivastava and D. Rotem. Analytical rnodeling of materialized view maintenance. In\nACM Symp. on Principles of Database Systems, 1988.\n[708] J. Srivastava, J. 'Tan, and V. Lum. Tbsam: An access nlethod for efficient processing of\nstatistical queries. IEEE Transactions on Knowledge and Data Engineering, 1(4):414-·-\n42~~, 1989.\n[709] D. Stacey. Replication: DB2 , Oracle or Sybase?\nDatabase Prngramrning and Design,\npages 42~50, December 1994.\n[710) P. Stachour and B. Thuraisingham.\nDesign of LDV: A multilevel secure relational\ndatabase managenlent system. IEEE 7iransactions on Knowledge and Data Engineering,\n2(2), 1990.\n[71l} .J. Stankovic andW. Zhao. On real-time transactions. In Proc. ACAJ SIGMOD Conf.\non the AIanagernent of Data Record, 1988.\n[712] T. Steel. Interirn report of the ANSI-SPARe study group. In P7'oc. ACA1 8IGMOD\nConf. on the Managernent of Data, 197.5.\n[713] Ivt Stonebraker. ltnplernentation of integrity constraints and views by query lllodifica-\ntion. In Prnc. ACAI SIGAtfOD Conf. on the Managernent of Data, 1975.\n[714] M. Stonebraker.\nConcurrency control and consistency of rnultiple copies of data in\nDistributed Ingres. IEEE Transactions on Software BngineeT'ing, 5(3), 1979.\n[715] IvI. Stonebraker. Operating systern support for database ruanagement. Cornmunications\nof the ACAtf,\n14(7):412~··418, 1981.\n\n1040\nDATABASE l\\!IANAGENiENT SVS'I'g'iJ1S\n[716]\n?vI. Stonebraker. Inclusion of new types in relational database systerns. In Proc. IEEE\nInti. ()onf. on Data Engineering, 1986.\n[717]\n1v1. Stonebraker.\nThe INGRBS Papers:\nAnatorny of a Relational Database Systern.\nAddison-Wesley, 1986.\n[718J\nIV!. Stonebraker. 'The design of the Postgres storage systern.\nIn Proc. Inti. Conf.\nOTt\n\\le'f7.) Large Do,tabases, 1987.\n[719] IvI. Stonebraker. Db,iect-relational DBA1Ss-,-,,-The NeTt Great Wave. J\\tlorgan Kaufrnann,\nH)96.\n[720]\n:LvI. Stonebraker, J.f.):ew, K. Gardels, and .1. l\\'leredith.\nThe Sequoia 2000 storage\nbenchrnark. In Proc. AClv[ SIGi\\,fOD Conf. on the AIanagernent of Data,\n199~3.\n[721 J wI. Stonebraker and .J. Hellerstein (eds). Read'ings in Database System,s. Nlorgan Kauf-\nrnann, 2 edition, 1994.\n[722]\nN1. Stonebraker, A. Jhingran, J. Goh, and S. Potarnianos. On rules, procedures, caching\nand views in data base systerns. In UCBERL Jv19086, 1990.\n[723] M. Stonebraker and G. Kernnitz. The Postgres next-generation database management\nsystem. Comrnunications of the ACM, 34(10):78--92, 1991.\n[724] B. Subramanian, T. Leung, S. Vandenberg, and S. Zdonik. The AQUA approach to\nquerying lists and trees in object-oriented databases. In Proc. IEEE Int!. Conf. on Data\nEngineering, 1995.\n[725] W. Sun, Y. Ling, N. Rishe, and Y. Deng.\nAn instant and accurate size estimation\nmethod for joins and selections in a retrieval-intensive environment.\nIn Proc. AC1\\1\n8IGMOD Conf. on the Managernent of Data, 1993.\n[726] A. Swami and A. Gupta. Optirnization of large join queries: Conlbining heuristics and\ncOlnbinatorial techniques. In Proc. ACiVf 8IGJtv10D Conf. on the Management of Data,\n1989.\n[727] T. Swift and D. Warren. An abstract rnachine for SLG resolution: Definite programs.\nIn Intl. Logic Prograrnming Syrnposium, 1994.\n[728] A. Tansel, J. Clifford, S. CacHa, S. Jajodia, A. Segev, and R. Snodgrass.\nTernporal\nDatabases: TheoT\"'y, Design and Im,plernentation. Benjarnin-Cummings, 1993.\n[729] Y. Tay, N. Coodrnan, and R. Suri. Locking perfornutnce in centralized databases. A CNI\nTransactions on Database Syste'fns, 10(4):415----462, 1985.\n(7~30] T. Teorey.\nDatabase lvlodeling and Design:\nThe E-R Approach.\nlVlorgan Kaufrnann,\n1990.\n[7:31] rr. Teorey, D.-Q. -Yang, and .T. Fry.\nA logical database design rnethodology for rela--\ntional databa..\"cs using the extended entity-relationship rHode!.\nACJlvf Computing Sur-\nveys, 18(2): 197----222, 1986.\n[7:32]\nH\" Thmnas.\nA rnajority consensus approach to concurrency control for nmltiple copy\ndatabases. ACl\\1 'Trnn8acLions on Database S'ysterns, 4(2):180---209, 1979.\n[7:3:3] S. A. Thomas. 88L E1 TL8 Es.sent'ial8: Secnring the\n~Veb. John \\Viley & Sons, 2000.\n[734]\nA. 'ThOInasian. Concurrency control:\n~1ethods, pcrforrnancc, and analysis. ACA1 Com-\nputing Surueys, 30(1):70----119, 1998.\n[7:35] A. T'l10111asian. Two-phase locking performance and its thrashing behavior A()A1 Com-\n]J'l1t:ing Surveys, :30(1):70-119, 1998.\n[7:36] S. 'I'hOIua.'), S. Bodagala, K. Alsabti, and S. Ranka.\nAn efficient algorithrn for the\nincremental upclation of a..ssociation rules in large databases.\nIn Proc. Intl. Conf. on\nj{nowledgc !Ji8covcrl/ and Data A1'ining. AAAIPress, 1997.\n\n1041\n[737] S. Todd.\nThe Peterlee relational test vehicle.\nIBAJ 8ysterns Jour'no.l, 15(4):285··-:307,\nHl76.\n(738) H. Toivonen. Sarnpling large databases for association rules. In Pn)c. Inti. GonIon\nYery Large Databases, 1996.\n[739] TP Perforrnance Council. TPC Benclllnark D: Standard specification, rev. 1.2. Technical\nreport, http://www .tpc. org/dspec .html, 1996.\n[740] 1. Traiger, J. Gray, C. Galtieri, and B. Lindsay. T'ransactions and consistency in dis-\ntributed databa.,se systems. ACAI Transactions on Databa8e Systerns, 25(9), 1982.\n[741]\n1'vi. Tsangaris and J. Naughton. On the performance of object clustering techniques. In\nProc. AClVI SIGl\\10D Conf. on the ]Vlanagement of Data, 1992.\n[742] D.-:N1. Tsou and P. Fischer.\nDecomposition of a relation scheme into Boyce-C odd\nnonnal form. SIGACT News, 14(3):23-29, 1982.\n[743) D. Tsur, J. D. Ullman, S. Abiteboul, C. Clifton, R. Motwani, S. Nestorov, and A. Rosen-\nthal. Query flocks: A generalization of association-rule mining. In Proc. ACM SIGMOD\nConf. on Managernent of Data, pages 1-12,1998.\n[744] A. Tucker (ed.). Computer- Science and Engineering Handbook. CRC Press, 1996.\n[745] J. W. Thkey. Exploratory Data Analysis. Addison-Wesley, 1977.\n[746] J. Ullman. The U.R. strikes back. In ACM Symp. on Principles of Database Systems,\n1982.\n[747] J. Ullman. Principles of Database and Knowledgebase Systems, Vols. 1 and 2. Computer\nScience Press, 1989.\n[748) J. Ullman.\nInfonnation integration using logical views.\nIn Intl. Conf. on Database\nTheory, 1997.\n(749) S. Urban and L. Delcarnbre.\nAn analysis of the structural, dynamic, and temporal\naspects of semantic data models. In Proc. IEEE Intl. Conf. on Data Engineering, 1986.\n[750] G. Valentin, M. Zuliani, D. C. Zilio, G. M. Lohman, and A. Skelley. Db2 advisor: An\noptirnizer smart enough to recomrnend its own indexes. In Proc. Intl. Conf. on Data\nEngineering (ICDE), pages 101-110. IEEE COlnputer Society, 2000.\n[751]\nlVI. Van Emden and R. Kowalski. The semantics of predicate logic as a prograrrnning\nlanguage. Journal of the ACM, 23(4):7:~3~742, 1976.\n[752] A. Van Gelder. Negation as failure using tight derivations for general logic programs. In\nJ. Minker, editor, Fo'undations of Deductive Databases and Logic ProgTarnrn'ing. i\\1organ\nKaufmann, 1988.\n[753) C. J. van Rijsbergen. Infomwtion Retrieval. Butterworths, London, United Kingdorn,\n1990.\n[754] Jvt Vardi. Incomplete information and default reasoning. In ACM Symp. on Principles\nof Database Syste'ms, 1986.\n[755] M. Vardi.\nFundamentals of dependency theory.\nIn Trends in Theoretical CornputeT\nScience. ed. E. Borger, Computer Science Press, 1987.\n[756] L. Vieille. Recursive axioms in deductive databases: The query-subquery approach. In\nIntl. Conf. on Expert Database Systems, 1986.\n(757] L. Vieille. FraIn QSQ towards QoSaQ: global optimization of recursive queries. In Intl.\nCan]. on El:pert Database Systerns, 1988.\n[758] L. Vieille, P. Bayer, V. Kuchenhoff, and A. Lefebvre. EKS-VI , a short overview. In\nAAAI-90 Wor-kshop on Knowledge Base lvlanageutent Systerns, 1990.\n\n1042\nDATABASE MANAGEMENT SYSTEMS\n[759] .J. S. Vitter and Ivi. Wang. Approxirnate conlputation of rnultidinlensional aggregates\nof sparse data using wavelets. In Proc. ACM SICA10D Conf. on the ft..1anagc7nent of\nData, pages 193-204. ACtvI Press, 1999.\n[760] G. von Bultzingsloewen. Translating and optinlizing SQL queries having aggregates. In\nProc. Intl. Conf. on Very Large Databases, 1987.\n[761] G. von Bultzingsloewen, K. Dittrich, C. Iochpe, R.-P. Liedtke, P. Lockemaun, and\nM. Schryro. Kardamom······-A dataflow database machine for real-tiule applications. In\nProc. ACM SICNfOD Conf. on the Management of Data, 1988.\n[762] G. Vossen.\nData A10dels, Database Languages and Database lvlanagement Systems.\nAddison-Wesley, 1991.\n[763] N. Wade.\nCitation analysis:\nA new tool for science administrators.\nScience,\n188(4183) :429-432, 1975.\n[764] R. Wagner. Indexing design considerations. IBM Systems Journal, 12(4):351-367, 1973.\n[765] X. Wang, S. Jajodia, and V. Subrahmanian. Temporal modules: An approach toward\nfederated temporal databases. In Proc. ACM SIGMOD Conf. on the Management of\nData, 1993.\n[766] K. Wang and H. Liu. Schema discovery for semistructured data. In Third International\nConference on Knowledge Discovery and Data Mining (KDD -97), pages 271-274, 1997.\n[767] R. Weber, H. Sehek, and S. Blott. A quantitative analysis and performance study for\nsimilarity-search methods in high-dimensional spaces. In Proc. Inti. Conf. on Very Large\nData Bases, 1998.\n[768] G. Weddell.\nReasoning about functional dependencies generalized for semantic data\nmodels. ACM Transactions on Database Systems, 17(1), 1992.\n[769] W. Weih!. The impact of recovery on concurrency control. In ACM Symp. on Principles\nof Database Systems, 1989.\n[770] G. Weikum and G. Vossen.\nTransactional Information Systems.\nMorgan Kaufrnann,\n2001.\n[771] R. Weiss, B. V. lez, M. A. Sheldon, C. Manprenlpre, P. Szilagyi, A. Duda, and D. K.\nGifford.\nHyPursuit: A hierarchical network search engine that exploits content-link\nhypertext clustering. In Proc. A CM Conf. on Hypertext, 1996.\n[772] C.White. Let the replication battle begin. In Database Progrnmm,ing and Design, pages\n21-24, May 1994.\n[773] S. White, M. Fisher, R. Cattell, G. Hanlilton, and IvL Hapner. JDBC API Tutorial and\nReference: Universal Data Access for the Java 2 Platform. Addison-Wesley, 2 edition,\n1999.\n[774] J. Widorn and S. Ceri. Active Database Systerns. Morgan KaufInann, 1996.\n[775] G. Wiederhold. Database Design (2nd cd.).\n~/1cGraw-Hill, 1983.\n[776] G. Wiederhold, S. Kaplan, and D. Sagalowicz.\nPhysical database design research at\nStanford. IEEE Database Engineering, 1:117--·-119, 1983.\n(777] R. Williams,\nD. Daniels, L.\nH8.<1.S,\nG. Lapis, B. Lindsay, P. Ng, R. Oberrnarck,\nP. Selinger, A. v\\lalker, P. \"\"1'\"ilrns, and R. Yost. R*: An overview of the architecture.\nTechnical report, IBIvI RJ3325, San Jose, CA, 1981.\n[778] 1\\1. S. Winslett. A rnodel-based approach to updating databases with 1ncornplete infor-\nmation. A CA1 Transactions on Database Systerns, 1i3(2):167--196, 1988.\n\nREFERENCES\n1043\n[779J G. vViorkowski and D. KulI. DB2: Design and Developrnent Guide (8rd ed.). Addison-\nWesley, 1992.\n[780] 1. H. Witten, A. rvloffat, and T. C. Bell. Alanaging Gigabytes: Compressing and Indexing\nDocuments and Images. Van Nostrand Reinhold, 1994.\n[781] 1. H. Witten and E. Frank. Data Afining: Pr'actical Machine Learning Tools and Tech-\nniques with Java Im,plementations. Morgan Kaufmann Publishers, 1999.\n[782] O. Wolfson, A. Sistla, , B. Xu, J. Zhou, and S. Chamberlain. Domino: Databases for\nmoving objects tracking. In Prnc. ACM SIGlvIOD Int. Conf. on Afanagement of Data,\n1999.\n[783] Y. Yang and R. lI1iller. Association rules over interval data. In Proc. ACM SIGlv/OD\nConf. on the Management of Data, 1997.\n[784] K. Youssefi and E. Wong. Query processing in a relational database management system.\nIn Proc. Intl. Conf. on Very Larye Databases, 1979.\n[785] C. Yu and C. Chang.\nDistributed query processing.\nACM Computing Surveys,\n16(4):399~433, 1984.\n[786] O. R. Zaiane, M. EI-Hajj, and P. Lu. Fast Parallel Association Rule Mining Without\nCandidacy Generation. In Proc. IEEE Intl. Conf. on Data Mining (ICDM), 200l.\n[787] M. J. Zaki.\nScalable algorithms for association mining.\nIn IEEE Transactions on\nKnowledge and Data Engineering, volume 12, pages 372-390, May/June 2000.\n[788] M. J. Zaki and C.-T. Ho, editors. Large-Scale Parallel Data Mining. Springer Verlag,\n2000.\n[789] C. Zaniolo. Analysis and design of relational schemata. Technical report, Ph.D. Thesis,\nUCLA, TR UCLA-ENG-7669, 1976.\n[790] C. Zaniolo.\nDatabase relations with null values.\nJournal of Computer and System\nSciences,\n28(1):142~166, 1984.\n[791] C. Zaniolo. The database language GEM. In Readings in Object-Oriented Databases.\neds. S.B. Zdonik and D. Maier, Morgan Kaufmann, 1990.\n[792] C. Zaniolo. Active databa~e rules with transaction-conscious stable-model semantics.\nIn Intl. Conj. on Deductive and Object-Oriented Databases, 1996.\n[793] C. Zaniolo, N. Arni, and K. Ong.\nNegation and aggregates in recursive rules:\nthe\nLDL++ approach. In Intl. Conf. on Deductive and Object-Oriented Databases, 1993.\n[794] C. Zaniolo, S. Ceri, C. Faloutsos, R. Snodgrass, V. Subrahmanian, and R. Zicari. Ad-\nvanced Database Systems. Morgan Kaufmann, 1997.\n[795] S. Zdonik, U. Cetintemel, M. Cherniack, C. Convey, S. Lee, G. Seidlnan, M. Stone-\nbraker, N. Tatbul, and D. Carney Monitoring streams--·..··A new class of data manage-\nment applications. In Proc. Intl. Conf. on Very Large Data Bases, 2002.\n[796] S. Zdonik and D. lVfaier (eds.). Readings in Object-Oriented Databases. Morgan Kauf-\nrnann, 1990.\n[797] A. Zhang, 1.1. Nodine, B. Bhargava, and O. Bukhres. Ensuring relaxed atornicity for\n£le\"'Cible transactions in multidatabao:;e systerns. In PTOC. A CAf SIGMOD Conf. on the\nlvlanaqe-rnent of Data, 1994.\n[798] T. Zhang, R. Hamakrishnan, and IV!. Livny. BIRCH: an efficient data clustering rnethod\nfor very large databases. In Proc. AClvf SIGA10D Conf. on Alanagement of Data, 1996.\n[799] Y. Zhao, P. Deshpande, J. F. Naughton, and A. Shukla.\nSilllultaneous optirnization\nand evaluation of multiple dirnensional queries. In Proc. AClvl SIGlvlOD Intl. C'onj. on\nlvIanagernent of Data, 1998.\n\n1044\nD.ATABASE ~{ANAGErvIENT SYSTEItlS\n[800] Y. Zhuge, H. Garcia-1Vlo1ina, J. Hanuner, and .1. Widom. View maintenance in a ware-\nhousing enviroIllnent.\nIn Pl'oc. AGArf SIGAI0D Conf. on the A,lanagernent of Data,\n1995.\n[801J\n111. ,?vL Zloof. Query-by-exaIllple: a database language. IBlv! System,s JO'1rmal, 16(4):324--\n~~43, 1977.\n[802J J. Zobel, ,A. tvloffat, and K. RalnarIlohanarao. Inverted files versus signature files for\ntext indexing. A CA1 Transactions on Database System,s, 23, 1998.\n[80~{] .J. Zobel, A. lVlotfat, and R. Sacks-Davis. An efficient indexing technique for full text\ndatabases. In Froc. Intl. Co'nj. on Very Large Databases., Iv!07:qan Kaufman pubs. (San\n})'Q,ncisco\" CAy is] Vancou'veT, 1992.\n[804] U. Zukowski and B. Freitag. The deductive database system LOLA. In Proc. Intl. Conf.\non Log'ic Programming and N'on-A10notonic Reasoning, 1997.\n\nAUTHOR INDEX\nAbbott, R., 578, 1005, 1001\nAbdali, K., 270, 1015\nAbdellatif, A., 771, 1029\nAbiteboul, S., 24, 98, 648, 816,\n844, 925, 967, 1005, 1030,\n1033, 1041, 1001\nAboulnaga, A., 967, 1005\nAcharya, S., 888, 1005\nAchyutuni, K.J., 578, 1005\nAckaouy, E., xxix\nAdali, S., 771, 1005\nAdiba, M.E., 771, 1005, 1038\nAdleman, L.I\\i1., 722, 1036\nAdya, A., 815, 1029\nAgarwal, R.C., 924, 1005\nAgarwal, S., 887, 1005\nAggarwal, C.C., 924, 1005\nAgrawal, D., 578, 771, 1005\nAgrawal, R., 181, 602, 815, 887,\n924925,\n1005~1006, 1008,\n1024, 1030,\n1037-~1039\nAhad, R., 516, 1006\nAhlberg, C., 1006, 1001\nAhmed, R., 1026, 1001\nAho, A.V., 303, 516, 648, 1006\nAiken, A., 181, 1006, 1001\nAilamaki, A., 1006\nAlameldeen, A.R., 9G7, 1005\nAlbert, J.A., xxxi, 1026, 1001\nAlon, N., 887\nAlonso, R., 966, 10:35\nAlsabti, K., 925, 1041\nAnupam, V., 10:34, 1001\nAnwar, E., 181, 1007\nApt, K.R., 845, 1007\nArmstrong, W.W., 648, 1007\nArni, N., 816, 1044\nArocena, G., 967, 1007\nAsgarian, Iv1., 691, 1012\nAstrahan, MJvL 98, 180, 516,\n1007, 10121013, 1038\nAtkinson, M.P., 815816, 1007\nAttar, R., 771,\n100~\nAtzeni, P., 24, 98, 648, 8Ui,\n967, 1007\nAvnur, R., 888, 1007\nBabcock, B., 1007\nBalm, S., 888, 1007\nBadal, D.Z., 98, 1007\nBadia, A., 129, 887, 1007, 10:35\nBadrinath, B.R., 1025, 1001\nBaeza-Yates, R., 966, 1019\nBailey, P., 1007\nBalbin, 1., 845, 1008\nBallou, N., 815, 1026\nBalsters, H., xxxi\nBancilhon, F., 99, 816, 845,\n1008\nBapaRao, K.V., 516, 1006\nBaralis, E., 181, 1008\nBarbara, D., 771, 888, 966,\n1008, 1020, 1035\nBarclay, T., 438, 1033\nBarnes, I\\iLG., :30:3, 1039\nBarnett, J.R., 337, 1008\nBarquin, R., 887, 1008\nBatini, C., 55--56, 1008\nBatory, D.S., 516, 1008, 1026\nBaugsto, B.A.W., 438, 1008\nBaum, M.S., 722, 1019\nBawa, Iv!., 924, 1038\nBayardo, R.J., 924----·925, 1008,\n10:35\nBayer, P., 844, 1042\nBayer, R., 369, 1008\nBeck, M., 438, 1008\nBeckmann, N., 991, 1008\nBeech, D., 815, 1018\nBeeri, C., 648, 816, 845, 1006,\n1009\nBektas, H., xxix\nBell, D., 771, 1009\nBell, T.e., 966, 104:3\nBentley, J. L.,\n~369, 1009\nBerchtold, S., 991, 1009\nBergstein, P., xxxii\nBernstein, A.J., 24, 771,\n1027-1028\nBernstein, P.A., 99, 548, 576,\n578, 648, 771, 1007,\n10091010, 1015, HX36\nBeyer, K.S., 887, 991, 1010,\n1029, 10:'35, lOCH\nBhalotia, G., 924, 10:38\nBhargava, 13.K., xxxii, 771,\nHHO\nBiJiris, A., 337, 1010\nBiskup, J., 56, 648, UHO\nBitton, [)., /1:38, 477, 1008, 1010\nBlajr, H., 845, 1007\n1045\nBlakeley, J.A., 887, 1010\nBlanchard, L., xxx\nBlasgen, M.W., 98, 477,602,\n1007, 1010, 1012, 1022\nBlaustein, B.T., 99, 1009\nBlott, S., 991, 1043\nBodagala, S., 925, 1041\nBohannon, P., 967, 1010, 1026,\n1001\nBohm, C., 991, 1009\nBonaparte, N., 847\nBonnet, P., 1010\nBooeh, G., 56, 1010, 1036\nBoral, H., 477, 516, 1027\nBoroclin, A., 966, 1010\nBosworth, A., 887, 1022\nBoyce, R.F., 180, 1010\nBradley, P.S., 887, 1010, 1038,\n925\nBratbergsengen, K., 477, 1010\nBreiman, L., 925, 1010\nBreitbart, Y., 771, 1010--1011,\n1030\nBrin, S., 924, 966, 1011\nBrinkhoff, T., 991, 1011\nBrown, K.P., :337, 1011\nBruno, N., 888, 1011\nBry, F., 99, 845, 1011\nBukhres, O.A., 771, 1017\nBuneman, a.p., 56, 181,\n815·816, 967, 1007, 1011,\n1032\nBunker, R., 477, 1021\nBurdick, D., 924, 1011\nBurger, J., 10:34, IDOl\nBurke, E., 422\nCabibbo, L., 816, 1007\nCai, L., xxxi\nCalimlinl, M., 924, 1011\nCa.mpbell, D., xxxi\nCandan, K.S., 771, 1005\nCarey, 1'1,11..1., xxix, xxxi....·xxxii,\n:3:37, 578, 691, 771,\n815816, 888, 967, 1004,\n1006, 1011-1012, 1019,\n1022-102:3, J(J:39\nCarney, D., 888, 1044\nCarroll, L., 440\nCasanova, M.A., 56, 99, 1012,\n1019\n\n1046\nCa.,>tano~ S., 722, 1012\nCastro, :tv!., 815, 1029\nCate, H.P., 815, 1018\nCattell, R.G.G., 219, 816, 1012,\n102:3, 1043\nCeri, S., 55, 99, 181, 771, 816,\n844, 925, 1008, 1012, 1031,\n1043·\"'·1044, 1001\nCesarini, F., 691, 1012\nCetintemel, U., 888, 1044\nChakravarthy, U.S., 181, 516,\n578, 1007, 1012, 1024,\n1036\nChamberlain, S., 991, 1043\nChamberlin, D.D., 98-99,\n180-181, 516, 816, 967,\n1007, 1010--1013, 1017,\n1038\nChan, 11.C., 771\nChan, P., 924, 1025\nChandra, A.K., 516, 845, 1013\nChandy, M.K., 771, 1013\nChang, C.C., 771, 1013, 1043\nChang, D., 270, 1013\nChang, S.K., 771\nChang, W., 815, 1022\nChanliau, M., xxxi\nChao, D., xxxi\nCharikar, M., 888, 1013\nChatziantoniou, D., 887, 1013\nChaudhuri, S., 691, 816,\n887--888, 924, 1011, 1013\nChawathe, S., 967, 1032\nCheiney, J.P., 477, 1013\nChen, C.M., 337, 516, 1013\nChen, G., 1029, 1001\nChen, H., xxxi\nChen, J., 1006, 1001\nChen, P.M., 337, 1014\nChen, P.P.S., 1014\nChen, Y., 887, 1014\nCheng, W.H., 771\nCherniack, :tv'L, 888, 1044\nCheung, D.J., 925, 1014\nChilds, D.L., 98, 1014\nChimenti, D., 844, 1014\nChin, F.Y., 722, 1014\nChisholm, K., 1007\nChiu, D.W., 771, 1009\nChiueh, T-C., 966, 1014\nChomicki, .1., 99, 1014\nChou, H.,\n:3~H, 815, 101.4, 1016\nChow, E.C., 815, 1018\nChristodoulakis, S., 516, 966,\n1025\nChrysanthis, P.K., 548, 1014\nChu, F., 516, 1014\nChu, !')., 5Ui, l(l:30\nChurchill, W., 992\nCivelek, F.N., 56, 1014\nClarke, EJvL, 99, 1009\nClemons, E.K., 181, 1011\nClifford, J., 1041, 1001\nClifton, C., 925, 1041\nCochrane, R.J., 181, 1014\nCockshott, P., 1007\nCodd, E.F., 98, 129,648,887,\n1014..·..1015\nColby, L.S., 887, 1015\nCollier, R., 25\nComer, D., 369, 1015\nConnell, C., 516\nConnolly, D., 270, 1015\nConnors, T., 815, 1018\nConvent, B., 56, 1010\nConvey, C., 888, 1044\nCooper, B., 967, 1015\nCooper, S., 477, 1021\nCopeland, D., 815, 1015\nCornelio, A., 1039, 56\nCornell, C., 516, 1034\nCornell, G., 270, 1015\nCortes, C., 888\nCosmadakis, 8.S., 99\nCristian, F., 771, 1017\nCristodoulakis, S., 1018\nCvetanovic, Z., 438, 1033\nDadam, P., 337, 816, 1028\nDaemen, J., 722, 1015\nDaniels, D., 771, 1043\nDar, S., 887, 1040\nDas, G., 888, 1013\nDatal', M., 888, 1007\nDate, C.J., 24, 98--99, 637, 648,\n1015\nDavidson, S., 967, 1011\nDavis, J.W., 815, 1018\nDavis, K.C., xxxi\nDayal, D., 99, 181, 516, 648,\n771, 887, 1010, 1013, 1015,\n10301031\nDay, M., 815, 1029\nDe Antonellis, V., 24, 98, 648,\n1007\nDe i\\1aindreville, C., 181, 1039\nDeBono, E., 304\nDeBra, P., 648, 1015\nDeep, J., 270, 1015\nDelcambre, L.M.L., xxxi, 56,\n1042\nDelobel, C., 648, 816, 1008,\n1015\nDeng, Y., 516, 1041\nDenning, D.E., 722, 1015, 1029\nDeppisch, U., 3:37, 10;{4\nDerr, M., 1016\nDerrett, N., 815, 1018\nDersta,dt, .J., 267\nAUTH(}R, INDF~X\nDeshpande, A., 816, 1016\nDeshpande, P., 887, 1005, 1016,\n1039, 1044\nDeutsch, A., 967, 1016\nDeux, 0., 815, 1016\nDeWitt, D..J.) xxviii, 337, 438,\n477, 516, 602, 691,\n770'--771, 815--816, 1004,\n1006, 1010···1012, 1014,\n1016, 1021, 1024--1025,\n1030, 1032, 10:34, 1001\nDiaz, 0., 181, 1016\nDickens, C., 605\nDietrich, S.W., 845, 887, 1016,\n1023\nDiffie, W., 722, 1016\nDimino, L., xxxi\nDittrich, K.R., 816, 1042, 1001\nDogac, A., 56, 771, 1014, 1023\nDomingos, P., 925, 1016\nDomingos, R., 925, 1024\nDong, G., 887, 1014\nDonjerkovic, D., xxix, 516,\n887···-888, 1016, 1029, 1035,\n1001\nDonne, J., 726\nDoole, D., 816, 1011\nDoraiswamy, S., xxxi\nDoyle, A.C., 773\nDubes, R., 925\nDubes, R.C., 1016, 1025\nDu, K., 1033, 1001\nDu, W., 771, 1016\nDuda, A., 966, 1043\nDuMouchel, W., 888, 1008\nDupont, L., 967, 1037\nDupont, Y., 56, 1039\nDuppel, N., 477, 1016\nEaglin, R., xxxii\nEdelstein, H., 771, 887, 1008,\n1017\nEffelsberg, W., :-3;37, 1017\nEich, M.H., xxxi, 602, 1017\nEisenberg, A., 180, 816, 1017\nEl Abbadi, A., 578, 771, 1005,\n1017\nEI-Hajj, 11., 924, 1043\nEllis, C.8., 578, 1017\nEllman, C.,\n10~34, 1001\nElmagarmid, A.K., 771, 1000,\n1015-1017\nElmasri, R., 24, 5Ei, 1017\nEpstein, R., 477, 771, 1017\nErbe, R., 3:n, 816, 1028\nEster, M., 925, 1017, 1037\nEswaran, K.P., 98, 180181,\n477, 548, 1007, uno, 101':-3,\n1017\n\nA UTHOR INDEX\nFagin, R., xxix, 390, 637, 648,\n1009, 1015, 1017--1018\nFalonts().,;, C., 181, :3:W, :369,\n816, 844, 888, 925, 966,\n991, 1008, 1018, 1027,\n1044, 1001\nFan, C., 967, 1038\nFang, N!., 924, 1018\nFandemay, P., 477, 101:3\nFayyad, U.M., 887, 924--925,\n1006, 1010, 1018,\n1038~-1039\nFendrich, .1., xxxii\nFernandez, M., 967, 1016, 1018\nFinkelstein, S.J., 516, 691, 845,\n1018, 1032\nFischer, C.N., xxx\nFischer, P.C., 648, 1025, 1041\nFisher, K., 888\nFisher, iVI., 219, 1023, 1043\nFishman, D.H., 815, 1018\nFitzgerald, E., 817\nFleming, C.C., 691, 1019\nFlisakowski, S., xxix~xxx\nFlorescu, D., 967, 1012--1013,\n1016, 1018-1019\nFord, W., 722, 1019\nFotouhi, F., 477, 1019\n:Fowler, M., 56, 1019\nFox, S., 771, 1036\nFrakes, W.B., 966, 1019\nFranaszek, P.A., 578, 1019\nFranazsek, P.A., 1019\nFrank, E., 924, 1043\nFranklin, M.J., 771, 815816,\n967, 1011, 1015, 1019\n:Fraternali, P., 181, 1012, 1019\nFrawley, W.J., 924, 1034\nFreeston,\n~1.'W., 991, 1019\nI~eire, .1., 967, 1010\nFreitag, B., 844, 1044\nFrench, .1., 1020\nFrew, .1., 691, 1040\nFreytag, J.C., 516, 1019\nFriedman, J.B., :369, 924 925,\n1009----1010, 102:3\nFriesen, 0., 816, 1019\nFry, J.P., 24, 56, 99, 1019, 1041\nFuchs, IV!., 270, 1028\nFu, Y., 925, 1023\nFugini, M.G., 722, 1012\nFuhr, N., 966, 1019\nFukuda, 1'.,924, 1019\nFunderburk, .I., 967, 10:38\nFurtado, A.1., 99, 1012, 1019\nFushimi, S., 477, 1019\nGacHa, S., 1041, lOCH\nGaede, V., 991, 1020\nGallaire, H., 98.._·99, 648, 844,\n1020\nGaltieri, C.A., 602, 771, 1028,\n1041\nGamboa, R., 844, 1014\nGanguly, S., 771, 1020\nGanski, R.A., 516, 1020\nGanti, V., 925, 1020\nGarcia-:Molina, H., 24, 578, 771,\n887, 924, 966----967, 1005,\n1010, 1018, 1020·..·1021,\n1033-1035, 1044, 1001\nGardels, K., 691, 1040\nGarfield, E., 966, 1020\nGarg, A.K., 390, 1020\nGarza, J.F., 337, 815, 1008,\n1026\nGehani, N.H., 181, 815, 1006\nGehrke, .I.E., 691, 888,\n924-925, 1006,\n1011-~1012,\n1020\nGerber, R.H., 477, 770, 1016\nGhemawat, S., 815, 1029\nGhosh, S.P., 303, 1020\nGibbons, P.B., 887-888, 1005,\n1021\nGibson, D., 925, 966, 1021\nGibson, G.A., 337, 1014, 1021,\n1034\nGifford, D.K., 771, 1021, 1043\nGifford, K., 966\nGilbert, A.C., 888\nGionis, A., 888\nGoh, .1., 181, 1040\nGoldfarb, C.F., 270, 1021\nGoldman, R., 967, 1021, 1030\nGoldstein, .1., xxxi, 991, 1010,\n1021\nGoldweber, M., xxix\nGoodman, N., 57(), 578, 771,\n1007, 1009, 1036, 1038,\n1041\nGopalan, H... , xxxi\nGotlieb, C.C., 390, 1020\nGottlob, G., 844, 1012\nGraefe, G., xxxi, 4:38, 477, 51G,\n770--771, 815, 1011, 1016,\n1021, 1028\nGraham, M.H., 648, 1021\nGrahne, G., 98, 1021\nGrant, .J., 516, 1012\nGravano, L., 888, 966, 1011,\n1021\nGray, .LN., 98,\n4:_~8, 548, (i02,\n691, 770--771, 887, 1000,\nl007, ]012,\n10l(i~1017,\n1021-1022, 1028, 103:3,\n1037, 1041\n1047\n$\nGray, PJvl.D., 24, 181, IOI6,\n1022\nGreenwald, :NI., 887, 1022\nGreipsland, J.F., 438, 1008\nGriffin, T., 887, 1015\nGriffiths, P.P., 98, 180, 602,\n722, 1007, lCll:3, 1022\nGrimson, J., 771, 1009\nGrinstein, G., 1022, 1001\nGrosky, W., xxxi\nGruber, R., 815, 1029\nGuenther, 0., 991, 1020\nGuha, S., 888, 925, 1022, 1033\nGunopulos, D.,\n924-~925, 1006,\n1008, 1022\nGuo, S., 1034, 1001\nGupta, A., 516, 887, 1005,\n1022, 1041\nGuruswamy, S., 1033, 1001\nGuttman, A., 991, 1022\nGyssens, M., 129, 1007\nHaas, L.M., 771, 815, 1013,\n1022, 1043\nHaas, P.J., 516, 888,\n1022---1023, 1034\nHaber, E., xxx\nHaderle, D., 602, 771, 1031\nHadzilacos, V., 576, 578, 1009\nHaerder, T., 337, 602, 1017,\n1023\nHaight, D.M., 815, 1011\nHaines, M., xxix\nHalici, U., 771, 1023\nHall, M., 270, 1023\nHall, N.E., 815, 1011, 1034,\n1001\nHall, P.A.V., 477, 102:'3\nHalpern, J.Y., 516, 1014\nHamilton, G., 219, 102:'3, 104;3\nHammer, .1., xxxi, 887, 1044\nHammer, M., 98, 771, 102:3,\n1036\nHan, J., 887, 924·-925, 1014,\n102:3, 1028, 10:'32, :10:34\nHand, D.J., 924925, 1023,\n1D::W\nHanson, E.N., 181, 887, 102:3\nHapner, M., 219, 104:3\nBarel, D., 845, 101:3\nHarinarayan, V., 887,\n102::~\nHaritsa, J., 578,924,1023, l(X38\nHarkey, D., 270, 101:3\nHarrington, .I., xxx\nHarris, S., xxix\nHarrison, .I., 887, 1023\nHa..san, \"V., 771, 1020\nHass, P.or., 888, 1008\nHa.stie, T., 924, 1023\nHearst, M_, xxxii\n\n1048\nBeckerman, D., !J24, 1014,\n102:,3, 1041\nHeckman, ?v1., 722, 1029\nHeH<:md, P., 771\nHellerstein, JJv1., xxix, 181,\n516, 772, 816, 845, 888,\n967, 991, 1()06·_·1008,\n10221024, 1027,\n1034,,·,,1035, 1038, 1040, 77:3\nHellman, lVLE., 722, 1016\nHeilschen, L.J., 99, 10:30\nHeytens, l\\ILL., 477, 770, 1016\nHidber, C., 925, 1024\nHill, IvLD., 1006\nHillebrand, G., 967, 1011\nHimmeroeder, R., 967, 1024\nHinterberger, H., 991, 1033\nHjaltason, G.R., 967, 1015\nHoch, C.G., 815, 1018\nHo, C-T., 887, 924, 1024, 1044\nHolfelder, P., 270, 1015\nHollaar, L.A., 966, 1036, 1001\nHolzner, S., 270, 1024\nHoneyman, P., 648, 1009\nHong, D., 578, 1024\nHong, W., 771, 1024\nHopcroft, J.E., 303, 1006\nHou, W-C., 516, 1024, 1033,\n1001\nHoward, .J.H., 648, 1009\nHsiao, H., 771, 1024\nHsu, C., xxxii\nHuang, .1.,578, 1024\nHuang, L., 966, 1014\nI-Iuang, W., xxix\nHuang, Y., 771, 1024, 1001\nHull, H.. , 24, 56, 98, 648, 816,\n844, 1005, 1024, 1001\nHulten, G., 925, 1016, 1024\nHunter, .1., 270, 1024\nImielinski, T., 98, 924, 1006,\n1024-1025, 1001\nJoannidis, Y.E., xxix, 56, 516,\n888, 1008, 1025, 10:-n,\n10:34\nIochpe, C., 1042, lOCH\nIves, Z., 967, 1012\nJacobson, 1., 56, 1010, Hn6\nJacobsson, H., xxxi\nJagadish, H.V., :3:31, :369,\n887-,·888, 991, 1008, 1018,\n1025, 1027, 1040, lOCH\n.Ja-in, A.K., 925, 1016, 1025\nJajodia, S., 722, 771, 1025,\n1041,1042, 1001\n.larke, 1vt, 516, 1025\nJean, 1\"., 10:34, 1001\nJeffers, R., 578, 1005\n.Jhingran, A., 181, 1040\nJing, .J., 771, 1017\nJohnson, T., 578, 888, 1008,\n1024\nJones, K.S., 966, 1025\nJonsson, B.T., 771, 1019\nJou, J.H., 648, 1025\nKabra, N., 516, 1025, 10:34,\n1001\nKambaya,shi, Y., 771, 1025\nKamber, NI., 924, 1023\nKane, S., xx.xii\nKanellakis, P.C., 98, 648, 816,\n1005, 1008, 102,5, 1001\nKang, J., 967, 1018\nKang, Y.C., 516, 1025\nKaplan, S..1., 1043\nKarabatis, G., 771, 1036, 1001\nKargupta, H., 924, 1025\nKatz, R.H., 337, 477, 1014,\n1016, 1034\nKaufman, L., 925, 1025\nKaushik, R., xxxii, 967, 1026,\n926\nKawaguchi, A., xxxii, 887, 1015\nKeats, J., 130\nKedem, Z.I'v1., 924, 1028\nKeim, D.A., 1026, 1001\nKeller, A.M., 99, 1026\nKemnitz, G., 815, 1040\nKemper, A.A., 3:37, 816, 1028\nKent, W., 24, 616, 815, 1018,\n1026, 1001\nKerisit, J.M., 845, 10:36\nKerschberg, L., 24, 1026\nKetabchi, M.A., 1026, 1001\nKhanna, S., 887, 1022\nKhardon, R.., 924, 1022\nKhayyam, 0., 817\nKhoshafian, S., 816, 1008\nKiernan, J., 181, 967,\n10:38-1039\nKiessling, \\V., 516, 1026\nKifer, Tvi., 24, xxix, 816, 845,\n1026, 1028\nKimball, H.., 887, 1026\nKim, vV., 516, 771, 815 ..816,\n1017, 1026\nKimmel, W., xxx\nKing, .LT., 516, 1026\nKing, R., 50, 1024\nKing, VvT.F., 98, 1007, 1012\nKirk, (:., 270, 10:34\nJ<'itsuregawa, ?Y1., 477, 1019\nKleinberg, .1.R'!., 925,966, 1021,\n1026\nKlf.!in, J.D., xxxi\nKlug, A.C., 24, 129, :,337, 516,\n1016, 1026\nKnapp, E., 1027\nAlJ'rH()R INI)EX\nKnuth, D.E., 30:3, 4:38, 1027\nKoch, G., 99, 1027\nKoch, J., 5Ui, 1025\nKodavalla, H., xxxi\nKohler, \\V.H., 1027\nKonopnicki, D., 967, 1027, 1001\nKornacker, M., 816,\n991~ 1027\nKorn, F., 888, 1027\nKorth, II.F., 24, 578, 771, 967,\n1024, 1026-1027, HX30,\n1039, IDOl\nKossman, D., 771, 888,\n10l2~\n1019, 1027\nKotidis, Y., 887--888, 1027,\n1036\nKoudas, N., 888, 1022\nKoutsoupias, E., 967, 991,\n1023·--1024\nKowalski, R.A., 844, 1042\nKriegel, H-P., 925, 991,\n1008--1009, 1011, 1017,\n1026, 1037, 1001\nKrishnakumar, N., 771, 1027\nKrishnamurthy, R., 516, 771,\n844, 1014, 1016, 1020,\n1027\nKrishnaprasad, NL, xxxi, 887,\n10~35\nKuchenhoff, V., 844, 1042\nKuhns, J.L., 98, 129, 1027\nKulkarni, K., xxix\nKull, D., 691,\n104~3\nKumar, K.B., 477, 770, 1016\nKumar, V., 578, 1027\nKunchithapadarn, K., xxx\nKung, H.T., 578, 1027\nKuo, D., 1027\nKupsch, J., 1034, 1001\nKuspert, IC, 337, 816, 1028\nLaCroix, M., 129, 1027\nLadner, R.E., 578,\n10~30\nLai, :tv'L, 578, 1027\nLakslnnanan, L.V.S., 924925,\n967, 1027-1028,\n10~32, IO:~4\nLaIn, C., 815, 1028\nLamport, L., 771, 1028\nLampson, B.vV., 771, 1028\nLanders, 'I'.A., 771, 10:36\nLandis, G., 815, 1028\nLandwehr, C.L., 722\nLangE~rak, R., 99, 1028\nLapis, G., 771, 815, 1022, 1043\nLarson, J.A., 56, 771,\nlO::~9\nLarson, P., 390, '·1:38, 887, 1010,\n1028, HX35\nL1St, M., xxxii\nLausen, G., 8Ui, 967, 1024,\n1026\nLawan(lE~, S., 1029, 1001\n\n~4lJT}I()R INDE~Y\nLayman, A., 887, 1022\nLebowitz, F., 100\nLee, E.K., 337, 1014\nLee, 1V1., xxix\nLee, S., 888, 1044\nLefebvre, A.) 816, 844, 1019,\n1042\nLeff, A., 771, 1034\nLehman, P.L., 578, 1027-··1028\nLeinbaugh, P., 1010, 1001\nLenzerini, M., 56, 1008\nLescoeur, F., 845, 1036\nLeu, D.F., 1013\nLeung, T.vV., 816, 1040\nLeung, T.Y.C., 516, 845, 887,\n1028, 1038\nLeventhal, JV1., 270, 1028\nLevine, F., 578, 602, 1032\nLevy, A.Y., xxxii, 887, 967,\n1018'-1019, 1040\nLewis, D., 270, 1028\nLewis, P.M., 24, 771, 1028,\n1036\nLey, lVI., xxix\nLibkin, L., 887, 1015\nLiedtke, H., 1042, 1001\nLieuwen, D.F., 337, 887, 1015,\n1025, 1034, 1001\nLim, E-P., 771, 1028, 1001\nLin, D., 924, 1028\nLin, K-I., 991\nLindsay, B.G., xxxi, 98, 337,\n602, 771, 815, 887, 1012,\n1022, 1028,\n10~30-1032,\n1041, 1043\nLing,Y., 516, 1041\nLinnemann, V., :3:37, 816, 1028\nLipski, vV., 98, 1024\nLipton, R.J., 1020, 516, 1028,\nlOCH\nLiskov, 13.,815, 1029\nLitwin, \\V., 390, 771, 1029\nLiu, H., 967, 104:3\nLiu, 1'/1.1'., 771, 1017, 1029\nLivny, tv'L, :3:37, 578, 771, 816,\n887, 925, 1006, 10111012,\n1019, 102:3, 1029, 10:38,\n1044, 1001\nLoclu)vsk~ F., 816, 1026\nLockemann, P.C., 1042, 1001\n1..,0, B., 888, 1007\nLoh, \\>\\7-Y., 925, 1020\nLohman, G.!vI., 516, 691, 771,\n815, 1022, 1029, 1042\nLornet, D.B.,\n4~38, 578, 771,\n991, 10281029, 10:3:3\nLone~ K., 99, 1027\nLoo, B.'1'., 771, 10:n\nLorie, R.A., 98, 180, 438, 51n,\n548, 602, 1007, 1012,,··1013,\n1017, 1022, 1028-····1029,\n10:38\nLou, Y., 816, 1029\nLozinskii, E.L., 845, 1026 .\nLucchesi, C.L., 648, 1029\nLu, H., 770, 1029\nLu, P., 924,\n104~3\nLu, Y., 967, 1012\nLudaescher, B., 967, 1024\nLueder, R.,\n10~~4, 1001\nLum, V.Y., 369, 887, 1029,\n1040\nLunt, T., 722, 1029\nLupash, E., xxxii\nLyngbaek, P., 815, 1018\nMackert, L.F., 771, 1029\nNlacNicol, R., xxxi\nMadigan, D., 924, 1013\nMahbod, E., 815, 1018\nMah, T., 924\nMaheshwari, D., 815, 1029\nNlaier, D., 24, 98, 648, 815-·-816,\n844-845, 1008, 1015,\n1029-·10:30, 1044\nMakinouchi, A., 816, 1030\nJVlanber, D., 578, 1030\nManku, G., 887, 1030\nJVlannila, H., 648, 924··925,\n1006, 1014, 1022--1024,\n1030, 1041\nMannino, M.V., 516, 1030\nManolopoulos, Y., 925, 1018\nNlanprempre, C., 966, 1043\nManthey, H., 99, lOll\nMark, 1., 771, 1029\nIvIarkowitz, V.JV!., 56, 99, 1030\n1vlartella, G., 722, 1012\nMaryanski, P., 55, 1034\nMatias, Y., 887···-888, 1021\nMatos, V., 129, 816, 10;n\n~vlattos, N., 181,816, 1011,\n1014\nIVlaugis, 1.,., 181, 1007\nIVIcAuliffe, :N1.L., 815, 1011\nl\\ilcCarthy, D.R., 181, 1030\nlVIcCreight, E.l'vl., 369, 1008\nMcCune, V'll. V'!. , 99,\n10~'W\nMcGill, M.J., 966, 10:37\nIVIcGoveran, D., 99, 1015\nMcHugh, J., 967, lo:m\nl\\'1cJones, P.R., 98, 602, 1007,\n1022\nMclj~od, I)., 98, 516, 1006,\n102:3\nMcPherson, .J., 3:37, 815, 1022,\n1028\nI:vlecca, G., 816, 967, 1007\n1049\nwleenakshi, K., 845, 1008\nIviegiddo, N., 887, 1024\n?vlehl, J.\\N., 98, 180, 1007,\n1012..··1.01:3\nIVlehrotra, S., 771, 10:.m\n?vIehta, lvI., 770, 925, uno, HX38\n:NIelton, J., xxix, xx.-xii, 180,\n816, 887, 1017, 1030···-1031\nN:lenasce, D.A., 771, 1mn\nNlendelzon, A.O., 648, 925,\n967, 1007, 10l9, 1021,\n1029, 1031, 1035, 1001\n:NIeo, R.., 925, 1031\nMeredith, J., 691, 1040\nMerialdo, P., 967, 1007\nIvlerlin, P.M., 516, 1013\nMerrett, T.H., 129, 303, 1031\nMeyerson, A., 925, 1033\n11ichel, H., 477, 1013\nMichie, D., 925, 1031\nMihaila, G.A., 967, 1031\nJVIikkilineni, K.P., 477, 10:31\nIvIiller, R.J., 56, 924, 1031, 1043\nIvIilne, A.A., 550\nMilo, T., 816, 967, 1009, 1031,\n1.001\nMinker, J., 98·····99, 516, 648,\n844, 1007, 1012, 1020,\n1031\nJVlinoura, T., 771, 1031\nMishra, N., 925, 1022, 1033\nMisra, J., 771, 1013\nMissikoff, M., 691, 1012\nMitchell, G., 516, 10:,31\nMoffat, A., 966, 1031,\n104:3··..·1044\nMohan, (;., xxix, xxxi, 578,\n602, 771, 816, 991, 1027,\n10~n··l032\nMoran, J., xxxii\n1'l/lorimoto, Y., 924, 1019\nMorishita, S., 844, 924, 10If>,\n1019\nMorris, K.A., 844, 1032\nI\\!lorrison, R., 1007\nf\\·10tro, A., 56, 1032\nMotwarli, IL, 888, 924 925,\n1007, 1011, 101:3, 1018,\n1022, 103:3, 1041\nMukkamala, R., 771, HX32\nMumick, 1.S., 516, 816, 845,\n887, 1015, 1022, 10:32\nf\\·luntz, ILIL, 771, 887, 1028,\n10~31\nl\\iIuralikrishna,M., xxxi, 477,\n516, 770, 1016,\n10~12\nlvlutchlm', D., 771., 1025\nMuthukrishnan, S., 888\nlvIyers, A.C., 815, 1029\n\n1050\n?vlyllymaki, J., 1029, 1001\nNag, B., 1034, 1001\nNaqvi, S.A., 816, 844·--845,\n1009, 1011, 1014, 1032\nNarang, L, 578, 10:32\nNarasayya, V.R., 691, 888, 1013\nNarayanan, S., 816, 1011\nNash, 0., 338\nNaughton, J.F., 1026, xxix,\n438, 477, 516, 691, 770,\n815-816, 844, 887, 967,\n991, 1005, 1011~··1012,\n1016, 1022, 1024, 1028,\n1032, 1034, 1039, 1041,\n1044, 1001\nNavathe, S.B., 24, 55~56, 578,\n924, 1005, 1008, 1017,\n1037, 1039\nNegri, M., 180, 1032\nNeimat, M-A., 390, 815, 1018,\n1029\nNestorov, S., 925, 967, 1032,\n1041\nNewcomer, E., 548, 1009\nNg, P., 771, 1043\nNg, R.T., 337, 888, 924--925,\n1008, 1018, 1025, 1028,\n1032\nNg, V.T., 925, 1014\nNguyen, T., 967, 1033, 1037,\n1001\nNicolas, J-M., 99, 648, 1020\nNievergelt, J., 390, 991, 1018,\n1033\nNodine, M.H., 771\nNoga, A., 887\nNyberg, C., 438, 1033\nObermarck, R., 771,\n10321033, 1043\nO'C(lllaghan, L., 925, 1022,\n10:3:3\nOlken, F., 477, 516, 887, 1016,\n10:3:3\nOlshen, R.A., 925, 1010\nOlston, C., 771, 10:3:3, 888, 1007\nOrniecinski, E., 578, 924, 1005,\n10:37\nOnassis, A., 889\nO'Neil, E., 24, 1033\nO'Neil, P., 24, 771, 887, HJ:3:3\nOng, K., 816, 1044\nOoi, B-C., 770, 1029\nOracle, 651\nOrenstein, .I., 815, 1028\nOsborn, S.1., 648, 1029\nOsborne, R., xxxii\nOzden, B., 1033, lOCH\nOzsoyoglu, G., 129, 516, 722,\n816, 1014, 1024, 10:3:3,\nlOOI\nOzsoyoglu, 2.1\\11., 129, 516, 816,\n1029, 1033, 1038\nOzsu, M.T., 771, 1033\nPage, L., 966, 1011\nPang, A., 924925, 1028, 1032\nPapadimitriou, C.H., 99, 548,\n578, 967, 991, 1023-1024,\n1033\nPapakonstantinou, Y., 771,\n967, 1005, 1033,...10:M\nParaboschi, S., 181, 1008, 1012\nParedaens, J., 648, 1015\nParent, C., 56, 1039\nPark, J., 516, 887, 10:34, 1038\nPatel, JJvl., 1034, 1001\nPaton, N., 181, 1016\nPatterson, D.A., 337, 1014,\n1034\nPaul, H., 337, 815, 1034, 1037\nPeckham, J., 55, 1034\nPei, J., 924, 1023, 1034\nPelagatti, G., 180, 771, 1012,\n1032\nPetajan, E., 1034, 1001\nPetrov, S.V., 648, 1034\nPetry, F., xxxi\nPfeffer, A., 816, 991, 1024\nPhipps, G., 844, 1016\nPiatetsky-Shapiro, G., 516,\n924, 1006, 1018, 1034\nPiotr, 1., 888\nPippenger, N., :390, 1018\nPirahesh, H., 181, 3:37, 516,\n602, 771, 815, 845, 887,\n1014, 1022, 1028,\n1031-1032, 1034, 1038\nPirotte, A., 129, 1027\nPistol', P.,\n~337, 816, 1028\nPitts-Moultis, N., 270, 1034\nPoosala, V., 516, 888, 1005,\n1008, 10:34\nPope, A., :370\nPopek, G.,1.,\n~)8, 1007\nPort, G.S., 845, 1008\nPotarnianos, S., 181, 1040\nPowell, A., 1020\nPramanik, S., 477, 1019\nPrasad, V.V.V., 924, 1005\nPregibon, 1)., 888, 924, 1014,\n102:3, 1041\nI)rescod, P., 270, 1021\nPrice, T.G., 98, 516, 602, 1012,\n1022, 1m{8\nProck, A., xxx\nPruyn.e, J., xxix\nPsaila, G., 925, 1006, 1031\nPu, C., 771, 10:34\nAlJTHOR, INDeX\nPutzolu, G.R., 98, 578, 602,\n1007, 1022, 1028\nQian, X., 887, 1034\nQuass, D., 887, 967, 1030,\n10:331034\nQuinlan, J.R., 925, 1035\nRafiei, D., xxxi, 925, 1035\nRaghavan, P., 925, 966, 1006,\n1021\nRaiha, K-J., 648, 10:30\nRajagopalan, S., 887, 1030\nRajaraman, A., 887, 967, 1023,\n1034\nRamakrishna, J\\lLV., 390, 1035\nRamakrishnan, LV., 845, 1035\nRamakrishnan, R., 56, 516,\n816, 844--845,\n887~-888,\n924-\"925, 991, 1004--1005,\n1008--1010, 1016,\n102(}·1021, 1025, 1029,\n1031'-1032, 1035, 1038,\n1040, 1044, 1001\nRamamohanarao, K., 390,\n844~845, 966, 1008, 1035,\n1044\nRamamritham, K., 548, 578,\n1014, 1024\nRamamurty, R., xxx\nRaman, B., 888, 1007\nRaman, V., 888, 1007, 1035\nRamasamy, K., 887, 1016,\n10:34, 1039, 1001\nRamaswamy, S., 888, 1005\nRanganathan, A., 887, 1035\nRanganathan, M., 925, 1018\nRanka, S., 925, 1041\nRao, P., 845, 1035\nRae, S.G., 887, 10:35\nRastogi, R., :337, 771, 925,\n1010, 1.022, 1025,\n10~~0,\n10:3:3, 10:35, 1001\nRcarnes, Iv1., xxx\nReed, D.P., 578, 771, 1.035\nRees(~, G., 219, 10:35\nReeve, C.L., 771., 1009, 1036\nReina, C., 925, uno\nReiner, D.S., 516, 1026\nReisner, P., 180, 101:1\nReiter, R., 98, 10:35\nRengarajan, 1'., xxxi\nR(~scorla, E., 722, HX35\nlleuter, A., 548, 602, 1000,\n1022...,102:3,\n10~l6\nRichardson, J.E., 815, 337,\n1011·····1012\nRiehuI, S., 816, 1011\nRijrnen, V., 722, 1015\nRiloff, E., 96G, l():.l6, 1001\nRishe, N., 516, 1041\n\nAUTHOR INDEX\nRis.<mnen, J., 648, 925, 1030,\n1036\nRivest, R.L., 390, 722, 1036\nRoberts, G.O., 966\nRobie, J., 967, 1013\nRobinson, J.T., 578, 991, 1019,\n1027, 1036\nRogers, A., 888\nRohmer, J., 845, 10;36\nRosemall, S., 991, 1018\nRosenkrantz, D.J., 771, 10:36\nRosenthal, A., 516, 925,\n10~36~1037, 1041, IDOl\nRosenthal, J.S., 966, 1010\nRoss, K.A., 816, 887·..·-888, 1008,\n1013, 1015, 1032,\n10~36\nRotem, D., 516, 887, 1033, 1040\nRoth, T., 888, 1007\nRothnie, J.B., 771, 1009, 1036\nRousseeuw, P.J., 925, 1025\nRoussopoulos, M., 887, 1036\nRoussopoulos, N., 337, 516,\n771, 887, 991, 1013, 1027,\n1029, 1036\nRoy, P., 967, 1010\nRozen, S., 691, 1036\nRumbaugh, J., 56, 1010, 1036\nRusinkiewicz, M., 771, 1036,\n1001\nRyan, T.A., 815, 1018\nSacca, D., 845, 1036\nSacks-Davis, R., 390, 966, 1035,\n1044\nSadri, F., 967, 1027\nSagalowicz, D.,\n104~3\nSager, T., 516, 1030\nSagiv, Y., 516, 648, 816, 845,\n967, 1006, 1008, 1026,\n1029, 10:34, 10:36\nSagona.\" K.F., 844···845, 1035,\n1037\nSahuguet, A., 967, 10:37\nSalton, G., 966, l(X)7\nSaluja, S., 924, 1022\nSalveter, S., 516,\n10~39\nSalzberg, B.J., 30:3, :n7, 438,\n578, 991, 1029, 1037\nSarnarati, P., 722, 1012\nSamet, H., 991, 10:37\nSample, N., 9G7, 1015\nSander, .I., 925, 1017, 10:37\nSanders, It.E., 219, 10:37\nSandhu, R., 722, 1025\nSaraiya, Y., 844, 1032\nSarawagi, S., 816, 887, 925,\n1005, 10:37\nSathaye, A., xxxii\nSavasere, A., 924, 1037\nSbattella, L., 180, 10:32\nSchauble, P., 966, 1037\nSchek, H-J., 337, 815, 991,\n1034, 1037, 1043\nSchell, R., 722, 1029\nSchiesl, G., xxxii\nSchkolnick, IVLJVL, 98, 578, 691,\n1008, 1012, 1018, 1037\nSchlageter, G., 771, 1037\nSchlepphorst, C., 967, 1024\nSchneider, D.A., 390, 438, 477,\n516, 770, 1016, 1028··-1029\nSchneider, R., 991, 1008, 1011\nSchneier, B., 722,\n10~n\nScholl, NLH., 337, 815, 1034,\n1037\nSchrefl, M., xxxi\nSchryro, NI., 1042, 1001\nSchuh, D.T., 815, 1011\nSchumacher, L., xxix\nSchwarz, P., 602, 771, 1031\nSciore, E., 516, 648, 1037, 1039,\n1001\nScott, K., 56\nScott, S., 1019\nSeeger, B., 991, 1008\nSegev, A., 516, 887, 1034, 1038,\n1041, 1001\nSeidman, G., 888, 1044\nSelfridge, P.G., 924, 1038\nSelinger, P.G., 98, 180, 516,\n602, 722, 771, 1012, 1028,\n1038, 1043\nSellis, 1'.K., 337, 51G, 991,\n1018, 1025, 1038\nSeshadri, P., xxix, 516, 816,\n844'\nH B45, 887, 1014, 1035,\n10:38, 1040\nSeshadri, S., 477, 516, 1010,\n101(i, 1022, 1001\nSevcik, K.C., 888, 991, 1008,\n1033\nShaclrl1on, 1\\/1., 967, 1015\nShafer, J.C., 924925, 100G,\n10:38\nShaft, U., xxix xxx, 991, HnO,\n1021\nShah, D., 691, 924, 1012, 10:38\nShamir, A., 722, 10:36\nShan, lvi-C., 815, 1016, 1018,\n1026, 1.001\nShanmuga,'mndaram, :T., 887,\n967, 1012,\n10~38\nShapiro, L.D., xxix, 477, 101(:),\n10:38\nShasha, L)., xxix, 578, 691, 771,\n1010, lO:3E), 1<.n8\nShatkay, H., 925, 10:38\nSheard, '1'., 99, 10:38\n10~1\nShekita, E.J., 337, 51H, 815,\n967, 1011-·..·1012, 1022,\n1034, 1038\nSheldon, NLA., 966, 1043\nShelloy, P., 924, 1038\nShenoy, S.T., 516, 1038\nShepherd, J., 390, 1035\nSheth, A.P., 56, 771, 1017,\n1029, 103H, 1039, 1001,\n771\nShim, K., 816,\n887~-888, 925,\n1013, 1022, 1025, 1035\nShipman, D.W., 548, 771, 1009,\n1036\nShivakumar, N., 924, 1018\nShmueli, 0., 845, 967, 1009,\n1027, 1001\nShockley, W., 722, 1029\nShoshani, A., 887, 1038---1039\nShrira, L., 815, 1029\nShukla, A., xxix, 887, 1016,\n10~39, 1044\nSibley, E.H., 24, 1019\nSiegel, M., 516, 1037, 1039,\n1001\nSilberschatz, A., 24, xxx, 337,\n578, 771, 10101011, 1025,\n1027, 1030, 1033, 1039,\n1001\nSilverstein, C., 1011\nSimeon, J., 967, 1010, 1013\nSimoll, A.H.., 180, 816, 10:31\nSimon, E., 181, 691, 1038----1039\nSimoudis, E., 924, 1018, 1039\nSinghal, A., 771, 1029\nSistla, A.P., 991, 1024, 104:3,\n1001\nSkeen, D., 771, 1017, 1039\nSkounakis, 1'.1., 1006\nSlack, J.1L, xxxi\nSlutz, D.R., 98, 1012\nSmith, D.C.P., 55, 1039\nSmith, J.M., 56, 10:39\nSmith, K.P.,\n:3~37, 722, 1008,\n10:39\nSlnith, P.D.,\n:30~3,\n10~39\nSmyth, P., 924, 1006, 1018,\n10:30\nSnodgrass, R.T., 181, 816, 844,\n1041, 1044, lOCH\nSo, B., xxix\nSoda, G., 691, 1012\nSolomon, IvLH., 815, 1011\nSoloviev, V., 770, HX30\nSon, S.H., xxxi\nSoparka.r, N., 578, 1027,\n10~39,\n1001\nSorenson, P., 691, 1037\nSpaccapietra, S., 5G, 1014, 1039\n\n1052\nSpeegle, G., xxxi\nSpencer, L., 925, 1024\nSpertus, E., 9{j6, 1039\nSpiegelhalter, D.,]., 925, 1031\nSpiro, P., xxxi\nSpyratos, N., 99, 1008\nSrikant, R., 887, 924925, 1006,\n1024, 10:39\nSrinivasan, V., 578,\n10~3~3, 10:39,\n1001\nSriva..<;tava, D., 516, 816,\n844-845, 887--888, 924,\n1035·..·1036, 1038, 1040\nSrivastava, J., 771, 887, 1028,\n1040, 1001\nStacey, D., 771, 1040\nStachour, P., 722, 1040\nStankovic, J.A., 578, 1024,\n1040, 1001\nStavropoulos, H., xxix\nStearns, R., 771, 1036\nSteel, T.B., 1040\nStefanescu, M., 967, 1013\nStemple, D., 99, 1038\nStewart, 1\\11., 438, 10:37\nStokes, L., 516, 1022\nStolfo, S., 924, 1035\nStolorz, P., 924, 1006\nStonebraker, M., 24, 98 ..·99,\n181, 337, 477, 691, 771,\n815-\"-816, 887,,,888, 1006,\n1016--1017, 1024, 1037,\n1040, 1044, 1001\nStone, C.,J., 925, 1010\nStrauss, IvLJ., 888\nStrong, R.R., :390, 1018\nStuckey, 1:>..1.,516, 845, 1038\nSturgis, H.E., 771, 1028\nSubrahmanian, V.S., 181, 771,\n816, 844, 887, 1005, 1022,\n1042, 1044, 1001\nSubralnanian, B., 816, 1040\nSubramanian, LN., 967, 1027\nSubramanian, S., 967, 1012\nSueiu, D., xxxii, 967, 1011,\n1018, l();31\nSu, ,]., 816, 1024\nSu, S.1'.\\V., 477, 1031\nSudarshan, S., 10:35, 24, xxix,\n:B7, 516, 816, 844'''845,\n887, 924, 1025, 10351036,\n10:.38.. ·1040. 1001\nSudkamp, N., :3:37, 816, 1028\nSUll, \\V., 516, 1041\nSuri, R., 578, 1041\nSwagerman,R., 816, 1011\nSwarni, A., 516, 924, 1006,\n1022, 1041\nSwift, T., 844845,\nlO:~5, 1037,\n1041\nSzegedy, 1\\11., 887\nSzilagyi, P., 966, 104:3\nTam, B.\\V., 925, lOlA\nTanaka, H., 477, 1019\nTanca, L., 181, 844, 1012, 1019\n'ran, C.K., 815, 1011\nTan, J.S., 887, 1040\nTan, K-L., 770, 1029\nTan, \\V.C., 967, 1018\nTang, N., xxx\nTannen, V.B., 816, 1011\nTansel, A.D., 1041, 1001\nTatblll, N., 888, 1044\nTa~ 1'.C., 578, 1041\nTaylor, C.C., 925\nTaylor, C.C., 1031\nTeng, J., xxxi\nTeorey, T.J., 55--·56, 99, 1041\nTherber, A., xx..x\nThevenin, .I.Ivi., 477, 1013\nThomas, R.H., 771, 1041\nThomas, S., 925, 1037, 1041\nThomas, S.A., 722, 1041\nThomasian, A., xxxi-xxxii,\n568, 578, 1019, 1041\nThompson, C.R., 771,\n1010'''1011\nThuraisingham, B., 722, 1040\nTiberio, P., 691, 1018\nTibshirani, R., 924, 102:3\nTodd, S.J.P., 98, 1041\nToivonen, H., 924\nm -925, 1006,\n1022, 10:30, 1041\nTokuyama, T., 924, 1.019\nTomasic, A., 966, 1021\nTompa, F.W., 887, 1010\nTowsley, D., 578, 1024\nrn'aiger, I.L., 98, 548, 602, 771,\n1007, 1012, 1017, 1022,\n1028, 1041\nTrickey, H., 887, 1015\nTsangaris, :tv1., 816, 1041\nTsapara.s, P., 966, 1010\nTsatalos, O.G., 815, 1011\nTsatsoulis, C., xxxi\nTsichritzis, D.C., 24, 1026\nTsotras, V.; xxxii\nTsou, D., 648, 1041\nTsukerrnan, A., 4:38, 10:37\nrl'sukuda, K., ;3:37, 1008\n\"I'sur, D., 925, 1041\nTsur, S., 844-·845, 924, 1009,\n1011, 1014\n'l\\leherman, L., 99, 1012\nTucker, A.B., 24, 1041\nTufu~, K., 10:34, 10()}\nTukcy, J.\\N., 924, 1041\nAUTHOR,INDEK\n'-rwi.chell, B.C., 337, 1008\nUbelL 1'1., xxxi\nlJgur, A., xxxii\nUllman, J.D., 24, xxx, 56, 98,\n:303, :390, 5Hi, 648,\n844~845, 887, 924..··925,\n9G7, 1006, 1008, 1011,\n1018, 1020, 102:3, 1032,\n1034·--1035, 1041 ..1042\nUrban, S.D., 56, 1042\nlJren, S., 438, 1037\nUthurusamy, R., 924, 1006,\n1018\nValdes, .I., 1020, 1001\nValduriez, P., 691, 771, 1033,\n1038\nValentin, G., 691, 1042\nVan Emden, M., 844, 1042\nVan Gelder, A., 844....845, 1032,\n1042\nVan Gucht, D., xxix, 129, 816,\n887, 1007, 1035\nVan Rijsbergen, C.J., 966, 1042\nVance, B., 816, 1011\nVandenberg, S.1., xxxi,\n815~-.g16, 1011, 1040\nVardi, M.Y., 98, 648, 1021,\n1042\nVaughan, B., 438, 1037\nVe lez, B., 1043\nVelez, B., 966\nVerkamo, A.I., 924 925, 1006,\n1030\nVianu, V., 24, 98, 648, 8Ui,\n844, 967, 1005, 1001\nVidal, M., 56, 1012\nVieille, 1., 816, 844~845, 1019,\n1042\nViswanathan, S., 1025, 1001\nVitter, J.S., 888\nVon Bultzingsloewen, G., 516,\n1042, 1001\nVon Halle, B., 691, 1019\nVossen, G., 24, 548, 10421043\nVu, Q., 924, 1039\n\\Vade, B.W., 98, 180, 602, 722,\n1007,\n1012-~lOl:3, 1022,\n1028\n\\Vade, N., 966, 1042\nWagner, R.E., :369, 1042\n\\Vah, 13.\\V., 887, 1014\nWalch, G.,\n~n7, 816, 1028\n\\Valker, A., 771, 845, 1007,\n1.()!'l:3\n\\VaUrath, M., a:n l 81G, 1028\n\\\"'ang, .J., 887, 1014\n\\Vang, K., 967, 104:.3\n'Nang, M., xxxii, 888\n\\Vang, X.S., 771, 1042\n\nIi [fTH()R LNDE.:X;\n'Nang: H., 888 l 102:3\nWard, K., 516, 1021\n\\Varren, D.S., 844-845, 1030,\n10:35, 10:37, 1041\n\\Vatson, V., 98, 1007\n\\Neber, R., 991, 1043\n\\Veddell, G.E., 648, 1043\n\\Vei, J.,\n10~~9\n\\Veihl, \\V., 602, 1043\n\\Veikum, G., 3:37, 548, 815,\n1034, 1037, 1043\nWeiner, J., 967, 1032\n\\Veinreb, D., 815, 1028\n\\Veiss, R., 966, 1043\nWenger, K., 1029, 1001\nWest, IVI., 520\nWhitaker, M., xxxii\nWhite, C., 771, 104:3\nWhite, S., 219, 1043\nWhite, S.J., 815, 1011\nWidom, J., 24, 99, 181, 771,\n887---888, 967, 1006-1007,\n1012, 1020--·1021, 1030,\n1033·1034,\n1043~1044\nWiederhold, G., 24, xxix, 30:3,\n:337, 771, 887, 1020, 1031,\n1034, 1043\nWilkinson, W.K., 438, 477,\n578, 1008, 1027\nWillett, P., 966, 1025\nWilliams, R., 771, 1043\nWilms, P.P., 771, 815, 1022,\n1043\nWilson, L.O., 924, 1038\n\\Vinuner, 1'v1., 925\n\\Vimmers, E.L., 925, 1006\n\\Vinslett, M.S., 99, 722, 10:39,\n104:3\n\\iViorkowski, G., 691, 1043\n\\Vise, T.E., :3:n, 1008\n\\Vistrand, E., 1006, 1001\n\\Vitten, LH., 924, 966, 104:3\nWoelk, D., 815, 1026\nvVolfson, 0., 771, 991, 1024,\n1043, 1001\n\\Yong, C.Y., 925, 1014\nvVong, E., 516, 771, 1009, 1017,\n1036, 1043\nWong, H.K.T., 516, 1020\nWong, L., 816, 1011\n'Nang, W., 548, 1009\nWood, D., 477, 1016\nWoodruff, A., 1006, 1001\nWright, F.L., 649\nWu, J., 816, 1026\nWylie, K., 888, 1007\nXu, E., 991, 1043\nXu, X., 925, 1017, 1037\nYajima, S., 771, 1025\nYang, D., 56, 99, 1041\nYang, Y., 924, 1043\nYannakakis, Iv!., 516, 1036\nYao, S.B., 578, 1028\nYin, Y., 924, 1023\nYoshikawa, Iv!., 771, 816,\n1024---1025\nYossi, 11., 887---888\nYost, R.A., 98, 771, 1012, 104:3\nYoung, H.C., 4:38, 1029\nYoussefi, K., 516, 104:3\nYuan, L., 816, 10:3:3\nYa, C.T., 771, 104:31044\n'{u, J-B., m.n, 1021, 10:34, 1001\nVue, K.B., xxxi\nYurttas, S.: xx..:xi\nZaiane, O.R., 924,\n104~3\nZaki, IvLJ., 924, 1044\nZaniolo, C., 98, 181: 516, 648,\n816, 844--·--845, 1014, 1027,\n1036, 1044, 1001\nZaot, :M., 925, 1006\nZdonik, S.B., x,xix, 516, 816,\n888, 925, 1031, 1038, 1040,\n1044\nZhang, A., 771, 1017\nZhang, T., 925, 1044\nZhang, VV., 1044\nZhao, W., 1040, 1001\nZhao, Y., 887, 1044\nZhou, J., 991, 1043\nZhuge, Y., 887, 1044\nZiauddin, M., xxxi\nZicari, R., 181, 816, 844, 1044,\n1001\nZilio, D.C., 691, 1042\nZloof, Iv1.M., x,xix, 98, 1044\nZobel, J., 966, 1031, 1044\nZukowski, U., 844, 1044\nZuliani,\n~1., 691, 1042\nZwilling, M.J., 815, 1011\n\nSUBJECT INDEX\nINF, 615\n2NF, 619\n2PC, 759, 761\nblocking, 760\nwith Presumed Abort, 762\n2PL, 552\ndistributed databases, 755\n3NF, 617, 625, 628\n3PC, 762\n4NF, 636\n5NF, 638\nA priori property, 893\nAbandoned privilege, 700\nAbort, 522--523, 533, 535, 583,\n593, 759\nAbstract data types, 784--785\nACA schedule, 530\nAccess control, 9, 693-694\nAccess invariance, 569\nAccess mode in SQL, 538\nAccess path, 398\nmost selective, 400\nAccess privileges, 695\nAccess times for disks, 284, 308\nACID transactions, 521\nActive databases, 132, 168\nAdding tables in SQL, 91\nAdorned program, 839\nADTs, 784--785\nencapsulation, 785\nstorage issues, 799\nAdvanced Encryption Standard\n(AES), 710\nAES, 710\nAggregate functions in\nORDBMSs, 801\nAggregation in Datalog, 8:31\nAggregation in SQL, 151, 164\nAggregation in the ER model,\n:39,84\nAlgebra\nrelational, 102\nALTER, 696\nAlternatives for data entries in\nan index, 276\nAnalysis phase of recovery, 580,\n588\nANSI, 6, 58\nAPI, 195\nApplica.tion architectures, 2:36\nApplication programmers, 21\nApplication programming\ninterface, 195\nApplication servers, 251, 253\nArchitecture of a DBIvIS, 19\nARIES recovery algorithm,\n543, 580, 596\nArmstrong's Axioms, 612\nArray chunks, 800, 870\nArrays, 781\nAssertions in SQL, 167\nAssociation rules, 897, 900\nuse for prediction, 902\nwith calendars, 900\nwith item hierarchies, 899\nAsynchronous replication, 741,\n750--751, 871\nCapture and Apply, 752---753\nchange data table (CDT), 753\nconflict resolution, 751\npeer-to-peer, 751\nprimary site, 751\nAtomic formulas, 118\nAtomicity, 521-522\nAttribute, 11\nAttribute closure, 614\nAttributes in the ER model, 29\nAttributes in the relational\nmodel, 59\nAttributes in XNIL, 229\nAudit trail, 715\nAuthentication, 694\nAuthorities, 941\nAuthorization, 9, 22\nAuthorization graph, 701\nAuthorization ID, 697\nAutocommit in JDBC, 198\nAVC set, 909\nAVG, 151\nAvoiding cascading aborts,\n5;~0\nAxioms for FDs, 612\nB+ trees, 281, 344\nbulk-loading, :360\ndeletion, :352\nfor sorting, 4:33\nheight, ;3115\ninsertion,\n:.~48\nkey compression, :358\nlocking, 561\norder,\n;~45\n1054\nsearch, 347\nselection operation, 442\nsequence set, 345\nB+ trees vs.\nISA~1, 292\nBags, 780, 782\nBase table, 87\nBCNF, 616, 622\nBell-LaPadula security model,\n706\nBenchmarks, 506, 683, 691\nBinding\nearly vs. late, 788\nBioinformatics, 999\nBIRCH, 912\nBirth site, 742\nBit-sliced signature files, 939\nBitmap indexes, 866\nBitmapped join index, 869\nBitmaps\nfor space management, 317,\n328\nBlind writes, 528\nBLOBs, 775, 799\nBlock evolution of data, 916\nBlock nested loops join, 455\nBlocked I/O, 430\nBlocking, .5:3:3, 865\nBlocks in disks, 306\nBloomjoin, 748\nBoolean queries, 929\nBounding box, 982\nBoyce-Codd nonnal form, 616,\n622\nBuckets, 279\nin a hashed file, :371\nin histograms, 486\nBuffer frame, 318\nBuffer management\nDBMS VS. OS, 322\ndouble bufl'ering, 432\nforce approach, 541\nreal systems, :322\nreplacernent policy, :321\nsequential flooding, ;321\nsteal approach, 541\nBuffer m,luager, 20, :305, :318\nforcing a page, :323\npage replacement, :ng·320\npinning, ;U9\nprefetching, :322\n\nSUBJECT INDEX\nBuffer pool, ~n8\nBuffered writes, 571\nBuilding phase in hash join, 46:3\nBulk data types, 780\nBulk-loading 13+ trees, 360\nBushy trees, 415\nCaching of methods, 802\nCADjCA:M, 971\nCalculus\nrelational, 116\nCalendric a..ssociation rules, 900\nCandidate keys, 29, 64, 76\nCapture and Apply, 752\nCardinality of a relation, 61\nCartsian product, 105\nCASCADE in foreign keys, 71\nCascading aborts, 530\nCascading operators, 488\nCascading Style Sheets, 249\nCatalogs, 394--395, 480, 483,\n741\nCategorical attribute, 905\nCentralized deadlock detection,\n756\nCentralized lock management,\n755\nCertification authorities, 712\nCGI, 251\nChained transactions, 536\nChange data table, 753\nChange detection, 916·--917\nCharacter large object, 776\nCheckpoint, 19, 587\nfuzzy, 587\nCheckpoints, 543\nChecksum, 307\nChoice of indexes, 653\nChunking, 800, 870\nClass hierarchies, :37, 8:3\nClass interface, 806\nClassification, 904-905\nClassification rules, 905\nCla.'ssification trees, 906\nClearance, 706\nClient-server architecture, 2:37,\n738\nCL013, 776\nClock, 322\nClock policy,\n~321\nClose an iterator, 408\nClosure of 1\"Ds, 612\nCLR..s, 584, 592, 596\nClustered file, 277\nClustered files, 287\nClustering, 277,\n29:~, 660, 911\nCODASYL, D.B.T.G., 1014\nCollations in SQL, 140\nCollection hierarchies, 789\nCollection hierarchy, 789\nCollection types, 780\nCollisions, :379\nColumn, 59\nCommit, 523, 535, 58:3, 759\nCommit protocols, 751, 758\n2PC, 759, 761\n3PC,762\nCommunication costs, 7:39, 744,\n749\nCommunication protocol, 223\nCompensation log records, 584,\n592, 596\nComplete axioms, 613\nComplex types, 779, 795\nvs. reference types, 795\nComposite search keys, 295,\n297\nCompressed histogram, 487\nCompression in B+ trees, 358\nComputer aided design and\nmanufacturing, 971\nConcatenated search keys, 295,\n297\nConceptual design, 13, 27\ntuning, 669\nConceptual evaluation strategy,\n133\nConceptual schema, 13\nConcurrency, 9, 17\nConcurrency control\nmultiversion, 572\noptimistic, 566\ntimestamp, 569\nConcurrent execution, 524\nConflict equivalence, 550\nConflict resolution, 751\nConflict serializability vs.\nserializability, 561\nConflict serializable schedule,\n550\nConflicting actions, 526\nConjunct, 445\nprimary, :399\nConjunctive normal form\n(CNF), :398, 445\nConnection pooling, 200\nConnections in .IDBC, 198\nConservative 2PL, 559\nConsistency, 521\nContent types in X1vlL, 2:32\nContent-ba\",sed queries, 972, 988\nConvoy phenomenon, 555\nCookie, 259\nCookies,\n2,5~3\nCoordinator site, 758\nCorrelated queries, 147, 504,\n506\nCosine normalization, 9:32\nCost estirnatioIl, 48248:3\n1055\n~\nfor ADT methods. 803\nreal systems, 485\nCost model, 440\nCOUNT, 151\nCovering constraints, 38\nCovert channel, 708\nCrabbing, 5fi2\nCrash recovery, 9, 18, 22, 541,\n580,\n583~584, 587--588,\n590, 592, 595-596\nCrawler, 9:39\nCREATE DOfvlAIN, 166\nCREATE statement\nSQL, 696\nCREATE TABLE, 62\nCREATE TRIGGER, 169\nCREATE TYPE, 167\nCREATE VIEW, 86\nCreating a relation in SQL, 62\nCritical section, 567\nCross-product operation, 105\nCross-tabulation, 855\nC8564 at Wisconsin, xxviii\nCSS,249\nCUBE operator, 857, 869, 887\nCursors in SQL, 189, 191\nCylinders in disks, 306\nDali, 1001\nData definition language, 12\nData Definition Language\n(DDL), 12, 62, 131\nData dictionary, 395\nData Encryption Standard, 710\nData Encryption Standard\n(DES), 710\nData entries in an index, 276\nData independence, 9, 15, 74:3\ndistributed, 736\nlogical, 15, 87, 7:36\nphysical, 15, 736\nData integration, 995\nData fvlanipulatioll Language,\n16\nData ivianiplliation Language\n(D1vlL), 131\nData mining, 7, 849, 889\nData model, 10\nmultidimensional, 849\nsernantic, 10, 27\nData partitioning, 7:30\nskew, 7:30\nData reduction, 747\nData skew, 7:30, 73:3\nData source, 195\nData streams, 916\nData striping in RAID, 309 -:310\nData sllblanguage, 16\nData warehouse, 7, 678, 754,\n848, 870871\n\n1056\ndean, 871\nextract, 870\nload, 871\nmetadata, 872\npurge, 871\nrefresh. 871\ntransform, 871\nDatabase administrators, 21-·22\nDatabase architecture\nClient-Server VS.\nCollaborating Servers, 738\nDatabase design\nconceptual design, 13, 27\nfor an ORDBivIS,\n79~J\nfor OLAP,\n85~3\nimpact of concurrent access,\n678\nnormal forms, 615\nnull values, 608\nphysical, 291\nphysical design, 14, 28, 650\nrequirements analysis step, 26\nrole of expected workload, 650\nrole of inclusion dependencies,\n639\nschema refinement, 28, 605\ntools, 27\ntuning, 22, 28, 650, 667, 670\nDatabase management system,\n4\nDatabase tuning, 22, 28, 650,\n652, 667\nDataba...,es, 4\nDataflow for parallelism,\n7~31,\n733\nDataguides, 959\nDatalog, 818-·..·819, 822\naggregation, 8:31\ncomparison with relational\nalgebra, 830\ninput and output, 822\nleast fixpoint, 825-826\nlea..'3t rnodel, 824, 826\nmodel, 82:3\nrnultiset generation, 8:32\nnegation, 827·828\nrange-restriction and\nnegation, 828\nrules, 819\nsafety and range-restriction,\n826\nstratification, 829\nDataSpace, lOCH\nDates and times in SQL, 140\nDB2\nIndex Advisor, 665\nDBA. 22\nD BI Iibrary, 2.52\nDBMS. 4\nDBrviS architecture, 19\nDBl\\IS vs. as. :322\nDDL,12\nDeadlines\nhard VS. soft, 994\nDeadlock, 5:.n\ndetection, 556\ndistributed, 756\nglobal VS. local, 756\nphantom, 757\nprevention, 558\nDecision support, 847\nDecision trees, 906\npruning, 907\nsplitting attributes, 907\nDecompositions, 609\ndependency-preservation, 621\nhorizontal, 674\nin the absence of redundancy,\n674\ninto 3NF, 625\ninto BCNF, 622\nlossless-join, 619\nDecorrelation, 506\nDecryption, 709\nDeductions, 820\nDeductive databases, 820\naggregation, 831\nfixpoint semantics, 824\nleast fixpoint, 826\nleast model, 826\nleast model semantics, 82:3\n:Nlagic sets rewriting, 838\nnegation, 827·-828\noptimization, 834\nrepeated inferences, 834\nSeminaive evaluation, 836\nunnecessary inferences, 834\nDeep equality, 790\nDenormalization, 652, 6E)9, 672\nDependency-preserving\ndecomposition, 621\nDependent attribute, 904\nDES, 710\nDeskstar disk, 308\nDEVise, 1001\nDifference operation, 105, 141\nDigital Libraries project, 997\nDigital signatures, 71:3\nDimensions, 849\nDirectory\nof pages, :326\nof slots, :329\nDirectory doubling, :175\nDirty bit, :.H8\nDirty page table, 585, 589\nDirty read, 526\nDiscretionary access control,\n695\nSUBJECT INDF~\nDisjunctive selection condition,\n445\nDisk array,\n~309\nDisk spa.ce manager, 21, :304,\n316\nDisk tracks, 30ti\nDisks, :305\naccess times, 284, 308\nblocks, ;306\ncontroller, 307\ncylinders, tracks, sectors, :306\nhead, 307\nphysical structure, 306\nplatters, 306\nDistance function, 911\nDistinct type in SQL, 167\nDistributed data independence,\n736, 743\nDistributed databases, 726\ncatalogs, 741\ncommit protocols, 758\nconcurrency control, 755\ndata independence, 743\ndeadlock, 756\nfragmentation, 739\nglobal object names, 742\nheterogeneous, 737\njoin, 745\nlock management, 755\nnaming, 741\noptimizatioIl, 749\nproject, 744\nquery processing, 743\nrecovery, 755, 7,58\nreplication, 741\nscan, 744\nselect, 744\nSemijoin and Bloomjoin, 747\nsynchronous vs. asynchronous\nreplication, 750\ntransaction management, 755\ntransparency, 7:36\nupdates, 750\nDistributed deadlock, 756\nDistributed query processing,\n743\nDistributed transaction\nrnanagement I 755\nDistributed transactions, 73G\nDivision, 109\nin SQL, 150\nDivision operation, 109\nDl\\fL, 16\nDocument type declarations\n(DTDs),2:31\nDocurncnt vector, 9:30\nDoD security levels, 708\nDomain, 29, 59\n\nSU:BJECJ\n1 INDEX\nDomain constraints, 29, Gl, 7:3,\n166\nDomain relational calculus, 122\nDomain-key normal form, 648\nDouble buffering, <1:32\nDrill-down, 854\nDriver, 195·-196\nmanager,\n195~196\ntypes, 196\nDROP, 696\nDropping tables in SQL, 91\nDTDs, 231\nDuplicates in an index, 278\nDuplicates in SQL, 1:36\nDurability, 521--522\nDynamic databases, 560\nDynamic hashing, 373, 379\nDynamic indexes, 344, 373, :379\nDynamic linking, 786\nDynamic SQL, 194\nEarly binding, 788\nElectronic commerce, 221\nElements in X]\\;IL, 228\nEmbedded SQL, 187\nEncapsulation, 785\nEncryption, 709, 712\nEnforcing integrity constraints,\n70\nEntities, 4, 13\nEntity references in XML, 229\nEntity sets in the ER model, 28\nEnumerating alternative plans,\n492\nEquality\ndeep vs. shallow, 790\nEquality selection, 292\nEquidepth histogram, 487\nEquijoin, 108\nEquivalence of relational\nalgebra expressions, 414\nEquiwidth histogram, 487\nER model\naggregation, 39, 84\nattribute domains, 29\nattributes, 29\nclass hierarchies, :37, 8:3\ndescriptive attributes, 30\nentities and entity sets, 28\nkey constraints, :32:33\nkeys, 29\noverlap and covering, :38\nparticipation constraints, 34,\n79\nEH. rnodel\nrelationships\nand relationship sets, 29\nmany-to-many, :3:3\nmany-to-one, :t3\none-to-many, :.3:3\nroles, :32\nweak entities, :35, 82\nERP, 7\nEvent handler, 247\nEvents activating triggers, 168\nExample queries\nQ1, 110, 120, 123, 137, 145,\n147, 154\nQ2, 112, 120, 12:3, 1:39, 146\nQ3, 11:3, 139\nQ4, 113, I:N\nQ5, 113, 141\nQ6, 114, 142, 149\nQ7, 115, 121, 123\nQ8, 115\nQ9, 116, 121, 124, 150\nQ10, 116\nQ11, 117, 12:3, 135\nQ12, 119\nQ13, 120\nQ14, 121, 124\nQ15, 134\nQ16, 138\nQ17, 140\nQ18, 140\nQ19, 143\nQ20, 144\nQ21, 146\nQ22, 148\nQ23, 148\nQ24, 149\nQ25, 151\nQ26, 151\nQ27, 152\nQ28, 152\nQ29,\n15~3\nQ30, 153\nq:n, 154\nQ32, 155\nQ33, 158\nQ34, 159\nQ35, 160\nQ3G, 160\nQ37, 161\nExclusive locks,\n5~n\nEXEC SQL, 187\nExecution plan, 19\nExpensive predicates, 804\nExploratory data analysis, 849,\n890\nExpressions in SQL, L39, 16::J\nExpressiv€~ power\nalgebra VS. calculus, 124\nExtendible hashing, ::173\ndirectory doubling, 375\nglobal depth, ::376\nlocal deptlL 377\nExtensibility\nin an optimizer, 80:3\nlO5t7\nindexing ne,'\" types, 800\nExtensible Markup Language\n(XtvIL), 228, 231...··232\nExtensible Style Language\n(XSL), 228\nExternal schema, 14\nExternal sorting, 422, 424, 428,\n4::30. 4:32, 732\nFailure\nmedia, 541, 580\nsystem crash, 541, 580\nFalse positives, 938\nFan-out, 282,\n~345, 358-359\nFeature vectors, 970, 972\nField, 59\nFIFO, 322\nFifth normal form, 638\nFile, 20\nof records, 275\nFile organization, 274\nclustered, 287\nhashed, 279\nindexed, 276\nrandom, 284\nsorted, 285\ntree, 280\nFirst in first out (FIFO) policy,\n321\nFirst normal form, 615\nFixed-length records, 327\nFixpoint, 824\nNaive evaluation, 835\nSeminaive evaluation, 836\nFixpoint evaluation\niterations, 834\nFbrce vs. no-force, 586\nForce-write, 583, 759\n:Forced reinserts, 985\nForcing pages,\n:~23, 541, 583\nForeign key constraints, 6Ei\nForeign keys, 76\nversus aids, 796\nFormulas, 118\nFourth normal form, 6:36\nFragmentation, 7:39,··740\nFrequent itemsets, 89:3\na priori property, 893\nFully distributed lock\nmanagement, 756\nFunctional dependencies, 611\nArmstrong's Axioms, 612\nattribute closure, 614\nclosure, 612\nminimal cover, 625\nprojecti0I1; 621\n.Fuzzy checkpoint, 587\nGateways, 737\nGenBank, 997\nGeneralization,\n:~8\n\n1058\nGeneralized Search Tn..'es, 987\nGeographic Information\nSystems (GIS), 971, 998\nGet next tuple t 408\nGiST, 801, 987\nGlobal deadlock detection, 756\nGlobal depth in extendible\nha.shing, 376\nGRANT OPTION, 696\nGRANT statement\nSQL, 695, 699\nGranting privileges in SQL, 699\nGrid directory, 978\nGrid files, 978\nconvex regions, 981\nGroup commit, 996\nGrouping in SQL, 154\nHash functions, 279, 372, 379,\n735\nHash indexes, 279\nHash join, 463\nparallel databa.ses,\n733~~734\nHash partitioning, 730\nHashed files, 279\nHeap files, 20, 276, 284, 324\nHeight of a tree, 282, 345\nHeterogeneous databases, 737\ngateways, 737\nHierarchical clustering, 912\nHierarchical data model, 6\nHierarchical deadlock\ndetection, 757\nHistograms, 485,--486\ncompressed, 487\nequidepth, 487\nequiwidth, 487\nreal systems, 485\nHorizontal decomposition, 674\nHorizontal fragmentation,\n739~-740\nHost language, 16, 187\nHot spots, 535, 674, 678, 680\nHT~/lL, 226, 228, 1001\ntags, 226\nHTf,,1L Fonus, 242\nHTTP\nabsence of state, 258\nrequest, 224\nresponse, 224\nHTTP protocol, 223\nHubs, 941\nHmnan Genome project, 997\nHybrid ha..sh join, 465\nHyperText Markup Language\n(HTML), 226, 228\nIBlvI DB2, 167,\n~322,,32:3, :327,\n331, 333, 357, 359. 422,\n446, 45245:3, 485, 496,\n500, 506, 573, 582, 709,\n776, 780, 790, 818, 869,\n882\nIceberg queries, 896\nIdentifying owner, 36\nIDS, 6\n1mplementation\naggregation, 469\njoins, 455, 457..,·458, 465\nhash,\n46~3\nnested loops, 454\nprojections, 447--449\nhashing, 449\nsorting, 448\nselections, 401, 441---442,\n444---446\nwith disjunction, 446\nB+ tree, 442\nhash index, 444\nno disjunction, 445\nno index, 401, 441~442\nset-operations, 468\nIMS, 6\nInclusion dependencies, 639\nIncremental algorithms, 403\nIndex, 14, 276\nduplicate data entries, 278\nalternatives for data entries,\n276\nB+ tree, 344\nbitmap, 866\nclustered VB. unclustered, 277\ncomposite key, 295\nconcatenated key, 295\ndata entry, 276\ndynamic, 344, 373, 379\nequality query, 295\nequality vs. range selection,\n292\nextendible hashing, 373\nfan-out, 282\nh&,,;h, 279,\n~371\nbuckets, :371\nha.sh functions,\n~372\nprimary and overflow pages,\n371\nin SQL, 299\nISAM, 341\nlinear haBhing, :379\nmatching a selection, 296, 398\nmultidimensional, 97:3\nprimary VS. secondary, 277\nrange queries and composite\nkey indexes, 295\nspatial, 97:3\nstatic, :,341\nstatic hashing, :371\ntree, 280\nunclustered, 288 289\nunique, 278\nSlJBJECT INl)EOC\nIndex advisor, 66:3\nIndex configuration, 663\nIndex entries, 3:39\nIndex locking, 561\nIndex nested loops join, 402,\n457\nIndex selection, 65:3\nIndex tuning, 667\nIndex-only evaluation, 293, 402\nIndex-only plans, 662\nIndex-only scan, 452, 471, 495\nIndexes\nchoice, 291\nIndexing new data types, 800\nInference and security, 715\nInferences, 820\nInformation retrieval, 927\nInformix, 322-323, 327, 331,\n:333, 359, 422, 446,\n452~453, 485, 500, 506,\n573, 582, 709, 776, 780,\n866, 869\nInformix DDS, 167, 790\nInheritance hierarchies, 37, 83\nInheritance in object\ndatabases, 787\nInheritance of attributes, 37\nInsertable-into views, 89\nInstance of a relation, 59\nInstance of a relationship set,\n30\nIntegration, 995\nIntegrity constraints, 9, 12, 32,\n34, 38, 6:3, 79\nin SQL, 167\nspatial, 971\ndomain, 61, 73\nforeign key, 66\nin SQL, 165--·-166\nkey, 64\ntransactions in SQL, 72\nIntelligent Miner, 914\nInterface for a class, 80l)\nInterference, 728\nInternet databases, 7\nInterprocess communication\n(IPC), 802\nIntersection operation, 104, 141\nInverse document frequency\n(IDF), 9:31\nInverted indexes,\n~):35\nISA hierarchies, :37, 899\nISAtvl, 292, 341\nISO, 6, 58\nIsolation, 521\nIsolation level. 199\nIsolation level in SQL, 5:38\nREAD UNCO?vlMITTED.\n5:39\n\nSUBJECT INDEX\nREPEATABLE READ, 5:39\nSERlALIZABLE,\n5~39\nItemset,\n89~3\na priori property, 893\nfrequent, 893\nsupport,\n89~3\nIterations, 834\nIterator interface, 408\nIVEE, 1001\nJava\nservlet, 254\nJava Database Connectivity\n(JDBC), 195, 219, 737,\n870\nJava virtual machine, 786\nJ avaScript, 245\nJDBC, 195, 198, 219, 737, 870\narchitecture, 196\nautocommit, 198\nconnection, 198\ndata source, 196\nDatabase.lVletaData class, 205\ndriver management, 198\ndriver manager, 195-196\nExceptions, 203\nPreparedStatement class, 200\nResultSet class, 201\nWarnings, 203\nJDBC URL, 198\nJDs, 638\nJoin dependencies, 6:38\nJoins, 107\nBloomjoin, 748\ndefinition, 107\ndistributed databases, 745\nequijoin, 108\nimplementation, 454, 463\nblock nested loops, 455\nhybrid hash, 465\nindex nested loops, 457\nsort-nwrge, 458\nnatural join, 108\nouter, 164\nparallel databases, 732, 7:34\nSernijoin, 747\nKDD, 891\nKey, 29, 6ll\nKey compression, :358\nKey constraints, :32·<n\nKeys\ncandidate, 64, 76\ncandidate vs. search, 280\ncomposite search, 295\nforeign, 76\nforeign key, 6G\nprirnary, 65\nKeys constraints,\n64~-65\nKnowledge discovery, 890\nLarg(} object, 776\nLastLSN I 585\nLatch, 555\nLate binding, 788\nLeast fixpoints, 822, 825\nLeast model = least fixpoint,\n826\nLeast models, 822, 824\nLeast recently used (LRU)\npolicy, :321\nLeft-deep trees, 415\nLegal relation instance, 6:3\nLevel counter in linear ha..\"hing,\n~379\nLevels of abstraction, 12\nLexicon, 935\nLinear hashing, 379\nfamily of hash functions, 379\nlevel counter, 379\nLinear recursion, 831\nLinear scales, 979\nLOB, 776\nLocal deadlock detection, 756\nLocal depth in extendible\nhashing, 377\nLocators, 776\nLock downgrades, 556\nLock escalation, 566\nLock manager, 21, 554\ndistributed databases, 755\nLock upgrade, 555\nLock-coupling, 562\nLocking, 18\ndowngrading, 556\nB+ trees, 561\nconcurrency, 678\nConservative 2PL, 559\ndistributed databases, 755\nexclusive locks, 531\nlock escalation, 566\nlock upgrade, 555\nIIlultiple-granularity, 564\nperformance implications, 678\nshared locks, 5:n\nStrict 2PL, 5:31\nupdate locks, 556\nLocking protocol, 18, 5:30\nLog, 18, 522, 542, 582\nabort record,\n58~3\ncornrnit record, 58:3\ncompensation record (CLR),\n58:3\nend record, 58:3\nforce-write, 58:3\nlastLSN, 585\npageLSN, 582\nsequence number (LSN), 582\ntail, 582\nupdate record format, 58:.3\nWAL, 18\nlO~9\nLog record\nprevLSN field, 583\ntransID field, 583\ntype field, 583\nLog-based Capture, 752\nLogical data independence, 15,\n87, 7:36\nviews, 15\nLogical schema, 13, 27\nLossless-join decomposition,\n619\nLost update, 529\nLRU, :322\nMachine learning, 890\n.lViagic Sets, 506\nMagic sets, 837-838\nMain memory databases, 996\nNlandatory access control, 695\nobjects and subjects, 706\nlVlany-to-many relationship, 33\nMany-to-one relationship, 33\nMarket basket, 892\nMarkup languages, 226\nNlaster copy, 751\nMa.\"ter log record, 587\nMatching index, 398\nNlatching pha.\"e in hash join,\n463\nMaterialization of intermediate\ntables, 407\nMaterialization of views, 874\nMaterialized views\nrefresh, 876\nMathNIL, 2:35\nMAX, 151\n.Nlean-time-to-failure, 311\nIvleasures, 849\n:tY1edia failure, 541, 580, 595\nlVledia recovery, 595\nMedical imaging, 971\nlVlelton\n,1.,781\n:MeIIlory hierarchy, :305\nIVlerge operator, 731\nMerge sort, 424\nlVletadata, :.394, 872\nI'vlethods\ncaching, 802\ninterpreted VB. compiled, 802\nsecurity, 801\n.lVlicrosoft SQL Server, :322-<32:3,\n:327, :3::n, :3::3:3, :357, :359,\n422,\n446·~·447, 452--453,\n485, 49G, 500, 506, 57:3,\n582, 665, 709, 776, 8G6,\n869, 882\nI\\IIN, 151\nlV1ineset, 1001\nMiniba..se software, 1002\n\n1060\nMinimal cover, 625\nI'vIirroring in RAID,\n~n~3\nLvlobile databases, 995\nl\\'Iodel, 82:.3\nrVlodel maintenance. 916\nl'vlodifying a table in SQL, 62\nMOLAP,850\nj'viost recently llsed CMRU)\npolicy, 321\nlv'IRP, 7\n.MRU, :322\nl\\ilultidataba..se system, 737\n.lVlultidimensional data model,\n849\nMultilevel relations, 707\nIv1ultilevel transactions, 994\nMultimedia databases, 972, 997\nMultiple-granularity locking,\n564\nMultiple-query optimization,\n507\nMultisets, 135, 780, 782\nMultivalued dependencies, 634\nMultiversion concurrency\ncontrol, 572\nMVDs,634\nNaive fixpoint evaluation, 835\nNamed constraints in SQL, 66\nNaming in distributed systems,\n741\nNatural join, 108\nNatural language searches, 930\nNearest neighbor queries, 970\nNegation in Datalog, 828\nNegative border, 919\nNested collections, 783, 798\nNested loops join, 454\nNested queries, 145\nimplementation, 504\nN estf~d relations\nnesting, 784\nullIwsting, 78:3\nNested transactions, 535, 994\nNesting operation, 784\nNetwork data model, 6\nNO AC'rION in foreign key\"s,\n71\nNon-preemptive deadlock\nprevention, 559\nNonblocking algorithms, 865\nNonblocking comrnit protocol,\n76:3\nNolIVolatile storage, :306\nNormal forms, 615\nINF,615\nBC~NP, 616\n2NF,619\n:3NF, 617\nSynthesis, 628\n4NF, 6:36\n5NF,6:38\nDKNF,648\nnormalization, 622\nPJNF, 648\ntuning, 669\nNormalization, 622, 652\nNull values, 608\nimplementation, 3:32\nin SQL, 67, 69, 71, 162\nNumerical attribute, 905\nObject databases, 12\nObject exchange model\n(OEj\\iI), 947\nObject identifiers, 789\nObject manipulation language,\n806\nObject-oriented DBl\\iIS, 773,\n805, 809\nObject-relational DBJ.\\!IS, 773,\n809\nODBC, 195, 219, 737, 995\nODL, 805\nODMG data model\nattribute, 805\nclass, 805\ninverse relationship, 805\nmethod, 806\nobjects, 805\nrelationship, 805\nOEM,947\nOids, 789·790\nreferential integrity, 796\nversus foreign keys, 796\nversus URLs, 792\nOLAP, 684, 848· ..-849, 887\ncross-tabulation, 855\ndatab~<;;e design,\n85~~\ndimension table, 852\nfact table, 850\npivoting, 855\nroll-up and drill-down, 854\nSQL window queries, 8f)9\nOLTP, 847\nO?vfL, 80G\nOn-the-fly evaluation, 407\nOlle-to-rnany relationship,\n~3:3\nOne-to-one relationship, :.34\nOne-way' functions, 710\nOnline aggregation, 864\nOnline analytic processing\n(OLAP),848\nOnline transact.ion processing\n(OI:rp), 847\nOODB?vfS vs. ORDBMS. 809\nOpaque types, 785\nOpen an iterator, 408\nOpen Database Connectivity\n(ODBC), 195, 219, 7:37,\nSUBJECT INDEX\n995\nOptimistic concurrency control,\n566\nvalidation, 567\nOptimizers\ncost estimation, 482\nreal systems, 485\ndecomposing a query into\nblocks, 479\nextensibility, 80:3\nfor OHDBMSs, 803\nhandling expensive\npredicates, 804\nhistograms, 485\nnested queries, 504\noverview, 479\nreal systems, 485, 496, 500,\n506\nrelational algebra\nequivalences, 488\nrule-based, 507\nOQL, 805, 807\nOracle, 27, :322-,,323, :327, :331,\n333, 357, 359, 422,\n446··447, 452--453, 485,\n500, 506, 573, 582, 709,\n776, 780, 790, 803, 866,\n869, 882\nORDBIvIS database design, 793\nORDBMS implementation, 799\nORDBMS vs. OODBIVIS, 809\nORDBMS vs. RDB.lV1S, 809\nOrder of a B+ tree, 345\nOuter joins, 164\nOverflow in hash join, 464\nOverlap constraints, 38\nOverloading, 788\nOwner of a weak entity, :36\nPackages in SQL:1999, 131\nPage abstraction, 274, 316\nPage fonnats, 326\nfixed..·length records, 327\nvariable-length records,\n~328\nPage rephtcement policy,\n:.:~18 ..319,\n:~21\nPageLSN, 582\nParadise, 1001\nParaJlel database architecture\nshared-memory VS.\nshared-nothing, 727\nParallel databases, 726·727\nblocking, 729\nbulk loading, 731\ndata partitioning, 729·730\ninterference, 728\njoin, 7:32, 7:l1\nmerge and split, 7:31\noptimization, 7:.35\npipelining, 729\n\nSlfBJEC}T I1VDE)(\nscan, 7:31\nsorting, 732\nspeed-up VS. scale-up, 728\nParameteric query\noptimization, 507\nParity,\n~n1\nPartial dependencies, 617\nPartial participation,\n~34\nParticipation constraints, :34,\n79\nPartition views, 882\nPartitional clustering, 912\nPartitioned parallelism, 729\nPartitioning, 739\nhash VS. range,\n7~34\nPartitioning data, 730\nPartitioning phase in ha...,h join,\n463\nPath expressions, 781, 948\nPeer-to-peer replication, 751\nPerl modules, 252\nPhantom deadlocks, 757\nPhantom problem, 560, 986\nPhantoms, 538, 559\nSQL,538539\nPhysical data independence,\n15, 736\nPhysical database design, 14,\n28, 291, 650\nPhysical design\nchoices, 652\nclustered indexes, 293\nco-clustering, 660\nindex selection,\n65~3\nindex-only plans, 662\nmultiple-attribute indexes,\n297\nnested queries, 677\nquery tuning, 670, 675\nreducing hot spots, 679\nrole of expected workload, 650\ntuning queries, 670\ntuning the choice of indexes,\n667\ntuning the conceptual\nschema, 669\ntuning wizard, 6fj:3, 665\nPhysical schema, 14\nPin count, 318\nPinning pages,\n~n9\nPipelined €~valuation, 407, 4Hl,\n496\nPipelined parallelisrn, 729\nPivoting, 855\nPlatters on disks, ::W6\nPI\\'li\\lL, 891\nPoint data, 969\nl)ointer swizzling, 802\nPolyinstantiation, 708\nPostings file, 935\nPrecedence graph, 551\nPrecision, 934\nPrecommit, 76:3\nPredicate locking, 5tH\nPredictor attribute, 904\ncategorical, 905\nnumerical, 905\nPreemptive deadlock\nprevention, 559\nPrefetching\nreal systems, 323\nPrefetching pages, 322\nPrepare messages, 759\nPresumed Abort, 762\nPrevLSN, 583\nPrimary conjunct in a\nselection, 399\nPrimary copy lock\nmanagement, 755\nPrimary index, 277\nPRH.,,1ARY KEY constraint in\nSQL, 66\nPrimary keys, 29, 65\nin SQL, 66\nPrimary page for a bucket, 279\nPrimary site replication, 751\nPrimary storage, 305\nPrimary vs. overflow pages, 371\nPrivilege descriptor, 701\nProbing phase in hash join, 46:3\nProcedural Capture, 753\nProcess of knowledge discovery,\n891\nProject-join normal form, 648\nProjections, 744\ndefinition, 10:3\nilnplementation, 447\nProlog, 819\nPruning, 907\nPublic-key encryption, 710\nPublish and subscribe, 751\nPushing selections, 409\nQuantifiers, 118\nQuery, 16\nQuery block, 479\nQuery evaluation plan, 405\nQuery language, 16, 7:3, 100\nDatalog, 818..·819\ndomain relational calculus,\n122\nOQL, 807\nrelational algebra, 102\nrelational completeness, 126\nSQL, 130\ntuple relational calculus, 1.17\nXQuery, 948\nQuery rl1odification,\n87;.~\nQuery optirnization, 404, 507\n1061\nit\nbushy trees, 415\ndeductive databases, 8:34\ndistributed databases, 749\nenulneration of alternative\nplans, 492\nleft-deep trees, 415\novervievv, 405, 479\nparallel databases, 735\npushing selections, 409\nreduction factors, 483, 485\nrelational algebra\nequivalences, 488\nrule-ba..\"ed, 507\nSQL query block, 479\nstatistics, 395\nQuery optimizer, 19\nQuery pattern, 838\nQuery processing\ndistributed databases, 743\nQuery tuning, 670\nR trees, 982\nbounding box, 982\nR+ trees, 986\nRAID, 309--310\nlevels, :310\nmirroring, 313\nparity, ~H1\nredundancy schemes, :311\nreliability groups, 312\nstriping unit, 310\nRandomized plan generation,\n507\nRange partitioning, 730\nRange queries, 295, 970\nRange selection, 292\nRange-restriction, 826, 828\nRanked queries, 929\nRaster data, 969\nIlDBJvlS vs. ORDBJvlS, 809\nReal-time databases, 994\nRecall, 934\nRecord formats,\n~3:30\nfixed-length records,\n~3:31\nreal systems, :331,\n::3~n\nvariable-length records,\n:3~31\nRecord id, 275, :327\nRecord ids\nreal systems, :327\nRecords, 11, 60\nRecoverability, 5:30\nRecoverable schedulc l 530. 571\nRecovery, 9, 22, 54:3, 580\nAnalysis phase, 588\nAIUES, 580\ncheckpointing, 587\ncompensation log record, 584\ndistributed databa.\"3cs, 755,\n758\nfuzzy checkpoint, 587\n\n1062\nlog, 18, 522\nloser transactions, 592\nmedia failure, 595\nRedo pha.se, 590\nshadow pages, 596\nthree phases of restart, 587\nUndo phase, 592\nupdate log record,\n58~~\nRecovery manager, 21, 540, 580\nRecursive rules, 818\nRedo phase of recovery, 580,\n590\nReduction factor, 400\nReduction factors, 483, 485\nRedundancy and anomalies,\n607\nRedundancy in RAID, 309\nRedundancy schemes, 311\nReference types, 795\nReference types in SQL:1999,\n790\nReferential integrity, 70\nin SQL, 70\noids, 796\nviolation options, 70\nRefreshing materialized views,\n876\nRegion data, 970\nRegression rules, 905\nRegression trees, 906\nRelation, 11, 59\ncardinality, 61\ndegree, 61\ninstance, 60\nlegal instance, 63\nschema, 59\nRelational algebra, 103\ncomparison with Datalog,\n8~30\ndivision, 109\nequivalences, 488\nexpression, 102\nexpressive power, 124\njoin, 107\nprojection,\n10~1\nrenaming, 106\nselection, 10:1\nset-operatioIls, 104, 468\nHelationaJ calculus\ndomain, 122\nexpressive power, 124\nsafety, 125\ntuple, 117\nHelational completeness, 126\nRelational data model, 6\nH..elational database\ninstance, 61\nschema, 61\nRelational model, 10, 57\nllelationships, 4, 1;1, 29,\n:~:~\nRenaming in relational algebra,\n106\nRepeating history, 581, 596\nReplacement policy, 318~;n9\nReplacement sort, 428\nReplication, 739, 741\nasynchronous, 741, 750-·751,\n871\nma..ster copy, 751\npublish and subscribe, 751\nsynchronous, 741, 750\nResource managers, 99:3\nResponse time, 524\nRestart after crash, 587\nResult size estimation, 483\nREVOKE statement\nSQL, 699-700\nRevoking privileges in SQL, 700\nRid, 275, 327\nRids\nreal systems, 327\nROLAP, 852\nRole-based authorization, 697\nRoles in the ER model, 32\nRoll-up, 854\nROLLUP, 857\nRoot of an XML document, 231\nRotational delay for disks, 308\nRound-robin partitioning, 730\nRow-level triggers, 170\nRSA encryption, 710\nRule-based query optimization,\n507\nRules in Datalog, 819\nRunning information for\naggregation, 470\nRuns in sorting,\n42~~\nR* trees, 985\nSABRE, 6\nSafe queries, 125\nin Datalog, 826\nSafety, 826\nSampling\nreal systems, 485\nSavepoints, 5:35\nScalability, 890\nScale-up, 728\nScan, 744\nSchedule,\n52~3\navoid ca..\"icading abort, 5:30\nconflict equivalence, 550\nconflict serializahle, 550\nrecoverable, 530, 571\nserial, 524\nserializable, 525, 529\nstrict, 552\nview serializable, 55:3\nSchema, 11, 59, Gl\nSchema decomposition, 609\nSUBJECyr INDEX\nSchema evolution, 669\nSchema refinement, 28, 605\ndenormalizatioIl, 672\nSchema tuning, 669\nSearch key, 276\nSearch space of plans, 492\nSearch term, 928\nSecond normal form, 619\nSecondary index, 277\nSecondary storage, 305\nSecure Electronic Transaction,\n713\nSecure Sockets Layer, 712\nSecure Sockets Layer (SSL),\n223\nSecurity, 22, 694, 696\nauthentication, 694\nclasses, 695, 706\ndiscretionary access control,\n695\nencryption, 712\ninference, 715\nmandatory access control, 695\nmechanisms, 693\npolicy, 693\nprivileges, 695\nstatistical databa..\">es, 715\nusing views, 704\nSecurity administrator, 709\nSecurity levels, 708\nSecurity of methods, 801\nSeek time for disks, 284, :308\nSelection condition\nconjunct, 445\nconjunctive normal form, 445\nterm, 444\nSelection pushing, 409\nSelections, 744\ndefinition, 10;3\nSelectivity\nof an access path, 399\nSemantic data model, 10, 27\nSemantic integration, 995\nSemijoin, 747\nSemijoin reduction, 747\nSerninaive fixpoint evaluation,\n8:36\nSemistructured data, 946, 1001\nSequence data,\n91:~\nSequence of itemsets, 902\nSequence set in a B+ tree, 345\nSequential flooding,\n~321, 472\nSequential patterns, 901\nSerial schedule, 524\nSerializability, 525, 529, 550,\n55:3, 561\nSerializability graph, 551\nSerializahle schedule, 529\nServer-side processing, 254\n\nSUBJECT INDEX\nServlet, 254\nrequest, 255\nresponse, 255\nServlet interface, 255\nSession key, 712\nSession management,\n25~3\nSet comparisons in SQL, 148\nSET DEFAULT in foreign keys,\n71\nSet operators\nimplementation, 468\nin relational algebra, 104\nin SQL, 141\nSET protocol, 713\nSet-difference operation, 105\nSG1/IL, 228\nShadow page recovery, 596\nShallow equality, 790\nShared locks, 531\nShared-disk architecture, 727\nShared-memory architecture,\n727\nShared-nothing architecture,\n727\nSignature files, 937\nSingle-tier architecture, 236\nSkew, 730, 733\nSlot directories, 329\nSnapshots, 753, 882\nSnowflake queries, 869\nSOAP, 222\nSort-merge join, 403, 458\nSorted files, 285\nSorted runs, 423\nSorting, 732\napplications, 422\nblocked I/O, 430\ndouble buffering, 432\nexternal merge sort\nalgorithm, 424\nreplacement sort, 428\nusing B+ trees,\n4~33\nSound axioms, 613\nSpace-filling curves, 975\nSparse columns, 866\nSpatial data, 969\nboundary, 969\nlocation, 969\nSpatial extent, 969\nSpatial join queries, 971\nSpatial range queries, 970\nSpecialization, 38\nSpeed-up, 728\nSplit operator, 731\nSplit selection, 908\nSplitting attributes, 907\nSplitting vector,\n7~32\nSQL\nchained transactions,\n5:~6\naccess mode, 538\naggregate operations, 164\ndefinition, 151\nimplementation, 469\nALL, 148, 154\nALTER,696\nALTER TABLE, 91\nANY, 148, 154\nAS, 139\nauthorization ID, 697\nAVG, 151\nBETWEEN, 657\nCARDINALITY, 781\nCASCADE, 71\ncollations, 140\nCOMMIT, 535\nconformance packages, 131\ncorrelated queries, 147\nCOUNT, 151\nCREATE, 696\nCREATE DO:NIAIN, 166\nCREATE TABLE, 62\ncreating views, 86\nCUBE, 857\ncursors, 189\nholdability, 192\nordering rows, 193\nsensitivity, 192\nupdatability, 191\nData Definition Language\n(DDL), 62, 131\nData Manipulation\" Language\n(D:NIL), 131\nDATE values, 140\nDELETE, 69\nDISTINCT, 133, 136\nDISTINCT for aggregation,\n151\ndistinct types, 167\nDROP, 696\nDROP TABLE, 91\ndynamic, 194\nembedded language\nprogramming, 187\nEXCEPT, 141, 149\nEXEC, 187\nEXISTS, 141, 16:3\nexpressing division, 150\nexpressions, 139, 16:3\ngiving names to constraints,\n66\nGRANT, 695, 699\nGRANT OPTION, 696\nGROUP BY, 154\nHAVING, 154\nIN, 141\nindexing, 299\nINSERT, 52, 69\ninsertable-into views, 89\n1003\nSQL\nintegrity constraints\nassertions, 69, 167\nCHECK, 165\ndeferred checking, 72\ndomain constraints, 166\neffect on modifications, 69\nPRIMARY KEY, 5t)\ntable constraints, 69, 165\nUNIQUE, 66\nINTERSECT, 141, 149\nIS NULL, 163\nisolation level, 538\nrvIAX, 151\n:tvlIN, 151\nmultisets, 135\nSQL\nnested subqueries\ndefinition, 145\nimplementation, 504\nNO ACTION, 71\nNOT, 136\nnull values, 67, 69, 71, 162\nORDER BY, 193\nouter joins, 164\nphantoms, 538~539\nprivileges, 695\nDELETE, 696\nINSERT, t)96\nREFERENCES, 696\nSELECT, 695\nUPDATE, 696\nquery block, 479\nREAD UNCOMMITTED,\n539\nSQL\nreferential integrity\nenforcement, 70\nREPEATABLE READ, 539\nREVOKE, 699~700\nCASCADE, 700\nROLLBACK, 535\nROLLUP, 857\nsavepoints, 535\nsecurity, 696\nSELECT-FROM-\\VHERE,\n1:3:l\nSERIALIZABLE, 539\nSOrvIE, 148\nSQLCODE, 191\nSQLERROR, 189\nSQLSTA'I'E, 189\nstandardization, 58\nstandards, 180\nstrings, 139\n8l.r1\\1, 151\ntransaction support, 535\ntransactions and constraints,\n72\n\n1064\nUNION, 141\nUNI(~UE,\n16~1\nupdatable views, 88\nUPDATED,\n6~1, 69\nvie\\-\\'\" updates, 88\nviews, 90\nSQL Server\ndata mining, 914\nSQL/I'vHvI\nData I'vlining, 891\nFramework, 776\nFull Text, 944\nSpatial, 969\nSQL/PSNI, 212\nSQL/Xl\\iIL, 948\nSQL:1999, 58, 180, 816, 805\narray type constructor, 780\nreference types and oids, 790\nrole-based authorization, 697\nrow type constructor, 780\nstructured types, 780\nstructured user-defined types,\n779\ntriggers, 168\nSQL:2003, 180\nSQLCODE, 191\nSQLERROR, 189\nSQLJ, 206\niterators, 208\nSQLSTATE, 189\nSRQL, 887\nSSL protocol, 712\nStable storage, 542, 582\nStandard Generalized l\\ilarkup\nLanguage (SGML), 228\nStandardization, 58\nStar join queries, 869\nStar schema, 8,53\nStarvation, 554\nStateless communication\nprotocols, 225\nStatement-level triggers, 170\nStatic hashing, :371\nStatic indexes, :341\nStatistical databases, 715, 855\nStatistics IIHlintainecl by\nDBIVIS, :395\nStealing fnunes, 541\nStop words, 9:.n\nStorage\nnonvolatile, :306\nprirnarJr, secondary, and\ntertiary, :W5\nstable, 542\nStored procedures, 209\nStoring ADTs and structured\nt:;rpes, 799\nStratification, 829\ncmnparison to relational\nalgebra, 8:30\nStrearning data, 916\nStrict 2PL, 5:30···5:31, 551, 560\nStrict schedule, 552\nStrings in SQL, 139\nStriping unit, :310\nStructured types, 780\nstorage issues, 799\nStructured user-defined types,\n779\nStyle sheets, 247\nSubcl8..c\"is, :38\nSubstitution principle, 788\nSubtransaction, 755\nSUlvI, 151\nSuperclass, :38\nSuperkey, 65, 612\nSupport, 89:3\nassociation rule, 897\nclassification and regression,\n905\nfrequent itemset, 893\nitemset sequence, 902\nSwizzling, 802\nSybase, 27\nSybase ASE, 322--323, 327, 3:31,\n333, 357, 359, 422,\n446····447, 452453, 485,\n500, 506, 573, 582, 709,\n776\nSybase ASIQ, 446, 452-453\nSybase IQ, 447, 866, 869\nSymrnetric encryption, 710\nSynchronous replication, 741,\n750\nread-any write-all technique,\n751\nvoting technique, 750\nSystem catalog, :394\nSystem catalogs, 12, :3:30, :395,\n480, 48:3, 741\nSystem R, 6\nSystem response time, 524\nSystem throughput, 524\nTable, 60\n'I'ags in HTML, 226\nTemporal queries, 999\nTerm frequency, 9:31\n'.['ertiary storage,\n~l05\nThin clients, 237\nThird normal form, 617, 625,\nG28\n'T'hmnas \\Nrite Rule, 570\nrrhrashing, 534\nThree-I)hELse Cornmit, 762\nI'hree-tier architecture, 2:.39\nrniddle tier, 240\npresentation\nti(~r I 240\nSTJBJECT INI)EX\nThroughput, 524\nTime-out for deadlock\ndetection, 757\nTimestamp\nconcurrency control, 5G9 ·570\nbuffered writes, 571\nrecoverability, 571\ndeadlock prevention in 2PL,\n558\nTioga, 1001\nTotal participation, 34\nTP monitor I 993\nTPC-D,506\nTracks in disks, :306\nTrail, 582\nTransaction, 520--521\nabort, 523\nblind write, 528\ncommit, 523\nconflicting actions, 526\nconstraints in SQL, 72\ncustomer, 892\ndistributed, 736\nin SQL, 535\nlocks and performance, 678\nmanagement in a distributed\nDBl\\iIS, 755\nmultilevel and nested, 994\nproperties, 17, 521\nread, 523\nsehedule, 523\nwrite,\n52~3\nTransaction manager, 21, 541\nTransa.ction processing\nmonitor, 993\nrn'ansaction table, 553, 585, 589\nT'ransactions\nnested, 536\nsavepoints, 5:35\nTransactions and JDBC, 199\nrI'ransfer time for disks, 308\nTransID,\n5g;:~\nTransitive dependencies, 617\nTransparent data distribution,\n7~36\nTravelocity, 6\nrrree-based indexing, 280\n11\"ee8\nH trees, 982\nB-+ tree 1 344\nclassification and regression,\n906\nheight, 282\nISAIVI, :341\nnode forrnat for 13+ tree, :.316\n·Region Quad trees, 97G\nTriggers,l:32, l(38\nactivation, 168\nrow vs. statement level, 170\n\nuse in replication,\n75~:3\nTrivial FD, 61:3\nTSQL,1001\nTuning, 28, 650, 652, 667\n'runing for concurrency, 678\nTuning wizard,\n66~3, 665\nTllple, 60\n1\\lple relational calculus, 117\nTuring award, (-j\nTwo-PIHLSe Commit, 759, 761\nPresumed Abort, 762\nTwo-phase locking, 552\nTwo-tier architecture, 237\nType constructor, 779\nType extents, 789\nTypes\ncomplex vs. reference, 795\nobject equality, 790\nUDDI, 222\nUML, 47\nclass diagrams, 48\ncomponent diagrams, 49\ndatabase diagrams, 48\nUndo phase of recovery, 580,\n592\nUnicode, 230\nUnified 1v1odeling Language, 47\nUniform resource identifier\n(URI), 221\nUnion compatibility, 104\nUnion operation, 104, 141\nUNIQUE constraint in SQL, 66\nUnique index, 278\nUniversal resource locator\nCURL),223\nUnnesting operation, 783\nUnpinning pages, 319\nUnrepeatable read, 528\nUpdatable cursors, 191\nUpdatable views, 88\nUpelate locks, 556\nUpdate log record,\n58~~\nUpdates in distributed\ndatab<:LSes, 750\nUpgrading locks, 555\nURI, 221\nURL, 223\nURLs\nversus oids, 792\nUser-defined aggregates, 801\nUser-defined types, 784\nValid X1H.J documents, 231\nValidation in optimistic ee,\n567\nVariable-length fields, 332\nVariable-length records, 328\nVector data, 970\nVector space model, 930\nVertical fragmentation, 739----740\nVertical partitioning, 653\nView maintenance, 876, 881\nincremental, 877\nView materialization, 874\nView serializability, 553\nView serializable schedule, 553\nViews, 14, 86, 90, 653\nfor security, 704\nGRANT, 704\nquery modification, 873\nREVOKE, 704\nupdatable, 88\nupdates on, 88\n1065\nVisDB, lOCH\nVisualization. 1000\n\\Vait-die policy, 558\nWaits-for graph, 556, 756\nWAL, 18,\n~320, 581, 58G\n\\Varehouse, 754, 848, 870\n\\Veak entities,\n~~5, 82\n\\Veak entity set, 36\nWeb crawler,\n9~39\n\\Veb services, 222\n\\Vell-formed XML document,\n231\nvVindow queries, 859\nWizard\nindex tuning, 663\nWorkflow management, 993\nWorkload, 291\nWorkloads and databa..'3e\ndesign, 650\nWound-wait policy, 558\nWrite-ahead logging, 18,\n~320,\n581, 586\nWSDL, 222\nXML, 228\nentity references, 229\nroot, 231\nXNIL content, 232\nXNIL DTDs, 231\nXML Schema,\n2~34\nXPath, 250\nXQuery, 948\npath expressions, 948\nXSL, 228, 250\nXSLT, 250\nZ-order curve, 975",
          "pages": [
            2
          ]
        }
      },
      "educational_notes": {
        "definition": "f,;~tY'W';Yl~t';;:;,~7' A course that has a strong systems emphasis and assumes that students have good programming skills in C and C++.",
        "explanation": "It's your choice! New Modular Organization! Relational Model SQLDDL Infonnation Retrieval and XML Data Management ER Model Conceptual Design Appncatirms emphasis: A course that covers the principles of database systems and emphasizes how they are used in developing data-intensive applications. . f,;~tY'W';Yl~t';;:;,~7' A course that has a strong systems emphasis and assumes that students have good programming skills in C and C++.\n\nHybrid course: Modular organization allows you to teach the course with the emphasis you want. ......- := Dependencies ~~~ I v I II IV VIr III j DATABASE MANAGEMENT SYSTEMS DATABASE MANAGEMENT SYSTEMS Third Edition Raghu Ramakrishnan University of Wisconsin Madison, Wisconsin, USA • Johannes Gehrke Cornell University Ithaca, New York, USA Boston Burr Ridge, IL Dubuque, IA Madison, WI New York San Francisco St.\n\nLouis Bangkok Bogota Caracas Kuala Lumpur Lisbon London Madrid Mexico City Milan Montreal New Delhi Santiago Seoul Singapore Sydney Taipei Toronto McGraw-Hill Higher Education tz A Lhvision of The McGraw-Hill Companies DATABASE MANAGEMENT SYSTEMS, THIRD EDITION International Edition 2003 Exclusive rights by McGraw-Hill Education (Asia), for manufacture and export. This book cannot be re-exported from the country to which it is sold by McGraw-Hill.",
        "key_points": [
          "f,;~tY'W';Yl~t';;:;,~7' A course that has a strong systems emphasis and assumes that students have good programming skills in C and C++.",
          "Hybrid course: Modular organization allows you to teach the course with the emphasis you want."
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using Content in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "It's your choice!\nNew Modular Organization!\nRelational Model\nSQLDDL\nInfonnation Retrieval\nand XML Data\nManagement\nER Model\nConceptual Design\nAppncatirms emphasis: A course that covers the principles of database systems and emphasizes\nhow they are used in developing data-intensive applications.\n.\nf,;~tY'W';Yl~t';;:;,~7' A course that has a strong systems emphasis and assumes that students have\ngood programming skills in C and C++.\nHybrid course: Modular organization allows you to teach the course",
      "llm_error": "Error code: 401 - {'error': {'message': 'Invalid Authentication', 'type': 'invalid_authentication_error'}}"
    },
    "part-1": {
      "id": "part-1",
      "title": "part 1",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        288
      ],
      "sections": {
        "content": {
          "text": "$dataln = new CGI;\n$dataln-l,headerO;\n$authorName = $dataln-l,param('authorName');",
          "pages": [
            288
          ]
        }
      },
      "educational_notes": {
        "definition": "$dataln = new CGI; $dataln-l,headerO; $authorName = $dataln-l,param('authorName');",
        "explanation": "$dataln = new CGI; $dataln-l,headerO; $authorName = $dataln-l,param('authorName');",
        "key_points": [
          "Understanding part 1 is essential for working with databases"
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using part 1 in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "$dataln = new CGI;\n$dataln-l,headerO;\n$authorName = $dataln-l,param('authorName');"
    },
    "part-2": {
      "id": "part-2",
      "title": "part 2",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        288
      ],
      "sections": {
        "content": {
          "text": "print (II<HTML><TITLE>Argument passing test</TITLE> II) ;\nprint (II The user passed the following argument: II) ;\nprint (lI authorName: \", $authorName);",
          "pages": [
            288
          ]
        }
      },
      "educational_notes": {
        "definition": "print (II<HTML><TITLE>Argument passing test</TITLE> II) ; print (II The user passed the following argument: II) ; print (lI authorName: \", $authorName);",
        "explanation": "print (II<HTML><TITLE>Argument passing test</TITLE> II) ; print (II The user passed the following argument: II) ; print (lI authorName: \", $authorName);",
        "key_points": [
          "Understanding part 2 is essential for working with databases"
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using part 2 in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "print (II<HTML><TITLE>Argument passing test</TITLE> II) ;\nprint (II The user passed the following argument: II) ;\nprint (lI authorName: \", $authorName);",
      "llm_error": "Error code: 401 - {'error': {'message': 'Invalid Authentication', 'type': 'invalid_authentication_error'}}"
    },
    "part-3": {
      "id": "part-3",
      "title": "part 3",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        288
      ],
      "sections": {
        "content": {
          "text": "print (\"</HTML>\");\nexit;\nFigure 7.16\nA Simple Perl Script\nspecialized programs called application servers. An application server main-\ntains a pool of threads or processes and uses these to execute requests. Thus,\nit avoids the startup cost of creating a new process for each request.\nApplication servers have evolved into flexible middle-tier packages that pro-\nvide many functions in addition to eliminating the process-creation overhead.\nThey facilitate concurrent access to several heterogeneous data sources (e.g., by\nproviding JDBC drivers), and provide session management services. Often,\nbusiness processes involve several steps. Users expect the system to maintain\ncontinuity during such a multistep session. Several session identifiers such as\ncookies, URI extensions, and hidden fields in HTML forms can be used to\nidentify a session. Application servers provide functionality to detect when a\nsession starts and ends and keep track of the sessions of individual users. They",
          "pages": [
            288
          ]
        }
      },
      "educational_notes": {
        "definition": "print (\"</HTML>\"); exit; Figure 7.16 A Simple Perl Script specialized programs called application servers.",
        "explanation": "print (\"</HTML>\"); exit; Figure 7.16 A Simple Perl Script specialized programs called application servers. An application server main- tains a pool of threads or processes and uses these to execute requests. Thus, it avoids the startup cost of creating a new process for each request. Application servers have evolved into flexible middle-tier packages that pro- vide many functions in addition to eliminating the process-creation overhead.\n\nThey facilitate concurrent access to several heterogeneous data sources (e.g., by providing JDBC drivers), and provide session management services. Often, business processes involve several steps. Users expect the system to maintain continuity during such a multistep session. Several session identifiers such as cookies, URI extensions, and hidden fields in HTML forms can be used to identify a session.\n\nApplication servers provide functionality to detect when a session starts and ends and keep track of the sessions of individual users. They",
        "key_points": [
          "An application server main- tains a pool of threads or processes and uses these to execute requests.",
          "Thus, it avoids the startup cost of creating a new process for each request.",
          "Application servers have evolved into flexible middle-tier packages that pro- vide many functions in addition to eliminating the process-creation overhead.",
          "They facilitate concurrent access to several heterogeneous data sources (e.g., by providing JDBC drivers), and provide session management services.",
          "Often, business processes involve several steps."
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using part 3 in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "print (\"</HTML>\");\nexit;\nFigure 7.16\nA Simple Perl Script\nspecialized programs called application servers. An application server main-\ntains a pool of threads or processes and uses these to execute requests. Thus,\nit avoids the startup cost of creating a new process for each request.\nApplication servers have evolved into flexible middle-tier packages that pro-\nvide many functions in addition to eliminating the process-creation overhead.\nThey facilitate concurrent access to several heterogeneous ",
      "llm_error": "Error code: 401 - {'error': {'message': 'Invalid Authentication', 'type': 'invalid_authentication_error'}}"
    },
    "active-transactions": {
      "id": "active-transactions",
      "title": "Active transactions",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        569
      ],
      "sections": {
        "content": {
          "text": "Figure 16.9\nLock Thrashing\nIf a database system begins to thrash, the database administrator should reduce\nthe number of transactions allowed to run concurrently. Empirically, thrashing\nis seen to occur when 30% of active transactions are blocked, and a DBA should\nmonitor the fraction of blocked transactions to see if the system is at risk of\nthrashing.\nThroughput can be increa..c;ed in three ways (other than buying a fa..'3ter system):\nIIll\nBy locking the smallest sized objects possible (reducing the likelihood that\ntwo transactions need the same lock).\n..\nBy reducing the time that transaction hold locks (so that other transactions\nare blocked for a shorter time).\n3Ivlany common deadlocks can be avoided using a technique called lock downgrade8, implemented\nin most cOlnmercial systems (Section 17.3).",
          "pages": [
            569
          ]
        }
      },
      "educational_notes": {
        "definition": "Empirically, thrashing is seen to occur when 30% of active transactions are blocked, and a DBA should monitor the fraction of blocked transactions to see if the system is at risk of thrashing.",
        "explanation": "Figure 16.9 Lock Thrashing If a database system begins to thrash, the database administrator should reduce the number of transactions allowed to run concurrently. Empirically, thrashing is seen to occur when 30% of active transactions are blocked, and a DBA should monitor the fraction of blocked transactions to see if the system is at risk of thrashing. Throughput can be increa..c;ed in three ways (other than buying a fa..'3ter system): IIll By locking the smallest sized objects possible (reducing the likelihood that two transactions need the same lock).\n\n.. By reducing the time that transaction hold locks (so that other transactions are blocked for a shorter time). 3Ivlany common deadlocks can be avoided using a technique called lock downgrade8, implemented in most cOlnmercial systems (Section 17.3).",
        "key_points": [
          "Empirically, thrashing is seen to occur when 30% of active transactions are blocked, and a DBA should monitor the fraction of blocked transactions to see if the system is at risk of thrashing.",
          "By reducing the time that transaction hold locks (so that other transactions are blocked for a shorter time).",
          "3Ivlany common deadlocks can be avoided using a technique called lock downgrade8, implemented in most cOlnmercial systems (Section 17.3)."
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using Active transactions in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "Figure 16.9\nLock Thrashing\nIf a database system begins to thrash, the database administrator should reduce\nthe number of transactions allowed to run concurrently. Empirically, thrashing\nis seen to occur when 30% of active transactions are blocked, and a DBA should\nmonitor the fraction of blocked transactions to see if the system is at risk of\nthrashing.\nThroughput can be increa..c;ed in three ways (other than buying a fa..'3ter system):\nIIll\nBy locking the smallest sized objects possible (reduci",
      "llm_error": "Error code: 401 - {'error': {'message': 'Invalid Authentication', 'type': 'invalid_authentication_error'}}"
    },
    "ofcpus": {
      "id": "ofcpus",
      "title": "ofCPUs",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        762
      ],
      "sections": {
        "content": {
          "text": "SCALE-UP with DB SIZE\nLinear scale-up\n(ideal)\nSllblint~ar scale-up",
          "pages": [
            762
          ]
        }
      },
      "educational_notes": {
        "definition": "SCALE-UP with DB SIZE Linear scale-up (ideal) Sllblint~ar scale-up",
        "explanation": "SCALE-UP with DB SIZE Linear scale-up (ideal) Sllblint~ar scale-up",
        "key_points": [
          "Understanding ofCPUs is essential for working with databases"
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using ofCPUs in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "SCALE-UP with DB SIZE\nLinear scale-up\n(ideal)\nSllblint~ar scale-up"
    },
    "of-cpus-database-size": {
      "id": "of-cpus-database-size",
      "title": "of CPUs, database size",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        762
      ],
      "sections": {
        "content": {
          "text": "SCALI':-I]P with # KACTSiSEC\nSublinear scale-up\nLinear scale-up\n(ideal)",
          "pages": [
            762
          ]
        }
      },
      "educational_notes": {
        "definition": "SCALI':-I]P with # KACTSiSEC Sublinear scale-up Linear scale-up (ideal)",
        "explanation": "SCALI':-I]P with # KACTSiSEC Sublinear scale-up Linear scale-up (ideal)",
        "key_points": [
          "Understanding of CPUs, database size is essential for working with databases"
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using of CPUs, database size in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "SCALI':-I]P with # KACTSiSEC\nSublinear scale-up\nLinear scale-up\n(ideal)"
    },
    "of-cpus-transactions-per-second": {
      "id": "of-cpus-transactions-per-second",
      "title": "of CPUs, # transactions per second",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        762
      ],
      "sections": {
        "content": {
          "text": "Figure 22.2\nSpeed-up and Scale-up\nexecution of rnultiple queries, it is hard to identify in advance which queries\nwill run concurrently. So the ernphasis has been on parallel execution of a single\nquery.\nA relational query execution plan is a graph of relational algebra operators,\nand the operators in a graph can be executed in parallel.\nIf one operator\nconsurnes the output of a second operator, we have pipelined parallelism\n(the output of the second operator is worked on by the first operator as soon as\nit is generated); if not, the two operators can proceed esseptially independently.\nAn operator is said to block if it produces no output until 'it has conSUllled all\nits inputs. Pipelined parallelisrn is lirnited by the presence of operators (e.g.,\nsorting or aggregation) that block.\nIn addition to evaluating different operators in parallel, we can evaluate each\nindividual operator in a query plan in a parallel fashion. rrhe key to evaluating\nan operator in pa,rallel is to partition the input data; \\ve can then work on\neach partition in parallel and cornbine the results.\nThis approach is called\ndata-partitioned parallel evaluation.\nBy exercising sorne care, existing\ncode for sequentially evaluating relational operators can be ported easily for\ndata-partitioned parallel evaluation.\nAll inlportant observation, '\\vhich explains vvhy shaxed-nothing parallel databa\",;c\nsystelns have been very successful, is that database query evaluation is very\naxncll<tble to data-partitioned parallel evaluation. The goal is to nlinirnize elata,\nshipping b:y paTtitioning the data ,lnd structuring the algoritluns to do lnost of\nthe proc(\\'ssing at individual processors.\n(\\\\le lIse PTOCCS80T to refer to a. CPU\ntogether vvrith its local disk.)\n\\Ve nOVil (~onsid(~r elata paxtitioning and pa.rallelization of existing operator evaJ-\nuation cod(\\' ill rnore detail.",
          "pages": [
            762
          ]
        }
      },
      "educational_notes": {
        "definition": "A relational query execution plan is a graph of relational algebra operators, and the operators in a graph can be executed in parallel.",
        "explanation": "Figure 22.2 Speed-up and Scale-up execution of rnultiple queries, it is hard to identify in advance which queries will run concurrently. So the ernphasis has been on parallel execution of a single query. A relational query execution plan is a graph of relational algebra operators, and the operators in a graph can be executed in parallel. If one operator consurnes the output of a second operator, we have pipelined parallelism (the output of the second operator is worked on by the first operator as soon as it is generated); if not, the two operators can proceed esseptially independently.\n\nAn operator is said to block if it produces no output until 'it has conSUllled all its inputs. Pipelined parallelisrn is lirnited by the presence of operators (e.g., sorting or aggregation) that block. In addition to evaluating different operators in parallel, we can evaluate each individual operator in a query plan in a parallel fashion. rrhe key to evaluating an operator in pa,rallel is to partition the input data; \\ve can then work on each partition in parallel and cornbine the results.\n\nThis approach is called data-partitioned parallel evaluation. By exercising sorne care, existing code for sequentially evaluating relational operators can be ported easily for data-partitioned parallel evaluation. All inlportant observation, '\\vhich explains vvhy shaxed-nothing parallel databa\",;c systelns have been very successful, is that database query evaluation is very axncll<tble to data-partitioned parallel evaluation.",
        "key_points": [
          "So the ernphasis has been on parallel execution of a single query.",
          "A relational query execution plan is a graph of relational algebra operators, and the operators in a graph can be executed in parallel.",
          "An operator is said to block if it produces no output until 'it has conSUllled all its inputs.",
          "Pipelined parallelisrn is lirnited by the presence of operators (e.g., sorting or aggregation) that block."
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using of CPUs, # transactions per second in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "Figure 22.2\nSpeed-up and Scale-up\nexecution of rnultiple queries, it is hard to identify in advance which queries\nwill run concurrently. So the ernphasis has been on parallel execution of a single\nquery.\nA relational query execution plan is a graph of relational algebra operators,\nand the operators in a graph can be executed in parallel.\nIf one operator\nconsurnes the output of a second operator, we have pipelined parallelism\n(the output of the second operator is worked on by the first operator a",
      "llm_error": "Error code: 401 - {'error': {'message': 'Invalid Authentication', 'type': 'invalid_authentication_error'}}"
    },
    "the-enhanced-functionality-of-oildbivlss-raises-se": {
      "id": "the-enhanced-functionality-of-oildbivlss-raises-se",
      "title": "\nThe enhanced functionality of OIlDBIvlSs raises several irnp1ernentation chal-",
      "definition": "",
      "difficulty": "intermediate",
      "page_references": [
        832
      ],
      "sections": {
        "content": {
          "text": "lenges. SOlne of these are 'well understood and solutions have been irnp1enlented\nin products; others are subjects of current research. In this section \\ve exarnine\na few of the key challenges that arise in irnplernenting an efficient, fully func-\ntional OIlDBlvfS. l\\:lany rnore issues are involved than those discussed here; the\nI.-\n,\ninterested reader is encouraged to revisit the previous chapters in this book and\nconsider whether the irnplernentation techniques described there apply natu-\nrally to ORDBJ\\JISs or not.\n23.8.1\nStorage and Access Methods\nSince object-relational databases store new types of data, ORDBMS imple-\nrnentors need to revisit some of the storage and indexing issues discussed in\nearlier chapters. In particular, the system lllust efficiently store ADT objects\nand structured objects and provide efficient indexed access to both.\nStoring Large ADT and Structured Type Objects\nLarge ADT objects and structured objects cornplicate the layout of data on\ndisk.\nThis problern is well understood and has been solved in essentially all\nORDBMSs and OODBMSs. We present Sallie of the main issues here.\nUser-defined ADTs can be quite la,rge. In particular, they can be bigger than\na single disk page. Large ADTs, like BLOBs, require special storage, typically\nin a different location on disk frorn the tuples that contain them. Disk-based\npointers are rnaintained frorn the tuples to the objects they contain.\nStructured objects can also be large, but unlike ADrr objects, they often vary in\nsize during the lifetirne of a database. For exarnple, consider the stars attribute\nof the film,s table in l~igure 23.1. A.s the years pEt.'3S, SOlne of the 'bit actors' in\nan old rnovie rnay becorne farnous. 6 YVhen a bit actor hecornes farnous, Dinky\nrnight want to advertise his or her presence in the earlier filrns. This involves\nan insertion into the stars attribute of an individual tuple in filrns.\nBecause\nthese bulk attributes can grow arbitrarily, flexible disk layout rnechanisrns are\nrequired.\nGA famous example is Marilyn IVlonroe, who had a bit part in the Bette Davis da..ssic All About\n.E'/Jc.",
          "pages": [
            832
          ]
        }
      },
      "educational_notes": {
        "definition": "SOlne of these are 'well understood and solutions have been irnp1enlented in products; others are subjects of current research.",
        "explanation": "lenges. SOlne of these are 'well understood and solutions have been irnp1enlented in products; others are subjects of current research. In this section \\ve exarnine a few of the key challenges that arise in irnplernenting an efficient, fully func- tional OIlDBlvfS. l\\:lany rnore issues are involved than those discussed here; the I.- , interested reader is encouraged to revisit the previous chapters in this book and consider whether the irnplernentation techniques described there apply natu- rally to ORDBJ\\JISs or not.\n\n23.8.1 Storage and Access Methods Since object-relational databases store new types of data, ORDBMS imple- rnentors need to revisit some of the storage and indexing issues discussed in earlier chapters. In particular, the system lllust efficiently store ADT objects and structured objects and provide efficient indexed access to both. Storing Large ADT and Structured Type Objects Large ADT objects and structured objects cornplicate the layout of data on disk.\n\nThis problern is well understood and has been solved in essentially all ORDBMSs and OODBMSs. We present Sallie of the main issues here. User-defined ADTs can be quite la,rge. In particular, they can be bigger than a single disk page. Large ADTs, like BLOBs, require special storage, typically in a different location on disk frorn the tuples that contain them. Disk-based pointers are rnaintained frorn the tuples to the objects they contain.",
        "key_points": [
          "SOlne of these are 'well understood and solutions have been irnp1enlented in products; others are subjects of current research.",
          "In this section \\ve exarnine a few of the key challenges that arise in irnplernenting an efficient, fully func- tional OIlDBlvfS.",
          "In particular, the system lllust efficiently store ADT objects and structured objects and provide efficient indexed access to both."
        ],
        "examples": [
          {
            "title": "Example",
            "code": "-- See textbook for examples",
            "explanation": "Code examples available in the source material"
          }
        ],
        "common_mistakes": [
          {
            "mistake": "Not understanding the concept fully",
            "incorrect_code": "-- Incorrect usage",
            "correct_code": "-- Correct usage (see textbook)",
            "explanation": "Review the textbook explanation carefully"
          }
        ],
        "practice": {
          "question": "Practice using \nThe enhanced functionality of OIlDBIvlSs raises several irnp1ernentation chal- in your own SQL queries",
          "solution": "Try writing queries and compare with textbook examples"
        }
      },
      "llm_enhanced": false,
      "raw_text_preview": "lenges. SOlne of these are 'well understood and solutions have been irnp1enlented\nin products; others are subjects of current research. In this section \\ve exarnine\na few of the key challenges that arise in irnplernenting an efficient, fully func-\ntional OIlDBlvfS. l\\:lany rnore issues are involved than those discussed here; the\nI.-\n,\ninterested reader is encouraged to revisit the previous chapters in this book and\nconsider whether the irnplernentation techniques described there apply natu-\nrall",
      "llm_error": "Error code: 401 - {'error': {'message': 'Invalid Authentication', 'type': 'invalid_authentication_error'}}"
    }
  }
}